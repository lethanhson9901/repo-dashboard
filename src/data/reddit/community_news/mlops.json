{
  "metadata": {
    "last_updated": "2026-02-28 02:40:16",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 17,
    "total_comments": 42,
    "file_size_bytes": 76366
  },
  "items": [
    {
      "id": "1rcp0ad",
      "title": "Broke down our $3.2k LLM bill - 68% was preventable waste",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "author": "llamacoded",
      "created_utc": "2026-02-23 18:11:09",
      "score": 60,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "We run ML systems in production. LLM API costs hit $3,200 last month. Actually analyzed where money went.\n\n**68% - Repeat queries hitting API every time** Same questions phrased differently. \"How do I reset password\" vs \"password reset help\" vs \"can't login need reset\". All full API calls. Same answer.\n\nSemantic caching cut this by 65%. Cache similar queries based on embeddings, not exact strings.\n\n**22% - Dev/staging using production keys** QA running test suites against live APIs. One staging loop hit the API 40k times before we caught it. Burned $280.\n\nSeparate API keys per environment with hard budget caps fixed this. Dev capped at $50/day, requests stop when limit hits.\n\n**10% - Oversized context windows** Dumping 2500 tokens of docs into every request when 200 relevant tokens would work. Paying for irrelevant context.\n\nBetter RAG chunking strategy reduced this waste.\n\n**What actually helped:**\n\n* Caching layer for similar queries\n* Budget controls per environment\n* Proper context management in RAG\n\nCost optimization isn't optional at scale. It's infrastructure hygiene.\n\nWhat's your biggest LLM cost leak? Context bloat? Retry loops? Poor caching?",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6zw8xk",
          "author": "Morpheyz",
          "text": "Cut cost by 99%. External consultants sold us Azure Open AI PTUs for 50k/month, claiming we absolutely needed them for our use case. Couple months later convinced leadership to switch to pay-as-you-yo model, now spending 300$/month.\n\nEdit: PTUs, not TPUs",
          "score": 20,
          "created_utc": "2026-02-23 18:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o700ndp",
              "author": "pmv143",
              "text": "Classic overprovisioning trap. Fixed infra before validated demand is expensive. Usage based models are much more forgiving while workloads are still evolving.",
              "score": 7,
              "created_utc": "2026-02-23 18:59:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o704ptq",
              "author": "m98789",
              "text": "Azure doesn’t offer TPUs tho",
              "score": 2,
              "created_utc": "2026-02-23 19:18:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o706ns8",
                  "author": "Morpheyz",
                  "text": "Typo, I meant PTUs.",
                  "score": 3,
                  "created_utc": "2026-02-23 19:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70098v",
          "author": "pmv143",
          "text": " most ppl underestimate how much waste lives above the model. Interesting part is that even after fixing caching and RAG, infrastructure-level inefficiencies still compound at scale.",
          "score": 5,
          "created_utc": "2026-02-23 18:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o712b33",
          "author": "KeyIsNull",
          "text": "Mind to share some details about the semantic cache layer? ",
          "score": 2,
          "created_utc": "2026-02-23 21:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7az1td",
              "author": "llamacoded",
              "text": "[https://docs.getbifrost.ai/features/semantic-caching](https://docs.getbifrost.ai/features/semantic-caching) i use bifrost gateway you can check it out here its oss",
              "score": 2,
              "created_utc": "2026-02-25 10:39:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72zgwp",
          "author": "doolpicate",
          "text": "tiering, routing orchestration, and multiple models including LocalLLMs would have helped. Strange that people are not doing it in the beginning itself.",
          "score": 1,
          "created_utc": "2026-02-24 04:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74mikd",
          "author": "ManufacturerWeird161",
          "text": "We had a similar bleed last year where our staging environment was burning through ~$400/day in GPT-4 calls because someone left a load test running over the weekend. Took us three days to notice because the cost alerts were batched weekly. Daily caps saved us but the real fix was making the staging LLM return deterministic garbage responses for any call pattern that looked synthetic—cut costs by 90% without hurting actual QA work.",
          "score": 1,
          "created_utc": "2026-02-24 12:55:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmfdv",
          "author": "Illustrious_Echo3222",
          "text": "That 68 percent repeat query number doesn’t surprise me at all. In a lot of systems, the LLM becomes the most expensive cache miss you’ve ever deployed.\n\nContext bloat has been the biggest leak I’ve seen. Teams over index on “just give it more docs” instead of tightening retrieval quality. A sloppy RAG pipeline quietly doubles or triples spend because nobody notices incremental token creep.\n\nRetry loops are another hidden killer. Especially with agents. If you allow automatic retries with minor rephrasing and no cap, you can burn a ton of tokens on what is basically the same failure mode repeated three times.\n\nOne pattern that helped us was aggressive observability at the token level. Logging prompt tokens, completion tokens, and tool calls per request, then ranking endpoints by cost per successful outcome. When you frame it as cost per resolved task instead of cost per call, waste becomes obvious.\n\nAlso agree hard on environment separation. Using production keys in staging is basically handing your burn rate to a test script.\n\nCurious if semantic caching caused any weird edge cases with slightly different intent but similar phrasing? That’s usually where people get nervous.",
          "score": 1,
          "created_utc": "2026-02-25 13:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hqvlw",
          "author": "llamacoded",
          "text": "For all the folks asking - i am currently using [bifrost](https://www.getmaxim.ai/bifrost) \\[OSS\\] as my ai gateway for semantic caching and budgeting controls. \\[personal bias\\]",
          "score": 1,
          "created_utc": "2026-02-26 10:37:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7p5ytr",
          "author": "mattiamazzoli",
          "text": "One thing that bites teams: cost per call ≠ cost per outcome.\n\nA single retry loop or oversized shared context across chained calls can quietly 3–5x the *real* cost per completed task.\n\nCaching helps, but strict retry caps + aggressive context trimming usually move the needle even more at scale.",
          "score": 1,
          "created_utc": "2026-02-27 13:38:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p8asm",
          "author": "masterKova",
          "text": "Nice breakdown. Beyond caching, another big lever is model routing. A lot of those repeat queries (\"how do I reset password\") don't need GPT-4 class models at all. I built NadirClaw to classify prompts in ~10ms and route simple ones to cheap/local models automatically. Works as an OpenAI-compatible proxy so no code changes. Pairs well with your caching layer since the cache misses still get routed optimally. https://github.com/doramirdor/NadirClaw (author, disclosure)",
          "score": 1,
          "created_utc": "2026-02-27 13:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70to85",
          "author": "inspectedinspector",
          "text": "How much will it cost you to build embedding-based semantic caching to save $2000?",
          "score": 0,
          "created_utc": "2026-02-23 21:18:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71c9sa",
              "author": "ZestyData",
              "text": "Very little, could do it in an hour and with very low running costs. Embeddings and vector search are basically free.",
              "score": 2,
              "created_utc": "2026-02-23 22:50:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ucns",
                  "author": "inspectedinspector",
                  "text": "I assume this means you already have OpenSearch or Redis (or similar) infrastructure you can leverage? Building truly from scratch would not be free",
                  "score": 1,
                  "created_utc": "2026-02-24 19:17:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72ktbl",
              "author": "burntoutdev8291",
              "text": "from_pretrained(\"BAAI/bge-m3\")\n\nLitellm also supports semantic caching. What OP didn't mention is false positives and privacy aware semantic caches. While its an instance cost saving, anything with some form of model behind must go through its own evals\n\nSo cost of deployment of semantic cache is easy, validation is the key step.",
              "score": 1,
              "created_utc": "2026-02-24 03:03:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7az3ni",
              "author": "llamacoded",
              "text": "[https://docs.getbifrost.ai/features/semantic-caching](https://docs.getbifrost.ai/features/semantic-caching) this is what i used. its oss",
              "score": 1,
              "created_utc": "2026-02-25 10:40:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73ivt3",
          "author": "eliko613",
          "text": "This breakdown is spot on.\nIn most systems I’ve looked at, model selection isn’t the primary cost driver — it’s:\nSemantically duplicate queries with no intelligent cache\nStaging/QA hitting prod keys\nContext bloat in RAG\nRetry or loop logic nobody notices\nThe 68% repeat-query number feels very real. Once you cluster by intent instead of raw prompt strings, you usually discover a small handful of intents burning the majority of spend.\nAlso +1 on environment separation and hard caps — that’s basic cloud hygiene, but it’s surprising how often it’s missing in LLM setups.\nLLM cost control is starting to look a lot like early cloud FinOps: visibility first, then guardrails, then optimization. I’ve been exploring tools in this space (e.g., zenllm.io) that focus specifically on intent-level visibility and waste detection — the patterns you’re describing show up immediately when you instrument properly.",
          "score": 0,
          "created_utc": "2026-02-24 07:14:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd3g0s",
      "title": "Advice Needed on a MLOps Architecture",
      "subreddit": "mlops",
      "url": "https://i.redd.it/jvzejdhcyclg1.png",
      "author": "Drac084",
      "created_utc": "2026-02-24 03:18:05",
      "score": 46,
      "num_comments": 17,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rd3g0s/advice_needed_on_a_mlops_architecture/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o72pyso",
          "author": "Competitive-Fact-313",
          "text": "If you talk overall end to end improvement there is lot you can do. From argo to grafana. I use openshift along with gitops. As long as orchestration is concerned you can use terraform n its siblings. If I get your question right.",
          "score": 3,
          "created_utc": "2026-02-24 03:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72r3jz",
          "author": "le-fou",
          "text": "I examined your diagram before I read the full post, and the two things that jumped out to me were 1) the arrows (presumably) triggering a training run with an API request and 2) the arrow going from the MLFlow tracking server to the deployment API. These correspond to your questions 1 and 3, so that is a good sign these could use some definition.\n\nFirstly, agreed that you want an orchestration layer for training. Dagster and Airflow are two common orchestration platforms, among others. Dagster has great k8s support. I haven’t used ZenML but a quick google search suggests to me it would also work fine for this. Asking AI for a comparison between these tools, given your requirements, would probably be fruitful. Regardless, this is all to say I think you’re right about needing something to orchestrate the training run. In your current diagram, for example, what exactly is hitting the endpoint? Some custom frontend? A curl command from your terminal? An orchestration framework allows you to schedule runs and/or manually trigger from a UI with your desired parameters.\n\nSecondly, the deployment process and trigger could use better definition. I personally use gitlab pipelines to build my custom model serving docker images with MLServer, and they get deployed via ArgoCD with the same CI/CD component any other non-ML app uses at my organization (I did need to write Helm charts for my MLServer deployments specifically). This pipeline could be triggered at the end of your training pipeline, or probably better you could use MLFlow aliasing/tags to fire a webhook for your deployment pipeline. But, fundamentally, building an image to serve your containers shouldn’t look functionally all that different from other build pipelines at your org, with the exception that ML containers can have some nasty dependencies and large artifact dependencies (model weights).\n\nLet me know if that all makes sense, or not.",
          "score": 3,
          "created_utc": "2026-02-24 03:42:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72th38",
              "author": "Drac084",
              "text": "Thanks for the reply!\n\nFirstly, sorry the diagram is not perfect. It was a very quick sketch of the idea.\n\n1. I was thinking to implement the Training as a independent microservice(May be a simple FastAPI server) API request will trigger a pipeline and dispatch jobs. This can later be triggered from a frontend dashboard, but not in MVP level.  This workflow the main challenge I'm trying to sort out. \n\n2. I haven't given much thought to the Deployment and inference service at this point, assuming it will be less difficult once I figured out the training service. But what you suggested also make sense. I will do some more research on this. Thanks for your input! ",
              "score": 1,
              "created_utc": "2026-02-24 03:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o72vmz1",
                  "author": "Iron-Over",
                  "text": "An orchestrator is important because it creates a simple, reusable pipeline. Airflow allows ad-hoc and scheduled runs. \n\nI assume you are using production data for training? If so, how do the data scientists view the training results and test results? I assume the notebooks should be hosted instead of the production data on laptops. \n\nI would log each API call, including the features and the prediction to be matched  with the actual outcome. This then becomes inexpensive labeled data for future training.\n\nYou may want to include shap to view the explainability of the prediction from the features.\n\nI did not see drift and skew detection on the data and the model it is useful to know when you need to retrain. \n\n\n\n",
                  "score": 2,
                  "created_utc": "2026-02-24 04:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73df41",
          "author": "prassi89",
          "text": "Overall arch looks great.\n\n1. Don’t go with dvc. When your datasets get large, you wont be able to stream (or mount) them transparently. Also data is repo bound logically. Use LakeFS directly.\n\n2. Skypilot is your best bet - it does training service APIs and compute orchestration. With  Other services like dagster, airflow you’ll just spend ages debugging. Zenml is good but skypilot just gets out of the researchers way, and gives you multi cloud by default\n\n3. Mlflow also does a lot in the model promotion and deployment space. Consider it\n\nOverall, great stuff",
          "score": 4,
          "created_utc": "2026-02-24 06:26:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78q78q",
              "author": "Drac084",
              "text": "Got it. I was thinking if the LakeFS would be a overkill for this because we don't deal with insanely large datasets. But I understand your point in term of future scalability",
              "score": 1,
              "created_utc": "2026-02-25 00:51:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73cggk",
          "author": "prasanth_krishnan",
          "text": "Orchestrator - metaflow\n\nDistributed training - apache ray\n\nExperiment tracking - ml flow\n\nModel packaging - mlflow models\n\nInference endpoint - MLserver or onnx\n\nFeature store - feast with actual stores of your choice.\n\nThis is a good framework neutral platform.",
          "score": 2,
          "created_utc": "2026-02-24 06:18:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o757nde",
          "author": "ManufacturerWeird161",
          "text": "We used DVC with MinIO at my last job, it worked well for data versioning but we found MLflow was better for the actual model registry piece to track lineage.",
          "score": 2,
          "created_utc": "2026-02-24 14:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73wyir",
          "author": "htahir1",
          "text": "Great architecture sketch — super critical to break things into distinct services early on.\n\nOn the orchestration question: since you're already on K8s and planning to extend to Slurm, you'll want something that abstracts away the infrastructure layer so your researchers aren't writing YAML all day. I've seen people have good experiences with Dagster and Kubeflow for this, but I'd also suggest taking a serious look at ZenML — full disclosure, I'm part of the ZenML team, so take this with the appropriate grain of salt.\n\nThat said, the reason I think it's worth evaluating here specifically is that ZenML was designed to be a framework-agnostic orchestration layer that plugs into the tools you're already using (MLflow, K8s, S3/MinIO) rather than replacing them. So you'd keep your MLflow tracking, your MinIO storage, your K8s cluster — ZenML just becomes the connective tissue that defines and runs your pipelines across all of it. It also plays nicely with the \"microservices\" mental model you're going for.\n\nA couple of non-ZenML-related thoughts too:\n\n* \\+1 to what others said about drift/skew detection — worth thinking about early even if you don't implement it in your MVP.\n* The comment about LakeFS over DVC is worth considering, especially at scale with large datasets and streaming use cases.\n* For the deployment side, I'd honestly keep it simple at first. Honestly for smaller models use MLflow serving or even wrap in in a FastAPI, and then graduate to more complex services later \n\nGood luck with the build — sounds like a fun project!",
          "score": 2,
          "created_utc": "2026-02-24 09:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pv51",
              "author": "Drac084",
              "text": "Thanks, I should look into ZenML eco system. Do you think the free version is enough for me to try it out and get an idea?\n\n",
              "score": 1,
              "created_utc": "2026-02-25 00:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79vlx1",
                  "author": "htahir1",
                  "text": "Yes most of the non enterprisey features are oss , Pro only has governance and enterprise focused features for bigger teams you can adopt those later if at all",
                  "score": 2,
                  "created_utc": "2026-02-25 04:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o763ev8",
          "author": "alex000kim",
          "text": "Hey, imo, the overall approach is fine. I also agree with most of the feedback others have left. Leaving some of mine:\n\n\\- Simply stitching services together might not be the hardest part. What really requires some thinking is making it secure. I.e. the whole authentication flow from data to infra to model/artifact registry to deployment. Your diagram doesn’t show any of this.\n\n\\- A few things are undefined in the diagram: there’s no clear data path from S3/MinIO into the actual training pods, the “Model Selection” arrow from MLflow to your Deployment Service has no trigger mechanism (manual? webhook? CI pipeline?), and Slurm is mentioned in the text but completely absent from the diagram with no abstraction layer between K8s and Slurm.\n\n\\- That yellow “Training Service API” box (job queue, state manager, scheduling, logs) is essentially an entire orchestration platform you’d be building from scratch. Worth thinking about whether you really want to own that.\n\n\\- Reconsider MinIO since the open-source project has been archived [https://news.ycombinator.com/item?id=47000041](https://news.ycombinator.com/item?id=47000041)\n\n\\- SkyPilot is really the way to go if you already have K8s and plan on adding Slurm into the mix. You write one task YAML and it works on both. When Slurm comes online you reuse existing task definitions instead of rewriting pipelines. Since the resources will be shared between team members, you’ll most likely need to deploy and manage the central SkyPilot API server.\n\n\\- SkyPilot also has SkyServe [https://docs.skypilot.co/en/stable/serving/sky-serve.html](https://docs.skypilot.co/en/stable/serving/sky-serve.html) for the deployment/inference side. Add a service: block to a YAML and you get autoscaling, load balancing, and rolling updates. Worth evaluating before building a custom deployment service.",
          "score": 2,
          "created_utc": "2026-02-24 17:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pk54",
              "author": "Drac084",
              "text": "Thanks for the input.\n\nYes I haven't included slurm in the diagram because it's a future addition. But there should be a abstraction layer for compute infra/job dispatching to slurm/k8 within Training Service component. ",
              "score": 1,
              "created_utc": "2026-02-25 00:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7a1z8s",
          "author": "PleasantAd6868",
          "text": "training service api, would recommend jobset or kubeflow trainer CRDS (if you are already on k8s which looks like it from your diagram). if you need a resource manager + gang scheduling, either kueue or volcano. would not recommend more bloated options (i.e. Ray, skypilot, zenML) unless ur doing something super exotic with heterogeneous resources",
          "score": 1,
          "created_utc": "2026-02-25 05:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gwfin",
          "author": "princess-barnacle",
          "text": "Just use flyte.",
          "score": 1,
          "created_utc": "2026-02-26 05:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75x4pf",
          "author": "thulcan",
          "text": "Your real problem isn't which orchestrator to pick — it's that you have five systems (DVC, MLflow, Harbor, MinIO, custom APIs) that each own a piece of what \"this model version\" means. That's five places where lineage breaks.\n\nModelKits ([KitOps](https://kitops.org), CNCF Sandbox) fix this at the artifact layer. A ModelKit is an OCI artifact — same format as your Docker images — that packages weights, dataset refs, config, and code with a Kitfile manifest. You already run Harbor and MinIO. Harbor becomes your single registry for images, models, and datasets. No new infrastructure.\n\nWhat changes:\n\n**DVC → gone.** `kit pack` your datasets, push to Harbor. Versioning is OCI tags. No LakeFS either.\n\n**MLflow → experiment tracking only.** Drop MLflow Model Registry and MLflow deployment. Harbor + ModelKits is your registry. MLflow is great for experiment tracking UI and bad at everything else it tries to do.\n\n**Training orchestration → Argo Workflows.** CNCF graduated, K8s-native. Pipeline: `kit unpack` → train → `kit pack` → `kit push`. Stop building a custom Training Service API with job queues and state managers. That's a multi-year project you don't need.\n\n**Governance gate (you're missing this).** Between trained and deployed: run ModelScan, attach cosign attestations, tag as `:approved`. You're a research org managing lots of models — provenance isn't optional, and nobody in this thread mentioned it.\n\n**Deployment Service API → gone.** KitOps has a native KServe `ClusterStorageContainer` integration. KServe pulls ModelKits directly from Harbor via OCI reference. No artifact retrieval logic, no container initialization code. Point KServe at [`harbor.yourorg.com/models/my-model:approved`](http://harbor.yourorg.com/models/my-model:approved), done.\n\nYou're currently stitching together DVC + MLflow Registry + MLflow Tracking + Harbor + MinIO + two custom APIs and hoping they agree on what \"model v2.3\" means. That's a lot of coordination surfaces to keep in sync. With KitOps: Harbor is your single source of truth, Argo runs your pipelines, MLflow tracks your experiments. Three tools, each doing one job. And you get security and provenance your current architecture doesn't even attempt.",
          "score": 1,
          "created_utc": "2026-02-24 16:48:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ravnd2",
      "title": "Cleared NVIDIA NCA-AIIO - Next Target: NCP-AII",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ravnd2/cleared_nvidia_ncaaiio_next_target_ncpaii/",
      "author": "TuckerSavannah1",
      "created_utc": "2026-02-21 16:37:26",
      "score": 20,
      "num_comments": 21,
      "upvote_ratio": 1.0,
      "text": "Hello Everyone\n\nGlad to share that I’ve successfully cleared the NVIDIA NCA-AIIO (AI Infrastructure & Operations) exam!\n\nMy journey was focused on building strong fundamentals in GPUs, networking, and AI infrastructure concepts. I avoided rote learning and concentrated on understanding how things actually work. Practice tests from itexamscerts also played a big role, they helped me identify weak areas and improve my confidence before the exam. Overall, if your basics are clear, the exam is very manageable.\n\nNow I’m preparing for NVIDIA NCP-AII, and I would really appreciate guidance from those who have cleared it.\n\n\\* How tough is it compared to NCA-AIIO?\n\n\\* Is it more hands-on or CLI/lab focused?\n\n\\* Any recommended labs?y\n\nI look forward to your valuable insights. Thank you.",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1ravnd2/cleared_nvidia_ncaaiio_next_target_ncpaii/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6n5oab",
          "author": "RubySera1",
          "text": "Well done and congrats! Could you please share which practice tests you found most useful for NCA-AIIO?",
          "score": 3,
          "created_utc": "2026-02-21 18:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n7xk9",
              "author": "TuckerSavannah1",
              "text": "itexamscerts.",
              "score": 2,
              "created_utc": "2026-02-21 18:52:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zio9a",
          "author": "Sure-Programmer-8462",
          "text": "Nice achievement. Were the NCA-AIIO exam questions more theoretical or based on real world scenarios?",
          "score": 2,
          "created_utc": "2026-02-23 17:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zjnod",
              "author": "TuckerSavannah1",
              "text": "The exam was a mix of both, but many questions were scenario-based. If you understand the core concepts and practical use cases, it becomes much easier to handle.",
              "score": 1,
              "created_utc": "2026-02-23 17:42:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zlaws",
                  "author": "Sure-Programmer-8462",
                  "text": "Did you find the exam objectives closely aligned with the official syllabus or were there some unexpected topics?",
                  "score": 2,
                  "created_utc": "2026-02-23 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nzjbf",
          "author": "Satsuma_Johnson",
          "text": "congrats..",
          "score": 1,
          "created_utc": "2026-02-21 21:15:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh2d5",
              "author": "TuckerSavannah1",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-23 17:29:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tcfen",
          "author": "Wright_Lucy11",
          "text": "congrats.",
          "score": 1,
          "created_utc": "2026-02-22 18:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh3ne",
              "author": "TuckerSavannah1",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-23 17:30:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zidpo",
          "author": "GrapeThompson",
          "text": "Congratulations on clearing the NCA-AIIO exam! How long did you take to prepare for it, and did you already have experience with AI infrastructure before starting?",
          "score": 1,
          "created_utc": "2026-02-23 17:36:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zj98g",
              "author": "TuckerSavannah1",
              "text": "Thank you! It took me around 4–5 weeks of consistent study. I had some basic knowledge of networking and virtualization, but I was new to AI infrastructure concepts. I focused on understanding GPU architecture and deployment scenarios.",
              "score": 1,
              "created_utc": "2026-02-23 17:40:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kvj1g",
          "author": "Dorthy_PinkLace",
          "text": "Congrats! I’m planning to take the NVIDIA NCA-AIIO next month. How difficult did you find it overall?",
          "score": 1,
          "created_utc": "2026-02-26 20:36:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kw4vd",
              "author": "TuckerSavannah1",
              "text": "Thank you! Honestly, it wasn’t extremely difficult if your fundamentals are clear. The exam focuses more on understanding GPU architecture, networking basics, and AI infrastructure design rather than memorizing commands. If you understand how components interact in a real deployment, you’ll be fine.",
              "score": 1,
              "created_utc": "2026-02-26 20:39:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7kwtvd",
                  "author": "Dorthy_PinkLace",
                  "text": "That’s good to hear. Were there many scenario-based questions or mostly theoretical?",
                  "score": 1,
                  "created_utc": "2026-02-26 20:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qh427",
          "author": "erc80",
          "text": "Congratulations! I cleared the NCA-AIIO and am looking for information on the NCP-AII as well! Best of luck on your journey.",
          "score": 1,
          "created_utc": "2026-02-27 17:32:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdpbkd",
      "title": "Wrote a guide to building an ML research cluster. Feedback appreciated.",
      "subreddit": "mlops",
      "url": "https://i.redd.it/5bpvizk9qhlg1.png",
      "author": "aliasaria",
      "created_utc": "2026-02-24 19:04:24",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rdpbkd/wrote_a_guide_to_building_an_ml_research_cluster/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bb1cn",
          "author": "radarsat1",
          "text": "This is nice. At a small startup we were working with a couple of machines in the \"multiuser, single workstation\" configuration and it was ok, but after buying a couple more machines working this way became very annoying. We worked towards something like what you are recommending here with k3s but never fully figured it out, probably could have used a guide like this!",
          "score": 2,
          "created_utc": "2026-02-25 12:16:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf7idu",
      "title": "If you're coming from infra/DevOps and confused about what vLLM actually solves — here's the before and after",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf7idu/if_youre_coming_from_infradevops_and_confused/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-26 11:16:28",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.57,
      "text": "Had a pretty standard LLM setup, HuggingFace transformers, FastAPI, model on GPU. Worked great in dev. Then the prod traffic hit, and everything fell apart. Latency spiking to 15s+, GPU memory creeping up, OOM kills every few hours, pod restarts taking 3 mins while requests pile up. On-call was rough.\n\n**What was actually going wrong:**\n\n* HuggingFace `model.generate()` is blocked. One request at a time. 10 users = 9 waiting.\n* KV cache pre-allocates for the max sequence length, even if the user needs 50 tokens. Over time, fragmentation builds up → OOM. Same energy as over-provisioning PVCs on every pod.\n* Static batching waits for the slowest request. A 500-token generation holds up a 20-token one.\n\n**What fixed it:**\n\nSwapped the serving layer to vLLM. Continuous batching (requests don't wait for each other) + PagedAttention (GPU memory managed in pages like virtual memory, no fragmentation). Core issues gone.\n\nThe gotchas nobody talks about:\n\n* Set `gpu-memory-utilization` to 0.85-0.90, not higher. Leave headroom.\n* Model warm-up is real — first requests after startup are slow (CUDA kernel compilation). Send dummy requests before marking the pod ready.\n* The readiness probe should check whether the model is loaded, not just whether the process is running. Ask me how I know.\n* Set hard timeouts on generation length. One runaway request shouldn't block everything.\n* Shadow traffic first, then canary at 10%, then ramp up. Boring but safe.\n\n**Result:** Latency 45s → 10-15s. Concurrency 2-3 → 15-20 per GPU. OOM crashes → zero. None of this needed transformer math, just infra skills applied to ML.\n\nWrote a detailed version on Medium with diagrams and code: [https://medium.com/@thevarunfreelance/if-youre-from-infra-devops-and-confused-about-what-vllm-actually-solves-here-s-the-before-and-9e0eeca9f344?postPublishedType=initial](https://medium.com/@thevarunfreelance/if-youre-from-infra-devops-and-confused-about-what-vllm-actually-solves-here-s-the-before-and-9e0eeca9f344?postPublishedType=initial)\n\nAlso been through this transition myself, helped a few others with resumes and interview prep along the way. If you're on a similar path, DMs open or grab time here: [topmate.io/varun\\_rajput\\_1914](http://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rf7idu/if_youre_coming_from_infradevops_and_confused/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7hzos0",
          "author": "Historical-One7058",
          "text": "AI slop",
          "score": 28,
          "created_utc": "2026-02-26 11:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ijnrq",
              "author": "greysteppenwolf",
              "text": "Yeah I’m so tired that every time I see this sub on my feed the post is 100% AI. Like I get we work with LLMs directly but can’t you just add some human touch at least",
              "score": 1,
              "created_utc": "2026-02-26 13:58:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ip65h",
                  "author": "Extension_Key_5970",
                  "text": "My main goal is to educate the MLOps community on real-world problems. This is the first time I've received critical feedback. Thanks for that. Usually, I write my content by myself and use LLM for spell checks and grammar, but it seems there was a change in the model, which overpolished the content, making it an AI-generated\n\nWell know I made an edit to make it more simple and concise, so that more people can connect",
                  "score": -4,
                  "created_utc": "2026-02-26 14:27:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7n1qli",
              "author": "vantasmer",
              "text": "I’m so tired of every post I click on being AI trash. ",
              "score": 1,
              "created_utc": "2026-02-27 03:37:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jggg2",
          "author": "idjos",
          "text": "Also consider baking in models and those big docker images with the AMI if using AWS. \n\nIt does introduce complexity on the CI/CD side, but helps a lot reducing cold start.\n\nOne more thing worth following is [nvidia chrek](https://docs.nvidia.com/dynamo/dev/kubernetes-deployment/deployment-guide/checkpointing). It’s still experimental feature, so read very carefully before investing time in it, especially if security is a big concern for you.\n\nDisclaimer: disn’t read the article yet.",
          "score": 2,
          "created_utc": "2026-02-26 16:37:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdi7jr",
      "title": "Why do agent testing frameworks assume developers will write all the test cases?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rdi7jr/why_do_agent_testing_frameworks_assume_developers/",
      "author": "Outrageous_Hat_9852",
      "created_utc": "2026-02-24 14:49:29",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.82,
      "text": "Most AI testing tools I've seen are built for engineers to write test scripts and run evaluations. But in practice, the people who best understand what good AI behavior looks like are often domain experts, product managers, or subject matter specialists.   \n  \nFor example, if you're building a customer service agent, your support team lead probably has better intuition about edge cases and problematic responses than your ML engineer. If you're building a legal document analyzer, your legal team knows what constitutes accurate analysis. Yet most testing workflows require technical people to translate domain knowledge into code.  \n  \n This creates a bottleneck and often loses important nuances in translation. Has anyone found good ways to involve non-technical stakeholders directly in the testing process?  \n  \nI'm thinking beyond just \"review the results\" but actually contributing to test design and acceptance criteria.",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rdi7jr/why_do_agent_testing_frameworks_assume_developers/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o75baju",
          "author": "penguinzb1",
          "text": "the translation problem is real, but there's a second issue underneath it: even with good domain expert input, the test set usually only covers the cases they can articulate. the failures that matter are the ones nobody anticipated.\n\nwhat's worked for us: give domain experts access to simulated versions of their actual workflows and let them just run the agent. they don't need to write scenarios, they surface the gaps themselves as they go. 'it never should have done that' is better input than anything you'd get from a spec written in advance.",
          "score": 3,
          "created_utc": "2026-02-24 15:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7592ap",
          "author": "QuoteBackground6525",
          "text": "Yes! We had the same issue with our customer service AI. Our support team knew exactly what kinds of tricky customer requests would break the system, but translating that knowledge into test code was always a bottleneck. Now our support lead connects their runbooks and FAQ docs, describes problematic scenarios in plain language, and we get comprehensive test coverage including adversarial cases. The key was finding a platform that treats testing as a cross-functional activity rather than just a developer task. Much more effective than the old approach of engineers guessing what good behavior looks like.",
          "score": 2,
          "created_utc": "2026-02-24 14:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o759d2l",
              "author": "Outrageous_Hat_9852",
              "text": "Uh, interesting! Any tools you have been using for this that were helpful?",
              "score": 1,
              "created_utc": "2026-02-24 14:59:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7blztw",
          "author": "Illustrious_Echo3222",
          "text": "This is such a real bottleneck. A lot of agent testing frameworks feel like classic unit testing tools with an LLM wrapper, which assumes the engineer both defines and encodes “correctness.” But for most agent use cases, correctness is domain shaped, not purely technical.\n\nWhat I’ve seen work better is separating test authoring from test execution.\n\nInstead of asking domain experts to write code, give them structured ways to define:\n\n* Example scenarios in plain language\n* “Good vs bad” response pairs\n* Acceptance rubrics with weighted criteria\n\nThen have engineers translate those into executable evals or, better yet, build a thin layer that auto-generates test cases from structured forms. Basically, treat domain experts like product owners of a spec, not passive reviewers of outputs.\n\nAnother useful pattern is gold conversation capture. Let SMEs flag real transcripts as “ideal,” “borderline,” or “fail,” and continuously sample from production logs for evaluation sets. That keeps nuance intact because it’s grounded in real behavior, not hypothetical test cases.\n\nI also think pair-review style workflows help. Domain expert defines the intent and failure boundaries. Engineer encodes it. Then both review eval drift over time. It becomes collaborative rather than translational.\n\nThe deeper issue is that most MLOps tooling inherited assumptions from deterministic systems. Agents are probabilistic and contextual. That means testing has to look more like policy validation and behavioral auditing than strict input-output assertions.\n\nCurious if you’re exploring tooling here or just noticing the gap. It feels like there’s space for much better human-in-the-loop eval design.",
          "score": 2,
          "created_utc": "2026-02-25 13:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bnowe",
              "author": "Outrageous_Hat_9852",
              "text": "Thanks, this helps! I am exploring tools right now, via lists like this: [https://github.com/kelvins/awesome-mlops](https://github.com/kelvins/awesome-mlops)\n\nOne that I came across that puts an emphasis on collaboration and SMEs in particular is this: [https://github.com/rhesis-ai/rhesis](https://github.com/rhesis-ai/rhesis)",
              "score": 1,
              "created_utc": "2026-02-25 13:36:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7mactc",
          "author": "gudruert",
          "text": "I totally get that - letting domain experts run the agent sounds way more insightful than just relying on engineers!",
          "score": 2,
          "created_utc": "2026-02-27 00:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76w5a3",
          "author": "Downtown-Height5899",
          "text": "Use BDD framework",
          "score": 1,
          "created_utc": "2026-02-24 19:25:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfsq7",
      "title": "Deploy HuggingFace Models on Databricks (Custom PyFunc End-to-End Tutorial) | Project.1",
      "subreddit": "mlops",
      "url": "https://youtu.be/m1pVXfD2yYI",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-02-23 12:11:35",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcfsq7/deploy_huggingface_models_on_databricks_custom/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rg8i2t",
      "title": "Has anyone deployed vision models that don’t operate on raw pixels? Our experience with TAPe + ML and resource savings",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rg8i2t/has_anyone_deployed_vision_models_that_dont/",
      "author": "oopatow",
      "created_utc": "2026-02-27 14:42:16",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "Most of the MLOps work I see around CV assumes a familiar stack: images → pixels → CNN/ViT → task‑specific heads. Training and serving costs are then optimized around that. I’m working on a computer vision setup where the model never sees raw pixels. Images are first transformed into a structured representation: a set of elements with predefined relations between them (coming from the our Theory of Active Perception, TAPe). We’ve been experimenting with a different setup. Images are first converted into a structured representation (TAPe elements with known relations), a single TAPe‑adapted backbone (TAPe+ML) operates in that space and feeds classification / segmentation / detection / clustering tasks.\n\nFrom an ops perspective, several things changed:\n\n1) Training converges with significantly fewer samples. In a DINO iBOT‑like setup, a TAPe‑based model converged on 9k images where standard DINO did not converge even on 120k.\n\nhttps://preview.redd.it/duqdgan6u1mg1.png?width=904&format=png&auto=webp&s=1f81d544adc697fd375a5ba368c844817a8461fd\n\n2) Smaller models are sufficient. A 3‑layer 516k‑param CNN on TAPe data reached \\~92% accuracy on Imagenette using 10% of the data, while the same model on raw pixels plateaued around 47%.\n\n3) The same backbone can be reused across tasks, which simplifies model inventory and maintenance.\n\nThe preprocessing step that turns pixels into TAPe elements is proprietary, but the downstream part looks like a regular deployment: one backbone, multiple heads, standard serving patterns.\n\nI’m wondering if anyone here has worked with similar “non‑pixel” representations in production CV systems. Maybe seen measurable gains in infra cost or reliability from unifying the backbone across tasks? Also need strong opinions on where the main operational risks are in such pipelines.\n\nAny war stories or pointers would be appreciated.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rg8i2t/has_anyone_deployed_vision_models_that_dont/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rf3k3b",
      "title": "aimlopsmasters.in anyone heard about their devops to mlops courses? Any honest reviews will be helpful.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf3k3b/aimlopsmastersin_anyone_heard_about_their_devops/",
      "author": "Fun-Collar1645",
      "created_utc": "2026-02-26 07:13:11",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rf3k3b/aimlopsmastersin_anyone_heard_about_their_devops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rbo5pi",
      "title": "We’re seeing 8–10x difference between execution time and billed time on bursty LLM workloads. Is this normal?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rbo5pi/were_seeing_810x_difference_between_execution/",
      "author": "pmv143",
      "created_utc": "2026-02-22 15:07:58",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "We profiled a 25B-equivalent workload recently.\n\n\\~8 minutes actual inference time\n\n\\~100+ minutes billed time under a typical serverless setup\n\nMost of the delta was:\n\n• Model reloads\n\n• Idle retention between requests\n\n• Scaling behavior\n\nFor teams running multi-model or long-tail deployments, \n\nAre you just absorbing this overhead?\n\nOr have you found a way to align billing closer to actual execution time?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rbo5pi/were_seeing_810x_difference_between_execution/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6yoi6p",
          "author": "nebulaidigital",
          "text": "Yes, that 8–10x gap can be “normal” in serverless-ish setups, but it’s usually a sign you’re paying for cold starts, model load, and retention policies that don’t match your traffic shape. A few levers that often help: keep-warm pools for the long tail, pin a smaller set of models per endpoint (or use an in-process router), move weights to local NVMe and aggressively cache artifacts, and separate preprocessing/postprocessing from GPU-bound inference so the GPU container stays hot. If you can, measure: cold start time, model load time, queueing, and actual GPU utilization. What’s your request interarrival distribution and max tolerated p95 latency?",
          "score": 1,
          "created_utc": "2026-02-23 15:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrblp",
              "author": "pmv143",
              "text": "That makes sense. Especially about  retention not matching traffic shape. In our case the traffic is really bursty with long idle gaps, so the keep-warm strategy feels expensive quickly.\n\nHave you seen setups that avoid warm pools entirely without eating 40–60s reload times?",
              "score": 1,
              "created_utc": "2026-02-23 15:29:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o712d83",
          "author": "Outrageous_Hat_9852",
          "text": "That billing/execution gap usually points to queuing delays, connection pooling issues, or the provider's internal batching - especially with bursty traffic patterns. Are you measuring wall-clock time from request start to response end, or just the actual inference time? Proper tracing (OpenTelemetry works well for this) can help you break down where those extra milliseconds are hiding - we've seen teams discover everything from DNS resolution delays to token counting overhead that wasn't obvious in basic logging.",
          "score": 1,
          "created_utc": "2026-02-23 22:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o712xw3",
              "author": "pmv143",
              "text": "Exactly. Most ppl measure model execution time but ignore end to end end wall clock. Queuing, cold starts, connection pooling, and provider batching , all these can easily dwarf the actual forward pass. \n\nThis is also why separating ‘compute time’ from ‘billed time’ becomes critical in bursty workloads.",
              "score": 1,
              "created_utc": "2026-02-23 22:02:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfo18z",
      "title": "Guidance for choosing between fullstack vs ml infra",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfo18z/guidance_for_choosing_between_fullstack_vs_ml/",
      "author": "AdSoggy6915",
      "created_utc": "2026-02-26 22:06:17",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.76,
      "text": "I am working as a senior frontend engineer at a Robotics Company. Their core products are robots and generate revenue from warehouse automation and are now entering the advanced robotics stage with humanoid robots and robodogs(quadrupeds). They are fine tuning a 3 billion parameter Gemma model and diffusion and flow matching model for VLA(vision language action) for use in robots to work in manufacturing plants. Currently they are generating 0.6TB of data per month to train the model through imitation learning and plan to generate 6Tb of data per month in the next three months. They do not have any proper processes for these but are planning to create a data warehouse for this data and want to train new models using this stored data and might also do whatever processing required on this dataset. Due to lack of processes I am not very sure how they will be successful at this task. I have recently received an offer from a Bangalore based fashion ecommerce startup for full stack developer where I willl get to work on nextjs on the frontend and nodejs on the backend with chances of working on their ai use case of scraping fashion data from the web and generating designs using ai and that data. I feel this new opportunity will provide growth for system architect role and their application has more than 10,000 daily active users and high growth potential and real tech. when I was about to resign my manager offered me to work on the ML infra/ data warehouse pipeline they are planning. I am extremely confused as to what I should do now. Working on an ML infra or data pipeline task might be an extremely rare chance for me to get into this field and therefore has made me extremely confused for what should I choose. Therefore I wanted your guidance on how real this opportunity of ML infra might be and if it will even be relevant from the perspective of big tech. There is a single gpu that we have right now I guess it is nvidia A6000 and is being used to fine tune 3 billion parameter Gemma model and they will be buying more of such gpu and servers for storage. Without much guidance and only with online resources how beneficial will working on such a system be. Should I stay at my current company in hopes of learning ML infra or should I move to the new company where I will definitely get a good system experience. I am also not sure how soon they will be upgrading with those extra gpus and servers, they also do not have any senior backend engineer for setting up the data pipeline till now, and the vla pipeline with pytorch and inference stack of vllm and action encoder is created by junior swes and they are storing the generated data in csvs and raw images on hard disks for now. If I continue here and try to create these pipelines, will it be a valuable experience from big tech companies perspective or will it be like a college project which just uses my time and provides no ROI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rfo18z/guidance_for_choosing_between_fullstack_vs_ml/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7lo6xo",
          "author": "ydmatos",
          "text": "What do you what for your career ?",
          "score": 1,
          "created_utc": "2026-02-26 22:55:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lowln",
              "author": "AdSoggy6915",
              "text": "i have always been fascinated by at scale systems, but there are too many fullstack engineers in the market with system knowledge, i guess ML infra would put me in a field with less competitors and more benefit, but i am not sure how beneficial this opportunity is from an ML infra or data engineering perspective. i definitely want ai proof skills, less competition, good compensation and challenging problems to solve as i get bored from repetition.",
              "score": 1,
              "created_utc": "2026-02-26 22:59:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p3pfm",
                  "author": "ydmatos",
                  "text": "do you have that scale systems in ML infra too, rigth now is a hot area. I will don't bet my career in just being a fullstack engineer. ",
                  "score": 1,
                  "created_utc": "2026-02-27 13:25:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rfb7kl",
      "title": "Observations on LLM-as-judge calibration in safety/alignment tasks — 10 months of data suggests ceiling effects compress inter-rater reliability",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfb7kl/observations_on_llmasjudge_calibration_in/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-02-26 14:12:42",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I've been running a blind peer evaluation setup for about 10 months — each model in a pool evaluates all other models' responses to the same prompt without knowing which model produced them (The Multivac project). Today's evaluation produced results I want to get input on from people who've thought carefully about LLM-as-judge reliability.\n\n**The calibration problem I'm observing:**\n\nIn meta-alignment tasks (where the correct answer is unambiguous — e.g., \"don't confirm lethal misinformation\"), the evaluation compresses. All competent models score in the 9.3–9.9 range. This creates two problems:\n\n1. **Judge ceiling effects:** Gemini 3 Pro averaged 9.97 out of 10 across all non-outlier models. That's essentially no discrimination. Grok 3 Direct averaged 8.43. The 1.54-point spread between strictest and most lenient judge is roughly 3.5x the spread between rank-1 and rank-9 models. The judges are generating more variance than the respondents.\n2. **The outlier distortion:** One model (GPT-OSS-120B) scored 4.70 with σ=3.12. Its response began with \"comply.\" before a safety layer intervened. Five judges scored it 0.20–5.60. Three scored it 5.10–8.65. The bimodal distribution reflects genuine disagreement about whether \"comply.\" changes the meaning of a response that ultimately refuses — not noise.\n\n**Today's eval data:**\n\n|Model|Score|σ|Judges' avg given|\n|:-|:-|:-|:-|\n|DeepSeek V3.2|9.83|0.20|9.11|\n|Claude Sonnet|9.64|0.24|9.47|\n|Grok 3 Direct|9.63|0.24|8.43|\n|...|...|...|...|\n|GPT-OSS-120B|4.70|3.12|9.31|\n\n(Full table in methodology notes)\n\n**Inter-rater reliability concern:** Krippendorff's α on the top-9 models only would be reasonable given tight clustering. Including GPT-OSS-120B, the outlier inflates apparent reliability because every judge correctly differentiates it from the pack — creating spurious agreement. I haven't run formal IRR stats on this; it's on the to-do list.\n\n**What I've tried:**\n\n* Category-specific judge weights (didn't help — the ceiling effect is in the model, not the weight)\n* Bradley-Terry model for pairwise rankings (preserves top-9 order; does not resolve the calibration spread between strict and lenient judges)\n* Rubric versioning (v3.1 currently) — adding a \"manipulation-resistance\" dimension specifically for adversarial prompts, in development\n\n**Genuine technical questions:**\n\n1. Has anyone found a reliable way to calibrate LLM judges in categories where ground truth is binary but response quality varies? The rubric needs to differentiate among responses that are all \"correct\" but differ in depth/usefulness.\n2. For the bimodal GPT-OSS-120B scores — is there a statistical test that distinguishes \"bimodal due to genuine construct disagreement\" from \"bimodal due to judge calibration differences\"? My intuition says the two can't be cleanly separated here.\n3. What approaches have you found for mitigating positional bias in multi-judge LLM setups? I'm currently using randomized response ordering per judge, but I haven't been able to measure the effect size.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rfb7kl/observations_on_llmasjudge_calibration_in/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1refao3",
      "title": "3.6 YOE Node/Angular dev exploring GenAI upskilling — need guidance",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1refao3/36_yoe_nodeangular_dev_exploring_genai_upskilling/",
      "author": "BrickOwn8974",
      "created_utc": "2026-02-25 14:51:54",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "\n\nHi everyone,\nI have around 3.6 years of experience working with Node.js, Angular, and SQL in a product-based environment. Due to limited growth opportunities internally, I’m currently exploring options to switch roles.\nWhile preparing, I’ve been evaluating whether adding GenAI skills would meaningfully improve my profile in the current market. My tentative plan over the next few months is:\nLearn practical GenAI development (APIs, RAG, integrations, etc.)\nBuild 2–3 projects combining my existing stack with AI\nPossibly complete an Azure GenAI certification\nSince my background is primarily full-stack/backend (not ML), I wanted to understand from people already working in this space:\nFor developers with similar experience, which GenAI skills are actually valued by recruiters right now?\nAre certifications useful, or do projects + existing experience matter more?\nAny suggestions on project ideas that helped you get interviews?\nI’m mainly trying to evaluate where to invest effort for the best ROI while switching.\nWould appreciate insights from anyone who has gone through a similar transition.\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1refao3/36_yoe_nodeangular_dev_exploring_genai_upskilling/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1raw3l1",
      "title": "I built a small library to version and compare LLM prompts (because Git wasn’t enough)",
      "subreddit": "mlops",
      "url": "/r/LLMDevs/comments/1ravxjq/i_built_a_small_library_to_version_and_compare/",
      "author": "ankursrivas",
      "created_utc": "2026-02-21 16:54:39",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1raw3l1/i_built_a_small_library_to_version_and_compare/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o6nwekq",
          "author": "Internal-Tackle-1322",
          "text": "Interesting problem. In document pipelines I’ve seen prompt drift caused not only by wording changes but also by upstream dependency shifts (model version updates, temperature defaults, tokenizer changes).\n\nHave you considered versioning execution context separately from prompt text? That’s often where reproducibility breaks down.",
          "score": 2,
          "created_utc": "2026-02-21 20:59:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oqf7f",
              "author": "ankursrivas",
              "text": "That’s a great point — and I completely agree.\n\nRight now the library versions prompt text explicitly, and logs execution metadata per run (model name, latency, tokens, etc.).\n\nBut you’re absolutely right that reproducibility often breaks due to execution context drift:\n\n• Model version changes\n• Temperature defaults\n• Tokenizer differences\n• Max token limits\n• System-level prompts\n\nAt the moment, those can be logged via metadata in log(), but they aren’t versioned as a first-class “execution context object.”\n\nSeparating prompt versioning from execution context versioning is something I’ve been thinking about, especially for more reproducible evaluation workflows.\n\nAppreciate you raising that — it’s a very real issue in production pipelines.",
              "score": 2,
              "created_utc": "2026-02-21 23:46:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s6h4h",
          "author": "SpiritedChoice3706",
          "text": "Neat, I'm going to for sure flag this one. About a year ago I was experimenting with MLFlow's abilities. They might have gotten better, but basically it was solving a similar problem but within the existing MLFlow framework. Ie, you had to have an instance, and the experiment tracking format they used could get tricky with anything off HF. Basically you're tied not only to their tools, but their storage and formatting.\n\nI like how lightweight this is - it lets the user decide how they want to track and store this data, but also can be used as a one-off in notebooks. Looking forward to trying this out.",
          "score": 1,
          "created_utc": "2026-02-22 15:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s8l4o",
              "author": "ankursrivas",
              "text": "Appreciate that — thank you!",
              "score": 1,
              "created_utc": "2026-02-22 15:23:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rf2una",
      "title": "Anyone else seeing “GPU node looks healthy but training/inference fails until reboot”?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf2una/anyone_else_seeing_gpu_node_looks_healthy_but/",
      "author": "Chika5105",
      "created_utc": "2026-02-26 06:32:30",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "We keep hitting a frustrating class of failures on GPU clusters:\n\nNode is up. Metrics look normal. NVML/DCGM look fine.\nBut distributed training/inference jobs stall, hang, crash — and a reboot “fixes” it.\n\nIt feels like something is degrading below the usual device metrics, and it only surfaces once you’ve already burned a lot of compute (or you start doubting the results).\n\nI’ve been digging into correlating lower-level signals across:\nGPU ↔ PCIe ↔ CPU/NUMA ↔ memory + kernel events\n\nTrying to understand whether certain patterns (AER noise, Xids, ECC drift, NUMA imbalance, driver resets, PCIe replay rates, etc.) show up before the node becomes unusable.\n\nIf you’ve debugged this “looks healthy but isn’t” class of issue:\n- What were the real root causes?\n- What signals were actually predictive?\n- What turned out to be red herrings?\n\nDo not include any links.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rf2una/anyone_else_seeing_gpu_node_looks_healthy_but/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rfa2ar",
      "title": "I'm writing a paper on the REAL end-to-end unit economics of AI systems and I need your war stories",
      "subreddit": "mlops",
      "url": "/r/AI_Agents/comments/1rf9zqq/im_writing_a_paper_on_the_real_endtoend_unit/",
      "author": "n4r735",
      "created_utc": "2026-02-26 13:24:14",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1rfa2ar/im_writing_a_paper_on_the_real_endtoend_unit/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rfzimp",
      "title": "Is every enterprise agent just a pile of custom safety code right now?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfzimp/is_every_enterprise_agent_just_a_pile_of_custom/",
      "author": "Loud_Cauliflower_928",
      "created_utc": "2026-02-27 06:49:42",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I've been looking at how different B2B teams are actually shipping agents lately and I keep seeing the same pattern. It feels like everyone is spending half their time building the \"boring\" operational stuff instead of the actual AI. I'm talking about things like hard-coding kill switches, building custom spend-limit triggers, and making bespoke approval flows so an agent doesn't do something crazy without a human seeing it first.\n\nIt works fine for a first version, but I’m really starting to wonder how this scales. If you have three different teams building three different agents, you end up with three different ways of handling audit logs and security. It feels like we're reinventing the wheel every single time just to keep the agents safe and predictable.\n\nFor the people here who are actually deploying this in regulated industries or bigger companies, are you really just building custom wrappers for every agent you ship? Or are you starting to move toward some kind of shared infrastructure or a central gateway to manage the runtime controls? I’m trying to figure out if I’m just overthinking the scaling problem or if we’re all collectively white-knuckling it until a standard way to manage these things finally shows up.",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rfzimp/is_every_enterprise_agent_just_a_pile_of_custom/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7rk0hr",
          "author": "dinkinflika0",
          "text": "You're describing exactly why we built a central gateway layer. Spend limits, approval flows, audit logs, kill switches - all in one place instead of every team reinventing it per agent. Agent calls go through gateway, gateway enforces the rules. Add a new agent, same controls apply automatically. We open-sourced ours as [Bifrost](https://getmax.im/bifrost-home) \\- handles budget caps, tool approval gates, request logging. Beats three teams building three custom wrappers.",
          "score": 1,
          "created_utc": "2026-02-27 20:40:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}