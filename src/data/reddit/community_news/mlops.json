{
  "metadata": {
    "last_updated": "2026-01-22 08:58:07",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 9,
    "total_comments": 33,
    "file_size_bytes": 43403
  },
  "items": [
    {
      "id": "1qiqcl6",
      "title": "Coming from DevOps/Infra to MLOps? Here's what I learned after several interviews at product companies",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qiqcl6/coming_from_devopsinfra_to_mlops_heres_what_i/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-21 06:22:21",
      "score": 56,
      "num_comments": 16,
      "upvote_ratio": 0.97,
      "text": "I've been interviewing for MLOps and ML Platform Engineer roles over the past few months, and I wanted to share some observations that might help others make a similar transition.\n\n**The Interview Gap**\n\nMost interviewers I've faced come from research or pure ML engineering backgrounds. They think in terms of model architectures, feature engineering, and training pipelines. If you're coming from a pure infrastructure or DevOps background like me, there's often a disconnect.\n\nYou talk about Kubernetes orchestration, GPU cluster management, and cost optimisation. They ask about data drift, model retraining strategies, or how you'd debug a model's performance degradation. The conversation doesn't flow naturally because you're speaking different languages.\n\n**What Actually Helped**\n\nI realised I needed to invest time in **ML fundamentals** â€“ not to become a data scientist, but to bridge the communication gap. Understanding basic statistics, how different model types work, and what \"overfitting\" or \"data leakage\" actually mean made a huge difference.\n\nWhen I could frame infrastructure decisions in ML terms (\"this architecture reduces model serving latency by X%\" vs \"this setup has better resource utilisation\"), interviews went much more smoothly.\n\n**Be Strategic About Target Companies**\n\nNot all MLOps roles are the same. If you're targeting companies heavily invested in **real-time inferencing** (think fraud detection, recommendation engines, autonomous systems), the focus shifts to:\n\n* Data distribution and streaming pipelines\n* Low-latency prediction infrastructure\n* Real-time monitoring and anomaly detection\n* Data engineering skills\n\nIf they're doing **batch processing and research-heavy ML**, it's more about:\n\n* Experiment tracking and reproducibility\n* Training infrastructure and GPU optimization\n* Model versioning and registry management\n\nMatch your preparation to what they actually care about. Don't spray-and-pray applications.\n\n**MLOps Roles Vary Wildly**\n\nHere's something that actually helped my perspective: **MLOps means different things at different companies**.\n\nI've had interviews where the focus was 90% infrastructure (Kubernetes, CI/CD, monitoring). Others were 70% ML-focused (understanding model drift, feature stores, retraining strategies). Some wanted a hybrid who could do both.\n\nThis isn't because teams don't know what they want. It's because MLOps is genuinely different depending on:\n\n* Company maturity (startup vs established)\n* ML use cases (batch vs real-time)\n* Team structure (centralised platform vs embedded engineers)\n\nIf an interview feels misaligned, it's often a mismatch in role expectations, not a reflection of your skills. The \"MLOps Engineer\" title can mean vastly different things across companies.\n\n**Practical Tips**\n\n* Learn the basics: bias-variance tradeoff, cross-validation, common model types\n* Understand the ML lifecycle beyond just deployment\n* Be able to discuss model monitoring (not just infra monitoring)\n* Know the tools: MLflow, Kubeflow, Ray, etc. â€“ but more importantly, know *why* they exist\n* Read ML papers occasionally â€“ even if you don't implement them, you'll understand what your ML colleagues are dealing with\n\n**Final Thought**\n\nThe transition from DevOps to MLOps isn't just about learning new tools. It's about understanding a new domain and the people working in it. Meet them halfway, and you'll find the conversations get a lot easier.\n\nKeep learning, keep iterating.\n\nIf anyone's going through a similar transition and wants to chat, feel free to DM or connect here: [https://topmate.io/varun\\_rajput\\_1914/](https://topmate.io/varun_rajput_1914/)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qiqcl6/coming_from_devopsinfra_to_mlops_heres_what_i/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0tlygd",
          "author": "____Kitsune",
          "text": "How often was kubernetes a hard req vs EKS or other managed services?",
          "score": 3,
          "created_utc": "2026-01-21 08:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tt6cn",
              "author": "Extension_Key_5970",
              "text": "For MLOPs, I have not faced a deep dive wrt core on-prem K8, nowadays it's all EKS managed, of course, one should be good enough with the K8 ecosystem, as ultimately models and apps are deployed on K8, so one needs debugging and troubleshooting skills with respect to it.",
              "score": 3,
              "created_utc": "2026-01-21 09:34:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0tok4t",
          "author": "mwon",
          "text": "Very nice summary. What you think about the role AI Scientists that in many cases requires knowledge in all those skills?",
          "score": 3,
          "created_utc": "2026-01-21 08:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tswyg",
              "author": "Extension_Key_5970",
              "text": "Scientists focus on research; the skills I mentioned are engineering skills. I've seen companies expect research expertise from engineers and vice versa. Some overlap is fine, but fully merging these roles isn't ideal in the long term.\n\nThe engineering skills are accessible to anyone from a software background moving into ML â€“ even ML scientists can pick them up if transitioning from research to engineering.\n\nBe intentional about your path rather than being pushed into a hybrid role that doesn't align with your strengths.",
              "score": 2,
              "created_utc": "2026-01-21 09:32:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v0uqa",
          "author": "eemamedo",
          "text": "This is probably one of the best posts I have seen here for a while. I am going exactly the same. Some companies hire me for ML Infra but want me to live code LLM based application. Itâ€™s very rare that companies actually know what MLOps or ML Infra is supposed to do/be.Â ",
          "score": 2,
          "created_utc": "2026-01-21 14:39:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tc3eo",
          "author": "EviliestBuckle",
          "text": "Which ml course did you take?",
          "score": 1,
          "created_utc": "2026-01-21 06:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tlj5t",
              "author": "Extension_Key_5970",
              "text": "For specific ML knowledge, actually i havent follow any one course, instead I went for Top to bottom approach, I bought an practise exam for AWS ML Speciality, as it covers all ML foundations topics I suppose, went through exam scenarios one by one, learn from the answers and wrong choices, view YT videos - statsquest is awesome, if you want to dig in any of ml topics, explained very well by statsquest, these will create a strong base for ML",
              "score": 4,
              "created_utc": "2026-01-21 08:20:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0tmuef",
                  "author": "EviliestBuckle",
                  "text": "So in your experience how similar is llmops to mlops?",
                  "score": 1,
                  "created_utc": "2026-01-21 08:33:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tv57p",
          "author": "Competitive-Fact-313",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-01-21 09:53:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zmfj1",
          "author": "Friendly_Willow_8447",
          "text": "Thatâ€™s really good details\nThanks for sharing",
          "score": 1,
          "created_utc": "2026-01-22 04:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10f2ve",
          "author": "tortuga_me",
          "text": "Thats kind of true. Different companies view mlops tasks as something no other person can do. Part of the job is to advise management what is missing in their current setup and by default this means interviewers wont have that much insight into what they are looking for.",
          "score": 1,
          "created_utc": "2026-01-22 07:59:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhbtc9",
      "title": "MLOps vs MLE System Design Prep Dilemma for EM -> Which to Focus?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "author": "Low-Breakfast2018",
      "created_utc": "2026-01-19 18:11:06",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "Hi ML Leaders,  \n  \nI'm prepping for MLOps EM roles at FAANG/big tech + backups at legacy cos. But interviews seem split:  \n  \n1) SOP-hiring: Google & Meta, even \"MLOps\" JDs hit you with MLE-style system designs (classification/recommendation etc)  \n2) Team-oriented-hiring companies: Amazon/Uber/MSFT/Big Tech, more pure MLOps system design (feature stores, serving, monitoring, CI/CD).  \n3) Legacy (smaller/enterprise): Mostly general ML lead/director roles leaning MLE-heavy, few pure MLOps spots.  \n  \nDon't want to spread prep thin on two \"different\" system designs. How should I do to make sure to focus since the competition is high. Or any strategy or recommendation on double down on MLOps? How'd you balance? Seeking for experienced folks input.  \n  \nYOE: 13+ (non-FAANG)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0ir6fp",
          "author": "Comprehensive_Gap_88",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-01-19 18:27:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhfaj",
          "author": "dank_coder",
          "text": "!remind me in 1 day",
          "score": 1,
          "created_utc": "2026-01-19 20:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jhlgg",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-20 20:26:55 UTC**](http://www.wolframalpha.com/input/?i=2026-01-20%2020:26:55%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/o0jhfaj/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qhbtc9%2Fmlops_vs_mle_system_design_prep_dilemma_for_em%2Fo0jhfaj%2F%5D%0A%0ARemindMe%21%202026-01-20%2020%3A26%3A55%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qhbtc9)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-19 20:27:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kughz",
          "author": "Key_Base8254",
          "text": "following",
          "score": 1,
          "created_utc": "2026-01-20 00:35:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qebb90",
      "title": "hosted open source neptune.ai alternative?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qebb90/hosted_open_source_neptuneai_alternative/",
      "author": "Says_Watt",
      "created_utc": "2026-01-16 09:14:59",
      "score": 11,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "I would gladly pay for a hosted open source [neptune.ai](http://neptune.ai) alternative that's a drop in replacement for wandb / neptune experiment tracking. The OpenAI acquisition + shutdown of [neptune.ai](http://neptune.ai) is stupid. We as a community need a proper drop in replacement for the purposes of experiment tracking that has a performant UI. I just want to visualize my loss curve without paying w&b unacceptable pricing ($1 per gpu hour is absurd).\n\nThere's no way doing this is that hard. I would do it myself but am working on a different project right now.\n\nAlso aim is an open source alternative but it's not a drop in replacement and it's not hosted. I want to easily switch from wandb and neptune without losing quality UI, without hosting it myself, and without having to do a bunch of gymnastics to fit someone else's design patterns. It needs to be MIT license so that if you decide to sell out someone else can pick up where you left off. Please for the love of god can someone please create a mobile app so I can view my runs while on the go?\n\nedit: also there's [minfx.ai](http://minfx.ai) but their ui is terrible, why is it so hard to just clone wandb / neptune, the spec is there, someone please vibe code it lol",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qebb90/hosted_open_source_neptuneai_alternative/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzw9b7n",
          "author": "MarcelLecture",
          "text": "ML engineers and MLOps has been going with MLflow for at least 5 years.\nIts UI my not be the best and it tries to be a model registry IMHO badly (kitops ftw)\nBuuuut, it is reliable, open-source, has a great community, has managed solution, easy af to deploy and can integrate easily:\n- in your code\n- on your infra\n\nYou are not obliged to use its model registry btw\n\nI never had any big issues with 3 years of using it in production in multiple companies.",
          "score": 3,
          "created_utc": "2026-01-16 10:11:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwa2lh",
              "author": "Says_Watt",
              "text": "I think aim integrate with mlflow so I'll give it a shot, but it seems it's not as easy to use as wandb. So a UI that mimics wandb / [neptune.ai](http://neptune.ai) that uses mlflow and kitops would be great. Hosted of course.",
              "score": 1,
              "created_utc": "2026-01-16 10:18:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzw6y57",
          "author": "extreme4all",
          "text": "Mlflow is OSS mlops tracking, there is probably some managed mlflow services around",
          "score": 2,
          "created_utc": "2026-01-16 09:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw7cvu",
              "author": "Says_Watt",
              "text": "It's doing too much, can I use it as easily as wandb to track my experiments and visualize my training? If not then I'm not interested. I don't need the rest of the bloat. I want a sleak UI that I can visualize and track my experiment and a mobile app that has feature parity with wandb / [neptune.ai](http://neptune.ai) in terms of tracking the experiment. It needs to be open source because [neptune.ai](http://neptune.ai) has proven people are incapable of being reliable and wandb has proven that they're unwilling to solve the problem without gouging their customers.",
              "score": 1,
              "created_utc": "2026-01-16 09:53:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzw8dto",
                  "author": "d_lowl",
                  "text": "Yep, that's the intended usage of MLflow. You record your runs. You record your parameters and metrics (it does support metrics at steps, so you can plot your loss). You don't have to use other features if you don't want to (it's not really that bloated to be fair, it's mostly just an experiment tracker + model registry). It works locally, can be easily deployed too.\n\n\\>mobile app\n\nThat it doesn't have. But you can open it in your browser still.",
                  "score": 1,
                  "created_utc": "2026-01-16 10:03:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzw5c6l",
          "author": "mutlu_simsek",
          "text": "Check perpetual ml. It is not hosted but very cost effective.",
          "score": 1,
          "created_utc": "2026-01-16 09:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw6bts",
              "author": "Says_Watt",
              "text": "I don't understand why every company needs to be \"batteries included\" all in one solution. I just want to track my experiments. I'll deploy it myself by running a few terminal commands and pushing a container to some repository. It's not that difficult.",
              "score": 2,
              "created_utc": "2026-01-16 09:44:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwiw14",
          "author": "latent_signalcraft",
          "text": "its true that many experiment tracking tools dont offer the seamless integration and UI performance youre looking for especially when it comes to being hosted and open source. but beyond just swapping tools its crucial to consider how the underlying infrastructure and data flow will handle scaling with AI workloads. a well integrated experiment tracking system is only as effective as the data and governance practices supporting it. having solid data pipelines and robust MLOps practices in place can significantly improve the user experience especially as experimentation complexity grows. if you're looking for a drop in solution focusing on tools that also prioritize data quality and traceability might be key to a more stable and reliable solution long term.",
          "score": 1,
          "created_utc": "2026-01-16 11:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwjmzf",
              "author": "Says_Watt",
              "text": "data quality? I'm pretty new to mlops in general. I just manage my data myself, store in s3 and train. So in my case I just want a nice UI to visualize the training part.",
              "score": 1,
              "created_utc": "2026-01-16 11:39:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwsqjs",
          "author": "clever_entrepreneur",
          "text": "Why they don't share a docker compose file?",
          "score": 1,
          "created_utc": "2026-01-16 12:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxi8a9",
              "author": "Says_Watt",
              "text": "what?",
              "score": 1,
              "created_utc": "2026-01-16 15:01:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwxdzz",
          "author": "alderteeter",
          "text": "Maybe this will work for you. Seems more appropriate for an individual user than production service, but maybe Iâ€™m misreading it. \n\nhttps://huggingface.co/docs/trackio/en/index",
          "score": 1,
          "created_utc": "2026-01-16 13:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07yg1c",
          "author": "burntoutdev8291",
          "text": "I recently had this issue, moving to mlflow from wandb. The paid services are really so much better, but mlflow has been here for very long.",
          "score": 1,
          "created_utc": "2026-01-18 02:26:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgdznn",
      "title": "Thin agent / heavy tools + validation loops + observability: what would you add for prod?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/gallery/1qgdznn",
      "author": "OnlyProggingForFun",
      "created_utc": "2026-01-18 17:05:03",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qgdznn/thin_agent_heavy_tools_validation_loops/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0bi737",
          "author": "OnlyProggingForFun",
          "text": "If anyone wants the PDF, I can share it too :)",
          "score": 1,
          "created_utc": "2026-01-18 17:05:21",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0cqote",
          "author": "Revolutionary-Bet-58",
          "text": "I would say check for infinite loops/recursion, does it meet regulatory requirements and no token bombing patterns",
          "score": 1,
          "created_utc": "2026-01-18 20:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ekdhj",
          "author": "sapiensush",
          "text": "What kind of eval you follow to be specific?",
          "score": 1,
          "created_utc": "2026-01-19 02:18:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wh4ui",
          "author": "Competitive-Fact-313",
          "text": "Amazing",
          "score": 1,
          "created_utc": "2026-01-21 18:36:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfy95h",
      "title": "I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "mlops",
      "url": "https://i.redd.it/h7duxwz871eg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-18 03:59:55",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qfy95h/i_built_a_tool_that_forces_5_ais_to_debate_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0alini",
          "author": "functionalfunctional",
          "text": "So your solution to llms being bad at facts is to spend 5x as much $ and power to get a consensus on bad facts?  Maybe should have asked 6 if that was a good product idea.",
          "score": 5,
          "created_utc": "2026-01-18 14:23:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nt6ax",
              "author": "Hyperventilater",
              "text": "I don't think that's 100% a fair criticism.\n\nThe idea behind this tool is a good one, power demands not taken into account. How do you determine consensus among a group of experts? You have them do honest debate and get to the bottom of it. \n\nThe tool might be more of \"have 5 lay-people debate until consensus\", but it's definitely a good idea in theory.",
              "score": 1,
              "created_utc": "2026-01-20 13:13:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0apsu4",
              "author": "S_Anv",
              "text": "You can enable/disable any model. the minimum is 2 models.Â ",
              "score": 0,
              "created_utc": "2026-01-18 14:46:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xhi0b",
          "author": "SnooOwls966",
          "text": "How do you mitigate context pollution?",
          "score": 1,
          "created_utc": "2026-01-21 21:20:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qixc5n",
      "title": "Looking for consulting help: GPU inference server for real-time computer vision",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qixc5n/looking_for_consulting_help_gpu_inference_server/",
      "author": "bix_mobile",
      "created_utc": "2026-01-21 13:04:09",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "We're building a centralized GPU server to handle inference requests from multiple networked instruments running YOLO-based object detection and classification models. Looking for someone with relevant experience to consult on our architecture.\n\n**What we're trying to optimize:**\n\n* End-to-end latency across the full pipeline: image acquisition, compression, serialization, request/response, deserialization, and inference\n* API design for handling concurrent requests from multiple clients\n* Load balancing between two RTX 4500 Blackwell GPUs\n* Network configuration for low-latency communication\n\n**Some context:**\n\n* Multiple client instruments sending inference requests over the local network\n* Mix of object detection and classifier models\n* Real-time performance mattersâ€”we need fast response times\n\nIf you have experience with inference serving (Triton, TorchServe, custom solutions), multi-GPU setups, or optimizing YOLO deployments, I'd love to connect. Open to short-term consulting to review our approach and help us avoid common pitfalls.\n\n**If you're interested, please DM with your hourly rate.**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qixc5n/looking_for_consulting_help_gpu_inference_server/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0zl93r",
          "author": "Friendly_Willow_8447",
          "text": "Are you building this on top of any cloud providers or you have your own infrastructure? Also do you plan or are you doing kubernetes?",
          "score": 1,
          "created_utc": "2026-01-22 04:09:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhesvw",
      "title": "Setup a data lake",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "author": "Subatomail",
      "created_utc": "2026-01-19 19:56:01",
      "score": 5,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nIâ€™m a junior ML engineer, I have 2 years experience so Iâ€™m not THAT experienced and especially not in this.\n\nIâ€™ve been asked in my current job to design some sort of data lake to make the data independent from our main system and to be able to use this data for future projects in ML.\n\nTo give a little context, we already have a whole IT department working with the â€œmainâ€ company architecture. We have a very centralized system with one guy supervising every in and out. Itâ€™s a mix of AWS and on-prem.\n\nEverytime we need to access data, we either have to export them manually via the software (like a client would do) or if we are lucky and there is already an API that is setup we get to use it too.\n\nSo my manager gave me the task to try to create a data lake (or whatever the correct term might be for this) to make a copy of the data that already exists in prod and also start to pump data from the sources used by the other software. And by doing so, weâ€™ll have the same data but weâ€™ll have it independently whenever we want.\n\nThe thing is I know that this is not a simple task and other than the courses I took on DBs at school, I never designed or even thought about anything like this. I donâ€™t know what would be the best strategy, the technologies to use, how to do effective logsâ€¦.\n\nThe data is basically a fleet management, there are equipment data with gps positions and equipment details,  there are also events like if equipment are grouped together then they form a â€œjobâ€ with ids, start date, locationâ€¦ so itâ€™s a very structured data so I believe a simple sql db would suffice but Iâ€™m not sure if itâ€™s scalable.\n\nI would appreciate it if I could get some kind of books to read or leads that I should follow to at least build something that might not break after two days and that will be a good foundation long term for ML.",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0os2hr",
          "author": "ClearML",
          "text": "Youâ€™re not wrong, as this is a big ask, especially for a junior role. From an ML standpoint, donâ€™t overthink â€œdata lakeâ€ yet.\n\nFor structured fleet/event data, a simple SQL store is fine to start. What matters more for ML is: having reproducible snapshots of data, knowing which model trained on which version, avoiding manual exports long-term\n\nif you want something outside SQL that still works well for ML, a common choice is an object-store â€œlakeâ€:\n\n* Land raw data as files in S3 (or MinIO on-prem), partitioned by date/entity (e.g., events/date=.../, gps/date=.../).\n* Use a table format like Delta Lake / Apache Iceberg / Apache Hudi on top so you get versioning + schema evolution + time travel (super helpful for reproducible training datasets).\n* Query it later with Trino/Athena/Spark when needed, without locking yourself into one database.\n\nThe hard parts arenâ€™t scale, theyâ€™re ingestion, schema changes, and data ownership. Start with append-only ingestion from prod (even on a schedule), keep it boring, and design for traceability first.\n\nIf you build something reliable and reproducible, youâ€™ll have a solid ML foundation, you can always optimize later.",
          "score": 4,
          "created_utc": "2026-01-20 16:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pizjw",
              "author": "Subatomail",
              "text": "Thank you for the tips. Iâ€™ll look more into the steps you proposed and Iâ€™ll give extra attention to reliability and reproducibility",
              "score": 1,
              "created_utc": "2026-01-20 18:16:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s478f",
          "author": "HC-Klown",
          "text": "As a former ML Engineer I agree with u/ClearML about the fact that the most important thing you need is a way to track your ML Experiments and to add on that, a way to monitor deployments (your an ml engineer primarily and not a data scientist). \n\nBut as a data engineer who took part in designing and implementing my company's data platform, my advice is to NOT try to build your own version of a data platform.\nIf i understand correctly, there is a centralized team in charge of gathering the data and hopefully doing efforts to establish a source of truth for data about important entities and processes in the company.\n\nOther than ingesting already existing data from their platform, you are suggesting to also ingest data from other sources which they have already ingested, figured out potentially complex source data models, quality tested and likely implemented business logic which you do not know about. \nSo, your statement that \"we will have the same data but we'll have it independently\" is a highly unlikely scenario. Data ia not extracted and 'voila\" ready to use, there is likely mane steps inbetween.\nYou are risking:\n1. Redoing work that has already been done by another team\n2. Training your models on data that does not represent an already established and potentially evolving truth. Effectively building a shadow data platform will in the long run not be beneficial for you or the company\n\nSo my advice would be to:  \n* Focus your efforts on building a bridge between your team and the centralized data team, and trying to get the data you need from the centralized platform. I know this might take time and managers want quick results. But doing this is better in the long run. Moreover, you should be able to get support from your manager and higher stakeholders on this approach. As an ML engineer you cannot be starved of the data you need. Try doing this in parallel to starting your \"shadow data lake\" if you really need quick results.\n* From this data build a feature store, and advice like using Open Table Formats like delta or iceberg that support time travel is a nice to have and not a MUST at the beginning.",
          "score": 1,
          "created_utc": "2026-01-21 02:00:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ksynn",
          "author": "eemamedo",
          "text": "Data Lake as a technology is fairly simple. Think, S3 Buckets but many of them. \n\n\"Simple DB\" would be Data Warehouse. \n\n\nNeither of them are suitable as is for ML workloads.",
          "score": 1,
          "created_utc": "2026-01-20 00:27:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kxl59",
              "author": "Subatomail",
              "text": "What would be suitable for ML then ? Or would the data lake be a first step and then there should be an intermediate between the data lake and the ml pipeline ? Then what technology would be used for this intermediate step ?",
              "score": 1,
              "created_utc": "2026-01-20 00:51:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lik4o",
                  "author": "eemamedo",
                  "text": "Yup. Data lake is the first step. Usually, data lake is used for raw, unprocessed data that you then clean using ETL or ELT pipeline and load into data warehouse. After that, you do ML modeling. I skipped couple of steps but those steps depend on the company. For example, in my previous work, we used OLTP DB to process data from DWH and then ML consumes that data. Some companies use Feature Stores.",
                  "score": 1,
                  "created_utc": "2026-01-20 02:46:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qhtov0",
      "title": "Releasing KAOS - The K8s Agent Orchestration System",
      "subreddit": "mlops",
      "url": "https://i.redd.it/ypcs28cn7geg1.gif",
      "author": "axsauze",
      "created_utc": "2026-01-20 06:34:03",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhtov0/releasing_kaos_the_k8s_agent_orchestration_system/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mlq55",
          "author": "RetiredApostle",
          "text": "How does it differ from kagent by Solo.io?",
          "score": 1,
          "created_utc": "2026-01-20 07:08:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdtely",
      "title": "Does anyone else feel like Slurm error logs are not very helpful?\"",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qdtely/does_anyone_else_feel_like_slurm_error_logs_are/",
      "author": "Valeria_Xenakis",
      "created_utc": "2026-01-15 19:35:49",
      "score": 3,
      "num_comments": 22,
      "upvote_ratio": 0.72,
      "text": "I manage a small cluster (64 GPUs) for my lab, and I swear 40% of my week is just figuring out why a job is `Pending` or why NCCL timed out.  \n  \nYesterday, a job sat in queue for 6 hours. Slurm said `Priority`, but it turned out to be a specific partition constraint hidden in the config that wasn't documented.  \n  \nIs it just our setup, or is debugging distributed training a nightmare for everyone? What tools are you guys using to actually see *why* a node is failing? `scontrol show job` gives me nothing.\n\n>",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qdtely/does_anyone_else_feel_like_slurm_error_logs_are/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzsst5b",
          "author": "cipioxx",
          "text": "Its very frustrating.",
          "score": 2,
          "created_utc": "2026-01-15 21:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzsujeu",
              "author": "Valeria_Xenakis",
              "text": "I feel like I spend more time than necessery just grepping logs on random nodes.\n\nI really want to know if there are better ways that are industry standard to track down the root cause and would appreciate any guidance. \n\nOr are you guys stuck doing it manually too?",
              "score": 1,
              "created_utc": "2026-01-15 21:12:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzsux4x",
                  "author": "cipioxx",
                  "text": "Manually and guessing.  I have started using llms to get ideas about some issues that pop up.  14 prolog errors now.  I drained the machines last week for maintenance.  I dont know whats going on",
                  "score": 2,
                  "created_utc": "2026-01-15 21:14:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxcbyk",
          "author": "ConcertTechnical25",
          "text": "Slurm error logs are essentially an Information Black Hole. When a job is stuck on Pending, the \"Priority\" label is often a mask for a hard partition constraint or a hardware mismatch. Distributed training requires more than just job status; it requires real-time monitoring of NCCL state and hardware metrics. If youâ€™re manually grepping slurmd logs on random nodes, youâ€™re playing a game of whack-a-mole that Slurm was never designed to help you win.",
          "score": 2,
          "created_utc": "2026-01-16 14:32:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzy19ku",
              "author": "Valeria_Xenakis",
              "text": "Yes, i agree and this is pretty annoying. I was wondering if this is how people go about fixing issues or if there is any better way.",
              "score": 1,
              "created_utc": "2026-01-16 16:26:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o021zl1",
                  "author": "burntoutdev8291",
                  "text": "The reply felt very AI",
                  "score": 1,
                  "created_utc": "2026-01-17 05:12:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt3lxt",
          "author": "cipioxx",
          "text": "Hmmm.  Ok.  I need to build a machine to test this on.  Thank you",
          "score": 1,
          "created_utc": "2026-01-15 21:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqfmy",
              "author": "cipioxx",
              "text": "Thank you my friend",
              "score": 1,
              "created_utc": "2026-01-15 23:50:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02632x",
          "author": "rishiarora",
          "text": "Nice cluster.",
          "score": 1,
          "created_utc": "2026-01-17 05:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hlybj",
          "author": "traceml-ai",
          "text": " I have  been thinking a lot about this class of problem.\n\nI am currently working on an open-source approach to make debugging distributed PyTorch jobs easier: starting with single-GPU today, and gradually moving toward multi-node setups.\n\nThe idea is to surface whatâ€™s actually happening during training (step timing, dataloader stalls, GPU memory pressure, per-rank behavior) so you donâ€™t have to guess from logs. \n\nIf you would  be open to it, I would love to DM and learn a bit more about your workflow and the kinds of failures you see. I am  just trying to build something that works for real clusters like yours.",
          "score": 1,
          "created_utc": "2026-01-19 15:22:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}