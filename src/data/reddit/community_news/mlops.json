{
  "metadata": {
    "last_updated": "2025-12-30 21:17:00",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 4,
    "total_comments": 11,
    "file_size_bytes": 17799
  },
  "items": [
    {
      "id": "1pvygot",
      "title": "Feature Stores: why the MVP always works and that's the trap (6 years of lessons)",
      "subreddit": "mlops",
      "url": "https://mikamu.substack.com/p/feature-store-the-sprawl",
      "author": "Valuable-Cause-6925",
      "created_utc": "2025-12-26 07:24:13",
      "score": 23,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pvygot/feature_stores_why_the_mvp_always_works_and_thats/",
      "domain": "mikamu.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw0uk45",
          "author": "stratguitar577",
          "text": "Sounds like the requirements were not defined at all before building. If you approach a feature store without planning on what it needs to solve, it’s pretty obvious you’ll run into these problems. Point in time training data is a key requirement so not sure why it’s talked about as an afterthought. ",
          "score": 8,
          "created_utc": "2025-12-26 13:50:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0xm2i",
              "author": "Valuable-Cause-6925",
              "text": "I totally agree that dataset construction is a must-have capability for a Feature Store.  \n  \nIn practice, though, the teams care much more about \"getting to production\" first. Figuring out how to make sure that training is consistent with serving is a secondary concern that some teams solve and others leave to their DS counterparts.",
              "score": 1,
              "created_utc": "2025-12-26 14:10:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw13wxq",
                  "author": "stratguitar577",
                  "text": "That’s where the feature “platform” takes over vs just the storage piece feature stores deal with. It should abstract the compute to allow writing the transformation once and running across both training and serving contexts for eliminating skew. ",
                  "score": 1,
                  "created_utc": "2025-12-26 14:50:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1qynv",
          "author": "DangKilla",
          "text": "An acquaintance is implementing too many features before launching, instead of focusing on the problem being solved",
          "score": 2,
          "created_utc": "2025-12-26 16:57:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1t7jw",
              "author": "Valuable-Cause-6925",
              "text": "Feature Store related?",
              "score": 1,
              "created_utc": "2025-12-26 17:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwitrbv",
          "author": "seunosewa",
          "text": "This might as well be an advertisement for storing everything in standard relational databases like PostgreSQL and MySQL.",
          "score": 1,
          "created_utc": "2025-12-29 10:13:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwivbr9",
              "author": "Valuable-Cause-6925",
              "text": "By everything you mean what exactly?",
              "score": 1,
              "created_utc": "2025-12-29 10:27:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwjces7",
                  "author": "seunosewa",
                  "text": "Data.",
                  "score": 1,
                  "created_utc": "2025-12-29 12:50:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg54z",
      "title": "Production ML Serving Boilerplate - Skip the Infrastructure Setup",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pyg54z/production_ml_serving_boilerplate_skip_the/",
      "author": "Extension_Key_5970",
      "created_utc": "2025-12-29 07:39:01",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "MLOps engineer here. Built this after setting up the same stack for the 5th time.\n\nWhat it is:\n\nInfrastructure boilerplate for MODEL SERVING (not training). Handles everything between \"trained model\" and \"production API.\"\n\nStack:\n\n\\- MLflow (model registry)\n\n\\- FastAPI (inference API)\n\n\\- PostgreSQL + Redis + MinIO\n\n\\- Prometheus + Grafana\n\n\\- Kubernetes (tested on Docker Desktop K8s)\n\nWhat works NOW:\n\nFull stack via \\`docker-compose up -d\\`\n\nK8s deployment with HPA (2-10 replicas)\n\nEnsemble predictions built-in\n\nHot model reloading (zero downtime)\n\nE2E validation script\n\nProduction-grade health probes\n\nKey features for MLOps:\n\n\\- Stage-based deployment (None → Staging → Production)\n\n\\- Model versioning via MLflow\n\n\\- Prometheus ServiceMonitor for auto-discovery\n\n\\- Rolling updates (maxUnavailable: 0)\n\n\\- Resource limits configured\n\n\\- Non-root containers\n\n5-minute setup:\n\n\\`\\`\\`bash\n\ndocker-compose up -d\n\npython3 scripts/demo-e2e-workflow.py  # Creates model, registers, serves\n\n\\`\\`\\`\n\nProduction deploy:\n\n\\`\\`\\`bash\n\n./scripts/k8s-bootstrap.sh  # One-command K8s setup\n\n./scripts/validate-deployment.sh --env k8s\n\n\\`\\`\\`\n\nHonest question: What's the most significant pain point in your ML deployment workflow that this doesn't solve?\n\nGitHub: [https://github.com/var1914/mlops-boilerplate](https://github.com/var1914/mlops-boilerplate)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pyg54z/production_ml_serving_boilerplate_skip_the/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pxpzeu",
      "title": "Built a small production-style MLOps platform while learning FastAPI, Docker, and CI/CD – looking for feedback",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pxpzeu/built_a_small_productionstyle_mlops_platform/",
      "author": "Melodic_Struggle_95",
      "created_utc": "2025-12-28 12:14:33",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.78,
      "text": "I’ve been learning MLOps and wanted to move beyond notebooks, so I built a small production-style setup from scratch.\n\n\n\nWhat it includes:\n\n\\- Training pipeline with evaluation gate\n\n\\- FastAPI inference service with Pydantic validation\n\n\\- Dockerized API\n\n\\- GitHub Actions CI pipeline\n\n\\- Swagger UI for testing predictions\n\n\n\nThis was mainly a learning project to understand how models move from training to deployment and what can break along the way.\n\n\n\nI ran into a few real-world issues (model loading inside Docker, environment constraints on Ubuntu, CI failures) and documented fixes in the README.\n\n\n\nI’d really appreciate feedback on:\n\n\\- Project structure\n\n\\- Anything missing for a “real” MLOps setup\n\n\\- What you’d add next if this were production\n\n\n\nRepo: [https://github.com/faizalbagwan786/mlops-production-platform](https://github.com/faizalbagwan786/mlops-production-platform)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pxpzeu/built_a_small_productionstyle_mlops_platform/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwcrjyb",
          "author": "raiffuvar",
          "text": "Not gonna put it in production. Split training and platform into different repos.\nGrab a real training repo, stick it here, and see what happens.",
          "score": 3,
          "created_utc": "2025-12-28 12:32:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcs8hh",
              "author": "Melodic_Struggle_95",
              "text": "Fair point. This repo isn’t meant to represent a final production setup, but a learning-focused MLOps platform where I can iterate end to end.\nIn a real production environment, I agree that training and serving would typically live in separate repos or at least separate deployment units, often owned by different teams.\nFor now, I kept them together to understand the full lifecycle and the interfaces between training, evaluation, and inference. My next step is to split training and serving and treat the trained model as an external artifact to the platform.\nAppreciate the feedback.",
              "score": 2,
              "created_utc": "2025-12-28 12:38:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh2vk2",
          "author": "BackgroundLow3793",
          "text": "Oh that's nice. Thank you. I'm learning MLOps recently too. I think next thing is MLFlow, understand why people use MLFlow. I mean it doesn't have to be MLFLow, but the core idea is tracking and model versioning I guess. \n\nThere is also a good article here: [https://docs.databricks.com/aws/en/machine-learning/mlops/mlops-workflow](https://docs.databricks.com/aws/en/machine-learning/mlops/mlops-workflow)",
          "score": 2,
          "created_utc": "2025-12-29 02:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjldtq",
              "author": "Melodic_Struggle_95",
              "text": "Thanks! Yeah, I’m on the same page. The main value is tracking and versioning, not MLflow itself. I’m planning to add that next, and the Databricks article looks solid. Appreciate you sharing it.",
              "score": 1,
              "created_utc": "2025-12-29 13:48:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgfa6",
          "author": "wallesis",
          "text": "Where's the \"platform\" part?",
          "score": 1,
          "created_utc": "2025-12-28 15:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdl5gc",
              "author": "Melodic_Struggle_95",
              "text": "Right now the “platform” part is still small by design. At this stage I’m focusing on building the core pieces first a clean training pipeline, an evaluation gate, a consistent model loading layer, and a serving API with clear contracts.The idea is to treat this as the foundation, and then gradually add real platform features like CI/CD, model registry, monitoring, and automated retraining. This repo shows the early platform core, not the final version.",
              "score": 1,
              "created_utc": "2025-12-28 15:40:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwozj3m",
          "author": "Significant-Fig-3933",
          "text": "How do you handle data lineage, code tracking, orchestration, and data/model monitoring?\nLike what exact data and what exact code (model design, features, etc) was the model trained with?\n\nAnd how do you coordinate/track retraining etc (ie orchestration)?",
          "score": 1,
          "created_utc": "2025-12-30 07:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoztqm",
              "author": "Melodic_Struggle_95",
              "text": "Right now the project tracks data and code through the training pipeline and Git commits, with retraining handled manually, and the next planned step is adding MLflow and orchestration to properly manage lineage, versioning, monitoring, and automated retraining.",
              "score": 2,
              "created_utc": "2025-12-30 07:27:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqwnyh",
          "author": "bad_detectiv3",
          "text": "Can you provide resource where you built up on MLOps material? I want to give this a try beyond using LLM in application use case",
          "score": 1,
          "created_utc": "2025-12-30 15:49:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyuobm",
      "title": "I got tired of burning money on idle H100s, so I wrote a script to kill them",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pyuobm/i_got_tired_of_burning_money_on_idle_h100s_so_i/",
      "author": "jordiferrero",
      "created_utc": "2025-12-29 18:54:21",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.84,
      "text": "[https://github.com/jordiferrero/gpu-auto-shutdown](https://github.com/jordiferrero/gpu-auto-shutdown)\n\nGet it running on your ec2 instances now forever:\n\n    git clone https://github.com/jordiferrero/gpu-auto-shutdown.git\n    cd gpu-auto-shutdown\n    sudo ./install.sh\n\n\n\n\n\nYou  \nknow  \nthe feeling in ML research. You spin up an H100 instance to train a model, go to sleep expecting it to finish at 3 AM, and then wake up at 9 AM. Congratulations, you just paid for 6 hours of the world's most expensive space heater.\n\nI did this way too many times. I must run my own EC2 instances for research, there's no other way.\n\nSo I wrote a simple daemon that watches nvidia-smi.\n\nIt’s not rocket science, but it’s effective:\n\n1. It monitors GPU usage every minute.\n2. If your training job finishes (usage drops compared to high), it starts a countdown.\n3. If it stays idle for 20 minutes (configurable), it kills the instance.\n\n**The Math:**\n\nAn on-demand H100 typically costs around $5.00/hour.\n\nIf you leave it idle for just 10 hours a day (overnight + forgotten weekends + \"I'll check it after lunch\"), that is:\n\n* $50 wasted daily\n* up to $18,250 wasted per year per GPU\n\nThis script stops that bleeding. It works on AWS, GCP, Azure, and pretty much any Linux box with systemd. It even checks if it's running on a cloud instance before shutting down so it doesn't accidentally kill your local rig.\n\nCode is open source, MIT licensed. Roast my bash scripting if you want, but it saved me a fortune.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pyuobm/i_got_tired_of_burning_money_on_idle_h100s_so_i/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwo5pms",
          "author": "kamelkev",
          "text": "Good budget solution, but limited. You can do better analysis by leveraging cloudwatch.\n\nYou have to configure collection of “nvidia_gpu,” but once done you can do run historic analytics in a way you can’t really do with a script.\n\nFor example you could leverage a combination of cpu, network and gpu activity to truly understand if a job was done, or if it was simply a pipeline stage that had completed.\n\nWhich AMI are you using?",
          "score": 2,
          "created_utc": "2025-12-30 03:47:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9l4g",
          "author": "Dramatic_Hair_3705",
          "text": "Why don't you use apache airflow for task scheduling?",
          "score": 2,
          "created_utc": "2025-12-30 04:10:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpa3pl",
              "author": "Popular-Direction984",
              "text": "Because he can use a simple script. Why use some “solution”, where three lines of shell will do?:)",
              "score": 2,
              "created_utc": "2025-12-30 09:02:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpbjln",
                  "author": "Dramatic_Hair_3705",
                  "text": "You are a very smart guy.",
                  "score": 2,
                  "created_utc": "2025-12-30 09:15:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwmjedp",
          "author": "KeyIsNull",
          "text": "mmmm you might kill the instance in the middle of something (e.g. uploading the model somewhere), wouldn't it better to wrap your training script with a shell script that automatically shuts down the instance when the script is done?\n\n    #!/bin/bash\n    python my_training.py\n    shutdown\n\n  \nedit: code block",
          "score": 2,
          "created_utc": "2025-12-29 22:27:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmndid",
              "author": "jordiferrero",
              "text": "Absolutely. But I often use UI/fromtends that are always running so there are no scripts per se (e.g. ComfyUI)",
              "score": 2,
              "created_utc": "2025-12-29 22:48:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}