{
  "metadata": {
    "last_updated": "2026-01-14 16:46:42",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 34,
    "total_comments": 90,
    "file_size_bytes": 167911
  },
  "items": [
    {
      "id": "1q3fyvh",
      "title": "Production MLOps: What breaks between Jupyter notebooks and 10,000 concurrent users",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q3fyvh/production_mlops_what_breaks_between_jupyter/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-04 04:09:51",
      "score": 34,
      "num_comments": 7,
      "upvote_ratio": 0.93,
      "text": "Been working in ML infrastructure for a while now. Wrote some posts on the practical side of MLOps that don't get covered in tutorials\n\n  \n**Model Inferencing in Production: What MLOps Interviews Really Test**\n\nThe gap between training a model with 95% accuracy in a notebook and serving it to 10,000 simultaneous API requests. This is where most MLOps interviews actually start.\n\n[https://medium.com/p/239b151cd28d](https://medium.com/p/239b151cd28d)\n\n**How Distributed ML Training Survives GPU Crashes: A Deep Dive into Checkpoints and Shared Storage**\n\nWhat happens when GPU #3 dies 12 hours into training your LLM across 8 GPUs? Smart checkpointing is the difference between resuming in minutes versus starting over and burning thousands in compute.\n\n[https://medium.com/p/cca38d3390fb](https://medium.com/p/cca38d3390fb)\n\n**How a Cloud Engineer Can Help Build RAG and Vector DB Platforms**\n\nMoving past the buzzwords. Traditional search fails when documents say \"client reimbursement\" but you search \"customer refund.\" RAG solves this by searching your actual company data before generating answers.\n\n[https://medium.com/p/6b9c1ad5ee94](https://medium.com/p/6b9c1ad5ee94)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q3fyvh/production_mlops_what_breaks_between_jupyter/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nxl1du2",
          "author": "burntoutdev8291",
          "text": "I read the MLOps post, how do you scale GPU workloads and how can you speed up model loading? Especially for LLM workloads, the time taken to spin up a model can take minutes even. Do we do predictive, or scheduled scaling? Curious to hear how you'll solve it!\n\nHow do you deal with hardware failures? Cause if one fails, do you auto resume or anything? If it fails at 3am not many can respond",
          "score": 3,
          "created_utc": "2026-01-04 06:36:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxymkmx",
              "author": "Extension_Key_5970",
              "text": "You can adopt Kubernetes, and if it's AWS-managed EKS, then you can try Karpenter using node pools. Model loading can be speed up with model optimisation, one way is quantisation, which makes models lower in size.\n\nThen there is vLLM for LLM workloads, which can help in some way of caching LLM models, and use NVMe SSDs from the Infra perspective\n\nHardware failures are usually rare, especially if you are using any Cloud, but if they occur, have a checkpoint mechanism, as discussed in one of the blog posts, to resume processing from where it left off.",
              "score": 1,
              "created_utc": "2026-01-06 05:56:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxz69aq",
                  "author": "burntoutdev8291",
                  "text": "Despite that, just for Karpenter to spin up a node can take 5-15 mins. After that even with GDS and good caching strategies, like making sure HF HOME and VLLM cache are not in ephemeral, they can still take a while. Are there any strategies on CUDA checkpointing that you are aware of? I feel like there's potential there but there isn't much resources on these. \n\nCan agree that quantisation helps a lot, moving to FP8 is very seamless. \n\nI have had quite a lot of EFA failures causing model training to stop. Then we waste a few hours cause it happened at weird hours.\n\nYour posts are really informative btw, I'm considered quite fresh, like 2 YOE, just wanted to hear more about your complex cases if you have any. I know that it's probably very niche.",
                  "score": 1,
                  "created_utc": "2026-01-06 08:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxsb5qm",
          "author": "Broad-Disaster-3895",
          "text": "This is such a real set of issues as you move from a notebook to production scale, and the community is right to ask about scaling strategies. One thing I‚Äôve learned is that having clear automation around scaling and health checks makes it much easier to handle GPU workloads without everyone panicking at 3am. You might also think about managed [mlops as a service](https://www.clickittech.com/mlops-as-a-service/) platforms for predictable scaling, especially if your team is small. For hardware failures, automated rollbacks and warm standby workers can save a lot of headaches. It‚Äôs worth experimenting with predictive scaling and scheduled resource coverage so you‚Äôre not always in firefighting mode.",
          "score": 1,
          "created_utc": "2026-01-05 08:36:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvmhd7",
          "author": "pvatokahu",
          "text": "Good timing on these posts. We just had an incident where our inference pipeline started timing out because someone deployed a model that was doing synchronous database lookups inside the prediction loop. worked fine in testing with 10 requests, completely melted at scale.\n\nThe checkpoint thing is real - lost 3 days of fine-tuning once because our checkpoint strategy was garbage. Now we snapshot to both local NVMe and S3 every 30 minutes, plus keep the last 5 checkpoints rolling. The storage costs are nothing compared to rerunning failed jobs. Also learned the hard way that you need to checkpoint your optimizer state too, not just model weights... otherwise your learning rate schedule gets all messed up when you resume.",
          "score": 1,
          "created_utc": "2026-01-05 20:10:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxylefl",
          "author": "tortuga_me",
          "text": "RemindMe!  1 day",
          "score": 1,
          "created_utc": "2026-01-06 05:47:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxylikl",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-07 05:47:11 UTC**](http://www.wolframalpha.com/input/?i=2026-01-07%2005:47:11%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1q3fyvh/production_mlops_what_breaks_between_jupyter/nxylefl/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1q3fyvh%2Fproduction_mlops_what_breaks_between_jupyter%2Fnxylefl%2F%5D%0A%0ARemindMe%21%202026-01-07%2005%3A47%3A11%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q3fyvh)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-06 05:48:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0zedx",
      "title": "Please be brutally honest: Will I make it in MLOps?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q0zedx/please_be_brutally_honest_will_i_make_it_in_mlops/",
      "author": "OdinPupil",
      "created_utc": "2026-01-01 08:25:00",
      "score": 28,
      "num_comments": 37,
      "upvote_ratio": 0.87,
      "text": "**Strengths:**\n\n* Bachelors in mathematics from top 10 university in the us\n* PhD in engineering from top 10 also\n* 3 published papers (1 in ML, 1 in applied stats, 1 in optimization) however I will say the 1 ML paper did not impress anyone (only 17 citations)\n* Worked as a data scientist for \\~5 years upon graduation\n\n**Weaknesses:**\n\n* I have been unemployed for the last \\~5 years\n* I have ZERO letters of recommendation from my past job nor academia (I apologize for being vague here. Basically I went through a very dark and self-destructive period in my life, quit my job, and burned all my professional and academic bridges down in the process. Made some of the worst decisions of my life in a very short timespan. If you want more details, I can provide via DM/PM)\n* I have never worked with the cloud, with neural networks/AI, nor with anything related to devops. Only purely machine learning in its state circa 2021\n\n**My 6-12 month full-time study plan:**\n\n*(constructed via chatgpt, very open to critique)*\n\n* Refresher of classical ML (stuff I used to do everyday at work, stuff like kaggle and jupyter on one-time tabular data)\n* Certification 1: AWS Solutions Architect\n* Certification 2: Hashicorp Terraform Associate\n* Portfolio Project 1: Terraform-managed ML in AWS\n* Certification 3: Certified Kubernetes Administrator\n* Portfolio Project 2: Kubernetes-native ML pipeline with Inference-Feedback\n* Certification 4: AWS Data Engineer Associate\n* Portfolio Project 3: Automated Warehousing of Streaming Data with Schema Evolution and Cost-Optimization\n* Certification 5: AWS Machine Learning Engineer Associate\n* Portfolio Project 4: End-to-End MLOps in Production with Automated A/B testing and Drift detection\n* Mock Technical Interview Practice\n* Applying and Interviewing for Jobs\n\n**Please be brutally honest. What are my chances of getting into MLOps?**",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1q0zedx/please_be_brutally_honest_will_i_make_it_in_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nx2dhvj",
          "author": "juicymice",
          "text": "Tell the employers you were self-employed or running a business for five years. Don't tell them you were sitting idle. I know of one case where an applicant said he was running a chicken farm for seven years. He networked and got a C-level role.",
          "score": 21,
          "created_utc": "2026-01-01 11:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx24sut",
          "author": "infinity_magnus",
          "text": "I lead AI R&D at a company, have a PhD, have transitioned from academia to industry, and am pretty old. I would suggest the following.\n\n\\- Since you were unemployed for the last 5 years, the top priority is to get at least into a Junior ML job.  Think of the current situation as a reset of your career.  Find a position at a small/medium-sized, not-so-fancy company, even if it is in the service IT sector.\n\n\\- Focus on getting up-skilled on deep learning and related stuff quickly. You can do that because you have done even harder stuff - your PhD.\n\n\\- Don't worry about the pay, etc., get into a junior position. Given the red flags around you, a junior position is the best bet. You will need to be honest in interviews, and if they ask about the 5-year gap, speak in a way that shows you learned from your mistakes and are moving on to a new path. Companies value EQ more than IQ now, and a positive attitude is needed.   \n  \n Once you are in the job, you will get to know many recent tech stacks/cloud infra. Then learn about MLOps if you want to transition into it. I can tell you MLOps is not practised as a separate department in companies if the development team is not big. You will need to spend some time in the new job and work your way up gradually with the certifications and other training. Let me also tell you that just getting certified on some skills does not guarantee an interview - I would myself not hire a person with just certification and no previous production scale experience. \n\n  \nGood luck.",
          "score": 11,
          "created_utc": "2026-01-01 09:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25hfm",
              "author": "OdinPupil",
              "text": "ok thank you, and it seems to resound closely with what u/LoaderD, u/greysteppenwolf, and u/UnreasonableEconomy said above:\n\ndont aim for a MLops job, aim for a more junior, more ML-adjacent job first",
              "score": 2,
              "created_utc": "2026-01-01 09:58:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3hw8n",
                  "author": "infinity_magnus",
                  "text": "Correct !",
                  "score": 1,
                  "created_utc": "2026-01-01 16:13:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx8gejv",
                  "author": "MathmoKiwi",
                  "text": "Data Analyst is a good \"ML adjacent\" position to target initially.",
                  "score": 1,
                  "created_utc": "2026-01-02 10:52:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx219ji",
          "author": "greysteppenwolf",
          "text": "Genuine question: do you really think you can get all these certifications in 6-12 months without having any knowledge in devops now? Why don‚Äôt you consider just going back to DS/academia/etc?",
          "score": 7,
          "created_utc": "2026-01-01 09:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx225l7",
              "author": "OdinPupil",
              "text": "from the research i've done and also browsing in r/AWSCertifications i didnt think the 12 month time horizon would be the most unrealistic part of this plan\n\nand regarding returning to DS, i thought \"pure vanilla DS\" was a rapidly dying + oversaturated field, whereas MLops is the future",
              "score": 1,
              "created_utc": "2026-01-01 09:22:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx22txc",
                  "author": "greysteppenwolf",
                  "text": "I don‚Äôt know exactly what your previous experience with devops is but I think you are overestimating yourself, especially with Kubernetes admin certificate (I‚Äôm less familiar with AWS and don‚Äôt know how hard it is to get certs there). How proficient are you with k8s as a user, for starters? \n\nI also repeat my question about why you chose MLOps in particular. It‚Äôs a profession that requires broad technical knowledge in many different areas: ML, DevOps, system design, GPU architecture, etc.  If you don‚Äôt have recent knowledge in ANY of these, why would you choose a profession that requires you to be such a T-shaped specialist? Instead of returning to DS where you have the base knowledge and experience.\n\nAfter your edit: can‚Äôt you pivot to LLMs or something?",
                  "score": 4,
                  "created_utc": "2026-01-01 09:29:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2joy7",
          "author": "IronFilm",
          "text": "Brutally honest: Targeting MLOps is a bit of a reach too far for your current situation!\n\nAt the moment you just need \"a job\", *any* job.\n\nA good entry level job to target would be a Data Analyst role.\n\nOnce you get this job, work at it for a couple of years, *then* once you've built up that stability of work history and experience, then that is when you can start to think about targeting MLOps jobs",
          "score": 8,
          "created_utc": "2026-01-01 12:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx60wgh",
              "author": "OdinPupil",
              "text": "thank you and i appreciate it. I'd rather take a harsh truth now and be re-directed towards a goal with a higher cahnce of success",
              "score": 2,
              "created_utc": "2026-01-02 00:07:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3v3ou",
          "author": "Yarafsm",
          "text": "Just FYI - AWS certification is not really useful to what you are looking for and it will take lot of your time as it is hard for someone who has never worked in IT",
          "score": 6,
          "created_utc": "2026-01-01 17:23:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3mxic",
          "author": "Tennis-Affectionate",
          "text": "Proud of you OP, you‚Äôre gonna do great",
          "score": 4,
          "created_utc": "2026-01-01 16:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21a64",
          "author": "LoaderD",
          "text": "You‚Äôve been out of work for 5 years? Focus on getting a job, no one is going to hire you for mlops when there are this many red flags.",
          "score": 4,
          "created_utc": "2026-01-01 09:12:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx235ty",
              "author": "OdinPupil",
              "text": "but that was exactly my thinking. i have too many red flags to be hired now, so i should spend a year demonstrating im serious again through certs and projects",
              "score": 1,
              "created_utc": "2026-01-01 09:33:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2k8j0",
                  "author": "IronFilm",
                  "text": "Certs won't solve the red flags",
                  "score": 3,
                  "created_utc": "2026-01-01 12:28:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx27a8g",
                  "author": "LoaderD",
                  "text": "No, the issue is getting into an MLOPs role that you have no experience in. Work on certs and projects, sure, but try to network and get into a JR role in something more closely related to ML, then get on the job experience in ops, then move to MLOPs.\n\nGetting a job should be your first priority, because as much as it sucks, taking 5 years off is a really difficult pill for companies to swallow, having a tangentially related job shows you‚Äôre still able to hold a job.",
                  "score": 2,
                  "created_utc": "2026-01-01 10:17:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2k1y7",
              "author": "IronFilm",
              "text": "He has a whole Tiananmen Square parade full of red flags!",
              "score": 1,
              "created_utc": "2026-01-01 12:27:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx20a5x",
          "author": "UnreasonableEconomy",
          "text": "The bad news: you're right - there isn't much there, unfortunately. At least not for mlops. Even for ML, you have lots of catchup to do. Pretty much everything is transformers now (not just LLMs). Unless you can find some big corp that does old-school stuff.\n\nThe good news: I think the will to learn can 100% get you there. A lot of these certs are good goals, I think. They won't cover everything, and it's pretty AWS oriented, but it's definitely a start.\n\n> Portfolio Project\n\nWhile it's definitely not a bad idea, I don't know if that will do you all that much good in terms of hireability.\n\nIf you already have all that free time available to you, what *I* would do (and your mileage may vary) would be to attach myself to some startup. For sweat equity, I don't think many will say no. Don't expect the startup to make you tons of money (or any at all), but you'll learn (a little bit) to deal with the messy, dirty environments and constraints that involve other humans, which you won't get designing greenfield stuff.\n\nThe 'Rona did a lot of people dirty - welcome back :)",
          "score": 4,
          "created_utc": "2026-01-01 09:02:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx227g6",
              "author": "OdinPupil",
              "text": "Thank you for the honest feedback and the encouragement as well",
              "score": 1,
              "created_utc": "2026-01-01 09:23:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2fy0f",
          "author": "dukesb89",
          "text": "You should leverage your academic experience (which is very strong) to get a data science / research type role. Why are you trying to get into something you have no experience in?",
          "score": 2,
          "created_utc": "2026-01-01 11:47:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2ha8f",
              "author": "OdinPupil",
              "text": "tbh, it was from a counseling/career advice session with chatgpt. i input all my past stats, and asked what should i do, and it said MLops was a good goal",
              "score": 1,
              "created_utc": "2026-01-01 12:00:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5czet",
                  "author": "MathmoKiwi",
                  "text": "LLMs be hallucinating silly bad advice once again.",
                  "score": 1,
                  "created_utc": "2026-01-01 21:56:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2x69s",
          "author": "rishiarora",
          "text": "You give 10 more certifications but none of them come close to getting an actual job. Cut short the plan to 3 months and start giving interviews for junior roles first.",
          "score": 2,
          "created_utc": "2026-01-01 14:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbonoa",
          "author": "valuat",
          "text": "Personal issues aside, you‚Äôre way overqualified. Though ‚Äúproduction is hard‚Äù, to borrow from Musk and others, it is not a research job; it‚Äôs a production job and typically an IT job (it shouldn‚Äôt be but it is). My concern is that you‚Äôll get bored soon.",
          "score": 2,
          "created_utc": "2026-01-02 21:23:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbs9qa",
              "author": "OdinPupil",
              "text": "oh man... being overqualified/bored bc its too easy is the least of my concerns! haha but thank you for the insight",
              "score": 1,
              "created_utc": "2026-01-02 21:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6qol0",
          "author": "coinclink",
          "text": "Plenty of early-stage startups out there will hire you, you might have to work really hard at a startup though, which isn't for everyone",
          "score": 1,
          "created_utc": "2026-01-02 02:40:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8k3ae",
          "author": "eronlloyd",
          "text": "If you want to get back into the AI tech field and work your way up via some short-term training and long-term growth opportunities, consider becoming an entry-level data center technician in an AI factory. Here‚Äôs a great book to get you started: https://a.co/d/3lcKGqv.\n\nI have an MS in data science (2018) but decided to stay in my current career as a telecom engineer, and now I design AI data centers full time. The pay is as good or better than most data science positions, and you can bridge your academic knowledge with operations in creative ways that help you stay valuable and continue to advance.\n\nHappy to discuss with you further. Best of luck!",
          "score": 2,
          "created_utc": "2026-01-02 11:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxb46bi",
          "author": "Establishment_Unique",
          "text": "Stop planning and start doing. This is one thing that bugs me about AI assistants. They always want to create a 12 month timeline. That's not how it works. You start going in one direction and you learning. You learn more about what you are good at and what's valuable as you do it. Then you adjust your direction. You don't need the perfect plan you need feedback and iteration. I also agree with people saying just get a job. You don't even know of you like Mlops. If you do like it what kind?¬†\n\n\nApply for jobs NOW. If you don't get any interviews that's ok. That's feedback. Add one skill at a time until you DO get interviews. Then use feedback from those interviews (and what you are learning) to adjust your career direction. Keep doing that. Short term plans and iteration beat long term plans every time.",
          "score": 1,
          "created_utc": "2026-01-02 19:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxpteow",
          "author": "letsTalkDude",
          "text": "So As I understand, the rules of flexing have changed now.",
          "score": 1,
          "created_utc": "2026-01-04 23:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx254eo",
          "author": "Hpanduh",
          "text": "you got this it's never to late to goblin mate",
          "score": 1,
          "created_utc": "2026-01-01 09:54:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1vdh3",
      "title": "DevOps ‚Üí ML Engineering: offering 1:1 calls if you're making the transition",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q1vdh3/devops_ml_engineering_offering_11_calls_if_youre/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-02 10:34:47",
      "score": 25,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": "Spent 7 years in DevOps before moving into ML Platform Engineering. Now managing 100+ K8s clusters running ML workloads and building production systems at scale. \n\nThe transition was confusing - lots of conflicting advice about what actually matters. Your infrastructure background is more valuable than you might think, but you need to address specific gaps and position yourself effectively. \n\nSet up a Topmate to help folks going through this: [https://topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)\n\nWe can talk through skill gaps, resume positioning, which certs are worth it, project strategy, or answer whatever you're stuck on. \n\nAlso happy to answer quick questions here.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q1vdh3/devops_ml_engineering_offering_11_calls_if_youre/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nxa09ws",
          "author": "enemadoc",
          "text": "I'm a software developer looking to make this transition. I've worked on a wide variety of areas. I come from Java, and have built dev pipelines, cloud infrastructure, and worked on some bare metal Openshift clusters. I plan on getting the CKA certificate this year. Is that the cert you would go for? I assume that's the most impactful certificate I can get. Currently holding the AWS architect and Machine Learning speciality certs also.",
          "score": 3,
          "created_utc": "2026-01-02 16:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf7tzm",
              "author": "Extension_Key_5970",
              "text": "CKA level skills obviously worth it, not sure though, certification is actually crucial in grabbing a job, for early senior roles, maybe  \nFor MLOps, currently, most of the companies' focus is on inference, how to expose models with very low latency, as per my experience, and can handle ML pipelines with respect to batch and streaming data.\n\nWhere to start --> Python is a must, I would say, day to day, at least 50% learning should be using Python, the rest you can distribute across ML foundations, and System design scenarios wrt Inferencing and ML Pipelines",
              "score": 1,
              "created_utc": "2026-01-03 11:27:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbhk8r",
          "author": "Technical_Rutabaga67",
          "text": "Are you free lancing on this project?",
          "score": 1,
          "created_utc": "2026-01-02 20:48:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf7y6r",
              "author": "Extension_Key_5970",
              "text": "Currently yes, I am Freelancing",
              "score": 1,
              "created_utc": "2026-01-03 11:28:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdaj9p",
          "author": "rajeshThevar",
          "text": "Thanks for the post. \n\nCan we have your inputs on what is most needed skills on MLOps?",
          "score": 1,
          "created_utc": "2026-01-03 02:37:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf87ow",
              "author": "Extension_Key_5970",
              "text": "As said in the above comment, \"Where to start --> Python is a must, I would say, day to day, at least 50% learning should be using Python, the rest you can distribute across ML foundations, and System design scenarios wrt Inferencing and ML Pipelines\"\n\nTech stack --> Python, Kubernetes, Airflow, One ML Framework Pytorch or Tensorflow, MLFlow, Strong ML Foundations, ML Pipelines",
              "score": 1,
              "created_utc": "2026-01-03 11:30:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlsufe",
                  "author": "pm19191",
                  "text": "I'm a Senior MLOps Engineer and I've never used Kubernetes. Currently working for a 3000+ company, reporting to the CDO. Since all my projects are internal, the model system design exposes the results with a Dashboard - no Kubernetes needed. The rest seems accurate.",
                  "score": 1,
                  "created_utc": "2026-01-04 10:41:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxmcigu",
                  "author": "cosmic_timing",
                  "text": "That's wild to me. Why not just replace that entire stack with ai agents?",
                  "score": 1,
                  "created_utc": "2026-01-04 13:18:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q9dlxo",
      "title": "Confused about terminology in this area",
      "subreddit": "mlops",
      "url": "https://i.redd.it/g6p46x8hskcg1.png",
      "author": "KimchiFitness",
      "created_utc": "2026-01-10 19:47:06",
      "score": 25,
      "num_comments": 9,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q9dlxo/confused_about_terminology_in_this_area/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyukcxg",
          "author": "Fipsomat",
          "text": "I am an ML Engineer and we do everything in this chart to some degree.",
          "score": 6,
          "created_utc": "2026-01-10 20:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyydv3c",
              "author": "KimchiFitness",
              "text": "the unsatisfying (but unfortunately true) answer of: actually they are all the same thing and include everything\n\nechoing what u/vladlearns, u/MindlessYesterday459, /u/[an4k1nskyw4lk3r](https://www.reddit.com/user/an4k1nskyw4lk3r/) said",
              "score": 2,
              "created_utc": "2026-01-11 11:13:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyz0pec",
                  "author": "Low-Ad6158",
                  "text": "Yeah, it's kind of a mess. Everyone uses MLOps differently based on their workflows and needs. It helps to clarify what you mean when discussing it, especially with teams or in job descriptions.",
                  "score": 1,
                  "created_utc": "2026-01-11 14:03:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyv05gh",
          "author": "MindlessYesterday459",
          "text": "It depends on company scale. In a company with 5-30 ML engineers most MLE would also do both mlops levels. \n\nWhen the numbers approaches 100 MLEs some would specialize solely in mlops of both platform and app levels whilst the rest would only do mle work. \n\nWhen you have hundreds MLEs you would need a dedicated platform level mlops team and also some engineers in other teams (SDE, MLE, SRE) also specializing specifically in application  level mlops.\n\nThere is no consesus to what mlops really entails so it highly depends on company itself.",
          "score": 3,
          "created_utc": "2026-01-10 21:56:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuq586",
          "author": "vladlearns",
          "text": "Different scopes. Zoomcamp is teaching what‚Äôs basically full-stack data science: the idea of their stuff is that a DS person can train a model and ship their model on their own - so, yes, that‚Äôs very much app-lvl work\n\nWhen companies, especially big and mature ones, say mlops guy = devops + understands ml. They‚Äôre talking about platform work, someone owning k8s, gpu quotas, tf, and all the plumbing, that lets the rest of the team operate\n\nBTW, zoomcamp is honest about what it is, it‚Äôs not pretending to turn folks into platform engineers. it‚Äôs giving DS/ML folks the ability to ship without waiting for infra\n\nthe confusion only happens later, when the industry collapses everything into the word mlops and pretends it‚Äôs one job",
          "score": 3,
          "created_utc": "2026-01-10 21:06:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyv569r",
          "author": "MyBossIsOnReddit",
          "text": "What's in a name, huh?\n\nTypically ML Engineers bring software engineering practices to ML, and is much broader in scope than you have here. Think about stuff like optimizing hot loops, ml system design, optimizing a models performance..\n\nIn practise there's no difference between platform and application level mlops at most firms. You'd also likely see it merged in with what DS and MLEs do. Companies that do tend to make distinction tend to be industry leaders.",
          "score": 3,
          "created_utc": "2026-01-10 22:20:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyvs8yn",
          "author": "an4k1nskyw4lk3r",
          "text": "It depends. MLOps is application-level plus platform-level. But almost all MLOps Engineers is doing both. It‚Äôs not enterprises fault. It‚Äôs engineers fault.",
          "score": 2,
          "created_utc": "2026-01-11 00:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzh00t",
          "author": "sippin-jesus-juice",
          "text": "I do all of this but I‚Äôm also a startup founder \n\nDon‚Äôt ask me anything, I barely know what I‚Äôm doing and am navigating day by day.  We have a proper data pipeline and setup now, but it took a lot of learning and figuring out things",
          "score": 1,
          "created_utc": "2026-01-11 15:30:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzyc91",
          "author": "AIML_Tom",
          "text": "Integrating AI/ML with other systems faces major hurdles is the problem. Problems faced are data issues silos, poor quality, labeling, legacy system incompatibility, talent gaps, scalability limits, high costs, and challenges in model explainability. This problem has no early answers.",
          "score": 1,
          "created_utc": "2026-01-11 16:53:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1nzsp",
      "title": "How to deploy multiple Mlflow models?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q1nzsp/how_to_deploy_multiple_mlflow_models/",
      "author": "Plus_Cardiologist540",
      "created_utc": "2026-01-02 03:35:30",
      "score": 20,
      "num_comments": 5,
      "upvote_ratio": 0.96,
      "text": "So, I started a new job as a Jr MLOps. I've just entered a moment where the company is undergoing a major refactoring of its infrastructure, driven by new leadership and a different vision. I'm helping to change how we deploy our models. \n\nThe new bosses want to deploy all models in a single FastAPI server that consumes 7 models from MLflow. This is not in production yet. While I'm new and a Jr, I'm starting to implement some of the old code in this new server (validation, Pydantic, etc). \n\nBefore the changes, they had 7 different servers, corresponding to 7 FastAPI servers. The new boss says there is a lot of duplicated code, so they want a single FastAPI, but I'm not sure.\n\nI asked some of the senior MLOps, and they just told me to do what the boss wants. However, I was wondering whether there is a better way to deploy multiple models without duplicating code and having them all in a single repository? Because when a model needs to be retrained, it must restart the Docker container to download the new version. Also, some models (for some reason) have different dependencies, and obviously, each one has its own retraining cycles.\n\nI had the idea of having each model in its own container and using something like MLFlow Serve to deploy the models. With a single FastAPI, I could just route to the /invocation of each model.\n\nIs this a good approach to suggest to the seniors, or should I simply follow the boss's instructions?",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1q1nzsp/how_to_deploy_multiple_mlflow_models/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nx70bya",
          "author": "Salty_Country6835",
          "text": "\n  What you are reacting to is not wrong, but it helps to separate concerns.\n\n  A single FastAPI serving multiple models does reduce duplicated *interface* code,\n  but it also tightly couples model lifecycles, dependencies, and restart semantics.\n  That tradeoff usually shows up later during retraining, hotfixes, or uneven scaling.\n\n  One common middle ground is:\n  - shared FastAPI (or gateway) for auth, validation, routing\n  - one container per model (or per dependency class)\n  - shared libraries instead of shared runtimes\n\n  That way you remove duplication without forcing all models to restart or align dependencies.\n  Framing it as ‚Äúhere are the risks of full consolidation‚Äù rather than ‚Äúthis is better‚Äù\n  usually lands better with seniors.\n\n   Which failure modes are acceptable today but painful six months from now?\n   Is duplication happening in code, or in runtime responsibility?\n   What is the smallest step that preserves optionality?\n\n  If one model needs an urgent retrain or dependency bump, should all others be forced to restart?",
          "score": 15,
          "created_utc": "2026-01-02 03:40:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8o1wk",
          "author": "guardianz42",
          "text": "The decision to do 7 models in 1 server or 7 individual servers can come down to budget, speed or latency considerations. If there's not much traffic on any individual model and they all fit in memory, you can try a single server. \n\nOtherwise, 7 different servers might be work better depending on the traffic patterns. \n\nRegardless, the best thing you can do is experiment a lot to see what works best and have flexibility to change your approach later. We personally deploy thousands of models using litserve which removes a ton of duplicate code and can work in either setting described here. https://github.com/Lightning-AI/LitServe (built on FastAPI but specialized for AI). \n\nFor senior people, come back with options and a suggestion : \"I tried these various approaches and here are all the trade-offs, and my suggestion based on these results is X\" will land better. And if you can follow with \"but the way it's built allows us the flexibility to easily change the approach later\" then the decision of the approach right now matters less as long as you can change it later. \n\nUltimately, model serving approaches change with your product's maturity and usage scaling. What works for 100 users probably won't work for 10 million.",
          "score": 4,
          "created_utc": "2026-01-02 11:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9ek4y",
              "author": "Letzbluntandbong",
              "text": "Good call on experimenting! If you're considering multiple servers, think about the deployment and maintenance trade-offs. Using something like LitServe sounds solid; it could help simplify your codebase and manage dependencies better. Presenting a few options to your seniors shows initiative and might lead to a more flexible solution.",
              "score": 2,
              "created_utc": "2026-01-02 14:51:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbb0c7",
          "author": "pvatokahu",
          "text": "Your boss is making a classic mistake that I've seen play out badly before. Consolidating 7 services into one sounds great in theory until your first production incident when one model crashes and takes down all the others. Or when you need to scale just one model that's getting hammered but now you have to scale the entire monolith.\n\nThe container-per-model approach you're thinking about is way more sensible. At Microsoft we had similar debates about service boundaries and learned the hard way that coupling unrelated models creates more problems than it solves. Different dependency versions alone should be enough to kill this idea - wait till you hit a numpy version conflict between models and watch the fun begin. i'd document the risks clearly (restart downtime affecting all models, dependency hell, independent scaling issues) and present your alternative. Sometimes new leadership needs to learn these lessons themselves though.",
          "score": 2,
          "created_utc": "2026-01-02 20:16:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiw20p",
          "author": "dayeye2006",
          "text": "check out reverse proxy -- caddy, nginx. \n\nyou need one in front of your services, so you can do URL mapping -> services, as if they are deployed in the way you described.",
          "score": 1,
          "created_utc": "2026-01-03 22:58:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb5jto",
      "title": "Observability for AI Workloads and GPU Infrencing",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qb5jto/observability_for_ai_workloads_and_gpu_infrencing/",
      "author": "DCGMechanics",
      "created_utc": "2026-01-12 20:03:20",
      "score": 14,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "Hello Folks,\n\nI need some help regarding observability for AI workloads. For those of you working on AI workloads, handling your own ML models, and running your own AI workloads in your own infrastructure, how are you doing the observability for it? I'm specifically interested in the inferencing part, GPU load, VRAM usage, processing, and throughput. How are you achieving this?\n\nWhat tools or stacks are you using? I'm currently working in an AI startup where we process a very high number of images daily. We have observability for CPU and memory, and APM for code, but nothing for the GPU and inferencing part.\n\nWhat kind of tools can I use here to build a full GPU observability solution, or should I go with a SaaS product?\n\nPlease suggest.\n\nThanks",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qb5jto/observability_for_ai_workloads_and_gpu_infrencing/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nz8bfla",
          "author": "dayeye2006",
          "text": "I add metrics emitted to Prometheus in the code. Later you can monitor and visualize using grafana conveniently",
          "score": 6,
          "created_utc": "2026-01-12 20:55:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza54y1",
          "author": "Easy_Appointment_413",
          "text": "You want end-to-end GPU + inference visibility, not just ‚Äúis the box alive?‚Äù\n\n\n\nBaseline stack a lot of teams use: dcgm-exporter on each node to expose GPU metrics (util, memory, ECC, power, temps) into Prometheus, then Grafana dashboards and alerts. Pair that with nvidia-smi dmon logs for quick CLI debugging. For per-model / per-route latency and throughput, push custom metrics from the inference service (p95 latency, queue depth, batch size, tokens or images/sec) to Prometheus or OpenTelemetry, then join them with GPU metrics in Grafana so you can see ‚Äúthis model = this GPU pressure.‚Äù\n\n\n\nFor deeper profiling, Nsight Systems/Compute for sampling, and Triton Inference Server metrics if you‚Äôre using it. Datadog or New Relic can work fine if you‚Äôre already paying for them; I‚Äôve also seen people wire alerts into Slack via PagerDuty, plus use something like Pulse alongside Datadog and OpenTelemetry to watch user feedback on Reddit when latency or quality quietly degrades.\n\n\n\nMain thing: treat GPUs as first-class monitored resources with DCGM + Prometheus, then layer model-level metrics on top.",
          "score": 3,
          "created_utc": "2026-01-13 02:32:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzaj8th",
              "author": "DCGMechanics",
              "text": "And what about infrencing observability? Any idea about this?\n\nCurrently i use nvidia-smi or nvtop for GPU metrics but the real black box is infrencing.",
              "score": 1,
              "created_utc": "2026-01-13 03:49:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nza92kk",
          "author": "Past_Tangerine_847",
          "text": "This is a real gap most teams hit once models go into production.\n\n\n\nFrom what I‚Äôve seen, GPU observability and inference observability usually end up being two different layers:\n\n\n\n1. GPU-level metrics\n\nPeople typically use:\n\n\\- NVIDIA DCGM + Prometheus exporters\n\n\\- nvidia-smi / DCGM for VRAM, utilization, throttling\n\n\\- Grafana for visualization\n\n\n\nThis covers GPU load, memory, temps, and throughput reasonably well, but it doesn‚Äôt tell you if your model behavior is degrading.\n\n\n\n2. Inference-level observability (often missing)\n\nThis is where things usually break silently:\n\n\\- prediction drift\n\n\\- entropy spikes\n\n\\- unstable outputs even though GPU + latency look fine\n\n\n\nAPM and infra metrics won‚Äôt catch this.\n\n\n\nI ran into this problem myself, so I built a small open-source middleware that sits in front of the inference layer and tracks prediction-level signals (drift, instability) without logging raw inputs.\n\n\n\nIt‚Äôs intentionally lightweight and complements GPU observability rather than replacing it.\n\nRepo is here if useful: [https://github.com/swamy18/prediction-guard--Lightweight-ML-inference-drift-failure-middleware](https://github.com/swamy18/prediction-guard--Lightweight-ML-inference-drift-failure-middleware)\n\n\n\nCurious how others are correlating GPU metrics with actual model behavior in production.",
          "score": 2,
          "created_utc": "2026-01-13 02:53:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzajbdy",
              "author": "DCGMechanics",
              "text": "Thanks, will sure check it out!",
              "score": 1,
              "created_utc": "2026-01-13 03:49:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbyus0",
          "author": "latent_signalcraft",
          "text": "if you already have CPU and APM id frame GPU inference the same way resource metrics plus request level signals tied together with good labels. most teams ive seen use NVIDIA DCGM with Prometheus and Grafana for GPU load memory power and thermals then add inference metrics like latency queue time batch size and errors via app instrumentation or OpenTelemetry. GPU graphs alone wont tell you where you‚Äôre stuck so you need both layers. SaaS can help with polish but without consistent tagging by model version and input characteristics you still miss regressions and bottlenecks.",
          "score": 1,
          "created_utc": "2026-01-13 10:52:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhtun0",
          "author": "pvatokahu",
          "text": "Try monocle2ai for inference from Linux foundation.",
          "score": 1,
          "created_utc": "2026-01-14 06:09:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1sryi",
      "title": "When models fail without ‚Äúdrift‚Äù: what actually breaks in long-running ML systems?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q1sryi/when_models_fail_without_drift_what_actually/",
      "author": "Salty_Country6835",
      "created_utc": "2026-01-02 07:53:20",
      "score": 13,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "\nI‚Äôve been thinking about a class of failures that don‚Äôt show up as classic data drift or sudden metric collapse, but still end up being the most expensive to unwind.\n\nIn a few deployments I‚Äôve seen, the model looked fine in notebooks, passed offline eval, and even behaved well in early production. The problems showed up later, once the model had time to interact with the system around it:\n\nDownstream processes quietly adapted to the model‚Äôs outputs\n\nHuman operators learned how to work around it\n\nRetraining pipelines reinforced a proxy that no longer tracked the original goal\n\nMonitoring dashboards stayed green because nothing ‚Äústatistically weird‚Äù was happening\n\n\nBy the time anyone noticed, the model wasn‚Äôt really predictive anymore, it was reshaping the environment it was trained to predict.\n\nA few questions I‚Äôm genuinely curious about from people running long-lived models:\n\nWhat failure modes have you actually seen after deployment, months in, that weren‚Äôt visible in offline eval?\n\nWhat signals have been most useful for catching problems early when it wasn‚Äôt input drift?\n\nHow do you think about models whose outputs feed back into future data, do you treat that as a different class of system?\n\nAre there monitoring practices or evaluation designs that helped, or do you mostly rely on periodic human review and post-mortems?\n\n\nNot looking for tool recommendations so much as lessons learned; what broke, what surprised you, and what you‚Äôd warn a new team about before they ship.\n",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1q1sryi/when_models_fail_without_drift_what_actually/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nxbxyil",
          "author": "bobbruno",
          "text": "My experience is that every model fails after some time. Most drift will be in input, but not all - and not all input drift is that detectable.\n\nYou also get drift on results - the inputs are as previously seen, but the competition adjusted to provide better answers to your recommendations, and you lose. The virus mutates on a gene not in your feature set, and now it's resistant. Your business changes something that the model didn't account or track, and now its predictions don't work so well anymore. There are as many ways things can go wrong as there are scenarios for ML.\n\nI just accept that every model is wrong, some are useful, for some time.",
          "score": 2,
          "created_utc": "2026-01-02 22:08:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxc1ho2",
              "author": "Salty_Country6835",
              "text": "\n\nAgreed. That‚Äôs very much my experience as well.\n\nMost failures eventually present as some form of input or outcome drift, even if the root cause wasn‚Äôt detectable or measurable at the time. By the time it‚Äôs obvious, the model‚Äôs ‚Äúuseful window‚Äù has often already passed.\n\nWhere this post came from for me was noticing that some degradations aren‚Äôt driven by novel inputs so much as the environment adapting around the model; competitors adjusting, operators changing behavior, downstream processes re-optimizing. Eventually that does look like drift, but it can take a while to surface as something you can act on.\n\nIn practice I‚Äôve landed in the same place you describe: assume impermanence, expect degradation, and design processes that plan for retirement or rework rather than indefinite correction.\n\n‚ÄúAll models are wrong, some are useful, for some time‚Äù feels like the only stable stance.",
              "score": 2,
              "created_utc": "2026-01-02 22:26:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6jp20",
          "author": "toniperamaki",
          "text": "I‚Äôve run into this a few times where nothing you‚Äôd normally call ‚Äúdrift‚Äù was happening. Metrics were fine, inputs looked normal, evals still passed. And yet the system was clearly getting worse at the thing it was supposed to help with.\n\nWhat ended up changing wasn‚Äôt the model so much as everything around it. People figured out where they didn‚Äôt trust it and started routing those cases differently. Product changes shifted how predictions were used. None of that shows up as a clean distribution shift, but it absolutely changes the role the model plays.\n\nOne case that stuck with me was where downstream teams had quietly learned when to ignore outputs. No one thought of it as a workaround, it was just ‚Äúhow you use it‚Äù. Retraining then reinforced that behavior because the data reflected those choices. From the model‚Äôs point of view, it was doing great.\n\nBy the time it came up, there wasn‚Äôt a knob to turn or a threshold to tweak. It was more like realizing everyone had been solving a slightly different problem for months.\n\nThe annoying bit is that the early signals weren‚Äôt statistical at all. They were operational. Support tickets, escalation patterns, latency suddenly mattering in places it didn‚Äôt before. All the dashboards were green, so nobody was really looking there.",
          "score": 2,
          "created_utc": "2026-01-07 11:20:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyc0xez",
              "author": "Salty_Country6835",
              "text": "Yes, this is a perfect example of what I was trying to surface.\n\n> ‚ÄúFrom the model‚Äôs point of view, it was doing great.‚Äù\nThat line captures it. The model stayed locally correct while the problem definition drifted operationally.\n\n\n\nThe part about downstream teams learning when to ignore outputs without labeling it a workaround is especially familiar. Once that behavior feeds back into training data, you‚Äôve effectively trained the system to optimize for a different role than anyone thinks it has.\n\nAnd I completely agree on early signals. The first hints I‚Äôve seen in cases like this were never statistical, they showed up in support volume, escalation paths, latency suddenly mattering in odd places, or handoffs changing. All things most ML monitoring doesn‚Äôt touch.\n\nBy the time it‚Äôs visible as ‚Äúmodel degradation,‚Äù you‚Äôre already months into solving a slightly different problem than the one you thought you were.\n\nThanks for articulating it so clearly.",
              "score": 1,
              "created_utc": "2026-01-08 03:36:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8jlcg",
          "author": "UnreasonableEconomy",
          "text": "NGL sounds a bit like either scifi, or more charitably, like bad product management.\n\n> Human operators learned how to work around it\n\nThis has nothing really to do with machine learning. You need to talk with your stakeholdes...",
          "score": 1,
          "created_utc": "2026-01-02 11:20:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8kdeg",
              "author": "Salty_Country6835",
              "text": "\nI don‚Äôt disagree that stakeholder alignment and product design matter here, that‚Äôs kind of the point.\n\nWhere I‚Äôm pushing is that once a model is deployed, those human and organizational adaptations become part of the system the model operates in. From an ops perspective, that affects:\n\nwhat data you collect next\n\nwhat retraining reinforces\n\nwhat metrics remain meaningful over time\n\n\nIf operators learn to work around a model, that‚Äôs not sci-fi, it‚Äôs an observable feedback signal that often isn‚Äôt captured by standard ML monitoring. In practice, it can quietly invalidate offline assumptions while dashboards stay green.\n\nI‚Äôm less interested in whether this is ‚ÄúML vs PM‚Äù and more in how teams operationally detect and manage these effects once a model is in prod. If you‚Äôve seen concrete ways to handle that (or decided it‚Äôs out of scope for MLOps entirely), I‚Äôd genuinely like to hear how you draw that boundary.",
              "score": 2,
              "created_utc": "2026-01-02 11:27:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx8mujc",
                  "author": "UnreasonableEconomy",
                  "text": "> If operators learn to work around a model, that‚Äôs not sci-fi, it‚Äôs an observable feedback signal that often isn‚Äôt captured by standard ML monitoring. In practice, it can quietly invalidate offline assumptions while dashboards stay green.\n\nI don't wanna be rude, but is this your professional experience, or chatgpt agreeing with you?\n\n> and more in how teams operationally detect and manage these effects once a model is in prod\n\ndetect: if what users are saying and what the metrics show don't agree\n\nmanage: by talking to users... \n\n---\n\nwhat type of product are you talking about? what industry, what application? what were you trying to accomplish?\n\nIME it's rare that you go to online leaning straight out of eval or 'early production' unless it's a time series application. Even then, rare. Eventually, the product is dialed in well enough, and the processes are understood well enough that you can go online. The users rarely know the difference. Maybe someone else has a different experience.\n\nAt the end of the day you're developing a process. Yes, ML is doing a lot of heavy lifting 'figuring out' the process, but it can't do everything. If your process fails so catastrophically (as you describe, from a process perspective), someone didn't do a good job of figuring out the process. Blaming the model doesn't really fly as an excuse.\n\nIf a model fails as you describe, someone either didn't do the legwork to understand the requirements or the context, or stopped working the process altogether.\n\nThe reason I find this unrealistic is because this assumes software is 'set it and forget it'. It never is. When development stops, the product typically dies - unless it has been developed to a certain maturity. \n\nWhat you do next is you keep developing. What data you collect next depends on what you're doing. What metrics are meaningful depends on what you're trying to accomplish and where you're struggling.",
                  "score": 1,
                  "created_utc": "2026-01-02 11:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg54z",
      "title": "Production ML Serving Boilerplate - Skip the Infrastructure Setup",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pyg54z/production_ml_serving_boilerplate_skip_the/",
      "author": "Extension_Key_5970",
      "created_utc": "2025-12-29 07:39:01",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "MLOps engineer here. Built this after setting up the same stack for the 5th time.\n\nWhat it is:\n\nInfrastructure boilerplate for MODEL SERVING (not training). Handles everything between \"trained model\" and \"production API.\"\n\nStack:\n\n\\- MLflow (model registry)\n\n\\- FastAPI (inference API)\n\n\\- PostgreSQL + Redis + MinIO\n\n\\- Prometheus + Grafana\n\n\\- Kubernetes (tested on Docker Desktop K8s)\n\nWhat works NOW:\n\nFull stack via \\`docker-compose up -d\\`\n\nK8s deployment with HPA (2-10 replicas)\n\nEnsemble predictions built-in\n\nHot model reloading (zero downtime)\n\nE2E validation script\n\nProduction-grade health probes\n\nKey features for MLOps:\n\n\\- Stage-based deployment (None ‚Üí Staging ‚Üí Production)\n\n\\- Model versioning via MLflow\n\n\\- Prometheus ServiceMonitor for auto-discovery\n\n\\- Rolling updates (maxUnavailable: 0)\n\n\\- Resource limits configured\n\n\\- Non-root containers\n\n5-minute setup:\n\n\\`\\`\\`bash\n\ndocker-compose up -d\n\npython3 scripts/demo-e2e-workflow.py  # Creates model, registers, serves\n\n\\`\\`\\`\n\nProduction deploy:\n\n\\`\\`\\`bash\n\n./scripts/k8s-bootstrap.sh  # One-command K8s setup\n\n./scripts/validate-deployment.sh --env k8s\n\n\\`\\`\\`\n\nHonest question: What's the most significant pain point in your ML deployment workflow that this doesn't solve?\n\nGitHub: [https://github.com/var1914/mlops-boilerplate](https://github.com/var1914/mlops-boilerplate)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pyg54z/production_ml_serving_boilerplate_skip_the/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwz0cbl",
          "author": "pvatokahu",
          "text": "Nice stack choices. We've been running something similar at Okahu but ended up ditching MLflow for model registry after hitting some weird edge cases with versioning conflicts when multiple teams were pushing models. The prometheus + grafana setup looks solid though - that's one thing I wish more boilerplate repos would include by default.\n\nYour hot model reloading approach is interesting.. we tried implementing that but ran into memory leaks with certain model types (especially transformer-based ones). Had to build a whole sidecar pattern just to manage the lifecycle properly. How are you handling memory cleanup during the reload process? Also curious if you've tested this with models larger than a few GB - that's where things usually start breaking in my experience.",
          "score": 1,
          "created_utc": "2025-12-31 20:24:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1v6yi",
              "author": "Extension_Key_5970",
              "text": "These are a great set of realistic queries I was expecting. \n\nFor now, the boilerplate is relatively standard base infrastructure, which usually startups struggle to implement, or if they don't want to spend time on infra, but I will take these as an enhancement, of course, to make things scalable, even if with larger models, and test memory leak scenarios specifically. \n\nAlso, I am interested in the edge cases that led you to ditch MLflow for a model registry.",
              "score": 1,
              "created_utc": "2026-01-01 08:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0xybe",
      "title": "Finally released my guide on deploying ML to Edge Devices: \"Ultimate ONNX for Deep Learning Optimization\"",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q0xybe/finally_released_my_guide_on_deploying_ml_to_edge/",
      "author": "meet_minimalist",
      "created_utc": "2026-01-01 06:49:59",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI‚Äôm excited to share that I‚Äôve just published a new book titled¬†**\"Ultimate ONNX for Deep Learning Optimization\"**.\n\nAs many of you know, taking a model from a research notebook to a production environment‚Äîespecially on resource-constrained edge devices‚Äîis a massive challenge. ONNX (Open Neural Network Exchange) has become the de-facto standard for this, but finding a structured, end-to-end guide that covers the entire ecosystem (not just the \"hello world\" export) can be tough.\n\nI wrote this book to bridge that gap. It‚Äôs designed for ML Engineers and Embedded Developers who need to optimize models for speed and efficiency without losing significant accuracy.\n\n**What‚Äôs inside the book?**¬†It covers the full workflow from export to deployment:\n\n* **Foundations:**¬†Deep dive into ONNX graphs, operators, and integrating with PyTorch/TensorFlow/Scikit-Learn.\n* **Optimization:**¬†Practical guides on Quantization, Pruning, and Knowledge Distillation.\n* **Tools:**¬†Using ONNX Runtime and ONNX Simplifier effectively.\n* **Real-World Case Studies:**¬†We go through end-to-end execution of modern models including¬†**YOLOv12**¬†(Object Detection),¬†**Whisper**¬†(Speech Recognition), and¬†**SmolLM**¬†(Compact Language Models).\n* **Edge Deployment:**¬†How to actually get these running efficiently on hardware like the Raspberry Pi.\n* **Advanced:**¬†Building custom operators and security best practices.\n\n**Who is this for?**¬†If you are a Data Scientist, AI Engineer, or Embedded Developer looking to move models from \"it works on my GPU\" to \"it works on the device,\" this is for you.\n\n**Where to find it:**¬†You can check it out on Amazon here:[https://www.amazon.in/dp/9349887207](https://www.amazon.in/dp/9349887207)\n\nI‚Äôve poured a lot of experience regarding the pain points of deployment into this. I‚Äôd love to hear your thoughts or answer any questions you have about ONNX workflows or the book content!\n\nThanks!\n\n[Book cover](https://preview.redd.it/q5wo0ag5poag1.jpg?width=970&format=pjpg&auto=webp&s=b76d4d84c9fa70e6630d1b776389623a75397afe)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q0xybe/finally_released_my_guide_on_deploying_ml_to_edge/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q4vfg0",
      "title": "Need help designing a cost efficient architecture for high concurrency multi model inferencing",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q4vfg0/need_help_designing_a_cost_efficient_architecture/",
      "author": "Fearless_Peanut_6092",
      "created_utc": "2026-01-05 19:29:18",
      "score": 12,
      "num_comments": 21,
      "upvote_ratio": 0.88,
      "text": "I‚Äôm looking for some guidance on an inference architecture problem, and I apologize in advance if something I say sounds stupid or obvious or wrong. I‚Äôm still fairly new to all of this since I just recently moved from training models to deploying models.\n\nMy initial setup uses aws lambda functions to perform tensorflow (tf) inference. Each lambda has its own small model, around 700kb in size. During runtime, the lambda downloads its model from s3, stores it in the /tmp directory, loads it as a tf model, and then runs model.predict(). This approach works perfectly fine when I‚Äôm running only a few Lambdas concurrently.\n\nHowever, once concurrency and traffic increases, the lambdas start failing with /tmp directory full errors and occasionally out-of-memory errors. After looking into, it seems like multiple lambda invocations are reusing the same execution environment, meaning downloaded models by other lambdas remain in /tmp and also memory usage accumulates over time. My understanding was that lambdas should not share environments or memory and each lambda has its own /tmp folder?, but I now realize that warm lambda execution environments can be reused. Correct me if I am wrong?\n\nTo work around this, I separated model inference from the lambda runtime and moved inference into a sagemaker multi model endpoint. The lambdas now only send inference requests to the endpoint, which hosts multiple models behind a single endpoint. This worked well initially, but as lambda concurrency increased, the multi model endpoint became a bottleneck. I started seeing latency and throughput issues because the endpoint could not handle such a large number of concurrent invocations.\n\nI can resolve this by increasing the instance size or running multiple instances behind the endpoint, but that becomes expensive very quickly. I‚Äôm trying to avoid keeping large instances running indefinitely, since cost efficiency is a major constraint for me.\n\nMy target workload is roughly 10k inference requests within five minutes, which comes out to around 34 requests per second. The models themselves are very small and lightweight, which is why I originally chose to run inference directly inside Lambda.\n\n  \nWhat I‚Äôm ultimately trying to understand is what the ‚Äúright‚Äù architecture is for this kind of use case? Where I need the models (wherever I decide to host them) to scale up and down and also handle burst traffic upto 34 invocations a second and also cheap. Do keep in mind that each lambda has its own different model to invoke. \n\nThank you for your time!",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1q4vfg0/need_help_designing_a_cost_efficient_architecture/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nxw3r2b",
          "author": "pvatokahu",
          "text": "Lambda's execution environment reuse is definitely a thing - you're not wrong about that. Each concurrent execution gets its own container but yeah, if a container finishes processing and another request comes in, AWS will reuse that same container to save on cold start time. So your /tmp and memory state persists between invocations on the same container. I've seen people try to clean up /tmp at the end of each invocation but that adds latency and doesn't always work reliably.\n\nYour multi-model endpoint approach makes sense but those things get expensive fast. We had a similar issue at Okahu where we needed to run inference for multiple small models - ended up going with ECS Fargate tasks instead of Sagemaker. You can spin up containers on demand, they scale pretty well, and you only pay for what you use. Each task can handle multiple models if you want, or you can have one model per task. The nice part is you can set up auto-scaling based on request count or CPU usage, so it handles burst traffic without keeping expensive instances running all the time.\n\nAnother option that worked for us was using Step Functions to orchestrate Lambda functions differently. Instead of having each Lambda download its own model, we had a \"model loader\" Lambda that would download and cache models in EFS (Elastic File System). Then your inference Lambdas just mount the EFS and read the models from there - no more /tmp issues. EFS is pretty cheap for small models and the read performance is good enough for most inference workloads. Plus you can set up lifecycle policies to automatically delete models that haven't been accessed in a while.",
          "score": 11,
          "created_utc": "2026-01-05 21:31:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1e0fl",
              "author": "Fearless_Peanut_6092",
              "text": "I will definitely look into ecs fargate tasks. From what you said, it actually looks like a really good fit for this use case\n\nI also like the idea of \"model loader\" lambda. But I am afraid that the inference lambda will still error out due to OOM issues. A warm inference lambda will load multiple models over its lifetime, and tensorflow inference can have memory leak across predictions. No matter what I do, I can only minimize this memory leak and not fully avoid it.",
              "score": 1,
              "created_utc": "2026-01-06 17:10:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwz5kn",
          "author": "eagz2014",
          "text": "One option is to use a Dask cluster as your model store backend with some basic methods that determine which model you need for the current request, whether the model is already cached on the cluster, download the model if not cached, and how to use the model to score the current payload. That way, the client you write can be a super lightweight API that does basic authentication (if necessary), payload validation, and submission of requests to the Dask cluster. You may even be able to just do this lightweight layer in Lambda exactly as you currently have it minus the model loading and scoring part which would go on the dask cluster.\n\nThe Dask cluster becomes the primary resource you need to tune based on how many models you anticipate needing cached at a given time, how many workers to allocate can be tuned to match your desired throughput, etc. This post below might be overkill but it gives you an idea of an architecture in both Ray and Dask.\n\nhttps://emergentmethods.medium.com/ray-vs-dask-lessons-learned-serving-240k-models-per-day-in-real-time-7863c8968a1f",
          "score": 4,
          "created_utc": "2026-01-06 00:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1ela1",
              "author": "Fearless_Peanut_6092",
              "text": "Thank you for this, I will look into it.",
              "score": 1,
              "created_utc": "2026-01-06 17:13:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxycj2p",
          "author": "decentralizedbee",
          "text": "do u have to run on aws? can u on edge? fireworks? basten?",
          "score": 2,
          "created_utc": "2026-01-06 04:43:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1exa6",
              "author": "Fearless_Peanut_6092",
              "text": "not really. Since I am not too familiar with options outside of aws I chose to stick with it. But I will definitely look into those options you mentioned. Thank you",
              "score": 1,
              "created_utc": "2026-01-06 17:14:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyhl5l",
          "author": "prasanth_krishnan",
          "text": "Why not have separate lambda functions for each of the small models. You can then multiplex the lambdas with api gateway or a proxy in the front.",
          "score": 2,
          "created_utc": "2026-01-06 05:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1f8dm",
              "author": "Fearless_Peanut_6092",
              "text": "I could do this as well. But I can have upto 10,000 different small models and having each lambda function for them seems pricey?",
              "score": 1,
              "created_utc": "2026-01-06 17:16:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny1it9j",
                  "author": "prasanth_krishnan",
                  "text": "Technically you don't pay for number of lambda. You pay only for the invocation. But having 10k lambda is an ops burden I would avoid.",
                  "score": 3,
                  "created_utc": "2026-01-06 17:32:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny1mmt2",
                  "author": "prasanth_krishnan",
                  "text": "I would approach this in the following avenues:\n\nCan we reduce the number of models? You can make a medium-sized model that can handle the cases of a bunch of small models. Maintaining 10k models is an ops burden regardless of how good your ml eco system is. \n\nCan you tolerate cold start latency? If so, we can work towards efficiently loading the models during inference time.\n\nBeing small models, can all the models be loaded in memory without any compromise on the model inference side? If yes, we can then group models into a handful of lambda that keep around 1 to 2 gb of models loaded in memory. This way, you only need to manage 10s of lambdas.\n\nDo you have any hosted infra? Like k8s? If yes this opens up other avenues.",
                  "score": 3,
                  "created_utc": "2026-01-06 17:50:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxytc9e",
          "author": "Salty_Country6835",
          "text": "\n  You're not crazy: Lambda *invocations* don't share state, but *execution environments* do. Warm reuse means /tmp and any globals can persist until AWS recycles that environment. So if you're downloading models to /tmp (and/or keeping loaded models in memory) without cleanup/limits, bursty concurrency will surface /tmp-full and OOM.\n\n  The deeper issue is the load shape: you're paying \"download + load\" costs too often. With 700kb models, the compute is cheap; the setup churn is what explodes under concurrency.\n\n  Two practical directions:\n\n  1) Stay on Lambda, but treat it like a long-lived process:\n     - Cache the loaded model in a module-level global so a warm environment reuses it.\n     - If a single Lambda can hit multiple model IDs, use an LRU cache with a hard max (N models) and evict.\n     - Don't leave model files piling up in /tmp: use unique paths per model/version and delete the artifact after load, or periodically purge /tmp on cold path.\n     - Add metrics: cache hit rate, time-to-first-predict, max RSS, /tmp usage.\n\n     Caveat: if each \"different model\" is truly a different Lambda function, each function already has its own environment pool; then the big win is simply \"download once per warm env\" + cleanup. If a single function routes to many models, you need LRU caps.\n\n  2) If you want \"cheap + bursty + many small models\", a small always-on inference service is often the sweet spot:\n     - Container service (ECS/Fargate or EKS) with lazy-load + LRU in memory.\n     - Autoscale on RPS/CPU, and optionally place SQS in front to buffer bursts.\n     - This avoids the model load thrash you see with multi-model endpoints when concurrency spikes and the active model set churns.\n\n  With your stated peak (~34 RPS), you should be able to hit cost efficiency by minimizing model-load events, not by buying bigger instances. Get the cache/eviction and observability right first; then pick the runtime (Lambda w/ provisioned concurrency vs containers) based on tail-latency and \"scale to zero\" requirements.\n\n   How many distinct models can a *single* Lambda invocation path call (1 fixed model vs dynamic model_id routing)?\n   Is latency sensitivity strict (p95/p99 target), or can you buffer via SQS to smooth bursts?\n   Are you using TF Lite already? If not, would converting these to TFLite reduce memory footprint and load time?\n\n   At peak, what is the cardinality of models touched within a 5-minute burst window (e.g., 10 models vs 1,000), and do requests cluster on a small hot set or are they uniformly spread?",
          "score": 2,
          "created_utc": "2026-01-06 06:52:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1kc4e",
              "author": "Fearless_Peanut_6092",
              "text": "Thank you for the detailed response !!\n\n1. This approach will fix the /tmp full error but I am afraid I will still face OOM errors because a single warm inference lambda can predict multiple models and I will face memory leaks. I took steps to fix this memory leak but I could only minimize it. Maybe like you said, if I change it to TFLite, I will not have any memory leaks then this approach will work for my use case.  \n\n2. Yes, I will most likely go with something like this and add an sqs in the middle to to handle burst traffic. \n\nI have an upper limit of 10,000 lambda invocations in 5 mins that is 10,000 distinct models. then lambda will decide how many containers to spin up. Depending on that each lambda can invoke from 1 distinct model to 10,000 distinct models. \n\nThe latency is 5 mins since after that the lambda times out. So, yes I can have a buffer in the middle as long as all 10k invocations finish within 5 mins. (I have a separate service that can invoke 10k concurrent lambdas every 5 mins)",
              "score": 1,
              "created_utc": "2026-01-06 17:39:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzbeq7",
          "author": "geoheil",
          "text": "do not use plain lambda but the new [https://aws.amazon.com/de/blogs/aws/introducing-aws-lambda-managed-instances-serverless-simplicity-with-ec2-flexibility/](https://aws.amazon.com/de/blogs/aws/introducing-aws-lambda-managed-instances-serverless-simplicity-with-ec2-flexibility/)",
          "score": 2,
          "created_utc": "2026-01-06 09:40:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzbhe2",
              "author": "geoheil",
              "text": "But why dont you go with [https://docs.ray.io/en/latest/serve/index.html](https://docs.ray.io/en/latest/serve/index.html) on top of fargate or k8s?",
              "score": 2,
              "created_utc": "2026-01-06 09:41:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1a5m9",
                  "author": "Fearless_Peanut_6092",
                  "text": "tbh I have no idea what other options are out there for this kind of usecase. That is why I wanted to ask here to get some recommendations.   \n  \nThank you I will look into ray serve !",
                  "score": 1,
                  "created_utc": "2026-01-06 16:53:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny19rww",
              "author": "Fearless_Peanut_6092",
              "text": "yes, I actually looked into this recently. Right now the inference is running on lambda managed instances, and functionally it‚Äôs doing what I need it to so far. \n\nI‚Äôm currently stress testing for high concurrency and I‚Äôm starting to hit throttling limits. Currently I can work around it by adding retries and time.sleep, but idk if this is a good practice in production environment. \n\nI will continue playing around different configurations of lambda managed instances and go from there.",
              "score": 2,
              "created_utc": "2026-01-06 16:51:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1v34y",
          "author": "dayeye2006",
          "text": "What model are you serving? Vision, text, tabular? What hardware are you using? I assume CPU. Intel or AMD\n\nYou probably want to look into a serving framework - e.g., trt, onnxruntime, openvino. \n\nThis gives you a few optimizations you might be missing now\n1. lower and compile your model to Utilize hardware specific instructions like AVX512 , SIMD \n2. Multi threading \n3. Continuous batching for better hardware utilization",
          "score": 1,
          "created_utc": "2026-01-06 18:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycscen",
          "author": "ampancha",
          "text": "You are exactly right about environment reuse. AWS keeps the \"Warm\" container alive to reduce cold starts, but if your code doesn't explicitly manage the lifecycle of those /tmp files or the TensorFlow memory graph, you will hit those OOM and storage limits quickly. For 700kb models at 34 RPS, SageMaker MME is usually overkill. I sent you a DM with a more cost-efficient architecture that handles this kind of burst traffic without the overhead.",
          "score": 1,
          "created_utc": "2026-01-08 06:39:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvsc9k",
          "author": "Scared_Astronaut9377",
          "text": "This is not related to ML or Ops. You are having trouble understanding how an AWS service works, i.e. runs your code. It's more appropriate to discuss in the AWS subreddit.",
          "score": -9,
          "created_utc": "2026-01-05 20:38:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvzddx",
              "author": "nullpointer1866",
              "text": "What? They‚Äôre asking THE MLOps questions: how to orchestrate model serving within a given set of constraints",
              "score": 8,
              "created_utc": "2026-01-05 21:10:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q15dgo",
      "title": "How does everyone maintain packages?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q15dgo/how_does_everyone_maintain_packages/",
      "author": "ShakeDue8420",
      "created_utc": "2026-01-01 14:26:17",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "How do you guys source and maintain AI/ML dev packages (e.g., PyTorch/CUDA/transformers), and how do you ensure they‚Äôre safe and secure?\n\nI know there‚Äôs a lot of literature out there on the subject but I‚Äôm wondering what everyone‚Äôs source of truth is, what checks/gates do most people run (scanning/signing/SBOM), and what‚Äôs a typical upgrade + rollout process?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q15dgo/how_does_everyone_maintain_packages/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nx3c1fd",
          "author": "d1ddydoit",
          "text": "Can use a lockfile to maintain the state of my projects package dependencies, use a tool like renovate to make PRs to upgrade packages in that lockfile and SAST & SCA tools on each PR to scan for vulnerabilities.\n\nCan use an artifact manager to cache binaries and tensors from pypi, hugging face etc instead of going direct (with scanning on all packages loaded into artifact manager).\n\nShould always lag behind a dependency package version release however by a couple of weeks at least to ensure security researchers have time to discover vulnerabilities.\n\nI use safetensors for models where possible over pickle etc but that is not always an option (e.g xgboost models).\n\nIn cloud, run behind tight firewalls and define least privilege for all principals used in a service.\n\nEven if my packages are compromised, the networking design and access controls for all my resources helps form a tight data perimeter that helps me sleep at night.\n\nML services are generally deployed to a network, it is both the service and the network that we deploy to that must be secure in design. I would never take one without the other as an ML engineer even though networking design is not really my job function (tuning, training and deploying models is).",
          "score": 5,
          "created_utc": "2026-01-01 15:42:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3l24w",
              "author": "ShakeDue8420",
              "text": "Thanks! What artifact manager are you using?\n\nYou mentioned cloud, are you using sagemaker or some other similar service? I‚Äôm having a hard time finding where they talk about how they manage packages. I read a doc that made it seem like they just use pypi¬†",
              "score": 1,
              "created_utc": "2026-01-01 16:30:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3pa4r",
                  "author": "d1ddydoit",
                  "text": "If you use SageMaker and are worried about the lifecycle of packages behind your SageMaker workloads you can bring/build your own containers (BYOC) for processing/training/tuning jobs and endpoints. If you want to see how SageMaker maintain their own prebuilt containers, depending on the image you can see it all like [this repo](https://github.com/aws/sagemaker-xgboost-container). I do use SageMaker (but more often I‚Äôm using Ray now). For the SageMaker resources I do have running, it‚Äôs all BYOC. The lifecycle of my service dependencies can‚Äôt depend on that of AWS SageMaker prebuilt images - I appreciate they do offer the prebuilt containers to get up and running fast (and on secure VPCs they are fine for lots of companies).\n\nI use a few artifact managers depending on what I‚Äôm up to but Artifactory is my main one.",
                  "score": 3,
                  "created_utc": "2026-01-01 16:53:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7pss9",
          "author": "latent_signalcraft",
          "text": "most teams i have seen stay sane by treating packages as platform concerns not per-project decisions. they standardize on a small set of base environments upgrade on a regular cadence and test against real workloads before rollout. scanning and SBOMs matter but uncontrolled drift and ad hoc upgrades cause more pain than missing the latest version. stability usually wins over freshness in practice.",
          "score": 2,
          "created_utc": "2026-01-02 06:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx99rg9",
          "author": "diamond143420",
          "text": "I‚Äôd recommend using a third party to keep an eye on all packages. I started using Trace AI from Zerberus to monitor our SBOM. It spotted a bunch of abandoned packages and even a few package typos. If I am really sus'd out by a package, I usually do a manual reviews. Start with sandboxed environments, test the new packages thoroughly, and keep an eye on version mismatches or use a service to monitor for you",
          "score": 2,
          "created_utc": "2026-01-02 14:24:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0fr4z",
          "author": "iamjessew",
          "text": "Founder of Jozu (jozu.com) and project lead for KitOps (kitops.org) here - we've spent a lot of time on this exact problem with enterprise customers, so I'll share what I've seen work.\n\nMost teams are winging it, we see a lot of teams trying to cram everything into a requirements.txt or Docker file, even tracking in a spreadsheet (something a surprisingly big org is doing!!) Then a quick prayer that nobody on the team installed something sketchy from PyPI. Security scanning is often an afterthought, or left to something like a Snyk scan, which we know doesn't even cover ML CVEs and issues. \n\nThe teams that have their act \\*more\\* together usually use some combination of the following:\n\n1. Curated base environments - A platform team maintains blessed container images or conda environments with approved PyTorch/CUDA/transformers versions. Data scientists pull from internal registry, not public PyPI directly.\n2. Automated scanning in CI - pip-audit, safety, or Trivy runs on every push. Block merges on critical/high CVEs. This catches the obvious stuff.\n3. SBOM generation - More teams are doing this now, especially with regulatory pressure (EU AI Act, NIST AI RMF). Generate at build time, attach to your artifacts.\n4. Signing what matters - Cosign for container images is pretty standard. For models specifically, we built SBOM generation and signing into KitOps because the model files themselves are often the blindspot - everyone's scanning their Python deps but nobody's verifying the model weights/etc weren't tampered with.\n\nThe gap I keep seeing is that teams treat model artifacts differently than code artifacts. Your application code goes through rigorous CI/CD with scanning and signing, but model files get dumped in S3 and pulled directly into production with zero verification. That's the problem we're solving with KitOps - package everything together (model, code, data, config, prompt, weights ... all of it), it can then be signed and pushed to your trusted registry. \n\nI'll add (and this is where we leave open source for a product pump) that we created Jozu to further build on this, allowing you to version those KitOps Modelkits, automatically run them through 5 different security scanners, create a downloadable audit log of the projects full lineage, and add a governance layer to block deployments or add a human in the loop element. You can dig into that on our security page (jozu.com/security)",
          "score": 2,
          "created_utc": "2026-01-06 14:28:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyuobm",
      "title": "I got tired of burning money on idle H100s, so I wrote a script to kill them",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pyuobm/i_got_tired_of_burning_money_on_idle_h100s_so_i/",
      "author": "jordiferrero",
      "created_utc": "2025-12-29 18:54:21",
      "score": 11,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "[https://github.com/jordiferrero/gpu-auto-shutdown](https://github.com/jordiferrero/gpu-auto-shutdown)\n\nGet it running on your ec2 instances now forever:\n\n    git clone https://github.com/jordiferrero/gpu-auto-shutdown.git\n    cd gpu-auto-shutdown\n    sudo ./install.sh\n\n\n\n\n\nYou  \nknow  \nthe feeling in ML research. You spin up an H100 instance to train a model, go to sleep expecting it to finish at 3 AM, and then wake up at¬†9 AM. Congratulations, you just paid for 6 hours of the world's most¬†expensive space heater.\n\nI did this way too many times. I must run my own EC2 instances for research, there's no other way.\n\nSo I wrote a simple daemon that watches¬†nvidia-smi.\n\nIt‚Äôs not rocket science, but it‚Äôs effective:\n\n1. It monitors GPU usage every minute.\n2. If your training job finishes (usage drops compared to high), it starts a countdown.\n3. If it stays idle for 20 minutes (configurable), it kills¬†the instance.\n\n**The Math:**\n\nAn on-demand H100 typically costs around¬†$5.00/hour.\n\nIf you leave it idle for just¬†10 hours a day¬†(overnight + forgotten weekends + \"I'll check it after lunch\"), that is:\n\n* $50 wasted daily\n* up to $18,250 wasted per year per GPU\n\nThis script stops that bleeding. It works on AWS, GCP, Azure, and pretty much any Linux box with systemd. It even¬†checks if it's running on a cloud instance before shutting down so it doesn't accidentally¬†kill your local rig.\n\nCode is open source, MIT licensed. Roast my¬†bash scripting if you want, but it saved me a fortune.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pyuobm/i_got_tired_of_burning_money_on_idle_h100s_so_i/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwo9l4g",
          "author": "Dramatic_Hair_3705",
          "text": "Why don't you use apache airflow for task scheduling?",
          "score": 3,
          "created_utc": "2025-12-30 04:10:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpa3pl",
              "author": "Popular-Direction984",
              "text": "Because he can use a simple script. Why use some ‚Äúsolution‚Äù, where three lines of shell will do?:)",
              "score": 1,
              "created_utc": "2025-12-30 09:02:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpbjln",
                  "author": "Dramatic_Hair_3705",
                  "text": "You are a very smart guy.",
                  "score": 2,
                  "created_utc": "2025-12-30 09:15:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo5pms",
          "author": "kamelkev",
          "text": "Good budget solution, but limited. You can do better analysis by leveraging cloudwatch.\n\nYou have to configure collection of ‚Äúnvidia_gpu,‚Äù but once done you can do run historic analytics in a way you can‚Äôt really do with a script.\n\nFor example you could leverage a combination of cpu, network and gpu activity to truly understand if a job was done, or if it was simply a pipeline stage that had completed.\n\nWhich AMI are you using?",
          "score": 2,
          "created_utc": "2025-12-30 03:47:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx92jwu",
              "author": "NoobZik",
              "text": "At least the proposed solution is cloud agnostic",
              "score": 1,
              "created_utc": "2026-01-02 13:42:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwmjedp",
          "author": "KeyIsNull",
          "text": "mmmm you might kill the instance in the middle of something (e.g. uploading the model somewhere), wouldn't it better to wrap your training script with a shell script that automatically shuts down the instance when the script is done?\n\n    #!/bin/bash\n    python my_training.py\n    shutdown\n\n  \nedit: code block",
          "score": 2,
          "created_utc": "2025-12-29 22:27:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmndid",
              "author": "jordiferrero",
              "text": "Absolutely. But I often use UI/fromtends that are always running so there are no scripts per se (e.g. ComfyUI)",
              "score": 2,
              "created_utc": "2025-12-29 22:48:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q4f7eb",
      "title": "[Vendor] Building Machine Learning Systems with a Feature Store; free digital copy.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q4f7eb/vendor_building_machine_learning_systems_with_a/",
      "author": "logicalclocks",
      "created_utc": "2026-01-05 07:19:42",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Happy New Year, Everyone.\n\n*Marketing folks* from Hopsworks here, starting the new year :)\n\nWe wanted to share the newly minted digital copy of the O'Reilly book Building Machine Learning Systems with a Feature Store that is fully free. It covers the FTI (Feature, Training, Inference) pipeline architecture and practical patterns for batch/real-time systems.\n\nEdited Link to the book: [https://www.hopsworks.ai/lp/full-book-oreilly-building-machine-learning-systems-with-a-feature-store](https://www.hopsworks.ai/lp/full-book-oreilly-building-machine-learning-systems-with-a-feature-store)  \n\\- use the \"*RedditOps*\" code.\n\n\n\nP.S. And as we are Marketing folks - we would be shuned if we did not mention that if you want to test drive the code or concepts without setting up your own infra, we just opened our new SaaS platform: [run.hopsworks.ai](http://run.hopsworks.ai)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1q4f7eb/vendor_building_machine_learning_systems_with_a/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nxs8qsu",
          "author": "GhostGit",
          "text": "The link is only for the first chapter, and not the full book. Of course, I am doing something wrong.",
          "score": 1,
          "created_utc": "2026-01-05 08:13:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxsbdoc",
              "author": "logicalclocks",
              "text": "You are correct; being too eager to post - I've posted the wrong link !!! \n\n[https://www.hopsworks.ai/lp/full-book-oreilly-building-machine-learning-systems-with-a-feature-store](https://www.hopsworks.ai/lp/full-book-oreilly-building-machine-learning-systems-with-a-feature-store)  \nuse the \"redditOps\" code.",
              "score": 1,
              "created_utc": "2026-01-05 08:38:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxtusyo",
          "author": "kinglizarrr",
          "text": "Now that's a Christmas gift!",
          "score": 1,
          "created_utc": "2026-01-05 15:17:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qa2sww",
      "title": "Automating ML pipelines with Airflow (DockerOperator vs mounted project)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qa2sww/automating_ml_pipelines_with_airflow/",
      "author": "guna1o0",
      "created_utc": "2026-01-11 15:55:01",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hello everyone,\n\nIm a data scientist with 1.6 years of experience. I have worked on credit risk modeling, sql, powerbi, and airflow.\n\nI‚Äôm currently trying to understand end-to-end ML pipelines, so I started building projects using a feature store (Feast), MLflow, model monitoring with EvidentlyAI, FastAPI, Docker, MinIO, and Airflow.\n\n\nI‚Äôm working on a personal project where I fetch data using yfinance, create features, store them in Feast, train a model, model version ing using mlflow, implement a champion‚Äìchallenger setup, expose the model through a fastAPI endpoint, and monitor it using evidentlyAI.\n\nEverything is working fine up to this stage.\n\nNow my question is: how do I automate this pipeline using airflow?\n\n1. Should I containerize the entire project first and then use the dockeroperator in airflow to automate it?\n\n2. Should I mount the project folder in airflow and automate it that way?\n\n\nPlease correct me if im wrong.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qa2sww/automating_ml_pipelines_with_airflow/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nz4p3hl",
          "author": "Extension_Key_5970",
          "text": "Don't containerise the whole project; instead, break it into pieces, like separate containers for MLflow, model monitoring with EvidentlyAI, FastAPI, Docker, MinIO, and Airflow.\n\nIn the Airflow Docker file, you can either copy the Airflow DAGs (pipelines) or mount just the DAGs folders to avoid continuously pushing new images.",
          "score": 1,
          "created_utc": "2026-01-12 08:27:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q6di1a",
      "title": "Scaling ML Pipelines for the US CPG Market: Advice on MLflow vs. Kubeflow for high-scale drift monitoring?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q6di1a/scaling_ml_pipelines_for_the_us_cpg_market_advice/",
      "author": "nonExiestent",
      "created_utc": "2026-01-07 11:58:38",
      "score": 10,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "Currently refining the production stack in our Bangalore office. We handle heavy datasets for US retail/CPG clients and are moving toward a more robust CI/CD setup with GitHub Actions and Kubernetes.\n\nSpecifically, we‚Äôre looking at how to better automate retraining triggers when we hit data drift. For those of you managing 4+ years of production ML:\n\n1. Do you prefer DVC or something cloud-native like SageMaker for versioning at this scale?\n2. How are you handling LLM deployment monitoring compared to traditional XGBoost models?\n\n**Note:** I‚Äôm also looking for a Senior Analyst who has lived through these exact struggles. If you're in Bangalore and have 4+ years of exp in this stack, I'd love to swap notes and discuss the role we're filling. Drop me a DM.",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1q6di1a/scaling_ml_pipelines_for_the_us_cpg_market_advice/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "ny6wjwv",
          "author": "toniperamaki",
          "text": "A couple of thoughts, mostly from scars rather than preferences.\n\nOn the retraining-on-drift point: be careful not to wire that too directly. In CPG especially, a lot of ‚Äúdrift‚Äù is seasonal, promotional, or downstream behavior changing, not the model going stale. If you blindly retrain every time a signal trips, you can end up chasing noise or reinforcing short-term effects.\n\nWhat I‚Äôve seen working better is treating drift as a review trigger first, not a retrain trigger. Something flags, someone looks, then you decide whether this is ‚Äúnew reality‚Äù or just the system breathing.\n\nRe MLflow vs Kubeflow: MLflow tends to hold up fine as long as you‚Äôre disciplined about how much logic you pile around it. It starts to creak when people expect it to be an orchestrator or a policy engine. Kubeflow gives you more structure, but you pay for it in operational overhead and onboarding friction. Teams underestimate that cost.\n\nOn versioning, I‚Äôve seen DVC work at scale, but only when there‚Äôs a strong culture around what gets versioned and why. Without that, it turns into a dumping ground pretty fast. Managed cloud-native tools can reduce that friction, but then you‚Äôre trading control for convenience.\n\nLLM monitoring vs something like XGBoost is a different problem class altogether. With tabular models you can usually anchor on inputs and outputs. With LLMs, most of the useful signals end up being latency, cost, prompt churn, and human feedback. Drift exists, but it‚Äôs rarely the first thing that bites you.\n\nNone of this is meant to say ‚Äúdon‚Äôt automate‚Äù, just that the automation boundaries matter more than the specific tools once the system gets big.",
          "score": 7,
          "created_utc": "2026-01-07 12:52:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygn2iz",
              "author": "ummitluyum",
              "text": "Regarding drift in CPG, I'd add that it's critical to monitor drift relative to last year (Year-over-Year), not last week. Seasonality is cyclical here, and week-over-week comparisons are guaranteed to trigger false positives and force the model to chase noise",
              "score": 1,
              "created_utc": "2026-01-08 20:07:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny7pe3v",
          "author": "qwertying23",
          "text": "shift to ray serve and ray based scaling you will have easier time scaling things I have battle scars from kubeflow",
          "score": 3,
          "created_utc": "2026-01-07 15:27:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygnpkx",
              "author": "ummitluyum",
              "text": "Ray is an excellent tool for compute, but it doesn't replace MLflow for model registry and experiment tracking. The ideal stack often looks like MLflow for lifecycle management plus Ray for the heavy lifting on top of K8s",
              "score": 2,
              "created_utc": "2026-01-08 20:10:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nygs5ww",
                  "author": "qwertying23",
                  "text": "Yes exactly :) thank you for the call out.",
                  "score": 1,
                  "created_utc": "2026-01-08 20:30:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6takz",
          "author": "rishiarora",
          "text": "CFBR",
          "score": 1,
          "created_utc": "2026-01-07 12:31:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyengbu",
          "author": "geoheil",
          "text": "ray + dasger [https://github.com/l-mds/local-data-stack/](https://github.com/l-mds/local-data-stack/) see [https://georgheiler.com/event/magenta-data-architecture-25/](https://georgheiler.com/event/magenta-data-architecture-25/) for enterprise level and [https://georgheiler.com/event/vienna-data-engineering-meetup-simple-sovereign-scalable-data-stack/](https://georgheiler.com/event/vienna-data-engineering-meetup-simple-sovereign-scalable-data-stack/) for an AI demo - use a dedicated orchestrator not a ML tracking solution; kubeflow only works within k8s - can  be hard to debug or scale down to a single persons dev machine",
          "score": 1,
          "created_utc": "2026-01-08 14:51:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygm4en",
          "author": "ummitluyum",
          "text": "MLflow and Kubeflow solve different problems, so don't try to pick \"one or the other\". MLflow is for experiment tracking and model registry, Kubeflow is an orchestrator on Kubernetes steroids. For CPG, where data is noisy and seasonal, I strongly advise against automated retraining based on data drift. In retail, \"drift\" is often just \"Black Friday\". If automation triggers retraining on that data, the model will break for normal days. It's better to set up alerts for drift, but leave the deployment decision to a human or hard business metrics, not statistical data tests",
          "score": 1,
          "created_utc": "2026-01-08 20:03:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7bb2p",
      "title": "Am I thinking Straight ?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q7bb2p/am_i_thinking_straight/",
      "author": "theoneplusbot",
      "created_utc": "2026-01-08 13:06:46",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve worked in a .NET / microservices environment for about 8 years. Alongside that, I picked up DevOps skills because I wanted to learn Docker and AKS, which is where we deploy our applications.\nFor the past 3 years, I‚Äôve been doing more DevOps and architectural work than hands-on development. At this point, I‚Äôve mostly moved away from .NET development atleast on the day job and am focused on DevOps.\nNow, I‚Äôm considering a transition into MLOps, and I‚Äôm wondering if this is the right move. I‚Äôm concerned that it might look like I‚Äôm jumping from one area to another rather than building depth.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q7bb2p/am_i_thinking_straight/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nyes5fv",
          "author": "htahir1",
          "text": "MLOps is a subset of DevOps in many ways and is definitely a growing industry still. I think it won‚Äôt come across as jumpy if you stay and specialized in operations but rather a natural transition if you like the field and want to learn more about it",
          "score": 6,
          "created_utc": "2026-01-08 15:13:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5w7u6",
      "title": "On premise vs Cloud platform based MLOps for companies, which is better?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q5w7u6/on_premise_vs_cloud_platform_based_mlops_for/",
      "author": "ben1200",
      "created_utc": "2026-01-06 21:48:20",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "I have only experience in building on premise end to end ML pipelines within my company. I done this because we don‚Äôt need a massive amount of GPU‚Äôs, what we have on site is enough for training current models. \n\nWe use GCP for data storage, then pipelines pull data down and train locally on a local machine, results are pushed to a shared MLFlow server that is hosted on a VM on GCP. \n\nI haven‚Äôt used the likes of vertex AI or azure, but what would be the man rationale for moving across?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q5w7u6/on_premise_vs_cloud_platform_based_mlops_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "ny5r2s2",
          "author": "addictzz",
          "text": "I'd say if you need to scale but you are not on GPU so may not need that.\n\nMaybe when your infra team or you are tired of maintaining data platform infra.",
          "score": 1,
          "created_utc": "2026-01-07 07:01:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxpzeu",
      "title": "Built a small production-style MLOps platform while learning FastAPI, Docker, and CI/CD ‚Äì looking for feedback",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pxpzeu/built_a_small_productionstyle_mlops_platform/",
      "author": "Melodic_Struggle_95",
      "created_utc": "2025-12-28 12:14:33",
      "score": 8,
      "num_comments": 12,
      "upvote_ratio": 0.73,
      "text": "I‚Äôve been learning MLOps and wanted to move beyond notebooks, so I built a small production-style setup from scratch.\n\n\n\nWhat it includes:\n\n\\- Training pipeline with evaluation gate\n\n\\- FastAPI inference service with Pydantic validation\n\n\\- Dockerized API\n\n\\- GitHub Actions CI pipeline\n\n\\- Swagger UI for testing predictions\n\n\n\nThis was mainly a learning project to understand how models move from training to deployment and what can break along the way.\n\n\n\nI ran into a few real-world issues (model loading inside Docker, environment constraints on Ubuntu, CI failures) and documented fixes in the README.\n\n\n\nI‚Äôd really appreciate feedback on:\n\n\\- Project structure\n\n\\- Anything missing for a ‚Äúreal‚Äù MLOps setup\n\n\\- What you‚Äôd add next if this were production\n\n\n\nRepo: [https://github.com/faizalbagwan786/mlops-production-platform](https://github.com/faizalbagwan786/mlops-production-platform)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pxpzeu/built_a_small_productionstyle_mlops_platform/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwcrjyb",
          "author": "raiffuvar",
          "text": "Not gonna put it in production. Split training and platform into different repos.\nGrab a real training repo, stick it here, and see what happens.",
          "score": 4,
          "created_utc": "2025-12-28 12:32:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcs8hh",
              "author": "Melodic_Struggle_95",
              "text": "Fair point. This repo isn‚Äôt meant to represent a final production setup, but a learning-focused MLOps platform where I can iterate end to end.\nIn a real production environment, I agree that training and serving would typically live in separate repos or at least separate deployment units, often owned by different teams.\nFor now, I kept them together to understand the full lifecycle and the interfaces between training, evaluation, and inference. My next step is to split training and serving and treat the trained model as an external artifact to the platform.\nAppreciate the feedback.",
              "score": 2,
              "created_utc": "2025-12-28 12:38:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh2vk2",
          "author": "BackgroundLow3793",
          "text": "Oh that's nice. Thank you. I'm learning MLOps recently too. I think next thing is MLFlow, understand why people use MLFlow. I mean it doesn't have to be MLFLow, but the core idea is tracking and model versioning I guess. \n\nThere is also a good article here: [https://docs.databricks.com/aws/en/machine-learning/mlops/mlops-workflow](https://docs.databricks.com/aws/en/machine-learning/mlops/mlops-workflow)",
          "score": 2,
          "created_utc": "2025-12-29 02:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjldtq",
              "author": "Melodic_Struggle_95",
              "text": "Thanks! Yeah, I‚Äôm on the same page. The main value is tracking and versioning, not MLflow itself. I‚Äôm planning to add that next, and the Databricks article looks solid. Appreciate you sharing it.",
              "score": 1,
              "created_utc": "2025-12-29 13:48:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgfa6",
          "author": "wallesis",
          "text": "Where's the \"platform\" part?",
          "score": 1,
          "created_utc": "2025-12-28 15:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdl5gc",
              "author": "Melodic_Struggle_95",
              "text": "Right now the ‚Äúplatform‚Äù part is still small by design. At this stage I‚Äôm focusing on building the core pieces first a clean training pipeline, an evaluation gate, a consistent model loading layer, and a serving API with clear contracts.The idea is to treat this as the foundation, and then gradually add real platform features like CI/CD, model registry, monitoring, and automated retraining. This repo shows the early platform core, not the final version.",
              "score": 1,
              "created_utc": "2025-12-28 15:40:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwozj3m",
          "author": "Significant-Fig-3933",
          "text": "How do you handle data lineage, code tracking, orchestration, and data/model monitoring?\nLike what exact data and what exact code (model design, features, etc) was the model trained with?\n\nAnd how do you coordinate/track retraining etc (ie orchestration)?",
          "score": 1,
          "created_utc": "2025-12-30 07:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoztqm",
              "author": "Melodic_Struggle_95",
              "text": "Right now the project tracks data and code through the training pipeline and Git commits, with retraining handled manually, and the next planned step is adding MLflow and orchestration to properly manage lineage, versioning, monitoring, and automated retraining.",
              "score": 2,
              "created_utc": "2025-12-30 07:27:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqwnyh",
          "author": "bad_detectiv3",
          "text": "Can you provide resource where you built up on MLOps material? I want to give this a try beyond using LLM in application use case",
          "score": 1,
          "created_utc": "2025-12-30 15:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx86yrp",
          "author": "Temporary_Term_5093",
          "text": "Cool project ! Did you use any Command Line AIs ?",
          "score": 1,
          "created_utc": "2026-01-02 09:23:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx87o7r",
              "author": "Melodic_Struggle_95",
              "text": "Thanks! No, I didn‚Äôt use any command-line AI tools. I wanted to build things step by step myself so I could really understand how training, evaluation, serving, and deployment work together in a real setup",
              "score": 2,
              "created_utc": "2026-01-02 09:30:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx88dlb",
                  "author": "Temporary_Term_5093",
                  "text": "Yeah that makes sense. I am also getting started with an end-to-end project myself and was looking for some AI tools to help move things faster. But i like your approach, the goal for doing these projects is to learn the basics and AI probably wouldn't help much with that",
                  "score": 1,
                  "created_utc": "2026-01-02 09:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q7dndy",
      "title": "Looking for Job Opportunities ‚Äî Senior MLOps / LLMOps Engineer (Remote / Visa Sponsorship)",
      "subreddit": "mlops",
      "url": "https://i.redd.it/zz5ulrin15cg1.jpeg",
      "author": "Asleep-Technician-21",
      "created_utc": "2026-01-08 14:47:00",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q7dndy/looking_for_job_opportunities_senior_mlops_llmops/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyk59cs",
          "author": "Big_Bird_Pecker",
          "text": "Hey, are you open to relocating? And what industries do you have xp in?",
          "score": 1,
          "created_utc": "2026-01-09 07:46:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyk5qge",
              "author": "Asleep-Technician-21",
              "text": "Yes, I am open to relocate. I have xp in Telecom, Enterprise SaaS/Data Analytics, and FinTech, with a strong focus on production ML/MLOps, real-time systems, and LLM platforms.",
              "score": 1,
              "created_utc": "2026-01-09 07:51:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q7ymli",
      "title": "Looking for Advice: Transitioning to MLOps After Career Break",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q7ymli/looking_for_advice_transitioning_to_mlops_after/",
      "author": "Dorito_77",
      "created_utc": "2026-01-09 04:30:08",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.79,
      "text": "I have experience in deep learning and computer vision (perception domain) but took a two-year break after moving countries. I‚Äôm struggling to get callbacks for similar roles, which now seem to require PhDs or master‚Äôs degrees from top programs.\n\nI‚Äôm considering transitioning toward MLOps since I have some prior exposure to it. I‚Äôve built an end-to-end personal project (full pipeline, deployment, documentation), but I‚Äôm not sure how to make it compelling to recruiters since it wasn‚Äôt in production. I‚Äôve also tried freelance platforms like Upwork without success.\n\nI‚Äôm open to internships, contract work, or temporary positions.. I just need to break this loop and start getting callbacks. For those who‚Äôve recently been placed in MLOps or adjacent roles (especially with non-traditional backgrounds or after a gap), what actually helped you get through the door?\n\nAny guidance would be appreciated. Thank you!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q7ymli/looking_for_advice_transitioning_to_mlops_after/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nyk3jhs",
          "author": "Extension_Key_5970",
          "text": "Have you visited my post https://www.reddit.com/r/mlops/s/WdepxXancv",
          "score": 1,
          "created_utc": "2026-01-09 07:31:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5yb36",
      "title": "New Tool for Finding Training Datasets",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q5yb36/new_tool_for_finding_training_datasets/",
      "author": "NarutoLLN",
      "created_utc": "2026-01-06 23:08:07",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I am an academic that partnered with a software engineer to productionize some of my ideas. I thought it might be of interest to the community here.\n\nLink to Project:¬†[https://huggingface.co/spaces/durinn/dowser](https://huggingface.co/spaces/durinn/dowser)\n\n\n\nHere is a link to a proof-of-concept on Huggingface trying to develop the idea further. It is effectively a reccomender system for open source datasets. It doesn't have a GPU runtime, so please be patient with it.\n\nLink to Abstract:¬†[https://openreview.net/forum?id=dNHKpZdrL1#discussion](https://openreview.net/forum?id=dNHKpZdrL1#discussion)\n\n\n\nThis is a link to the Open Review. It describes some of the issues in calculating influence including inverting a bordered hessian matrix.\n\nIf anyone has any advice or feedback, it would be great. I guess I was curious if people thought this approach might be a bit too hand wavy or if there were better ways to estimate influence.\n\nOther spiel:\n\nThe problem I am trying to solve is to how to prioritize training when you are data constrained. My impression is that when you either have small specialized models or these huge frontier models, they face a similar set of constraints. The current approach to support gains in performance seems to be a dragnet approach of the internet's data. I hardly think this sustainable and is too costly for incremential benefit.\n\nThe goal is to approximate influence on training data for specific concepts to determine how useful certain data is to include, prioritize the collection of new data, and support adversial training to create more robust models.\n\nThe general idea is that influence is too costly to calculate, so by looking at subspaces and obserserving some additional constrains/simplications, one can derive a signal to support the different goals(filtering data, priorization, adversial training). The technique is coined \"Data Dowsing\" since it isn't meant to be particularly precise but useful enough to inform guidance for resources.\n\nWe have been attempting to capture the differences in training procedures using perplexity.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q5yb36/new_tool_for_finding_training_datasets/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q4royy",
      "title": "MLOps for agents: tool-call observability + audit logs (MCP proxy w/ latency + token profiling + exports)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q4royy/mlops_for_agents_toolcall_observability_audit/",
      "author": "PutPurple844",
      "created_utc": "2026-01-05 17:18:12",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.79,
      "text": "https://preview.redd.it/x27efjntdkbg1.png?width=1922&format=png&auto=webp&s=d2366784f27f9277f82cfc9fa0a7010ac6ca47a0\n\n  \nAs agent systems go into production, tool calls become the control plane:\n\n* incident response (what happened?)\n* cost control (where did tokens go?)\n* performance (what‚Äôs slow?)\n* governance/audit (what did the agent attempt?)\n\nI built **Reticle** (screenshot attached): an MCP proxy + UI that captures JSON-RPC traffic, correlates calls, profiles latency + token usage, captures stderr, and records/export sessions.\n\nRepo: [https://github.com/LabTerminal/mcp-reticle](https://github.com/LabTerminal/mcp-reticle)\n\nWhat would you require to call this ‚Äúproduction-ready‚Äù? (OTel, redaction, sampling, trace IDs, policy engine, RBAC?)",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1q4royy/mlops_for_agents_toolcall_observability_audit/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q855uo",
      "title": "Triton inference server good practices",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q855uo/triton_inference_server_good_practices/",
      "author": "Cleverarcher23",
      "created_utc": "2026-01-09 10:48:20",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I am working on a SaaS and I need to deploy a Triton Ensemble pipeline with SAM3 + Lama inpainting that looks like this: \n\n    name: \"inpainting_ensemble\"\n    platform: \"ensemble\"\n    max_batch_size: 8\n    \n    # 1. INPUTS\n    input [\n      { name: \"IMAGE\", data_type: TYPE_UINT8, dims: [ -1, -1, 3 ] },\n      { name: \"PROMPT\", data_type: TYPE_STRING, dims: [ 1 ] },\n      { name: \"CONFIDENCE_THRESHOLD\", data_type: TYPE_FP32, dims: [ 1 ] },\n      { name: \"DILATATION_KERNEL\", data_type: TYPE_INT32, dims: [ 1 ] },\n      { name: \"DILATATION_ITERATIONS\", data_type: TYPE_INT32, dims: [ 1 ] },\n      { name: \"BLUR_LEVEL\", data_type: TYPE_INT32, dims: [ 1 ] }\n    ]\n    \n    # 2. Final OUTPUT\n    output [\n      {\n        name: \"FINAL_IMAGE\"\n        data_type: TYPE_STRING  # Utilis√© pour le transport BYTES\n        dims: [ 1 ]             # Un seul objet binaire (le fichier JPEG)\n      }\n    ]\n    \n    ensemble_scheduling {\n      step [\n        {\n          # STEP 1 : Segmentation & Post-Process (SAM3)\n          model_name: \"sam3_pytorch\"\n          model_version: -1\n          input_map { key: \"IMAGE\"; value: \"IMAGE\" }\n          input_map { key: \"PROMPT\"; value: \"PROMPT\" }\n          input_map { key: \"CONFIDENCE_THRESHOLD\"; value: \"CONFIDENCE_THRESHOLD\" }\n          input_map { key: \"DILATATION_KERNEL\"; value: \"DILATATION_KERNEL\" }\n          input_map { key: \"DILATATION_ITERATIONS\"; value: \"DILATATION_ITERATIONS\" }\n          input_map { key: \"BLUR_LEVEL\"; value: \"BLUR_LEVEL\" }\n          output_map { key: \"REFINED_MASK\"; value: \"intermediate_mask\" }\n        },\n        {\n          # STEP 2 : Inpainting (LaMa)\n          model_name: \"lama_pytorch\"\n          model_version: -1\n          input_map { key: \"IMAGE\"; value: \"IMAGE\" }\n          input_map { key: \"REFINED_MASK\"; value: \"intermediate_mask\" }\n          output_map { key: \"OUTPUT_IMAGE\"; value: \"FINAL_IMAGE\" }\n        }\n      ]\n    }\n\nThe matter is that the Client is a Laravel backend and the input images are stored in a s3 bucket. Should I add a preprocessing step (CPU\\_KIND) at Triton level that downloads from S3 then convert to UINT8 tensor (with PIL) OR I should let Laravel convert to tensor (ImageMagick) and send the tensors over the network directly to the Triton server ?  ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q855uo/triton_inference_server_good_practices/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nykq21d",
          "author": "dayeye2006",
          "text": "1 sounds right. \nPreprocessing can be CPU heavy. Dealing it on the web backend side doesn't sound right",
          "score": 1,
          "created_utc": "2026-01-09 10:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykr758",
              "author": "Cleverarcher23",
              "text": "Thanks you for your response. 1 sounds better to me too. Also my Laravel backend is on another hosting provider than my server Triton server (RTX 4070). Images as UINT8 tensors have a much bigger size than compressed PNG/JPG images. Moving these tensors accross WAN network might be a bad smell.",
              "score": 1,
              "created_utc": "2026-01-09 11:04:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyl52hn",
          "author": "CleanSpray9183",
          "text": "For image preprocessing you can use DALI (from Nvidia), which makes use of your GPU.",
          "score": 1,
          "created_utc": "2026-01-09 12:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz54emp",
          "author": "TalkingJellyFish",
          "text": "Downloading images isn't Tritons ideal use case and has cases that would be hard. E.g.  kind of a long running job (100s of ms -> seconds) and can be flaky due to network or disk.  If your making requests to the ensemble and those errors happen, its hard to debug and error prone , IMO .\n\n \nI think a slightly cleaner solution is to do the downloading outside of triton, e.g. you'd have some queue reading worker(s) on the same machine as triton, that would download the images and then make the inference call to triton. You can get quite fancy with the handoffs (going over shared memory), but probably best to start simple and evolve as the needs arise.",
          "score": 1,
          "created_utc": "2026-01-12 10:53:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q87ytk",
      "title": "A practical 2026 roadmap for modern AI search & RAG systems",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q87ytk/a_practical_2026_roadmap_for_modern_ai_search_rag/",
      "author": "ReverseBlade",
      "created_utc": "2026-01-09 13:15:56",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I kept seeing RAG tutorials that stop at ‚Äúvector DB + prompt‚Äù and break down in real systems.\n\nI put together a roadmap that reflects how modern AI search actually works:\n\n‚Äì semantic + hybrid retrieval (sparse + dense)  \n‚Äì explicit reranking layers  \n‚Äì query understanding & intent  \n‚Äì agentic RAG (query decomposition, multi-hop)  \n‚Äì data freshness & lifecycle  \n‚Äì grounding / hallucination control  \n‚Äì evaluation beyond ‚Äúdoes it sound right‚Äù  \n‚Äì production concerns: latency, cost, access control\n\nThe focus is system design, not frameworks. Language-agnostic by default (Python just as a reference when needed).\n\nRoadmap image + interactive version here:  \n[https://nemorize.com/roadmaps/2026-modern-ai-search-rag-roadmap](https://nemorize.com/roadmaps/2026-modern-ai-search-rag-roadmap)\n\nCurious what people here think is still missing or overkill.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q87ytk/a_practical_2026_roadmap_for_modern_ai_search_rag/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nyleg6f",
          "author": "latent_signalcraft",
          "text": "this is one of the more grounded RAG roadmaps ive seen especially the emphasis on retrieval quality reranking and evaluation as first class concerns. where i still see teams struggle is less in adding agentic layers and more in operational discipline around data ownership change management and eval drift as sources evolve. data freshness and lifecycle are listed but in practice that is where most production systems quietly degrade. id be curious how you think about organizational readiness alongside the technical layers because even well designed RAG stacks tend to stall without clear ownership and feedback loops.",
          "score": 2,
          "created_utc": "2026-01-09 13:42:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqz8pa",
          "author": "ketankhairnar",
          "text": "I've created some stuff for my own understanding. Hope you find it useful\n\n  \n[https://www.ketankhairnar.com/deep-dives/ai-engineering/](https://www.ketankhairnar.com/deep-dives/ai-engineering/)\n\n[https://www.ketankhairnar.com/deep-dives/production-agents/](https://www.ketankhairnar.com/deep-dives/production-agents/)",
          "score": 2,
          "created_utc": "2026-01-10 07:14:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8cmpn",
      "title": "TabPFN deployment via AWS SageMaker Marketplace",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q8cmpn/tabpfn_deployment_via_aws_sagemaker_marketplace/",
      "author": "Diligent_Inside6746",
      "created_utc": "2026-01-09 16:20:07",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "TabPFN-2.5 is now on SageMaker Marketplace to address the infrastructure constraints teams kept hitting: compliance requirements preventing external API calls, GPU setup overhead, and inference endpoint management.\n\nContext: TabPFN is a pretrained transformer trained on more than hundred million synthetic datasets to perform in-context learning and output a predictive distribution for the test data. It natively supports missing values, categorical features, text and numerical features, is robust to outliers and uninformative features. Published in Nature earlier this year, currently #1 on TabArena: [https://huggingface.co/TabArena](https://huggingface.co/TabArena)\n\nThe deployment model is straightforward - subscribe through marketplace and AWS handles provisioning. All inference stays in your VPC.\n\nHandles up to 50k rows, 2k features. On benchmarks in this range it matches AutoGluon tuned for 4 hours.\n\nMarketplace: [https://aws.amazon.com/marketplace/pp/prodview-chfhncrdzlb3s](https://aws.amazon.com/marketplace/pp/prodview-chfhncrdzlb3s)  \n  \nDeployment guide: [https://docs.priorlabs.ai/integrations/sagemaker](https://docs.priorlabs.ai/integrations/sagemaker)\n\nWe welcome feedback and thoughts!",
      "is_original_content": false,
      "link_flair_text": "Tools: paid üí∏",
      "permalink": "https://reddit.com/r/mlops/comments/1q8cmpn/tabpfn_deployment_via_aws_sagemaker_marketplace/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q5uj2p",
      "title": "InfiniBand and High-Performance Clusters",
      "subreddit": "mlops",
      "url": "https://martynassubonis.substack.com/p/infiniband-and-high-performance-clusters",
      "author": "Martynoas",
      "created_utc": "2026-01-06 20:47:30",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1q5uj2p/infiniband_and_highperformance_clusters/",
      "domain": "martynassubonis.substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pzfdgi",
      "title": "need guidance regarding mlops",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pzfdgi/need_guidance_regarding_mlops/",
      "author": "not_popular_to_know",
      "created_utc": "2025-12-30 11:05:17",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "Hello everyone,  \nI‚Äôm an engineering student with a physics background. For a long time, I wasn‚Äôt sure about my future plans, but recently I‚Äôve started feeling that machine learning is a great field for me. I find it fascinating because of the strong mathematics involved and its wide applications, even in physics.\n\nNow, I want to build a career in MLOps. So far, I‚Äôve studied machine learning and DSA and have built a few basic projects. I have a decent grasp of ML fundamentals and I‚Äôm currently learning more about AI algorithms.\n\nIf there‚Äôs anyone who can guide me on how to approach advanced concepts and build more valuable, real-world projects, I‚Äôd really appreciate your help.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1pzfdgi/need_guidance_regarding_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwqvfdn",
          "author": "Valeria_Xenakis",
          "text": "This is an answer I gave earlier for a different post. There maybe some irrelevant sections for you so please weed out the required points.  \n\n\nTo be honest, there are effectively zero MLOps jobs for freshers in the industry right now. It is a relatively new field and it is almost entirely dominated by experienced DevOps engineers or data engineers who have pivoted.\n\nAs a fresher, breaking in as a backend/software engineer is a much more realistic path. You should focus on building full stack projects to get your foot in the door first.\n\nOnce you are hired, learn MLflow since it is the industry standard, or just master whatever tools your company is specifically using. The best way to get into MLOps is to slowly take on those responsibilities by showing initiative within a backend role.\n\nAs for cloud providers, there is no point in trying to learn all three. Most companies use AWS, so just focus on that. You can look at the AWS MLOps certification later if you want less friction while working in the field, but get the foundational experience first.\n\nYou need to be a Devops engineer first and then only you can be an Mlops engineer. And Devops is not a fresher's task, usually you do those tasks after about 1.5 years of experience in the same project.",
          "score": 3,
          "created_utc": "2025-12-30 15:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr10h9",
              "author": "not_popular_to_know",
              "text": "\nSo can u tell me where to start and roadmap\nAs I have given very much time learning maths and all\nCan u tell me if I can build a carrier in ai/ml\nBtw thanks for your advice",
              "score": 1,
              "created_utc": "2025-12-30 16:10:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwr8q0q",
                  "author": "Valeria_Xenakis",
                  "text": "Hey, I don't want to give bad advice so I will keep it simple. That way you will still do more research from your side and not take whatever I say at face value.\n\nIf you are a student who has no industry experience, please don't constraint yourself to MLOps. IMO MLOps is a skill and DevOps is the job. Now you said you liked the mathematics part, sorry to say if that is what interests you, you would find DevOps really boring.\n\nThe kind of roles you are looking for are ML engineer, AI engineer etc. In  these roles you would be using models created by people in research segment to solve problems for your organization. These roles require maths and ml/ai but not in the foundational sense. They require your applied  maths and ai chops to use those techniques for the company and not come up with new models or techniques as such. These roles will 100 percent require MLOps knowledge but you would be expected to handle these only after you have 2-3 years of exp.\n\nAs a green horn right of the block, focus on full stack dev and create projects related to full stack dev of AI based projects. Basically aim for a backend/full stack engineer in any company that uses a lot of AI for their core product offering (Not the one which use AI as tools, the ones that use AI as their main product offering like Amazon, AirBnB, Walmart etc).\n\nAs you gain exp you will 100 percent need to take on MLOps responsibilities, but right now you are not cut out for that.\n\nPlease keep asking your doubts. I like to keep individual replies short, they are more engaging.",
                  "score": 3,
                  "created_utc": "2025-12-30 16:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q4kmqx",
      "title": "Breaking into international remote MLOps roles from Peru - early career advice",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q4kmqx/breaking_into_international_remote_mlops_roles/",
      "author": "Osoyoguiz",
      "created_utc": "2026-01-05 12:39:10",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "Hi everyone, I am looking for advice from professionals working in MLOps roles for international companies.\n\nI currently work as a pre professional intern at a well-known bank in Peru, where I am involved in data science and machine learning workflows. I have around one year of experience, I am based in Peru, and I have an intermediate English level (B2).\n\nI am interested in transitioning toward MLOps oriented roles and eventually working remotely for foreign companies. From your experience, how feasible is this path at an early career stage? What skills, tools, or types of experience should I prioritize to be competitive?\n\nAny guidance or shared experience would be greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q4kmqx/breaking_into_international_remote_mlops_roles/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "ny0c1aa",
          "author": "rishiarora",
          "text": "CFBR",
          "score": 1,
          "created_utc": "2026-01-06 14:08:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q670z0",
      "title": "‚ÄúThe AI works. Everything around it is broken.‚Äù",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q670z0/the_ai_works_everything_around_it_is_broken/",
      "author": "Embarrassed-Radio319",
      "created_utc": "2026-01-07 05:34:19",
      "score": 4,
      "num_comments": 6,
      "upvote_ratio": 0.71,
      "text": "If you‚Äôre building AI agents, you know the hard part isn‚Äôt the model ‚Äî it‚Äôs integrations, infra, security, and keeping things running in prod.\n\nI‚Äôm building **Phinite**, a **low-code platform** to ship AI agents to production (orchestration, integrations, monitoring, security handled).\n\nWe‚Äôre opening a small **beta** and looking for **automation engineers / agent builders** to build real agents and give honest feedback.\n\nIf that‚Äôs you ‚Üí [https://app.youform.com/forms/6nwdpm0y](https://app.youform.com/forms/6nwdpm0y)  \nWhat‚Äôs been the biggest blocker shipping agents for you?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q670z0/the_ai_works_everything_around_it_is_broken/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nyb1d37",
          "author": "LordWitness",
          "text": "> If you're building Al agents, you know the hard part isn't the model - it's integrations, infra, security, and keeping things running in prod\n\nNo, that's the kind of thing an inexperienced developer would struggle with. Integrations, infrastructure, securit.. all of that is part of an experienced developer's daily routine. \n\nThe hardest part is the model itself. Imagine building a system, it works, and then it breaks down two weeks later, without any changes to the code or anything? It's every developer's nightmare.",
          "score": 4,
          "created_utc": "2026-01-08 00:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyb2iry",
              "author": "Embarrassed-Radio319",
              "text": "The model *is* hard, no argument there.  \nBut in practice, most projects stall not because the model can‚Äôt reason  but because keeping it **reliable, secure, observable, and adaptable in production** becomes a long-tail maintenance problem.\n\nPhinite isn‚Äôt trying to replace experienced engineers it‚Äôs about **standardizing the boring-but-critical glue** so teams can spend more time improving models and behavior instead of rebuilding the same scaffolding over and over.\n\nGenuinely appreciate the pushback  this kind of discussion is exactly why we‚Äôre talking to practitioners early.          \n\n  \nI would love to show you the platform we built and would users to give us a honest feedback.",
              "score": 2,
              "created_utc": "2026-01-08 00:35:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny9gp1v",
          "author": "pvatokahu",
          "text": "Security and compliance killed our first agent deployment at a Fortune 500 client last year. We had this beautiful RAG system that could pull insights from their internal docs, worked great in dev... then their security team saw it making API calls to 12 different systems and shut us down. Took 3 months just to get approval for read-only access to half of them.\n\nThe monitoring piece is what keeps me up at night though. When an agent hallucinates in prod and nobody catches it for 48 hours, that's when you get the angry phone calls. We're using a mix of LangSmith and some custom logging but it still feels like flying blind sometimes. Would love to see how you're thinking about observability in Phinite - that's where most platforms fall short imo.",
          "score": 3,
          "created_utc": "2026-01-07 20:08:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyazeuj",
              "author": "LordWitness",
              "text": "> then their security team saw it making API calls to 12 different systems and shut us down. \n\nWhat did you mean by that? Was your application making requests to unknown APIs?",
              "score": 2,
              "created_utc": "2026-01-08 00:19:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nydnfik",
          "author": "latent_signalcraft",
          "text": "the framing resonates but i usually see the blocker show up a bit earlier than infra. teams jump from a promising demo straight into orchestration without locking down ownership evaluation criteria, or failure modes. once that is fuzzy monitoring and security feel impossible because nobody agrees what working actually means. in practice the agents that make it to production tend to be the ones where interfaces permissions and success signals are boringly explicit before any automation magic happens.",
          "score": 2,
          "created_utc": "2026-01-08 11:12:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcl7f4",
      "title": "Verticalizing my career/Seeking to become an MLOps specialist.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qcl7f4/verticalizing_my_careerseeking_to_become_an_mlops/",
      "author": "an4k1nskyw4lk3r",
      "created_utc": "2026-01-14 11:45:43",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm looking to re-enter the job market. I'm a Machine Learning Engineer and I lost my last job due to a layoff. This time, I'm aiming for a position that offers more exposure to MLOps than experimentation with models. Something platform-level. Any tips on how to attract this type of job? Any certifications for MLOps?",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qcl7f4/verticalizing_my_careerseeking_to_become_an_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q6giv1",
      "title": "Why 4 GPUs training can be slower than 1 on budget clouds",
      "subreddit": "mlops",
      "url": "https://cortwave.github.io/posts/multi-gpu/",
      "author": "randmusr66",
      "created_utc": "2026-01-07 14:18:09",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.67,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q6giv1/why_4_gpus_training_can_be_slower_than_1_on/",
      "domain": "cortwave.github.io",
      "is_self": false,
      "comments": [
        {
          "id": "ny7br6f",
          "author": "randmusr66",
          "text": "Link to repo with raw data and code  \n[https://github.com/cortwave/distributed-training](https://github.com/cortwave/distributed-training)",
          "score": 1,
          "created_utc": "2026-01-07 14:19:16",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzupvx",
      "title": "Built spot instance orchestration for batch ML jobs‚Äîfeedback wanted",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pzupvx/built_spot_instance_orchestration_for_batch_ml/",
      "author": "HelpingForDoughnuts",
      "created_utc": "2025-12-30 21:44:57",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "Got tired of building the same spot instance handling code at work, so I made it a product. Submit a job, it runs on Azure spot VMs, handles preemption/retry automatically, scales down when idle.\nThe pitch is simplicity‚Äîmulti-GPU jobs without configuring distributed training yourself, no infrastructure knowledge needed. Upload your container, pick how many GPUs, click run, get results back.\nEarly beta. Looking for people who‚Äôve built this stuff themselves and can tell me what I‚Äôm missing. Free compute credits for useful feedback.\nRoast my architecture if you want, I can take it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pzupvx/built_spot_instance_orchestration_for_batch_ml/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwv2jdk",
          "author": "qwertying23",
          "text": "Have you tried ray ? It does this quite well",
          "score": 3,
          "created_utc": "2025-12-31 04:45:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy0692",
              "author": "HelpingForDoughnuts",
              "text": "Yeah, Ray is solid for distributed ML workloads, and Anyscale makes it more accessible.\n\nThe main difference is that Ray still requires learning the Ray framework - you‚Äôre writing Ray-specific code with decorators, clusters, etc. We‚Äôre targeting the layer above that: ‚ÄúI want to train a PPO agent to play Breakout‚Äù ‚Üí it just works, without learning new APIs.\n\nRay is great if you want that level of control and don‚Äôt mind the learning curve. We‚Äôre going after people who just want their training job to run without becoming Ray experts first.\n\nDifferent markets really - Ray for ML engineers, us for researchers/beginners who want to skip the infrastructure parts entirely.\n\nHave you used Anyscale? Curious how you found the setup experience.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2025-12-31 17:19:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwy4459",
          "author": "qwertying23",
          "text": "Yes I have used anyscale and their workspaces concept is pretty neat. I do agree with your points but in my experience we faced the same issue and got stuck in the future we had to redesign our entire stack again. The beauty of ray is the same code can run on your laptop a single gpu cluster or on 1000‚Äôs of gpu I would rather build this with ray from the get go rather than the way we did this which made distributed computing an after thought. But happy to chat and give feedback on your product.",
          "score": 2,
          "created_utc": "2025-12-31 17:38:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy4yff",
              "author": "HelpingForDoughnuts",
              "text": "That‚Äôs a really thoughtful point about distributed-first architecture. Your experience with having to redesign the entire stack later is exactly the kind of lesson that‚Äôs expensive to learn the hard way.\n\nYou‚Äôre absolutely right that Ray‚Äôs abstraction is powerful - write once, run anywhere from laptop to 1000 GPUs. And if we‚Äôre building orchestration that needs to scale, starting with Ray as the foundation makes way more sense than bolting on distributed later.\n\nThe differentiation would be more in the layer above Ray - instead of users learning Ray APIs and cluster management, they get the natural language interface that routes to Ray workloads under the hood. But you‚Äôre right that the underlying execution should be distributed-native from day one.\n\nI‚Äôd genuinely love to chat more about this. Your experience with both the technical implementation and the business realities is exactly what we need to hear. Happy to jump on a call if you‚Äôre interested - would love to get your perspective on where the real pain points are and how a Ray-based approach might solve them better.\n\nThanks for offering feedback - that kind of input from someone who‚Äôs actually built and scaled these systems is invaluable.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2025-12-31 17:43:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qatfet",
      "title": "kubesdk v0.3.0 ‚Äî Generate Kubernetes CRDs programmatically from Python dataclasses",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qatfet/kubesdk_v030_generate_kubernetes_crds/",
      "author": "steplokapet",
      "created_utc": "2026-01-12 12:19:49",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[Puzl Team](https://puzl.cloud) here. We are excited to announce kubesdk v0.3.0. This release introduces automatic generation of Kubernetes Custom Resource Definitions (CRDs) directly from Python dataclasses.\n\n**Key Highlights of the release:**\n\n* **Full IDE support:** Since schemas are standard Python classes, you get native autocomplete and type checking for your custom resources.\n* **Resilience:** Operators work in production safer, because all models handle unknown fields gracefully, preventing crashes when Kubernetes API returns unexpected fields.\n* **Automatic generation of CRDs** directly from Python dataclasses.\n\n**Target Audience**\n\nWrite and maintain Kubernetes operators easier. This tool is for those who need their operators to work in production safer and want to handle Kubernetes API fields more effectively.\n\n**Comparison**\n\nYour Python code is your resource schema: generate CRDs programmatically without writing raw YAMLs. See the usage example.\n\n**Full Changelog:** [https://github.com/puzl-cloud/kubesdk/releases/tag/v0.3.0](https://github.com/puzl-cloud/kubesdk/releases/tag/v0.3.0)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qatfet/kubesdk_v030_generate_kubernetes_crds/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q6fefx",
      "title": "I built an open-source library that diagnoses problems in your Scikit-learn models using LLMs",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q6fefx/i_built_an_opensource_library_that_diagnoses/",
      "author": "lc19-",
      "created_utc": "2026-01-07 13:30:38",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.67,
      "text": "Hey everyone, Happy New Year!\n\n\n\nI spent the holidays working on a project I'd love to share: **sklearn-diagnose** ‚Äî an open-source Scikit-learn compatible Python library that acts like an \"MRI scanner\" for your ML models.\n\n\n\n**What it does:**\n\nIt uses LLM-powered agents to analyze your trained Scikit-learn models and automatically detect common failure modes:\n\n\\- Overfitting / Underfitting\n\n\\- High variance (unstable predictions across data splits)\n\n\\- Class imbalance issues\n\n\\- Feature redundancy\n\n\\- Label noise\n\n\\- Data leakage symptoms\n\n\n\nEach diagnosis comes with confidence scores, severity ratings, and actionable recommendations. \n\n\n\n**How it works:**\n\n1. Signal extraction (deterministic metrics from your model/data)\n\n2. Hypothesis generation (LLM detects failure modes)\n\n3. Recommendation generation (LLM suggests fixes)\n\n4. Summary generation (human-readable report)\n\n\n\n**Links:**\n\n\\- GitHub: [https://github.com/leockl/sklearn-diagnose](https://github.com/leockl/sklearn-diagnose)\n\n\\- PyPI: pip install sklearn-diagnose\n\n\n\nBuilt with LangChain 1.x. Supports OpenAI, Anthropic, and OpenRouter as LLM backends.\n\n\n\nAiming for this library to be community-driven with ML/AI/Data Science communities to contribute and help shape the direction of this library as there are a lot more that can be built - for eg. AI-driven metric selection (ROC-AUC, F1-score etc.), AI-assisted feature engineering, Scikit-learn error message translator using AI and many more!\n\n\n\nPlease give my GitHub repo a star if this was helpful ‚≠ê",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1q6fefx/i_built_an_opensource_library_that_diagnoses/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    }
  ]
}