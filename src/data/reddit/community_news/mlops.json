{
  "metadata": {
    "last_updated": "2026-01-20 02:29:03",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 9,
    "total_comments": 32,
    "file_size_bytes": 48812
  },
  "items": [
    {
      "id": "1qhbtc9",
      "title": "MLOps vs MLE System Design Prep Dilemma for EM -> Which to Focus?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "author": "Low-Breakfast2018",
      "created_utc": "2026-01-19 18:11:06",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hi ML Leaders,  \n  \nI'm prepping for MLOps EM roles at FAANG/big tech + backups at legacy cos. But interviews seem split:  \n  \n1) SOP-hiring: Google & Meta, even \"MLOps\" JDs hit you with MLE-style system designs (classification/recommendation etc)  \n2) Team-oriented-hiring companies: Amazon/Uber/MSFT/Big Tech, more pure MLOps system design (feature stores, serving, monitoring, CI/CD).  \n3) Legacy (smaller/enterprise): Mostly general ML lead/director roles leaning MLE-heavy, few pure MLOps spots.  \n  \nDon't want to spread prep thin on two \"different\" system designs. How should I do to make sure to focus since the competition is high. Or any strategy or recommendation on double down on MLOps? How'd you balance? Seeking for experienced folks input.  \n  \nYOE: 13+ (non-FAANG)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0ir6fp",
          "author": "Comprehensive_Gap_88",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-01-19 18:27:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhfaj",
          "author": "dank_coder",
          "text": "!remind me in 1 day",
          "score": 1,
          "created_utc": "2026-01-19 20:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jhlgg",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-20 20:26:55 UTC**](http://www.wolframalpha.com/input/?i=2026-01-20%2020:26:55%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/o0jhfaj/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qhbtc9%2Fmlops_vs_mle_system_design_prep_dilemma_for_em%2Fo0jhfaj%2F%5D%0A%0ARemindMe%21%202026-01-20%2020%3A26%3A55%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qhbtc9)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-19 20:27:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kughz",
          "author": "Key_Base8254",
          "text": "following",
          "score": 1,
          "created_utc": "2026-01-20 00:35:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qebb90",
      "title": "hosted open source neptune.ai alternative?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qebb90/hosted_open_source_neptuneai_alternative/",
      "author": "Says_Watt",
      "created_utc": "2026-01-16 09:14:59",
      "score": 8,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I would gladly pay for a hosted open source [neptune.ai](http://neptune.ai) alternative that's a drop in replacement for wandb / neptune experiment tracking. The OpenAI acquisition + shutdown of [neptune.ai](http://neptune.ai) is stupid. We as a community need a proper drop in replacement for the purposes of experiment tracking that has a performant UI. I just want to visualize my loss curve without paying w&b unacceptable pricing ($1 per gpu hour is absurd).\n\nThere's no way doing this is that hard. I would do it myself but am working on a different project right now.\n\nAlso aim is an open source alternative but it's not a drop in replacement and it's not hosted. I want to easily switch from wandb and neptune without losing quality UI, without hosting it myself, and without having to do a bunch of gymnastics to fit someone else's design patterns. It needs to be MIT license so that if you decide to sell out someone else can pick up where you left off. Please for the love of god can someone please create a mobile app so I can view my runs while on the go?\n\nedit: also there's [minfx.ai](http://minfx.ai) but their ui is terrible, why is it so hard to just clone wandb / neptune, the spec is there, someone please vibe code it lol",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qebb90/hosted_open_source_neptuneai_alternative/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzw9b7n",
          "author": "MarcelLecture",
          "text": "ML engineers and MLOps has been going with MLflow for at least 5 years.\nIts UI my not be the best and it tries to be a model registry IMHO badly (kitops ftw)\nBuuuut, it is reliable, open-source, has a great community, has managed solution, easy af to deploy and can integrate easily:\n- in your code\n- on your infra\n\nYou are not obliged to use its model registry btw\n\nI never had any big issues with 3 years of using it in production in multiple companies.",
          "score": 3,
          "created_utc": "2026-01-16 10:11:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwa2lh",
              "author": "Says_Watt",
              "text": "I think aim integrate with mlflow so I'll give it a shot, but it seems it's not as easy to use as wandb. So a UI that mimics wandb / [neptune.ai](http://neptune.ai) that uses mlflow and kitops would be great. Hosted of course.",
              "score": 1,
              "created_utc": "2026-01-16 10:18:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzw6y57",
          "author": "extreme4all",
          "text": "Mlflow is OSS mlops tracking, there is probably some managed mlflow services around",
          "score": 2,
          "created_utc": "2026-01-16 09:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw7cvu",
              "author": "Says_Watt",
              "text": "It's doing too much, can I use it as easily as wandb to track my experiments and visualize my training? If not then I'm not interested. I don't need the rest of the bloat. I want a sleak UI that I can visualize and track my experiment and a mobile app that has feature parity with wandb / [neptune.ai](http://neptune.ai) in terms of tracking the experiment. It needs to be open source because [neptune.ai](http://neptune.ai) has proven people are incapable of being reliable and wandb has proven that they're unwilling to solve the problem without gouging their customers.",
              "score": 1,
              "created_utc": "2026-01-16 09:53:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzw8dto",
                  "author": "d_lowl",
                  "text": "Yep, that's the intended usage of MLflow. You record your runs. You record your parameters and metrics (it does support metrics at steps, so you can plot your loss). You don't have to use other features if you don't want to (it's not really that bloated to be fair, it's mostly just an experiment tracker + model registry). It works locally, can be easily deployed too.\n\n\\>mobile app\n\nThat it doesn't have. But you can open it in your browser still.",
                  "score": 1,
                  "created_utc": "2026-01-16 10:03:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzw5c6l",
          "author": "mutlu_simsek",
          "text": "Check perpetual ml. It is not hosted but very cost effective.",
          "score": 1,
          "created_utc": "2026-01-16 09:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw6bts",
              "author": "Says_Watt",
              "text": "I don't understand why every company needs to be \"batteries included\" all in one solution. I just want to track my experiments. I'll deploy it myself by running a few terminal commands and pushing a container to some repository. It's not that difficult.",
              "score": 2,
              "created_utc": "2026-01-16 09:44:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwiw14",
          "author": "latent_signalcraft",
          "text": "its true that many experiment tracking tools dont offer the seamless integration and UI performance youre looking for especially when it comes to being hosted and open source. but beyond just swapping tools its crucial to consider how the underlying infrastructure and data flow will handle scaling with AI workloads. a well integrated experiment tracking system is only as effective as the data and governance practices supporting it. having solid data pipelines and robust MLOps practices in place can significantly improve the user experience especially as experimentation complexity grows. if you're looking for a drop in solution focusing on tools that also prioritize data quality and traceability might be key to a more stable and reliable solution long term.",
          "score": 1,
          "created_utc": "2026-01-16 11:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwjmzf",
              "author": "Says_Watt",
              "text": "data quality? I'm pretty new to mlops in general. I just manage my data myself, store in s3 and train. So in my case I just want a nice UI to visualize the training part.",
              "score": 1,
              "created_utc": "2026-01-16 11:39:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwsqjs",
          "author": "clever_entrepreneur",
          "text": "Why they don't share a docker compose file?",
          "score": 1,
          "created_utc": "2026-01-16 12:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxi8a9",
              "author": "Says_Watt",
              "text": "what?",
              "score": 1,
              "created_utc": "2026-01-16 15:01:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwxdzz",
          "author": "alderteeter",
          "text": "Maybe this will work for you. Seems more appropriate for an individual user than production service, but maybe I‚Äôm misreading it. \n\nhttps://huggingface.co/docs/trackio/en/index",
          "score": 1,
          "created_utc": "2026-01-16 13:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07yg1c",
          "author": "burntoutdev8291",
          "text": "I recently had this issue, moving to mlflow from wandb. The paid services are really so much better, but mlflow has been here for very long.",
          "score": 1,
          "created_utc": "2026-01-18 02:26:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcl7f4",
      "title": "Verticalizing my career/Seeking to become an MLOps specialist.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qcl7f4/verticalizing_my_careerseeking_to_become_an_mlops/",
      "author": "an4k1nskyw4lk3r",
      "created_utc": "2026-01-14 11:45:43",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "I'm looking to re-enter the job market. I'm a Machine Learning Engineer and I lost my last job due to a layoff. This time, I'm aiming for a position that offers more exposure to MLOps than experimentation with models. Something platform-level. Any tips on how to attract this type of job? Any certifications for MLOps?",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qcl7f4/verticalizing_my_careerseeking_to_become_an_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzkofmh",
          "author": "d1ddydoit",
          "text": "Becoming confident writing infra as code and architecting secure cloud solutions for ML workloads and development is something you‚Äôre probably best off learning away from an MLOps pathway - ML engineering and MLOps already covers too many disciplines to cover well under a single route.\n\nI would take a look at something like a (insert cloud provider) solutions architect and devops pathway courses/exams before focusing further on ML system patterns and services  (e.g feature stores, model registries, experiment tracking, model monitoring, serving managed development environments).\n\nBecoming confident in using and administering managed cloud platform services like SageMaker, Vertex, Snowflake etc also will boost your chances as lots of companies use these off the shelf rather than maintain their own custom in-house platforms (like Uber for example).",
          "score": 3,
          "created_utc": "2026-01-14 17:32:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmeh8y",
          "author": "denim_duck",
          "text": "MLOps is as saturated as ML. There aren‚Äôt more opportunities. You should pivot harder- consider robotics or something",
          "score": 1,
          "created_utc": "2026-01-14 22:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp1qs7",
              "author": "EviliestBuckle",
              "text": "Need further clarification on this",
              "score": 1,
              "created_utc": "2026-01-15 08:21:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp1sic",
                  "author": "EviliestBuckle",
                  "text": "Why does everyone feels their field is saturated",
                  "score": 1,
                  "created_utc": "2026-01-15 08:21:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzwnxlt",
              "author": "an4k1nskyw4lk3r",
              "text": "It‚Äôs a good point from eviliest‚Ä¶ can you explain?",
              "score": 1,
              "created_utc": "2026-01-16 12:11:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdtely",
      "title": "Does anyone else feel like Slurm error logs are not very helpful?\"",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qdtely/does_anyone_else_feel_like_slurm_error_logs_are/",
      "author": "Valeria_Xenakis",
      "created_utc": "2026-01-15 19:35:49",
      "score": 6,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "I manage a small cluster (64 GPUs) for my lab, and I swear 40% of my week is just figuring out why a job is `Pending` or why NCCL timed out.  \n  \nYesterday, a job sat in queue for 6 hours. Slurm said `Priority`, but it turned out to be a specific partition constraint hidden in the config that wasn't documented.  \n  \nIs it just our setup, or is debugging distributed training a nightmare for everyone? What tools are you guys using to actually see *why* a node is failing? `scontrol show job` gives me nothing.\n\n>",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qdtely/does_anyone_else_feel_like_slurm_error_logs_are/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzsst5b",
          "author": "cipioxx",
          "text": "Its very frustrating.",
          "score": 2,
          "created_utc": "2026-01-15 21:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzsujeu",
              "author": "Valeria_Xenakis",
              "text": "I feel like I spend more time than necessery just grepping logs on random nodes.\n\nI really want to know if there are better ways that are industry standard to track down the root cause and would appreciate any guidance. \n\nOr are you guys stuck doing it manually too?",
              "score": 1,
              "created_utc": "2026-01-15 21:12:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzsux4x",
                  "author": "cipioxx",
                  "text": "Manually and guessing.  I have started using llms to get ideas about some issues that pop up.  14 prolog errors now.  I drained the machines last week for maintenance.  I dont know whats going on",
                  "score": 2,
                  "created_utc": "2026-01-15 21:14:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxcbyk",
          "author": "ConcertTechnical25",
          "text": "Slurm error logs are essentially an Information Black Hole. When a job is stuck on Pending, the \"Priority\" label is often a mask for a hard partition constraint or a hardware mismatch. Distributed training requires more than just job status; it requires real-time monitoring of NCCL state and hardware metrics. If you‚Äôre manually grepping slurmd logs on random nodes, you‚Äôre playing a game of whack-a-mole that Slurm was never designed to help you win.",
          "score": 2,
          "created_utc": "2026-01-16 14:32:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzy19ku",
              "author": "Valeria_Xenakis",
              "text": "Yes, i agree and this is pretty annoying. I was wondering if this is how people go about fixing issues or if there is any better way.",
              "score": 1,
              "created_utc": "2026-01-16 16:26:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o021zl1",
                  "author": "burntoutdev8291",
                  "text": "The reply felt very AI",
                  "score": 1,
                  "created_utc": "2026-01-17 05:12:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt3lxt",
          "author": "cipioxx",
          "text": "Hmmm.  Ok.  I need to build a machine to test this on.  Thank you",
          "score": 1,
          "created_utc": "2026-01-15 21:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqfmy",
              "author": "cipioxx",
              "text": "Thank you my friend",
              "score": 1,
              "created_utc": "2026-01-15 23:50:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02632x",
          "author": "rishiarora",
          "text": "Nice cluster.",
          "score": 1,
          "created_utc": "2026-01-17 05:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hlybj",
          "author": "traceml-ai",
          "text": " I have  been thinking a lot about this class of problem.\n\nI am currently working on an open-source approach to make debugging distributed PyTorch jobs easier: starting with single-GPU today, and gradually moving toward multi-node setups.\n\nThe idea is to surface what‚Äôs actually happening during training (step timing, dataloader stalls, GPU memory pressure, per-rank behavior) so you don‚Äôt have to guess from logs. \n\nIf you would  be open to it, I would love to DM and learn a bit more about your workflow and the kinds of failures you see. I am  just trying to build something that works for real clusters like yours.",
          "score": 1,
          "created_utc": "2026-01-19 15:22:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfy95h",
      "title": "I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "mlops",
      "url": "https://i.redd.it/h7duxwz871eg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-18 03:59:55",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qfy95h/i_built_a_tool_that_forces_5_ais_to_debate_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0alini",
          "author": "functionalfunctional",
          "text": "So your solution to llms being bad at facts is to spend 5x as much $ and power to get a consensus on bad facts?  Maybe should have asked 6 if that was a good product idea.",
          "score": 3,
          "created_utc": "2026-01-18 14:23:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0apsu4",
              "author": "S_Anv",
              "text": "You can enable/disable any model. the minimum is 2 models.¬†",
              "score": 0,
              "created_utc": "2026-01-18 14:46:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qbte3c",
      "title": "Seeking a lightweight orchestrator for Docker Compose (Migration path to k3s)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qbte3c/seeking_a_lightweight_orchestrator_for_docker/",
      "author": "m_gijon",
      "created_utc": "2026-01-13 14:54:14",
      "score": 6,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI‚Äôm currently building an MVP for a platform using **Docker Compose**. The goal is to keep the infrastructure footprint minimal for now, with a planned migration to **k3s** once we scale.\n\nI need to schedule several ETL processes. While I‚Äôm familiar with **Airflow** and **Kestra**, they feel like overkill for our current **resource constraints** and would introduce unnecessary operational overhead **at this stage**.\n\n**What I've looked at so far:**\n\n* **Ofelia:** I love the footprint, but I have concerns regarding robust log management and audit trails for failed jobs.\n* **Supervisord:** Good for process management, but lacks the sophisticated scheduling and observability I'd prefer for ETL.\n\n**My Requirements:**\n\n1. **Low Overhead:** Needs to run comfortably alongside my services in a single-node Compose setup.\n2. **Observability:** Needs a reliable way to capture and review execution logs (essential for debugging ETL failures).\n3. **Path to k3s:** Ideally something that won't require a total rewrite when we move to Kubernetes.\n\nAre there any \"hidden gems\" or lightweight patterns you've used for this middle ground between \"basic cron\" and \"full-blown Airflow\"?",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qbte3c/seeking_a_lightweight_orchestrator_for_docker/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzdb3z8",
          "author": "dayeye2006",
          "text": "Maybe dagster?",
          "score": 1,
          "created_utc": "2026-01-13 15:46:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdd5ce",
              "author": "m_gijon",
              "text": "I did not known it! thanks! :)\n\nI think I'm gonna try a bunch of solutions, measure how many resources consume, and share the results here",
              "score": 1,
              "created_utc": "2026-01-13 15:55:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdi30w",
          "author": "proof_required",
          "text": "There is also [prefect](https://github.com/PrefectHQ/prefect)\n\n[Docker compose set-up](https://docs.prefect.io/v3/how-to-guides/self-hosted/docker-compose#how-to-run-the-prefect-server-via-docker-compose)",
          "score": 1,
          "created_utc": "2026-01-13 16:18:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzias9w",
              "author": "m_gijon",
              "text": "Thanks, I wasn't aware of that option.   \n  \nHowever, this isn't a separate process I can launch independently from the ETL execution, right?   \n  \nI‚Äôm concerned about mixing responsibilities. I‚Äôd prefer to keep them decoupled: the ETL should only be responsible for processing data, while a separate process/orchestrator handles the execution logic.",
              "score": 1,
              "created_utc": "2026-01-14 08:42:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlnq15",
          "author": "ClearML",
          "text": "This is a pretty common spot to be in, and you‚Äôre right to avoid over-engineering this early.\n\nIf you squint a bit, what you‚Äôre describing isn‚Äôt really ‚ÄúETL orchestration‚Äù yet, but rather it‚Äôs reliable job scheduling + visibility + a clean migration path. That narrows the field a lot.\n\nA few thoughts, based on what I‚Äôve seen work: Firstly, Docker Compose isn‚Äôt the problem, as cron-like schedulers *inside* Compose usually fail once you care about auditability and debugging. Ofelia is fine until the first ‚Äúwhy did this fail last night?‚Äù incident. One pattern that works well in this middle ground is using a job-centric orchestrator instead of a DAG-centric one. You define jobs as containers/scripts, run them on demand or on schedules, and get logs + history per run, without standing up a full scheduler stack.\n\nThis is actually where tools like ClearML end up fitting better than people expect:\n\n* It runs comfortably in a single-node / Docker Compose setup.\n* Jobs are just containers or scripts, so it feels closer to cron/supervisord than Airflow.\n* Every run gets logs, status, artifacts, and retries by default (huge for ETL debugging).\n* When you move to k3s, you‚Äôre not rewriting logic; you‚Äôre just changing where agents run.\n\nThe key difference vs Airflow is that you‚Äôre not modeling complex DAGs upfront. You‚Äôre tracking and scheduling *executions*, which matches an MVP phase much better.\n\nIf you want something even lighter, some start with:\n\n* simple cron + structured logging + metadata\n* then graduate to something like ClearML once failures/debugging start to hurt\n\nThat way you‚Äôre not locking yourself into Airflow semantics before you actually need them.\n\nTL;DR: you‚Äôre right to avoid Airflow right now. Look for something that treats jobs as first-class, gives you observability out of the box, and doesn‚Äôt care whether it‚Äôs running under Compose or k3s. That‚Äôs the real middle ground.",
          "score": 1,
          "created_utc": "2026-01-14 20:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03o6ni",
          "author": "lastmonty",
          "text": "Hello, \n\nI have been working on a light weight, non intrusive orchestration for some time. \n\nCheck out [runnable](https://astrazeneca.github.io/runnable/). \n\nIt supports complex workflow or jobs and provides a easy path towards kubernetes based workloads. It gives you complete visibility on execution logs and retry capability on cases of failure.\n\nHappy to answer or expand.",
          "score": 1,
          "created_utc": "2026-01-17 13:29:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd4clh",
      "title": "Do you also struggle with AI agents failing in production despite having full visibility into what went wrong?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qd4clh/do_you_also_struggle_with_ai_agents_failing_in/",
      "author": "HonestAnomaly",
      "created_utc": "2026-01-15 00:20:27",
      "score": 5,
      "num_comments": 20,
      "upvote_ratio": 0.65,
      "text": "I've been building AI agents for last 2 years, and I've noticed a pattern that I think is holding back a lot of builders, at least my team, from confidently shipping to production.\n\nYou build an agent. It works great in testing. You ship it to production. For the first few weeks, it's solid. Then:\n\n* A model or RAG gets updated and behavior shifts\n* Your evaluation scores creep down slowly\n* Costs start climbing because of redundant tool calls\n* Users start giving conflicting feedback and explore the limits of your system by handling it like ChatGPT\n* You need to manually tweak the prompt and tools again\n* Then again\n* Then again\n\nThis cycle is exhausting. Given there are few data science papers written on this topic and all observability platforms keep blogging about self-healing capabilities that can be developed with their products, I‚Äôm feeling it's not just me.\n\nWhat if instead of manually firefighting every drift and miss, your agents could adapt themselves? Not replace engineers, but handle the continuous tuning that burns time without adding value. Or at least club similar incidents and provide one-click recommendations to fix the problems.\n\nI'm exploring this idea of connecting live signals (evaluations, user feedback, costs, latency) directly to agent behavior in different scenarios, to come up with prompt, token, and tool optimization recommendations, so agents continuously improve in production with minimal human intervention.\n\nI'd love to validate if this is actually the blocker I think it is:\n\n* Are you running agents in production right now?\n* How often do you find yourself tweaking prompts or configs to keep them working?\n* What percentage of your time is spent on keeping agents healthy vs. building new features?\n* Would an automated system that handles that continuous adaptation be valuable to you?\n\nDrop your thoughts below. If you want to dig deeper or collaborate to build a product, happy to chat.",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qd4clh/do_you_also_struggle_with_ai_agents_failing_in/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzn4dri",
          "author": "Effective-Total-2312",
          "text": "Working in GenAI systems, my biggest complain, is the architects don't have an idea of what exactly should be an agent, and what should be a workflow, what should be just a traditionally programmed software, etc.\n\nIn the last 2 years I've been asked to use agent frameworks a lot, and always, they weren't the correct \"tool\" for the task that they wanted to complete.\n\nAlso, I don't really like ***most*** libraries and frameworks out there for the GenAI ecosystem, I feel they only add complexities and noise, to an already uncertain nature in LLM systems. I don't like losing control over what's going on, and I don't like complexity, I like sophisticated solutions with very controllable and testable patterns.\n\nTo your specific question, I don't think the way forward is more complexity, I think it is removing all the noise and coming up with design patterns that allow for sophisticated ways in which agents can truly solve business problems.",
          "score": 12,
          "created_utc": "2026-01-15 00:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzn5l0t",
              "author": "HonestAnomaly",
              "text": "Completely agree with that. Bloated systems and over fitting solutions is a big issue.",
              "score": 4,
              "created_utc": "2026-01-15 00:32:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzoplmv",
              "author": "KeyIsNull",
              "text": "I‚Äôve built with Pydantic Agents and LangGraph and I spent a big amount of time thinking why do I need such a boilerplate to write a simple task. I‚Äôm positive that the ecosystem will be simpler in the future, because right now is a hot mess",
              "score": 2,
              "created_utc": "2026-01-15 06:32:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzsms5l",
                  "author": "HonestAnomaly",
                  "text": "Yes, for with one specific task and a simpler use-case, this is true. But when there is a requirement for multiple agents and dynamic orchestration that changes with different scenarios, there may not be a simple solution. Anthropic's skills concept is pretty promising though. Would love to learn what you built and how would you build it differently, if you had to start from scratch.",
                  "score": 1,
                  "created_utc": "2026-01-15 20:36:39",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzu6rf0",
                  "author": "laststand1881",
                  "text": "Agreed",
                  "score": 1,
                  "created_utc": "2026-01-16 01:19:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzu6qnd",
              "author": "laststand1881",
              "text": "Agreed",
              "score": 1,
              "created_utc": "2026-01-16 01:19:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzn4tvy",
          "author": "Ok_Revenue9041",
          "text": "Totally relate to the struggle of constantly tweaking agents after every shift in model behavior. Automating feedback loops with real signal data can save a ton of manual effort and improve reliability. If you want to boost how your agents surface across AI platforms and handle these adaptive changes more efficiently, check out MentionDesk. It focuses on optimizing brand presence within LLMs so your work is better recognized.",
          "score": 3,
          "created_utc": "2026-01-15 00:28:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzngzz0",
              "author": "HonestAnomaly",
              "text": "Nice. Didn't know about that. Will check it out.",
              "score": 2,
              "created_utc": "2026-01-15 01:37:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzovvf7",
          "author": "Latter_Court2100",
          "text": "Totally agree. Visibility alone often isn‚Äôt the blocker; the harder problem is turning those signals into actionable insights like ‚Äúwhat change improves outcomes and cost with the least risk.‚Äù\n\nIn my experience, tools that capture complete agent traces plus per-step cost, latency, and behavior patterns (not just logs) make it much easier to classify failure modes and cluster similar incidents before touching prompts. Debuging/tracing like vLLora helps here by making every decision path visible end to end.\n\nWhat I find especially interesting is the next step: building an agent that works *on top of those traces*. Instead of reacting manually, it can group recurring failure paths, correlate them with eval drift or cost spikes, and then propose specific deltas such as prompt changes, tool constraints, or routing tweaks. You can validate those changes on a small eval slice before shipping anything.\n\nCurious how you think about automated adaptation in practice. Would you prefer a system that only suggests fixes with human review, or one that can apply small changes autonomously under strict guardrails?",
          "score": 2,
          "created_utc": "2026-01-15 07:27:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzspxx7",
              "author": "HonestAnomaly",
              "text": "Love this take. You're spot on that visibility isn't the bottleneck anymore, it's the¬†actionability¬†gap. Most teams I worked with have great traces and metrics, and with manual effort they are able to even translate those into ‚Äúsafe, small deltas‚Äù that actually move performance or cost in the right direction without regressions. The real unlock, as you said, seems to be building a layer¬†on top¬†of those traces, metrics, and user behavior, one that detects patterns, clusters recurring failures, and proposes fixes that are both measurable and reversible. Less human dependent analysis and more human reviewed auto-optimization.\n\nOn your question: I‚Äôm leaning toward a hybrid approach for adaptation. To optimize a black box, last thing we want, is another black box. üòÑ IMO, early-stage systems need a ‚Äúco-pilot mode‚Äù that¬†suggests¬†deltas with explainability and clear diff previews (like, ‚Äúreduce retrieval top-k from 8 ‚Üí 5 to cut cost by 12%, confidence 0.8‚Äù), and over time, once confidence thresholds and rollback mechanisms prove reliable, move toward limited¬†auto-apply¬†under guardrails. I am also getting inspired by the concept of backtesting from algorithmic trading. Whatever optimization the system comes up with should have some proof associated that it will work.\n\nHow have you seen ‚Äúsafe autonomy‚Äù handled in similar setups, have you experimented with closed-loop systems that make micro-tuning adjustments live automatically?",
              "score": 1,
              "created_utc": "2026-01-15 20:51:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzx5n0u",
                  "author": "Latter_Court2100",
                  "text": "We aare in a similar direction. We‚Äôve been working on an agent that optimizes LLM requests by analyziing traces and then running controlled experiments on variations in a guided way. In practice it‚Äôs mostly ‚Äúco-pilot/cursor way‚Äù today: it proposes small deltas like prompt trims, tool-call schema changes or retrieval parameter changes, then we validate on a replay set or a small eval slice before rolling anything out. We‚Äôve been cautious about full closed-loop changes in production, but the goal is to gradually earn autonomy for narrow, reversible optimizations with clear rollback and measurable impact.",
                  "score": 1,
                  "created_utc": "2026-01-16 13:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvghiz",
          "author": "dinkinflika0",
          "text": "This is a real problem. We see it constantly at [Maxim](https://getmax.im/Max1m) working with production teams.\n\nThe drift issue is brutal - model updates, data distribution changes, user behavior evolving. Your agent that worked great in January starts degrading by March.\n\nWe built continuous monitoring with automated evals on production traffic (sampling works fine, don't need to eval everything). Set thresholds, get alerts when quality drops before it becomes a crisis.\n\nFor prompt optimization, we have an automated system that analyzes eval results and generates improved versions. You prioritize which metrics matter, it iterates and shows reasoning. Not fully autonomous but cuts the manual tweaking significantly.\n\nHonest take: full self-healing is hard. You still need human judgment for major changes. But automating the continuous tuning (config tweaks, parameter adjustments, identifying failure patterns) saves tons of time.\n\nTo your questions - most teams we work with spend like 40% of time maintaining agents vs building new stuff. That ratio is broken.\n\nWhat kind of agents are you running? Curious what failure modes you're seeing most frequently.",
          "score": 1,
          "created_utc": "2026-01-16 05:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o044sy6",
          "author": "Sudden_Painting3381",
          "text": "Honestly, you‚Äôve hit on a problem that resonates deeply with many teams running AI agents in production. Continuous firefighting takes time without always yielding lasting improvements.\n\nIn my experience, building systems to monitor metrics like latency, cost creep, and model drift in real time can help set threesholds that alert developers when adjustments are needed not after issues are seen. Sometimes, an automated adaptation system makes a lot of sense here rather than manual checks. If such a tool could aggregate signals like user feedback, evaluation scores, and cost spikes to suggest optimization strategies (versus manual tweaking), it could significantly improve runtime stability but can become costly. \n\nHow do you currently prioritize and triage these issues? Are there frameworks helping you already like LangChain for tool selection or custom monitoring dashboards?",
          "score": 1,
          "created_utc": "2026-01-17 15:01:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn8bls",
          "author": "LoaderD",
          "text": "> chatgpt sloppost\n\n> sales hook \n\nThe patented MLOPs sub duo",
          "score": 1,
          "created_utc": "2026-01-15 00:47:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgdznn",
      "title": "Thin agent / heavy tools + validation loops + observability: what would you add for prod?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/gallery/1qgdznn",
      "author": "OnlyProggingForFun",
      "created_utc": "2026-01-18 17:05:03",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qgdznn/thin_agent_heavy_tools_validation_loops/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0bi737",
          "author": "OnlyProggingForFun",
          "text": "If anyone wants the PDF, I can share it too :)",
          "score": 1,
          "created_utc": "2026-01-18 17:05:21",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0cqote",
          "author": "Revolutionary-Bet-58",
          "text": "I would say check for infinite loops/recursion, does it meet regulatory requirements and no token bombing patterns",
          "score": 1,
          "created_utc": "2026-01-18 20:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ekdhj",
          "author": "sapiensush",
          "text": "What kind of eval you follow to be specific?",
          "score": 1,
          "created_utc": "2026-01-19 02:18:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhesvw",
      "title": "Setup a data lake",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "author": "Subatomail",
      "created_utc": "2026-01-19 19:56:01",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI‚Äôm a junior ML engineer, I have 2 years experience so I‚Äôm not THAT experienced and especially not in this.\n\nI‚Äôve been asked in my current job to design some sort of data lake to make the data independent from our main system and to be able to use this data for future projects in ML.\n\nTo give a little context, we already have a whole IT department working with the ‚Äúmain‚Äù company architecture. We have a very centralized system with one guy supervising every in and out, he‚Äôs the one who designed it and he gives little to no access to other teams like mine in R&D. It‚Äôs a mix of AWS and on-prem.\n\nEverytime we need to access data, we either have to export them manually via the software (like a client would do) or if we are lucky and there is already an API that is setup we get to use it too.\n\nSo my manager gave me the task to try to create a data lake (or whatever the correct term might be for this) to make a copy of the data that already exists in prod and also start to pump data from the sources used by the other software. And by doing so, we‚Äôll have the same data but we‚Äôll have it independently whenever we want.\n\nThe thing is I know that this is not a simple task and other than the courses I took on DBs at school, I never designed or even thought about anything like this. I don‚Äôt know what would be the best strategy, the technologies to use, how to do effective logs‚Ä¶.\n\nThe data is basically a fleet management, there are equipment data with gps positions and equipment details,  there are also events like if equipment are grouped together then they form a ‚Äújob‚Äù with ids, start date, location‚Ä¶ so it‚Äôs a very structured data so I believe a simple sql db would suffice but I‚Äôm not sure if it‚Äôs scalable.\n\nI would appreciate it if I could get some kind of books to read or leads that I should follow to at least build something that might not break after two days and that will be a good foundation long term for ML.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0ksynn",
          "author": "eemamedo",
          "text": "Data Lake as a technology is fairly simple. Think, S3 Buckets but many of them. \n\n\"Simple DB\" would be Data Warehouse. \n\n\nNeither of them are suitable as is for ML workloads.",
          "score": 1,
          "created_utc": "2026-01-20 00:27:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kxl59",
              "author": "Subatomail",
              "text": "What would be suitable for ML then ? Or would the data lake be a first step and then there should be an intermediate between the data lake and the ml pipeline ? Then what technology would be used for this intermediate step ?",
              "score": 1,
              "created_utc": "2026-01-20 00:51:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}