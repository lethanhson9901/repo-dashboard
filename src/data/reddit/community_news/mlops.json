{
  "metadata": {
    "last_updated": "2026-01-20 16:48:09",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 9,
    "total_comments": 30,
    "file_size_bytes": 45926
  },
  "items": [
    {
      "id": "1qhbtc9",
      "title": "MLOps vs MLE System Design Prep Dilemma for EM -> Which to Focus?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "author": "Low-Breakfast2018",
      "created_utc": "2026-01-19 18:11:06",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "Hi ML Leaders,  \n  \nI'm prepping for MLOps EM roles at FAANG/big tech + backups at legacy cos. But interviews seem split:  \n  \n1) SOP-hiring: Google & Meta, even \"MLOps\" JDs hit you with MLE-style system designs (classification/recommendation etc)  \n2) Team-oriented-hiring companies: Amazon/Uber/MSFT/Big Tech, more pure MLOps system design (feature stores, serving, monitoring, CI/CD).  \n3) Legacy (smaller/enterprise): Mostly general ML lead/director roles leaning MLE-heavy, few pure MLOps spots.  \n  \nDon't want to spread prep thin on two \"different\" system designs. How should I do to make sure to focus since the competition is high. Or any strategy or recommendation on double down on MLOps? How'd you balance? Seeking for experienced folks input.  \n  \nYOE: 13+ (non-FAANG)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0ir6fp",
          "author": "Comprehensive_Gap_88",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-01-19 18:27:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhfaj",
          "author": "dank_coder",
          "text": "!remind me in 1 day",
          "score": 1,
          "created_utc": "2026-01-19 20:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jhlgg",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-20 20:26:55 UTC**](http://www.wolframalpha.com/input/?i=2026-01-20%2020:26:55%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/o0jhfaj/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qhbtc9%2Fmlops_vs_mle_system_design_prep_dilemma_for_em%2Fo0jhfaj%2F%5D%0A%0ARemindMe%21%202026-01-20%2020%3A26%3A55%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qhbtc9)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-19 20:27:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kughz",
          "author": "Key_Base8254",
          "text": "following",
          "score": 1,
          "created_utc": "2026-01-20 00:35:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qebb90",
      "title": "hosted open source neptune.ai alternative?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qebb90/hosted_open_source_neptuneai_alternative/",
      "author": "Says_Watt",
      "created_utc": "2026-01-16 09:14:59",
      "score": 8,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I would gladly pay for a hosted open source [neptune.ai](http://neptune.ai) alternative that's a drop in replacement for wandb / neptune experiment tracking. The OpenAI acquisition + shutdown of [neptune.ai](http://neptune.ai) is stupid. We as a community need a proper drop in replacement for the purposes of experiment tracking that has a performant UI. I just want to visualize my loss curve without paying w&b unacceptable pricing ($1 per gpu hour is absurd).\n\nThere's no way doing this is that hard. I would do it myself but am working on a different project right now.\n\nAlso aim is an open source alternative but it's not a drop in replacement and it's not hosted. I want to easily switch from wandb and neptune without losing quality UI, without hosting it myself, and without having to do a bunch of gymnastics to fit someone else's design patterns. It needs to be MIT license so that if you decide to sell out someone else can pick up where you left off. Please for the love of god can someone please create a mobile app so I can view my runs while on the go?\n\nedit: also there's [minfx.ai](http://minfx.ai) but their ui is terrible, why is it so hard to just clone wandb / neptune, the spec is there, someone please vibe code it lol",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qebb90/hosted_open_source_neptuneai_alternative/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzw9b7n",
          "author": "MarcelLecture",
          "text": "ML engineers and MLOps has been going with MLflow for at least 5 years.\nIts UI my not be the best and it tries to be a model registry IMHO badly (kitops ftw)\nBuuuut, it is reliable, open-source, has a great community, has managed solution, easy af to deploy and can integrate easily:\n- in your code\n- on your infra\n\nYou are not obliged to use its model registry btw\n\nI never had any big issues with 3 years of using it in production in multiple companies.",
          "score": 3,
          "created_utc": "2026-01-16 10:11:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwa2lh",
              "author": "Says_Watt",
              "text": "I think aim integrate with mlflow so I'll give it a shot, but it seems it's not as easy to use as wandb. So a UI that mimics wandb / [neptune.ai](http://neptune.ai) that uses mlflow and kitops would be great. Hosted of course.",
              "score": 1,
              "created_utc": "2026-01-16 10:18:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzw6y57",
          "author": "extreme4all",
          "text": "Mlflow is OSS mlops tracking, there is probably some managed mlflow services around",
          "score": 2,
          "created_utc": "2026-01-16 09:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw7cvu",
              "author": "Says_Watt",
              "text": "It's doing too much, can I use it as easily as wandb to track my experiments and visualize my training? If not then I'm not interested. I don't need the rest of the bloat. I want a sleak UI that I can visualize and track my experiment and a mobile app that has feature parity with wandb / [neptune.ai](http://neptune.ai) in terms of tracking the experiment. It needs to be open source because [neptune.ai](http://neptune.ai) has proven people are incapable of being reliable and wandb has proven that they're unwilling to solve the problem without gouging their customers.",
              "score": 1,
              "created_utc": "2026-01-16 09:53:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzw8dto",
                  "author": "d_lowl",
                  "text": "Yep, that's the intended usage of MLflow. You record your runs. You record your parameters and metrics (it does support metrics at steps, so you can plot your loss). You don't have to use other features if you don't want to (it's not really that bloated to be fair, it's mostly just an experiment tracker + model registry). It works locally, can be easily deployed too.\n\n\\>mobile app\n\nThat it doesn't have. But you can open it in your browser still.",
                  "score": 1,
                  "created_utc": "2026-01-16 10:03:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzw5c6l",
          "author": "mutlu_simsek",
          "text": "Check perpetual ml. It is not hosted but very cost effective.",
          "score": 1,
          "created_utc": "2026-01-16 09:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw6bts",
              "author": "Says_Watt",
              "text": "I don't understand why every company needs to be \"batteries included\" all in one solution. I just want to track my experiments. I'll deploy it myself by running a few terminal commands and pushing a container to some repository. It's not that difficult.",
              "score": 2,
              "created_utc": "2026-01-16 09:44:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwiw14",
          "author": "latent_signalcraft",
          "text": "its true that many experiment tracking tools dont offer the seamless integration and UI performance youre looking for especially when it comes to being hosted and open source. but beyond just swapping tools its crucial to consider how the underlying infrastructure and data flow will handle scaling with AI workloads. a well integrated experiment tracking system is only as effective as the data and governance practices supporting it. having solid data pipelines and robust MLOps practices in place can significantly improve the user experience especially as experimentation complexity grows. if you're looking for a drop in solution focusing on tools that also prioritize data quality and traceability might be key to a more stable and reliable solution long term.",
          "score": 1,
          "created_utc": "2026-01-16 11:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwjmzf",
              "author": "Says_Watt",
              "text": "data quality? I'm pretty new to mlops in general. I just manage my data myself, store in s3 and train. So in my case I just want a nice UI to visualize the training part.",
              "score": 1,
              "created_utc": "2026-01-16 11:39:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwsqjs",
          "author": "clever_entrepreneur",
          "text": "Why they don't share a docker compose file?",
          "score": 1,
          "created_utc": "2026-01-16 12:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxi8a9",
              "author": "Says_Watt",
              "text": "what?",
              "score": 1,
              "created_utc": "2026-01-16 15:01:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwxdzz",
          "author": "alderteeter",
          "text": "Maybe this will work for you. Seems more appropriate for an individual user than production service, but maybe Iâ€™m misreading it. \n\nhttps://huggingface.co/docs/trackio/en/index",
          "score": 1,
          "created_utc": "2026-01-16 13:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07yg1c",
          "author": "burntoutdev8291",
          "text": "I recently had this issue, moving to mlflow from wandb. The paid services are really so much better, but mlflow has been here for very long.",
          "score": 1,
          "created_utc": "2026-01-18 02:26:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcl7f4",
      "title": "Verticalizing my career/Seeking to become an MLOps specialist.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qcl7f4/verticalizing_my_careerseeking_to_become_an_mlops/",
      "author": "an4k1nskyw4lk3r",
      "created_utc": "2026-01-14 11:45:43",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.8,
      "text": "I'm looking to re-enter the job market. I'm a Machine Learning Engineer and I lost my last job due to a layoff. This time, I'm aiming for a position that offers more exposure to MLOps than experimentation with models. Something platform-level. Any tips on how to attract this type of job? Any certifications for MLOps?",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1qcl7f4/verticalizing_my_careerseeking_to_become_an_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzkofmh",
          "author": "d1ddydoit",
          "text": "Becoming confident writing infra as code and architecting secure cloud solutions for ML workloads and development is something youâ€™re probably best off learning away from an MLOps pathway - ML engineering and MLOps already covers too many disciplines to cover well under a single route.\n\nI would take a look at something like a (insert cloud provider) solutions architect and devops pathway courses/exams before focusing further on ML system patterns and services  (e.g feature stores, model registries, experiment tracking, model monitoring, serving managed development environments).\n\nBecoming confident in using and administering managed cloud platform services like SageMaker, Vertex, Snowflake etc also will boost your chances as lots of companies use these off the shelf rather than maintain their own custom in-house platforms (like Uber for example).",
          "score": 3,
          "created_utc": "2026-01-14 17:32:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmeh8y",
          "author": "denim_duck",
          "text": "MLOps is as saturated as ML. There arenâ€™t more opportunities. You should pivot harder- consider robotics or something",
          "score": 1,
          "created_utc": "2026-01-14 22:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp1qs7",
              "author": "EviliestBuckle",
              "text": "Need further clarification on this",
              "score": 1,
              "created_utc": "2026-01-15 08:21:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp1sic",
                  "author": "EviliestBuckle",
                  "text": "Why does everyone feels their field is saturated",
                  "score": 1,
                  "created_utc": "2026-01-15 08:21:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzwnxlt",
              "author": "an4k1nskyw4lk3r",
              "text": "Itâ€™s a good point from eviliestâ€¦ can you explain?",
              "score": 1,
              "created_utc": "2026-01-16 12:11:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdtely",
      "title": "Does anyone else feel like Slurm error logs are not very helpful?\"",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qdtely/does_anyone_else_feel_like_slurm_error_logs_are/",
      "author": "Valeria_Xenakis",
      "created_utc": "2026-01-15 19:35:49",
      "score": 7,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "I manage a small cluster (64 GPUs) for my lab, and I swear 40% of my week is just figuring out why a job is `Pending` or why NCCL timed out.  \n  \nYesterday, a job sat in queue for 6 hours. Slurm said `Priority`, but it turned out to be a specific partition constraint hidden in the config that wasn't documented.  \n  \nIs it just our setup, or is debugging distributed training a nightmare for everyone? What tools are you guys using to actually see *why* a node is failing? `scontrol show job` gives me nothing.\n\n>",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qdtely/does_anyone_else_feel_like_slurm_error_logs_are/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzsst5b",
          "author": "cipioxx",
          "text": "Its very frustrating.",
          "score": 2,
          "created_utc": "2026-01-15 21:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzsujeu",
              "author": "Valeria_Xenakis",
              "text": "I feel like I spend more time than necessery just grepping logs on random nodes.\n\nI really want to know if there are better ways that are industry standard to track down the root cause and would appreciate any guidance. \n\nOr are you guys stuck doing it manually too?",
              "score": 1,
              "created_utc": "2026-01-15 21:12:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzsux4x",
                  "author": "cipioxx",
                  "text": "Manually and guessing.  I have started using llms to get ideas about some issues that pop up.  14 prolog errors now.  I drained the machines last week for maintenance.  I dont know whats going on",
                  "score": 2,
                  "created_utc": "2026-01-15 21:14:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxcbyk",
          "author": "ConcertTechnical25",
          "text": "Slurm error logs are essentially an Information Black Hole. When a job is stuck on Pending, the \"Priority\" label is often a mask for a hard partition constraint or a hardware mismatch. Distributed training requires more than just job status; it requires real-time monitoring of NCCL state and hardware metrics. If youâ€™re manually grepping slurmd logs on random nodes, youâ€™re playing a game of whack-a-mole that Slurm was never designed to help you win.",
          "score": 2,
          "created_utc": "2026-01-16 14:32:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzy19ku",
              "author": "Valeria_Xenakis",
              "text": "Yes, i agree and this is pretty annoying. I was wondering if this is how people go about fixing issues or if there is any better way.",
              "score": 1,
              "created_utc": "2026-01-16 16:26:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o021zl1",
                  "author": "burntoutdev8291",
                  "text": "The reply felt very AI",
                  "score": 1,
                  "created_utc": "2026-01-17 05:12:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt3lxt",
          "author": "cipioxx",
          "text": "Hmmm.  Ok.  I need to build a machine to test this on.  Thank you",
          "score": 1,
          "created_utc": "2026-01-15 21:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqfmy",
              "author": "cipioxx",
              "text": "Thank you my friend",
              "score": 1,
              "created_utc": "2026-01-15 23:50:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02632x",
          "author": "rishiarora",
          "text": "Nice cluster.",
          "score": 1,
          "created_utc": "2026-01-17 05:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hlybj",
          "author": "traceml-ai",
          "text": " I have  been thinking a lot about this class of problem.\n\nI am currently working on an open-source approach to make debugging distributed PyTorch jobs easier: starting with single-GPU today, and gradually moving toward multi-node setups.\n\nThe idea is to surface whatâ€™s actually happening during training (step timing, dataloader stalls, GPU memory pressure, per-rank behavior) so you donâ€™t have to guess from logs. \n\nIf you would  be open to it, I would love to DM and learn a bit more about your workflow and the kinds of failures you see. I am  just trying to build something that works for real clusters like yours.",
          "score": 1,
          "created_utc": "2026-01-19 15:22:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfy95h",
      "title": "I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "mlops",
      "url": "https://i.redd.it/h7duxwz871eg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-18 03:59:55",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qfy95h/i_built_a_tool_that_forces_5_ais_to_debate_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0alini",
          "author": "functionalfunctional",
          "text": "So your solution to llms being bad at facts is to spend 5x as much $ and power to get a consensus on bad facts?  Maybe should have asked 6 if that was a good product idea.",
          "score": 5,
          "created_utc": "2026-01-18 14:23:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nt6ax",
              "author": "Hyperventilater",
              "text": "I don't think that's 100% a fair criticism.\n\nThe idea behind this tool is a good one, power demands not taken into account. How do you determine consensus among a group of experts? You have them do honest debate and get to the bottom of it. \n\nThe tool might be more of \"have 5 lay-people debate until consensus\", but it's definitely a good idea in theory.",
              "score": 1,
              "created_utc": "2026-01-20 13:13:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0apsu4",
              "author": "S_Anv",
              "text": "You can enable/disable any model. the minimum is 2 models.Â ",
              "score": 0,
              "created_utc": "2026-01-18 14:46:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgdznn",
      "title": "Thin agent / heavy tools + validation loops + observability: what would you add for prod?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/gallery/1qgdznn",
      "author": "OnlyProggingForFun",
      "created_utc": "2026-01-18 17:05:03",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qgdznn/thin_agent_heavy_tools_validation_loops/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0bi737",
          "author": "OnlyProggingForFun",
          "text": "If anyone wants the PDF, I can share it too :)",
          "score": 1,
          "created_utc": "2026-01-18 17:05:21",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0cqote",
          "author": "Revolutionary-Bet-58",
          "text": "I would say check for infinite loops/recursion, does it meet regulatory requirements and no token bombing patterns",
          "score": 1,
          "created_utc": "2026-01-18 20:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ekdhj",
          "author": "sapiensush",
          "text": "What kind of eval you follow to be specific?",
          "score": 1,
          "created_utc": "2026-01-19 02:18:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhesvw",
      "title": "Setup a data lake",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "author": "Subatomail",
      "created_utc": "2026-01-19 19:56:01",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nIâ€™m a junior ML engineer, I have 2 years experience so Iâ€™m not THAT experienced and especially not in this.\n\nIâ€™ve been asked in my current job to design some sort of data lake to make the data independent from our main system and to be able to use this data for future projects in ML.\n\nTo give a little context, we already have a whole IT department working with the â€œmainâ€ company architecture. We have a very centralized system with one guy supervising every in and out. Itâ€™s a mix of AWS and on-prem.\n\nEverytime we need to access data, we either have to export them manually via the software (like a client would do) or if we are lucky and there is already an API that is setup we get to use it too.\n\nSo my manager gave me the task to try to create a data lake (or whatever the correct term might be for this) to make a copy of the data that already exists in prod and also start to pump data from the sources used by the other software. And by doing so, weâ€™ll have the same data but weâ€™ll have it independently whenever we want.\n\nThe thing is I know that this is not a simple task and other than the courses I took on DBs at school, I never designed or even thought about anything like this. I donâ€™t know what would be the best strategy, the technologies to use, how to do effective logsâ€¦.\n\nThe data is basically a fleet management, there are equipment data with gps positions and equipment details,  there are also events like if equipment are grouped together then they form a â€œjobâ€ with ids, start date, locationâ€¦ so itâ€™s a very structured data so I believe a simple sql db would suffice but Iâ€™m not sure if itâ€™s scalable.\n\nI would appreciate it if I could get some kind of books to read or leads that I should follow to at least build something that might not break after two days and that will be a good foundation long term for ML.",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0ksynn",
          "author": "eemamedo",
          "text": "Data Lake as a technology is fairly simple. Think, S3 Buckets but many of them. \n\n\"Simple DB\" would be Data Warehouse. \n\n\nNeither of them are suitable as is for ML workloads.",
          "score": 2,
          "created_utc": "2026-01-20 00:27:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kxl59",
              "author": "Subatomail",
              "text": "What would be suitable for ML then ? Or would the data lake be a first step and then there should be an intermediate between the data lake and the ml pipeline ? Then what technology would be used for this intermediate step ?",
              "score": 1,
              "created_utc": "2026-01-20 00:51:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lik4o",
                  "author": "eemamedo",
                  "text": "Yup. Data lake is the first step. Usually, data lake is used for raw, unprocessed data that you then clean using ETL or ELT pipeline and load into data warehouse. After that, you do ML modeling. I skipped couple of steps but those steps depend on the company. For example, in my previous work, we used OLTP DB to process data from DWH and then ML consumes that data. Some companies use Feature Stores.",
                  "score": 2,
                  "created_utc": "2026-01-20 02:46:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0os2hr",
          "author": "ClearML",
          "text": "Youâ€™re not wrong, as this is a big ask, especially for a junior role. From an ML standpoint, donâ€™t overthink â€œdata lakeâ€ yet.\n\nFor structured fleet/event data, a simple SQL store is fine to start. What matters more for ML is: having reproducible snapshots of data, knowing which model trained on which version, avoiding manual exports long-term\n\nif you want something outside SQL that still works well for ML, a common choice is an object-store â€œlakeâ€:\n\n* Land raw data as files in S3 (or MinIO on-prem), partitioned by date/entity (e.g., events/date=.../, gps/date=.../).\n* Use a table format like Delta Lake / Apache Iceberg / Apache Hudi on top so you get versioning + schema evolution + time travel (super helpful for reproducible training datasets).\n* Query it later with Trino/Athena/Spark when needed, without locking yourself into one database.\n\nThe hard parts arenâ€™t scale, theyâ€™re ingestion, schema changes, and data ownership. Start with append-only ingestion from prod (even on a schedule), keep it boring, and design for traceability first.\n\nIf you build something reliable and reproducible, youâ€™ll have a solid ML foundation, you can always optimize later.",
          "score": 1,
          "created_utc": "2026-01-20 16:12:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd4clh",
      "title": "Do you also struggle with AI agents failing in production despite having full visibility into what went wrong?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qd4clh/do_you_also_struggle_with_ai_agents_failing_in/",
      "author": "HonestAnomaly",
      "created_utc": "2026-01-15 00:20:27",
      "score": 4,
      "num_comments": 20,
      "upvote_ratio": 0.63,
      "text": "I've been building AI agents for last 2 years, and I've noticed a pattern that I think is holding back a lot of builders, at least my team, from confidently shipping to production.\n\nYou build an agent. It works great in testing. You ship it to production. For the first few weeks, it's solid. Then:\n\n* A model or RAG gets updated and behavior shifts\n* Your evaluation scores creep down slowly\n* Costs start climbing because of redundant tool calls\n* Users start giving conflicting feedback and explore the limits of your system by handling it like ChatGPT\n* You need to manually tweak the prompt and tools again\n* Then again\n* Then again\n\nThis cycle is exhausting. Given there are few data science papers written on this topic and all observability platforms keep blogging about self-healing capabilities that can be developed with their products, Iâ€™m feeling it's not just me.\n\nWhat if instead of manually firefighting every drift and miss, your agents could adapt themselves? Not replace engineers, but handle the continuous tuning that burns time without adding value. Or at least club similar incidents and provide one-click recommendations to fix the problems.\n\nI'm exploring this idea of connecting live signals (evaluations, user feedback, costs, latency) directly to agent behavior in different scenarios, to come up with prompt, token, and tool optimization recommendations, so agents continuously improve in production with minimal human intervention.\n\nI'd love to validate if this is actually the blocker I think it is:\n\n* Are you running agents in production right now?\n* How often do you find yourself tweaking prompts or configs to keep them working?\n* What percentage of your time is spent on keeping agents healthy vs. building new features?\n* Would an automated system that handles that continuous adaptation be valuable to you?\n\nDrop your thoughts below. If you want to dig deeper or collaborate to build a product, happy to chat.",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qd4clh/do_you_also_struggle_with_ai_agents_failing_in/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzn4dri",
          "author": "Effective-Total-2312",
          "text": "Working in GenAI systems, my biggest complain, is the architects don't have an idea of what exactly should be an agent, and what should be a workflow, what should be just a traditionally programmed software, etc.\n\nIn the last 2 years I've been asked to use agent frameworks a lot, and always, they weren't the correct \"tool\" for the task that they wanted to complete.\n\nAlso, I don't really like ***most*** libraries and frameworks out there for the GenAI ecosystem, I feel they only add complexities and noise, to an already uncertain nature in LLM systems. I don't like losing control over what's going on, and I don't like complexity, I like sophisticated solutions with very controllable and testable patterns.\n\nTo your specific question, I don't think the way forward is more complexity, I think it is removing all the noise and coming up with design patterns that allow for sophisticated ways in which agents can truly solve business problems.",
          "score": 12,
          "created_utc": "2026-01-15 00:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzn5l0t",
              "author": "HonestAnomaly",
              "text": "Completely agree with that. Bloated systems and over fitting solutions is a big issue.",
              "score": 4,
              "created_utc": "2026-01-15 00:32:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzoplmv",
              "author": "KeyIsNull",
              "text": "Iâ€™ve built with Pydantic Agents and LangGraph and I spent a big amount of time thinking why do I need such a boilerplate to write a simple task. Iâ€™m positive that the ecosystem will be simpler in the future, because right now is a hot mess",
              "score": 2,
              "created_utc": "2026-01-15 06:32:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzsms5l",
                  "author": "HonestAnomaly",
                  "text": "Yes, for with one specific task and a simpler use-case, this is true. But when there is a requirement for multiple agents and dynamic orchestration that changes with different scenarios, there may not be a simple solution. Anthropic's skills concept is pretty promising though. Would love to learn what you built and how would you build it differently, if you had to start from scratch.",
                  "score": 1,
                  "created_utc": "2026-01-15 20:36:39",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzu6rf0",
                  "author": "laststand1881",
                  "text": "Agreed",
                  "score": 1,
                  "created_utc": "2026-01-16 01:19:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzu6qnd",
              "author": "laststand1881",
              "text": "Agreed",
              "score": 1,
              "created_utc": "2026-01-16 01:19:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzn4tvy",
          "author": "Ok_Revenue9041",
          "text": "Totally relate to the struggle of constantly tweaking agents after every shift in model behavior. Automating feedback loops with real signal data can save a ton of manual effort and improve reliability. If you want to boost how your agents surface across AI platforms and handle these adaptive changes more efficiently, check out MentionDesk. It focuses on optimizing brand presence within LLMs so your work is better recognized.",
          "score": 3,
          "created_utc": "2026-01-15 00:28:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzngzz0",
              "author": "HonestAnomaly",
              "text": "Nice. Didn't know about that. Will check it out.",
              "score": 2,
              "created_utc": "2026-01-15 01:37:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzovvf7",
          "author": "Latter_Court2100",
          "text": "Totally agree. Visibility alone often isnâ€™t the blocker; the harder problem is turning those signals into actionable insights like â€œwhat change improves outcomes and cost with the least risk.â€\n\nIn my experience, tools that capture complete agent traces plus per-step cost, latency, and behavior patterns (not just logs) make it much easier to classify failure modes and cluster similar incidents before touching prompts. Debuging/tracing like vLLora helps here by making every decision path visible end to end.\n\nWhat I find especially interesting is the next step: building an agent that works *on top of those traces*. Instead of reacting manually, it can group recurring failure paths, correlate them with eval drift or cost spikes, and then propose specific deltas such as prompt changes, tool constraints, or routing tweaks. You can validate those changes on a small eval slice before shipping anything.\n\nCurious how you think about automated adaptation in practice. Would you prefer a system that only suggests fixes with human review, or one that can apply small changes autonomously under strict guardrails?",
          "score": 2,
          "created_utc": "2026-01-15 07:27:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzspxx7",
              "author": "HonestAnomaly",
              "text": "Love this take. You're spot on that visibility isn't the bottleneck anymore, it's theÂ actionabilityÂ gap. Most teams I worked with have great traces and metrics, and with manual effort they are able to even translate those into â€œsafe, small deltasâ€ that actually move performance or cost in the right direction without regressions. The real unlock, as you said, seems to be building a layerÂ on topÂ of those traces, metrics, and user behavior, one that detects patterns, clusters recurring failures, and proposes fixes that are both measurable and reversible. Less human dependent analysis and more human reviewed auto-optimization.\n\nOn your question: Iâ€™m leaning toward a hybrid approach for adaptation. To optimize a black box, last thing we want, is another black box. ðŸ˜„ IMO, early-stage systems need a â€œco-pilot modeâ€ thatÂ suggestsÂ deltas with explainability and clear diff previews (like, â€œreduce retrieval top-k from 8 â†’ 5 to cut cost by 12%, confidence 0.8â€), and over time, once confidence thresholds and rollback mechanisms prove reliable, move toward limitedÂ auto-applyÂ under guardrails. I am also getting inspired by the concept of backtesting from algorithmic trading. Whatever optimization the system comes up with should have some proof associated that it will work.\n\nHow have you seen â€œsafe autonomyâ€ handled in similar setups, have you experimented with closed-loop systems that make micro-tuning adjustments live automatically?",
              "score": 1,
              "created_utc": "2026-01-15 20:51:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzx5n0u",
                  "author": "Latter_Court2100",
                  "text": "We aare in a similar direction. Weâ€™ve been working on an agent that optimizes LLM requests by analyziing traces and then running controlled experiments on variations in a guided way. In practice itâ€™s mostly â€œco-pilot/cursor wayâ€ today: it proposes small deltas like prompt trims, tool-call schema changes or retrieval parameter changes, then we validate on a replay set or a small eval slice before rolling anything out. Weâ€™ve been cautious about full closed-loop changes in production, but the goal is to gradually earn autonomy for narrow, reversible optimizations with clear rollback and measurable impact.",
                  "score": 1,
                  "created_utc": "2026-01-16 13:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvghiz",
          "author": "dinkinflika0",
          "text": "This is a real problem. We see it constantly at [Maxim](https://getmax.im/Max1m) working with production teams.\n\nThe drift issue is brutal - model updates, data distribution changes, user behavior evolving. Your agent that worked great in January starts degrading by March.\n\nWe built continuous monitoring with automated evals on production traffic (sampling works fine, don't need to eval everything). Set thresholds, get alerts when quality drops before it becomes a crisis.\n\nFor prompt optimization, we have an automated system that analyzes eval results and generates improved versions. You prioritize which metrics matter, it iterates and shows reasoning. Not fully autonomous but cuts the manual tweaking significantly.\n\nHonest take: full self-healing is hard. You still need human judgment for major changes. But automating the continuous tuning (config tweaks, parameter adjustments, identifying failure patterns) saves tons of time.\n\nTo your questions - most teams we work with spend like 40% of time maintaining agents vs building new stuff. That ratio is broken.\n\nWhat kind of agents are you running? Curious what failure modes you're seeing most frequently.",
          "score": 1,
          "created_utc": "2026-01-16 05:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o044sy6",
          "author": "Sudden_Painting3381",
          "text": "Honestly, youâ€™ve hit on a problem that resonates deeply with many teams running AI agents in production. Continuous firefighting takes time without always yielding lasting improvements.\n\nIn my experience, building systems to monitor metrics like latency, cost creep, and model drift in real time can help set threesholds that alert developers when adjustments are needed not after issues are seen. Sometimes, an automated adaptation system makes a lot of sense here rather than manual checks. If such a tool could aggregate signals like user feedback, evaluation scores, and cost spikes to suggest optimization strategies (versus manual tweaking), it could significantly improve runtime stability but can become costly. \n\nHow do you currently prioritize and triage these issues? Are there frameworks helping you already like LangChain for tool selection or custom monitoring dashboards?",
          "score": 1,
          "created_utc": "2026-01-17 15:01:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn8bls",
          "author": "LoaderD",
          "text": "> chatgpt sloppost\n\n> sales hook \n\nThe patented MLOPs sub duo",
          "score": 1,
          "created_utc": "2026-01-15 00:47:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhtov0",
      "title": "Releasing KAOS - The K8s Agent Orchestration System",
      "subreddit": "mlops",
      "url": "https://i.redd.it/ypcs28cn7geg1.gif",
      "author": "axsauze",
      "created_utc": "2026-01-20 06:34:03",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhtov0/releasing_kaos_the_k8s_agent_orchestration_system/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mlq55",
          "author": "RetiredApostle",
          "text": "How does it differ from kagent by Solo.io?",
          "score": 1,
          "created_utc": "2026-01-20 07:08:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}