{
  "metadata": {
    "last_updated": "2026-02-18 17:29:58",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 12,
    "total_comments": 24,
    "file_size_bytes": 44630
  },
  "items": [
    {
      "id": "1r3kxcd",
      "title": "What YouTube content actually helped you in your MLOps journey? And what's still confusing?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r3kxcd/what_youtube_content_actually_helped_you_in_your/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-13 09:37:26",
      "score": 34,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I've been in the ML/DevOps space for 11+ years and recently started doing 1:1 calls helping people transition into MLOps. One thing I keep noticing, almost everyone I talk to is overwhelmed.\n\nNot because they're not smart. But because MLOps is so vast, with batch vs. real-time ML pipelines, inference, infrastructure, and monitoring, every course teaches it differently. One guy will say start with Kubeflow, another says MLflow, another says forget tools, learn fundamentals first.\n\nI genuinely want to understand from this community:\n\n1. When you search MLOps on YouTube, what kind of videos do you actually watch fully? Tool-specific tutorials? Career roadmaps? Architecture walkthroughs?\n2. What's your biggest struggle right now ‚Äî is it picking the right tools? Understanding how pieces connect end to end? Or knowing what the market actually wants vs what courses teach?\n3. Is there a video or channel that genuinely helped you \"get it\"? Not just theory, but actually made something click?\n4. What's missing? What video do you wish existed but doesn't?\n\nAsking because I see so much content out there, but people on my calls are still confused. \n\nSomething is clearly not working. Curious what you all think.\n\nI've been thinking of creating some content on this myself, but before that, I just want to understand the current situation and where people are really stuck. No point in adding more noise if the real gap is elsewhere.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r3kxcd/what_youtube_content_actually_helped_you_in_your/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o557kiv",
          "author": "ConsciousML",
          "text": "Youtube was not very helpful on my side for this as there‚Äôs very little quality content in my opinion.\n\nWhat worked best for me is reading quality articles:\n- [ml-ops.org](https://ml-ops.org/) is great for the basics\n- [Neptune.ai](https://neptune.ai/blog) is great but they surf a lot on the GenAI wave so I don‚Äôt read much anymore\n- [ZenML](https://docs.zenml.io/stacks) has amazing doc for the MLOps components (experiment tracker, feature store, orchestrator, etc.)\n- [Hopsworks](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines) explains the three pipeline paradigm better than anyone else\n\n\nOnce you know the theory well and you had some good hands-on experience, I‚Äôve found that the big tech engineering blogs are the best source of trusted information.\n\nI‚Äôve compiled a list of [interesting blogs](https://foremost-tea-3e3.notion.site/Resources-1973205f20be80d6923bd4a18ee62cd6?source=copy_link).\n\nI also try my best to share useful content on my [personal blog](https://www.axelmendoza.com/) if that‚Äôs helpful!\n\nI‚Äôm really interested in your process to try helping people getting into MLOps.\n\nDM me if you want to share some thoughts!",
          "score": 27,
          "created_utc": "2026-02-13 11:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56d3gt",
              "author": "Neither_Film_8641",
              "text": "I like your Blog!",
              "score": 3,
              "created_utc": "2026-02-13 15:17:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56mm3h",
                  "author": "ConsciousML",
                  "text": "Thanks man! I enjoy to write. Even better if it‚Äôs helpful ;)",
                  "score": 1,
                  "created_utc": "2026-02-13 16:02:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55w9ul",
          "author": "TruDanceCat",
          "text": "DataCamp is my go-to. Tons of great courses on ML and MLOPS. Theory videos, coding exercises, labs, and practice quizzes all based on career tracks like ML, MLOps, Data Scientist, Data Analytics, Data Engineering and more.\n\nWe have a learning budget at work, so they reimburse us for the cost, but I would pay for the subscription myself if they didn‚Äôt- it‚Äôs totally worth it.",
          "score": 3,
          "created_utc": "2026-02-13 13:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dmkaw",
              "author": "JayRathod3497",
              "text": "What's a subscription piece ?",
              "score": 1,
              "created_utc": "2026-02-14 18:18:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55nfka",
          "author": "DifficultDifficulty",
          "text": "I found AWS/GCP tech blogs and OSS repos useful, particularly those laying out architecture blueprints",
          "score": 2,
          "created_utc": "2026-02-13 12:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56sp6s",
          "author": "Inevitable_Resort902",
          "text": "I am a SWE looking to transition into MLOps and would love to get some advice. Can I get in touch with you?",
          "score": 1,
          "created_utc": "2026-02-13 16:31:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5739xf",
              "author": "Extension_Key_5970",
              "text": "sure, you can DM me",
              "score": 1,
              "created_utc": "2026-02-13 17:22:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o59do9g",
          "author": "Ambitious-Estate4356",
          "text": "I would say claude learning + claude code. They try to make you learn each and every concepts from ground up with some example and code.",
          "score": 1,
          "created_utc": "2026-02-14 00:25:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g5zzf",
              "author": "CupFine8373",
              "text": "what is claude learning ? ",
              "score": 1,
              "created_utc": "2026-02-15 03:12:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k2vpf",
          "author": "Gaussianperson",
          "text": "The biggest gap I see is that most content teaches MLOps tool-by-tool (\"here's MLflow\", \"here's Kubeflow\") but almost nobody explains how real production ML systems are actually designed end-to-end: the trade-offs, the architecture decisions, why companies pick certain patterns over others.\n\nI also feel like youtube videos are too long...\n\nThat's what I try to cover in my newsletter Machine Learning at Scale production ML patterns, system design deep dives (rec systems, ads, serving infra), and how things actually work inside big tech.\n\nWritten from the perspective of someone building these systems daily, not teaching them in a course: [https://machinelearningatscale.substack.com/](https://machinelearningatscale.substack.com/)",
          "score": 1,
          "created_utc": "2026-02-15 19:23:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r52ypf",
      "title": "Practical SageMaker + MLflow Stage/Prod Workflow for Small MLOps + DS Team?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r52ypf/practical_sagemaker_mlflow_stageprod_workflow_for/",
      "author": "ZeroSilver87",
      "created_utc": "2026-02-15 02:25:32",
      "score": 20,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hey all ‚Äî As the title says, looking for practical input from teams operating at a similar scale...\n\nWe have a small MLOps team supporting a small Data Science team... \\~4-6 per team. We‚Äôre enabling SageMaker + MLflow this year and trying to move toward more sustainable, repeatable ML workflows.\n\nHistorically, our ML efforts have been fairly ad hoc and home-grown. We‚Äôre now trying to formalize things and improve R&D velocity without overburdening either the DS team or our platform engineers.\n\nOne major constraint is that our DevOps/infra process is heavily gated. New AWS resources require approvals outside our teams and move slowly. So we‚Äôre trying to design something clean and safe that doesn‚Äôt require frequent new infrastructure or heavyweight process for each new model.\n\nI‚Äôm aware of the AWS-recommended workflows, but they seem optimized for larger teams or environments with more autonomy than we have.\n\nSome Additional Context:\n\n* Data lake on S3 (queried via Athena)\n* Models are often entity-specific (i.e., many model instances derived from a shared training pipeline)\n\nCurrent thinking:\n\n* Non-Prod:\n   * EDA + pipeline development + model experimentation\n   * read-only access to prod archive data to remove need to set up complicated replication from prod to non-prod\n* Prod:\n   * Inference endpoints\n   * Single managed MLflow workspace\n      * DS can log runs + register models (from non-prod or local)\n      * Only a prod automation role can promote models to ‚ÄúProduction‚Äù\n      * Production Inference services only load models marked \"Production\"\n   * Automated retraining pipelines\n\nThoughts or suggestions on this setup?\n\nThe goal is to embed sustainable workflows and guardrails without turning this into a setup that requires large teams to support it.\n\nWould love to hear what‚Äôs worked (or failed) for teams in similar size ranges or if you have any good experience with AWS Sagemaker to suggest good workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r52ypf/practical_sagemaker_mlflow_stageprod_workflow_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5hxwfs",
          "author": "Bezza100",
          "text": "It seems good, you will need to invest significant time on the CI/CD for promoting to make sure it's robust. Also consider standard examples and templates for the DS team so there isn't too much refactoring to use your CI/CD.",
          "score": 3,
          "created_utc": "2026-02-15 12:34:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lpux4",
              "author": "ZeroSilver87",
              "text": "Good points. Yes I‚Äôm not sure exactly what the CI/CD will look like for this but some templates to make it faster is a good idea",
              "score": 1,
              "created_utc": "2026-02-16 00:41:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qki7y",
          "author": "Gaussianperson",
          "text": "You should definitely use the MLflow Model Registry as the central hub for model handoffs. When a data scientist is happy with a model, they register it in MLflow, and that acts as the signal for your staging or production pipelines to pick it up and deploy it to a SageMaker endpoint. This keeps the roles clear and avoids a lot of back and forth between the teams.  \n  \nOne thing to watch out for is over engineering your internal tools too soon.\n\nStick to the native MLflow and SageMaker APIs before building complex wrappers.\n\nIf you are looking for more examples of how different companies handle these kinds of architectural choices, check out machinelearningatscale.substack.com. (author here)\n\nIt has some solid case studies on how bigger shops manage their production infrastructure which can give you some good ideas for your own roadmap.",
          "score": 3,
          "created_utc": "2026-02-16 19:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qnu67",
              "author": "ZeroSilver87",
              "text": "Excellent, thanks for the advice. Agree on not over engineering too soon. We may have some hurdles to creating SageMaker endpoints on the fly via CI/CD but I think there is a reasonable work around with MME since they feed off an S3 bucket‚Ä¶ assuming we can maintain a consistent set of requirements.",
              "score": 1,
              "created_utc": "2026-02-16 19:51:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o62qkl5",
          "author": "gdnmaia",
          "text": "You should be able to access prd data from nonprd via access not replication - the architecture you are using is dated, pre data science. How can you test AI models that are probabilistic in nature (I am not talking about probabilistic models) with partial data? Read this for inspiration (https://www.databricks.com/sites/default/files/2024-06/2023-10-EB-Big-Book-of-MLOps-2nd-Edition.pdf?itm_source=www&itm_category=resources&itm_page=thank-you&itm_location=body&itm_component=hero&itm_offer=2023-10-eb-big-book-of-mlops-2nd-edition.pdf) and don‚Äôt worry about the platform used in this documentation, focus on the paradigm across environments (model promotion versus replication, code promotion, data access versus copy, and data lineage).",
          "score": 1,
          "created_utc": "2026-02-18 16:15:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7s2ni",
      "title": "[D] We tested the same INT8 model on 5 Snapdragon chipsets. Accuracy ranged from 93% to 71%. Same weights, same ONNX file.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r7s2ni/d_we_tested_the_same_int8_model_on_5_snapdragon/",
      "author": "NoAdministration6906",
      "created_utc": "2026-02-18 03:30:42",
      "score": 19,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "We've been doing on-device accuracy testing across multiple Snapdragon SoCs and the results have been eye-opening.\n\nSame model. Same quantization. Same ONNX export. Deployed to 5 different chipsets:\n\n|Device|Accuracy|\n|:-|:-|\n|Snapdragon 8 Gen 3|91.8%|\n|Snapdragon 8 Gen 2|89.1%|\n|Snapdragon 7s Gen 2|84.3%|\n|Snapdragon 6 Gen 1|79.6%|\n|Snapdragon 4 Gen 2|71.2%|\n\nCloud benchmark reported 94.2%.\n\nThe spread comes down to three things we've observed:\n\n1. **NPU precision handling**¬†‚Äî INT8 rounding behavior differs across Hexagon generations. Not all INT8 is created equal.\n2. **Operator fusion differences**¬†‚Äî the QNN runtime optimizes the graph differently per SoC, sometimes trading accuracy for throughput.\n3. **Memory-constrained fallback**¬†‚Äî on lower-tier chips, certain ops fall back from NPU to CPU, changing the execution path entirely.\n\nNone of this shows up in cloud-based benchmarks. You only see it when you run on real hardware.\n\nCurious if others are seeing similar drift across chipsets ‚Äî or if anyone has a good strategy for catching this before shipping. Most CI pipelines we've seen only test on cloud GPUs and call it a day.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r7s2ni/d_we_tested_the_same_int8_model_on_5_snapdragon/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o60owaq",
          "author": "KeyIsNull",
          "text": "Wow that's a huge performance drop, it is always a good idea to measure on device but I'd never expect a 15/20% drop. Have you also tested the model in full precision?",
          "score": 1,
          "created_utc": "2026-02-18 08:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60wvu3",
          "author": "datashri",
          "text": "Did you publish anywhere?",
          "score": 1,
          "created_utc": "2026-02-18 09:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o629f1b",
          "author": "Commercial-Fly-6296",
          "text": "Maybe the chip wear and tear also matters ?",
          "score": 1,
          "created_utc": "2026-02-18 14:56:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r312b5",
      "title": "The agent security landscape is kind of a mess and I'm not sure what to do about it",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r312b5/the_agent_security_landscape_is_kind_of_a_mess/",
      "author": "Exact-Literature-395",
      "created_utc": "2026-02-12 18:21:21",
      "score": 16,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "So my team has been pushing me to evaluate autonomous agents for some of our workflow automation. Specifically looking at OpenClaw since it has massive traction (something like 160k+ GitHub stars) and can connect LLMs to local files, browsers, Slack, Discord, etc. Our ops lead is really excited about using it to auto-triage the \\~200 support tickets we get daily, basically having it read incoming tickets, check our internal docs, and route them to the right team with a priority score. Also been talking about automating the data validation checks we run every Monday where someone manually compares CSV exports against our postgres tables. Tedious stuff that would be perfect for an agent.\n\nBut honestly? The more I dig into this, the more I want to pump the brakes.\n\nI stumbled across some security research that genuinely unsettled me. Apparently there are tens of thousands of OpenClaw instances just... exposed directly to the internet. But the number that really stopped me was this: something like 15% of community built skills contain malicious instructions. Prompts designed to download malware or steal data. And when these get flagged and removed, they apparently just reappear under new identities pretty quickly.\n\nThe project's own FAQ literally describes this as a \"Faustian bargain\" with no \"perfectly safe\" setup. I appreciate the honesty but also... what am I supposed to do with that? How do I bring this to my team without sounding like I'm just being obstructionist?\n\nWhat's frustrating from an MLOps perspective is that this completely changes how I think about threat modeling. We've spent so much time worrying about model poisoning, adversarial inputs, data drift. With agents though, the attack surface just explodes. Prompt injection could come through any email or webpage the agent processes. If someone compromises the agent itself they basically inherit every permission we've granted it. And the plugin ecosystem? Nobody has time to audit all that, but you're essentially running untrusted code with access to your systems.\n\nThere's also this concept I keep seeing called \"judgment hallucination\" where the agent appears trustworthy but lacks genuine reasoning, so users just... hand over more and more authority. That one hits different because I can already see how it would play out with some of the less technical folks on our team who already treat ChatGPT like its omniscient.\n\nI looked at some alternatives like AutoGPT and BabyAGI but they seem to have similar issues, maybe even less mature from a security standpoint. A coworker mentioned something called Agent Trust Hub that supposedly scans skills for hidden logic and data exfiltration patterns before you install them, still need to actually try it though. The usual advice seems to be run everything in containers, dont expose default ports, start with read only permissions and expand from there. Basically treat it like you would any untrusted code I guess.\n\nBut I'm genuinely torn. The capability is exciting and I get why leadership wants this. The current state of the ecosystem though... it feels like we'd be taking on a lot of risk that we're not equiped to manage yet. Maybe I'm being too conservative here.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r312b5/the_agent_security_landscape_is_kind_of_a_mess/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o53p7au",
          "author": "KarmaIssues",
          "text": "I suppose the only real change would have to be sandboxing it. That would atleast help with some of the more damaging actions.",
          "score": 1,
          "created_utc": "2026-02-13 03:27:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5aecc4",
          "author": "South-Opening-9720",
          "text": "You're not being conservative ‚Äî you're threat-modeling correctly. I'd start with an 'assistive' agent first (summarize + classify + suggest KB links), keep it read-only, and lock it in a sandbox with tight egress + allowlisted tools; actions/handoffs only after evals + red-team prompt injection tests. I use chat data for the safer 'draft + route' step and it helps without giving it keys to the kingdom. What systems would it touch on day 1?",
          "score": 1,
          "created_utc": "2026-02-14 04:23:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fboic",
          "author": "CompelledComa35",
          "text": "You're right to pump the brakes. That 15% malicious skill rate is insane and judgment hallucination is a real problem. Sandboxing helps but doesn't solve prompt injection through emails/web content. Check out alice's caterpillar for agentic scanning, it surfaces these attack vectors before production. Run it against your planned workflows and show leadership the actual risk exposure with evidence. ",
          "score": 1,
          "created_utc": "2026-02-14 23:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pfpy1",
          "author": "penguinzb1",
          "text": "you're not being too conservative, you're being realistic. the 15% malicious plugin rate is wild and the \"faustian bargain\" framing from the project itself should tell you everything you need to know about production readiness.\n\nthe thing that jumps out to me is that you're thinking about this correctly but the solution isn't just sandboxing or read-only permissions. those help, but they don't address the core problem which is that you have no way to validate agent behavior before it hits production. if someone compromises a plugin or the agent hallucinates a bad judgment call, you won't know until it's already happened.\n\nwhat you actually need is a way to test failure scenarios before deployment. run simulations where the agent processes malicious emails, gets fed prompt injection attempts through support tickets, or encounters plugins that try to exfiltrate data. see if your sandboxing actually catches it, or if the agent routes something incorrectly because it got confused by adversarial input.\n\nthe \"assistive agent\" approach someone mentioned is smart, keep it read-only and limit blast radius. but even that needs validation. you can't just hope the agent summarizes tickets correctly or routes them to the right team. you need to inject edge cases and see where it breaks. what happens when a ticket contains sql injection attempts disguised as support questions? what if someone writes a ticket specifically designed to trick the agent into misclassifying priority?\n\nfor the monday data validation checks, same thing. test scenarios where the csv and postgres deliberately disagree, or where the data has been subtly corrupted. does the agent catch it, or does it just report everything looks fine because it's operating on vibes?\n\nthe judgment hallucination thing is real and it's exactly what makes this hard. people will trust the agent more than they should because it sounds confident. the only way to combat that is to have evidence that the agent actually behaves correctly under adversarial conditions, not just happy path demos.\n\ncurious what your current testing strategy looks like for the rest of your mlops stack. if you're already doing adversarial testing on models, this is basically the same mindset but applied to agent workflows instead of model outputs.",
          "score": 1,
          "created_utc": "2026-02-16 16:26:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w4qcl",
          "author": "yottalabs",
          "text": "The adversarial testing point is key. A lot of teams treat agents as application features, but they behave more like distributed systems with untrusted inputs at every boundary.  \n  \nWe‚Äôve seen the biggest gaps not in sandboxing itself, but in observability ‚Äî being able to trace why an agent made a specific decision, what signals it consumed, and whether it behaved within its intended policy envelope.  \n  \nWithout that, even good containment controls can fail quietly.",
          "score": 1,
          "created_utc": "2026-02-17 16:48:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r64jfl",
      "title": "We built hardware-in-the-loop regression gates for AI models on Snapdragon ‚Äî here's what we learned",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r64jfl/we_built_hardwareintheloop_regression_gates_for/",
      "author": "NoAdministration6906",
      "created_utc": "2026-02-16 08:49:34",
      "score": 10,
      "num_comments": 4,
      "upvote_ratio": 0.92,
      "text": "We deploy AI models to Snapdragon devices and got tired of cloud tests passing while real hardware failed. Built a CI tool that runs your model on physical Snapdragon devices and blocks the PR if gates fail.\n\nBiggest surprise: same INT8 model showed 23% accuracy variance across 5 Snapdragon chipsets. Cloud benchmarks predicted none of this.\n\nFull disclosure: I built this (EdgeGate). Happy to answer questions about the architecture or edge AI testing in general.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r64jfl/we_built_hardwareintheloop_regression_gates_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5ovn0i",
          "author": "Comfortable_Holiday3",
          "text": "Looks good. I built something similar: an RPC with telemetry that also supports non-OS hardware abstracting the physical layer (USB/OTA streaming) and the model runtime/engine/interpreter. Interested in the architecture. Curiously, what do you think is the cause of the large accuracy variance? Is it something related to optimized kernel implementation in the Snapdragon?",
          "score": 1,
          "created_utc": "2026-02-16 14:49:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p2vg6",
              "author": "NoAdministration6906",
              "text": "Thanks! Your setup sounds really cool ‚Äî abstracting the physical layer for non-OS hardware is no joke.\n\nOur approach is more cloud-first ‚Äî we go through Qualcomm AI Hub's API for compilation and on-device execution, then wrap results in signed evidence bundles so CI gates can't be spoofed. Think \"unit tests but for model quality on real hardware.\"\n\nOn the accuracy variance ‚Äî I think you're on the right track with the kernel implementations. Different Hexagon NPU generations (v69, v73, v75) likely use different fixed-point arithmetic paths. But it's probably a mix of that plus compiler-level graph optimizations varying per target, and possibly quantization calibration differences during compilation. We're also not pinning the runtime library version across devices yet, so that's another uncontrolled variable.\n\nHonestly, isolating *which* of these contributes *how much* is half the reason we built this. Would love to compare notes on what you've seen on the bare-metal side.",
              "score": 2,
              "created_utc": "2026-02-16 15:25:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p5gg6",
          "author": "penguinzb1",
          "text": "the 23% variance is wild. i've been thinking about hardware-in-the-loop testing for agents that need to run on different devices, and this kind of variance is exactly what makes it hard to trust cloud benchmarks for anything that ships to physical hardware.\n\ncurious how you're handling the gate thresholds across different chipsets. do you set per-device accuracy targets, or do you have a single gate that accounts for the worst-case variance? seems like the latter would be overly conservative but the former creates a maintenance nightmare.\n\nalso wondering if you're seeing variance patterns that correlate with specific model architectures or quantization approaches. like, does the variance show up more in certain layer types or is it pretty uniform across the whole model?",
          "score": 1,
          "created_utc": "2026-02-16 15:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ryosw",
              "author": "NoAdministration6906",
              "text": "Yeah, we ran into the same tradeoff. We don‚Äôt do ‚Äúworst-case single gate‚Äù ‚Äî it‚Äôs too conservative.\n\nWe keep per-chipset baselines (golden outputs/metrics) and gate on regression vs that device + an absolute floor. Fleet-wise we require key ‚Äúrelease‚Äù devices to pass, and use a percentile-ish rule for the rest so one flaky/outlier chipset doesn‚Äôt block everything.\n\nOn patterns: it‚Äôs not uniform ‚Äî INT8 variance usually clusters around specific ops/kernels (backend differences) and calibration-sensitive layers, not the whole network.",
              "score": 1,
              "created_utc": "2026-02-16 23:50:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4dcq9",
      "title": "Transitioning into MLOps from API Gateway background ‚Äî looking for realistic paths & pitfalls",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r4dcq9/transitioning_into_mlops_from_api_gateway/",
      "author": "EntropyTamer-007",
      "created_utc": "2026-02-14 06:26:58",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "Hi everyone,\n\nI‚Äôm looking for advice from people actually working in MLOps / ML platform roles, especially those who transitioned from non-ML backgrounds.\n\nMy current background (honest assessment):\n\n\\~4 years of experience working with Axway API Gateway\n\nMost of my work has been configuration-focused (policies in Policy Studio)\n\nI understand concepts like OAuth2, JWT, rate limiting, traffic mediation, etc., but mainly at a conceptual / tool-usage level\n\nI haven‚Äôt owned end-to-end systems, production ML pipelines, CI/CD, Kubernetes, or cloud infrastructure yet\n\nBeginner-level Python\n\nNo hands-on AWS/Azure/GCP or IaC experience so far\n\nSo while I‚Äôm not new to tech, I‚Äôm aware that my system ownership depth is limited.\n\nWhat I‚Äôm doing currently:\n\nI‚Äôm enrolled in a Data Science with Generative AI course\n\nI‚Äôm trying to avoid rushing into ‚ÄúML titles‚Äù without the necessary platform depth\n\nMy goal (longer-term):\n\nTransition into MLOps / ML Platform Engineering\n\nWork closer to model deployment, reliability, governance, and infrastructure, not pure research\n\nPrefer roles that are remote-friendly and have long-term growth\n\nFrom my background, \n\nwhat are the most realistic entry points into MLOps?\n\nIs it better to first transition into a Cloud / Platform / DevOps role and then move into MLOps, or are there viable direct bridges?\n\nWhich skills tend to be non-negotiable for MLOps roles that people often underestimate?\n\nWhat are common mistakes people make when trying to move into MLOps without prior ML ownership?\n\nIf you had to do this transition again, what would you focus on first vs ignore initially?\n\nI‚Äôm deliberately trying to avoid hype-driven decisions and would really value advice grounded in real hiring and on-the-job experience.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r4dcq9/transitioning_into_mlops_from_api_gateway/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5bgie5",
          "author": "flyingPizza456",
          "text": "Common \"mistake\" (and mistake is not really the right word here) is to think that MLOps is something you do as a profession. It is like devops, it is about how to do things, not what things have to be done.\n\nIt is an approach, which still can be backed with specific tools, processes, skills, roles etc.\n\nYou do not transition into it in my opinion. It is more like you are an ML engineer or data scientist or infrastructure engineer or sulution architect or whatever. Even having a role named MLOps or Devops engineer is totally fine but your main tasks with regards to MLOps thinking is to support the reliability of services that are realized through Ai technologies (mostly ML as the name suggests).\n\nThen this comes down to you are doing machine learning or infrastructure work and you want to professionalize this even more.\n\nOne cannot say this often enough: MLOps is NOT FOR BEGINNERS\n\nDo all the other work that relates to it and after some years of experience you will realize what the important bits of the MLOps approach are. Also: MLOps is, like often times with other approaches, highly individual and has to be adapted for the setting / service / organization (factors could be size of service, number of people using the pipelines and many more)\n\nAnd: I really don't want to dampen your ambitions, but I think you'll find it easier if you don't view it as a dedicated profession, but rather as a task that must be fulfilled by everyone involved. That's why it's so difficult to introduce it into organisations and apply it successfully.",
          "score": 8,
          "created_utc": "2026-02-14 10:06:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jhd6s",
              "author": "EntropyTamer-007",
              "text": "Yes! Agree with you as I am learing about MLOps the common thing I found was same the MLOps is not for beginner and its not a profession. \nThe particular thing I was trying to figure out is having invested API Gateway (particularly Axway API gateway) what could be my way forwards and MLOps was one of the long term path that was in my options. \n\n\nThanks for you insights!!",
              "score": 1,
              "created_utc": "2026-02-15 17:38:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6yai3",
      "title": "How deeply should an SRE understand PyTorch for ML production environments?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r6yai3/how_deeply_should_an_sre_understand_pytorch_for/",
      "author": "Simple-Toe20",
      "created_utc": "2026-02-17 06:10:03",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r6yai3/how_deeply_should_an_sre_understand_pytorch_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r2t5sn",
      "title": "Seeking deep 1:1 mentoring (Databricks / Snowflake / Azure ML)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r2t5sn/seeking_deep_11_mentoring_databricks_snowflake/",
      "author": "Admirable-Crab-9908",
      "created_utc": "2026-02-12 13:17:02",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.75,
      "text": "Looking for structured 1:1 mentoring to go from implementation-level expertise to platform-level mastery.\n\nFocus areas:\n\n\t‚Ä¢\tDatabricks MLOps (Unity Catalog, MLflow, CI/CD, governance)\n\n\t‚Ä¢\tSnowflake ML (Snowpark ML, feature pipelines, deployment patterns)\n\n\t‚Ä¢\tAzure ML (enterprise pipelines, model serving, security)\n\nKindly DM. Will be happy to pay hourly. ",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r2t5sn/seeking_deep_11_mentoring_databricks_snowflake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4zpz76",
          "author": "kchandank",
          "text": "I will DM you",
          "score": 2,
          "created_utc": "2026-02-12 15:10:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4f6uw",
      "title": "Passed NVIDIA NCA-AIIO and now need Guidance for NCP-AII",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r4f6uw/passed_nvidia_ncaaiio_and_now_need_guidance_for/",
      "author": "Sufficient_Berry_311",
      "created_utc": "2026-02-14 08:14:04",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.72,
      "text": "Hello Everyone \n\nI had passed the NCA-AIIO on 12th Feb 2026. The questions are simple and you can pass the exam using your logic. You can ask me questions about the exam. I have used notebooklm for study, if you want I can give it also. \n\n  \nI need help to clear the NCP-AII. Is there any person here who cleared it. I wanted to know how hard is this and how many questions we need to solve in cli (provided in exam) or is there any lab related work?  \n  \nAny help from where I will get the lab access?\n\nThank you üôè  ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r4f6uw/passed_nvidia_ncaaiio_and_now_need_guidance_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5cw4cw",
          "author": "Fantastic-Chicken748",
          "text": "Hey!!! I‚Äôm also studying to ncp-aai. If you have any tips that you want to share, I would be very much appreciated",
          "score": 2,
          "created_utc": "2026-02-14 16:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dqe4c",
              "author": "Sufficient_Berry_311",
              "text": "I am from infrastructure side not from application side, cannot help on that. You can follow the nvidia study guide to notebooklm and you can ask question.\nNCA-AIIO is very easy, if you have knowledge how gpu works thats it.",
              "score": 2,
              "created_utc": "2026-02-14 18:37:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3s22c",
      "title": "How did you learn Ray Serve? Any good resources?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r3s22c/how_did_you_learn_ray_serve_any_good_resources/",
      "author": "FreshIntroduction120",
      "created_utc": "2026-02-13 15:25:21",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\nI‚Äôm trying to learn Ray Serve for model deployment and scaling, but I‚Äôm a bit lost on where to start.\nIf you‚Äôve learned it before, how did you do it? Are there any good tutorials, courses, blog posts, or documentation that really helped you?\nAlso, if you know any good YouTube channels or resources to learn more about MLOps in general (model serving, deployment, infrastructure, etc.), I‚Äôd really appreciate it.\nI‚Äôm looking for practical, real-world examples if possible.\nThanks a lot üôè",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r3s22c/how_did_you_learn_ray_serve_any_good_resources/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o56zfwj",
          "author": "Delicious-One-5129",
          "text": "I learned it by working straight through the Ray Serve docs and deploying a small project end to end.\n\nI started with a single local model, then added replicas and autoscaling, and finally containerized it and ran it on a small cluster. For broader MLOps context, the main Ray docs and hands on demos combining it with FastAPI, Docker, and Kubernetes helped a lot.",
          "score": 2,
          "created_utc": "2026-02-13 17:03:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c8y6w",
              "author": "ConsciousML",
              "text": "Second this! Always work from the official docs and then use additional content when necessary.",
              "score": 1,
              "created_utc": "2026-02-14 13:57:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r68h1l",
      "title": "MLflow on Databricks End-to-End Tutorial | Experiments, Registry, Serving, Nested Runs",
      "subreddit": "mlops",
      "url": "https://youtu.be/9AenofD8GZ8",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-02-16 12:35:18",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r68h1l/mlflow_on_databricks_endtoend_tutorial/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": [
        {
          "id": "o5pd2m7",
          "author": "penguinzb1",
          "text": "mlflow experiment tracking gets tricky when you're trying to validate model behavior under different data conditions. logging metrics is the easy part, but testing whether your registered model actually handles edge cases correctly is where most pipelines fall apart.\n\ncurious how you're handling regression testing for models in the registry. do you have automated checks that run when a new version gets registered, or is it more manual validation before moving to serving?\n\nthe nested runs setup is interesting for hyperparameter sweeps. we've been testing agents that optimize ml workflows and the hard part is catching when a sweep finds technically better metrics but the model actually performs worse on production-like scenarios.",
          "score": 2,
          "created_utc": "2026-02-16 16:13:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r87p2u",
      "title": "The Human Elements of the AI Foundations",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/the-human-elements-of-the-ai-foundations",
      "author": "growth_man",
      "created_utc": "2026-02-18 16:30:28",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r87p2u/the_human_elements_of_the_ai_foundations/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": []
    }
  ]
}