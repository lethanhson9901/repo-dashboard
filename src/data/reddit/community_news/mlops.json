{
  "metadata": {
    "last_updated": "2026-02-15 16:43:41",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 16,
    "total_comments": 40,
    "file_size_bytes": 69177
  },
  "items": [
    {
      "id": "1r3kxcd",
      "title": "What YouTube content actually helped you in your MLOps journey? And what's still confusing?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r3kxcd/what_youtube_content_actually_helped_you_in_your/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-13 09:37:26",
      "score": 31,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "I've been in the ML/DevOps space for 11+ years and recently started doing 1:1 calls helping people transition into MLOps. One thing I keep noticing, almost everyone I talk to is overwhelmed.\n\nNot because they're not smart. But because MLOps is so vast, with batch vs. real-time ML pipelines, inference, infrastructure, and monitoring, every course teaches it differently. One guy will say start with Kubeflow, another says MLflow, another says forget tools, learn fundamentals first.\n\nI genuinely want to understand from this community:\n\n1. When you search MLOps on YouTube, what kind of videos do you actually watch fully? Tool-specific tutorials? Career roadmaps? Architecture walkthroughs?\n2. What's your biggest struggle right now â€” is it picking the right tools? Understanding how pieces connect end to end? Or knowing what the market actually wants vs what courses teach?\n3. Is there a video or channel that genuinely helped you \"get it\"? Not just theory, but actually made something click?\n4. What's missing? What video do you wish existed but doesn't?\n\nAsking because I see so much content out there, but people on my calls are still confused. \n\nSomething is clearly not working. Curious what you all think.\n\nI've been thinking of creating some content on this myself, but before that, I just want to understand the current situation and where people are really stuck. No point in adding more noise if the real gap is elsewhere.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r3kxcd/what_youtube_content_actually_helped_you_in_your/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o557kiv",
          "author": "ConsciousML",
          "text": "Youtube was not very helpful on my side for this as thereâ€™s very little quality content in my opinion.\n\nWhat worked best for me is reading quality articles:\n- [ml-ops.org](https://ml-ops.org/) is great for the basics\n- [Neptune.ai](https://neptune.ai/blog) is great but they surf a lot on the GenAI wave so I donâ€™t read much anymore\n- [ZenML](https://docs.zenml.io/stacks) has amazing doc for the MLOps components (experiment tracker, feature store, orchestrator, etc.)\n- [Hopsworks](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines) explains the three pipeline paradigm better than anyone else\n\n\nOnce you know the theory well and you had some good hands-on experience, Iâ€™ve found that the big tech engineering blogs are the best source of trusted information.\n\nIâ€™ve compiled a list of [interesting blogs](https://foremost-tea-3e3.notion.site/Resources-1973205f20be80d6923bd4a18ee62cd6?source=copy_link).\n\nI also try my best to share useful content on my [personal blog](https://www.axelmendoza.com/) if thatâ€™s helpful!\n\nIâ€™m really interested in your process to try helping people getting into MLOps.\n\nDM me if you want to share some thoughts!",
          "score": 23,
          "created_utc": "2026-02-13 11:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56d3gt",
              "author": "Neither_Film_8641",
              "text": "I like your Blog!",
              "score": 3,
              "created_utc": "2026-02-13 15:17:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56mm3h",
                  "author": "ConsciousML",
                  "text": "Thanks man! I enjoy to write. Even better if itâ€™s helpful ;)",
                  "score": 1,
                  "created_utc": "2026-02-13 16:02:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55w9ul",
          "author": "TruDanceCat",
          "text": "DataCamp is my go-to. Tons of great courses on ML and MLOPS. Theory videos, coding exercises, labs, and practice quizzes all based on career tracks like ML, MLOps, Data Scientist, Data Analytics, Data Engineering and more.\n\nWe have a learning budget at work, so they reimburse us for the cost, but I would pay for the subscription myself if they didnâ€™t- itâ€™s totally worth it.",
          "score": 4,
          "created_utc": "2026-02-13 13:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dmkaw",
              "author": "JayRathod3497",
              "text": "What's a subscription piece ?",
              "score": 1,
              "created_utc": "2026-02-14 18:18:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55nfka",
          "author": "DifficultDifficulty",
          "text": "I found AWS/GCP tech blogs and OSS repos useful, particularly those laying out architecture blueprints",
          "score": 2,
          "created_utc": "2026-02-13 12:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56sp6s",
          "author": "Inevitable_Resort902",
          "text": "I am a SWE looking to transition into MLOps and would love to get some advice. Can I get in touch with you?",
          "score": 1,
          "created_utc": "2026-02-13 16:31:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5739xf",
              "author": "Extension_Key_5970",
              "text": "sure, you can DM me",
              "score": 1,
              "created_utc": "2026-02-13 17:22:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o59do9g",
          "author": "Ambitious-Estate4356",
          "text": "I would say claude learning + claude code. They try to make you learn each and every concepts from ground up with some example and code.",
          "score": 1,
          "created_utc": "2026-02-14 00:25:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g5zzf",
              "author": "CupFine8373",
              "text": "what is claude learning ? ",
              "score": 1,
              "created_utc": "2026-02-15 03:12:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1hcgr",
      "title": "What's your Production ML infrastructure in 2026?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1hcgr/whats_your_production_ml_infrastructure_in_2026/",
      "author": "Repulsive_Ad_9950",
      "created_utc": "2026-02-10 23:46:35",
      "score": 27,
      "num_comments": 14,
      "upvote_ratio": 0.94,
      "text": "I'm currently studying the tools generally associated with MLOps. Some stuff seem to be non-negotiable: Cloud provides like AWS, GCP and Azure, Kubernetes, Docker, CI/CD and monitoring/observability. I'd like to hear about the tooling your company use to handle ML workflows, so I can have some direction in my studies. Here are my questions. \n\n**CI/CD**  \nGithub Actions, GitLab or other? Do you use different CI/CD tools depending for training and deployment?\n\n**Orchestration for training models**  \nWhat actually runs your training jobs? Airflow, Prefect, Kubeflow Pipelines, Argo, or something else?  \nHow does the flow work? For example, GitHub Actions -> Airflow DAG -> SageMaker job, or the pipeline occur integrated within Kubeflow?  \n\n\n**Serving**  \nAre your inference endpoints deployed with FastAPI, KServe or other (like Lambda)? I heard that KServe has the advantage of batching requests, which is more compute-efficient for fetching data from database, feature engineering, and making predictions, and as well as some automated A/B and canary deployments, which seems a big advantage to me. \n\n**Monitoring and Observability**  \nCloud-nativa services like CloudWatch or Prometheus+Grafana?\n\n**Integrated or Scattered?**  \nAll-in on one platform (Kubeflow end-to-end, or everything in SageMaker), or scattered (Airflow/Prefect, Kubernetes, etc)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r1hcgr/whats_your_production_ml_infrastructure_in_2026/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4qvk1q",
          "author": "pmv143",
          "text": "For serving LLMs at scale, the stack starts to diverge from traditional ML serving pretty quickly.\n\nFastAPI or KServe work fine for classic models, but large LLM inference introduces different constraints:\n\nâ€“ GPU residency and memory fragmentation\nâ€“ cold start latency\nâ€“ multi model scheduling on shared GPUs\nâ€“ batching vs interactive latency tradeoffs\n\nIn our case we ended up building a snapshot based runtime specifically for inference. Instead of treating endpoints as long lived pods, we snapshot the fully initialized CUDA graph and restore directly into GPU memory. That lets us scale to zero without warm pools and still keep cold start under ~2 seconds for 70B class models.\n\nMonitoring wise we still rely on Prometheus style metrics and GPU level telemetry, but the core efficiency gains come from how the runtime manages GPU state rather than from orchestration layers.\n\nThe training stack can stay fairly conventional. Inference is where the architecture really changes.",
          "score": 11,
          "created_utc": "2026-02-11 04:39:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rrcin",
              "author": "elsatan666",
              "text": "That snapshot approach sounds very cool, was that something you built yourselves or is it off the shelf?",
              "score": 1,
              "created_utc": "2026-02-11 09:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4s8275",
                  "author": "pmv143",
                  "text": "We built it ourselves.\n\nThere isnâ€™t really an off-the-shelf solution that snapshots a fully initialized CUDA graph and restores it directly into GPU memory for large LLMs.\n\nContainer images snapshot disk state.\nCheckpointing saves weights.\nBut neither preserves the full runtime state required for fast GPU restore.\n\nThis took several years of work around GPU memory management, CUDA initialization, and state restoration. Itâ€™s very inference-specific. We didnâ€™t touch the training stack much. The real complexity was managing GPU residency and multi-model state transitions safely.\n\nStill early, but itâ€™s been working well for 30Bâ€“300B class models so far.",
                  "score": 2,
                  "created_utc": "2026-02-11 11:44:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ppj8q",
          "author": "ixrequalv",
          "text": "GitHub, harness for cicd\nSagemaker for everything it can, also bedrock and lambdas / other AWS services \nPrometheus cloud watch and grafana",
          "score": 2,
          "created_utc": "2026-02-11 00:19:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qu00i",
          "author": "Scared_Astronaut9377",
          "text": "\nLet me better describe what makes sense and what makes less sense. \n\n> Some stuff seem to be non-negotiable: Cloud provides\n\nI mean, there are many jobs that are MLOps in nature that require Spark, Flink, etc. instead of clouds. But they typically hire Data Engineers to do that lol.\n\n> Github Actions, GitLab or other? \n\nBoth work + many other building tools including those in major clouds.\n\n> For example, GitHub Actions -> Airflow DAG -> SageMaker job, or the pipeline occur integrated within Kubeflow? \n\nBoth work, action -> airflow -> training job is a very solid pattern for many use cases.\n\n> Are your inference endpoints deployed with FastAPI, KServe\n\nDepends on the requirements and the framework. Sometimes just throwing things in FastAPI is ok, sometimes you want, say, tf serving. I wouldn't touch lambda, always make your own containers. KServe is indeed solid for many use cases.\n\n> Cloud-nativa services like CloudWatch or Prometheus+Grafana?\n\nMostly what the company is already using lol. If I am designing from scratch, yeah, prometheus+grafana. \n\n> Kubeflow end-to-end\n\nKubeflow pipelines are good. Kubeflow as a platform is garbage. \n\n> everything in SageMaker\n\nWhen I was new to MLOps in cloud. Never again. First-class sitizen cloud services + k8s.",
          "score": 2,
          "created_utc": "2026-02-11 04:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xy6fx",
          "author": "NoobZik",
          "text": "Kedro + MLFlow + Airflow + NannyML\nScattered but cloud agnostic\n\nFor inference Iâ€™m using FastAPI",
          "score": 1,
          "created_utc": "2026-02-12 07:04:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0tdmj",
      "title": "If you're struggling with ML foundations for MLOps, there's another path, the inference & serving side",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0tdmj/if_youre_struggling_with_ml_foundations_for_mlops/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-10 06:28:20",
      "score": 20,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "In my last post, I discussed the importance of ML foundations and Python as key aspects of MLOps. But I realised I left out the other side of the coin, one that's equally valid and may be a better fit for many of you.\n\nIf math and stats aren't your thing and you dread memorising gradient descent variants or probability distributions, hear me out: there's a whole side of MLOps where that's not the focus.\n\nThis side focuses on **model serving, inference optimisation, and production scaling**. \n\nCompanies need people who can:\n\n* Expose models via FastAPI\n* Optimise inference latency and throughput using vLLM, TensorRT, or Triton\n* Manage serving infrastructure with KServe, Seldon, or Ray Serve\n* Handle autoscaling, batching strategies, A/B deployments, and canary rollouts\n* Build observability, monitoring drift, tracking latency p99s, and managing GPU utilisation\n\nNone of this requires you to derive backpropagation from scratch. What it *does* require is strong production engineering instincts, the kind you already have if you've been in DevOps, SRE, or platform engineering.\n\nSo if you're coming from an infrastructure background and feel overwhelmed trying to learn ML theory just to break into MLOps, know that there's a legit path that maps directly to your existing skills. Inference at scale is genuinely hard engineering, and most ML teams desperately need people who can do it well.\n\nThe ML foundations will come naturally over time through exposure. You don't need to master them before you start contributing meaningfully.\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:Â [topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0tdmj/if_youre_struggling_with_ml_foundations_for_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4kodi4",
          "author": "Extension_Key_5970",
          "text": "For those who are thinking, on how to start and where to explore, read my detailed blog post: [https://medium.com/@thevarunfreelance/you-dont-need-to-master-ml-theory-to-break-into-mlops-here-s-the-other-path-no-one-talks-about-56bc6fb45319](https://medium.com/@thevarunfreelance/you-dont-need-to-master-ml-theory-to-break-into-mlops-here-s-the-other-path-no-one-talks-about-56bc6fb45319)",
          "score": 2,
          "created_utc": "2026-02-10 06:42:30",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4kvk18",
          "author": "--Thunder",
          "text": "Thank you for sharing, I was also looking to dive into mlOps stuff & come from a strong infra background.\n\nCan you recommend any book or some videos which might have helped you as well. I have read the doc on medium, itâ€™s precise & awesome.\n\nThank you ðŸ™ðŸ¼",
          "score": 1,
          "created_utc": "2026-02-10 07:48:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r312b5",
      "title": "The agent security landscape is kind of a mess and I'm not sure what to do about it",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r312b5/the_agent_security_landscape_is_kind_of_a_mess/",
      "author": "Exact-Literature-395",
      "created_utc": "2026-02-12 18:21:21",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "So my team has been pushing me to evaluate autonomous agents for some of our workflow automation. Specifically looking at OpenClaw since it has massive traction (something like 160k+ GitHub stars) and can connect LLMs to local files, browsers, Slack, Discord, etc. Our ops lead is really excited about using it to auto-triage the \\~200 support tickets we get daily, basically having it read incoming tickets, check our internal docs, and route them to the right team with a priority score. Also been talking about automating the data validation checks we run every Monday where someone manually compares CSV exports against our postgres tables. Tedious stuff that would be perfect for an agent.\n\nBut honestly? The more I dig into this, the more I want to pump the brakes.\n\nI stumbled across some security research that genuinely unsettled me. Apparently there are tens of thousands of OpenClaw instances just... exposed directly to the internet. But the number that really stopped me was this: something like 15% of community built skills contain malicious instructions. Prompts designed to download malware or steal data. And when these get flagged and removed, they apparently just reappear under new identities pretty quickly.\n\nThe project's own FAQ literally describes this as a \"Faustian bargain\" with no \"perfectly safe\" setup. I appreciate the honesty but also... what am I supposed to do with that? How do I bring this to my team without sounding like I'm just being obstructionist?\n\nWhat's frustrating from an MLOps perspective is that this completely changes how I think about threat modeling. We've spent so much time worrying about model poisoning, adversarial inputs, data drift. With agents though, the attack surface just explodes. Prompt injection could come through any email or webpage the agent processes. If someone compromises the agent itself they basically inherit every permission we've granted it. And the plugin ecosystem? Nobody has time to audit all that, but you're essentially running untrusted code with access to your systems.\n\nThere's also this concept I keep seeing called \"judgment hallucination\" where the agent appears trustworthy but lacks genuine reasoning, so users just... hand over more and more authority. That one hits different because I can already see how it would play out with some of the less technical folks on our team who already treat ChatGPT like its omniscient.\n\nI looked at some alternatives like AutoGPT and BabyAGI but they seem to have similar issues, maybe even less mature from a security standpoint. A coworker mentioned something called Agent Trust Hub that supposedly scans skills for hidden logic and data exfiltration patterns before you install them, still need to actually try it though. The usual advice seems to be run everything in containers, dont expose default ports, start with read only permissions and expand from there. Basically treat it like you would any untrusted code I guess.\n\nBut I'm genuinely torn. The capability is exciting and I get why leadership wants this. The current state of the ecosystem though... it feels like we'd be taking on a lot of risk that we're not equiped to manage yet. Maybe I'm being too conservative here.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r312b5/the_agent_security_landscape_is_kind_of_a_mess/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o53p7au",
          "author": "KarmaIssues",
          "text": "I suppose the only real change would have to be sandboxing it. That would atleast help with some of the more damaging actions.",
          "score": 1,
          "created_utc": "2026-02-13 03:27:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5aecc4",
          "author": "South-Opening-9720",
          "text": "You're not being conservative â€” you're threat-modeling correctly. I'd start with an 'assistive' agent first (summarize + classify + suggest KB links), keep it read-only, and lock it in a sandbox with tight egress + allowlisted tools; actions/handoffs only after evals + red-team prompt injection tests. I use chat data for the safer 'draft + route' step and it helps without giving it keys to the kingdom. What systems would it touch on day 1?",
          "score": 1,
          "created_utc": "2026-02-14 04:23:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fboic",
          "author": "CompelledComa35",
          "text": "You're right to pump the brakes. That 15% malicious skill rate is insane and judgment hallucination is a real problem. Sandboxing helps but doesn't solve prompt injection through emails/web content. Check out alice's caterpillar for agentic scanning, it surfaces these attack vectors before production. Run it against your planned workflows and show leadership the actual risk exposure with evidence. ",
          "score": 1,
          "created_utc": "2026-02-14 23:53:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r52ypf",
      "title": "Practical SageMaker + MLflow Stage/Prod Workflow for Small MLOps + DS Team?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r52ypf/practical_sagemaker_mlflow_stageprod_workflow_for/",
      "author": "ZeroSilver87",
      "created_utc": "2026-02-15 02:25:32",
      "score": 15,
      "num_comments": 1,
      "upvote_ratio": 0.95,
      "text": "Hey all â€” As the title says, looking for practical input from teams operating at a similar scale...\n\nWe have a small MLOps team supporting a small Data Science team... \\~4-6 per team. Weâ€™re enabling SageMaker + MLflow this year and trying to move toward more sustainable, repeatable ML workflows.\n\nHistorically, our ML efforts have been fairly ad hoc and home-grown. Weâ€™re now trying to formalize things and improve R&D velocity without overburdening either the DS team or our platform engineers.\n\nOne major constraint is that our DevOps/infra process is heavily gated. New AWS resources require approvals outside our teams and move slowly. So weâ€™re trying to design something clean and safe that doesnâ€™t require frequent new infrastructure or heavyweight process for each new model.\n\nIâ€™m aware of the AWS-recommended workflows, but they seem optimized for larger teams or environments with more autonomy than we have.\n\nSome Additional Context:\n\n* Data lake on S3 (queried via Athena)\n* Models are often entity-specific (i.e., many model instances derived from a shared training pipeline)\n\nCurrent thinking:\n\n* Non-Prod:\n   * EDA + pipeline development + model experimentation\n   * read-only access to prod archive data to remove need to set up complicated replication from prod to non-prod\n* Prod:\n   * Inference endpoints\n   * Single managed MLflow workspace\n      * DS can log runs + register models (from non-prod or local)\n      * Only a prod automation role can promote models to â€œProductionâ€\n      * Production Inference services only load models marked \"Production\"\n   * Automated retraining pipelines\n\nThoughts or suggestions on this setup?\n\nThe goal is to embed sustainable workflows and guardrails without turning this into a setup that requires large teams to support it.\n\nWould love to hear whatâ€™s worked (or failed) for teams in similar size ranges or if you have any good experience with AWS Sagemaker to suggest good workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r52ypf/practical_sagemaker_mlflow_stageprod_workflow_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5hxwfs",
          "author": "Bezza100",
          "text": "It seems good, you will need to invest significant time on the CI/CD for promoting to make sure it's robust. Also consider standard examples and templates for the DS team so there isn't too much refactoring to use your CI/CD.",
          "score": 1,
          "created_utc": "2026-02-15 12:34:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1tlnt",
      "title": "Need some suggestions on using Open-source MLops Tool",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1tlnt/need_some_suggestions_on_using_opensource_mlops/",
      "author": "NetFew2299",
      "created_utc": "2026-02-11 10:13:22",
      "score": 11,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "I am a Data scientist by Profession. For a project, I need to setup a ML Infrastructure in a local VM. I  am working on  A daily prediction /timeseries analysis. In the case of Open-Source, I have heard good things about ClearML (there are others, such as ZenML/MLrun), to my [knowledge.It](http://knowledge.It) is simply because it offers a complete MLops solution\n\nApart from this, I know I can use a combination of Mlflow, Prefect, Evidently AI, Feast, Grafana, as well. I want suggestions in case of ClearML, if any, on ease of use. Most of the Softwares claim, but I need your feedback.\n\nI am open to using paid solutions as well. My major concerns:\n\n1. Infrastructure cannot run on the cloud\n2. Data versioning\n3. Reproducible Experiment\n4. Tracking of the experiment\n5. Visualisation of experiment\n6. Shadow deployment\n7. Data drift",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r1tlnt/need_some_suggestions_on_using_opensource_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4up367",
          "author": "niek29",
          "text": "Hey! This is pretty much the exact problem weâ€™re building LUML to solve.\n\nhttps://github.com/luml-ai/luml\n\nWe already have experiment tracking and a deployment module you can self-host wherever you want, so the no-cloud constraint isnâ€™t a problem. Weâ€™re also building a new MLflow-like module with an easy transition to the full platform - centralized registry, deployments, and monitoring, all out of the box. Data drift monitoring is actively in development too.\n\nWeâ€™re onboarding early users right now and your use case is exactly what weâ€™re designing for. Happy to jump on a quick call to walk you through it, DM me if youâ€™re interested!",
          "score": 2,
          "created_utc": "2026-02-11 19:26:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4s4xi3",
          "author": "kayhai",
          "text": "It sounds like you are keen on ClearML. Iâ€™ve tried ML Flow (model registry and experiment tracking) + a scheduling tool of your choice (prefect, airflow or dagster etc). Iâ€™m not familiar with ClearML, may I ask which features of ClearML stands out to you?",
          "score": 1,
          "created_utc": "2026-02-11 11:19:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sb0a9",
              "author": "NetFew2299",
              "text": "It is simply because it offers a complete MLops solution.\n\n",
              "score": 2,
              "created_utc": "2026-02-11 12:07:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sbcu1",
          "author": "Fritos121",
          "text": "This is almost exactly what I came here looking for. A lot of focus on Cloud, but itâ€™s been a bit harder for me to find resources on how best to deploy locally. Thanks for asking the question!",
          "score": 1,
          "created_utc": "2026-02-11 12:09:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sqchp",
          "author": "Garbatronix",
          "text": "I have had positive experiences using LakeFS in conjunction with MinIO. It enables you to version data in a similar way to Git. With an MLFlow server, I can log all the relevant parameters, such as branch and ref. MLFlow enables models to be versioned and stored. An MLFlow Docker image can then be generated and easily deployed on a Docker host or Kubernetes. \n\nDrift detection and data visualisation can be implemented in Python scripts prior to training and stored as artefacts in MLFlow. I have created a custom Python model in MLFlow by generating my own Prometheus metrics. These can then be collected via Prometheus and visualised in Grafana.",
          "score": 1,
          "created_utc": "2026-02-11 13:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sru98",
          "author": "DifficultDifficulty",
          "text": "\"I need to setup a ML infrastructure in a local VM\" -> is this infra mostly for your own VM-local experiments, and is there no need to distribute workloads in the cloud where the infra would be shared by multiple team members?",
          "score": 1,
          "created_utc": "2026-02-11 13:52:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4qym",
              "author": "NetFew2299",
              "text": "No,, I don't need it for multiple teams....I just need to setup an API, currently being done with flask later being changed to fastapi.",
              "score": 1,
              "created_utc": "2026-02-12 13:15:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4z8nw7",
                  "author": "DifficultDifficulty",
                  "text": "I see. I've spoken to a few people who described a similar need to yours, and they spoke well about Kedro + MLFlow for this kind of VM-local experience. Please see [https://docs.kedro.org/en/stable/integrations-and-plugins/mlflow/](https://docs.kedro.org/en/stable/integrations-and-plugins/mlflow/) ",
                  "score": 1,
                  "created_utc": "2026-02-12 13:38:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yubra",
          "author": "SassFrog",
          "text": "My suggestion is that in order to use ClearML, or another tool reliably you likely need kubernetes set up beneath it (or another scheduler, which I'd recommend against). Once you have kubernetes you're 90% of the way there, you can easily delploy ClearML through a helmchart, another component besides ClearML or you could replace ClearML.",
          "score": 1,
          "created_utc": "2026-02-12 12:04:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tvtv",
          "author": "Iron-Over",
          "text": "You are missing explainability, understand which features determined the decision.Â \n\nA prediction store where you can store every prediction to map to an actual result down the road, which helps you create lots of labeled data for future training.Â \n\nYou may want a feature store.Â \n\nModel registry is highly recommended.Â \n\nNot sure if you need bias checks.Â \n\nAre you serving via batch or api?",
          "score": 1,
          "created_utc": "2026-02-12 18:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54ig61",
              "author": "NetFew2299",
              "text": "Serving via batchÂ ",
              "score": 1,
              "created_utc": "2026-02-13 07:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54h4zu",
          "author": "NoobZik",
          "text": "My stack cloud agnostic \nKedro, MLFlow, Airflow. \n\nMinio is dead actually so I shifted to rustfs",
          "score": 1,
          "created_utc": "2026-02-13 06:56:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0269t",
      "title": "What LLM workloads are people actually running asynchronously?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0269t/what_llm_workloads_are_people_actually_running/",
      "author": "NewClaim7739",
      "created_utc": "2026-02-09 11:50:20",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 0.87,
      "text": "Feels like most AI infra is still obsessed with latency when it isn't always the thing that moves the needle. The highest-volume workloads we're seeing are offline:\n\nâ€¢ eval pipelines  \nâ€¢ dataset labeling  \nâ€¢ synthetic data  \nâ€¢ document processing  \nâ€¢ research agents\n\nOnce you stop caring about milliseconds, the economics change completely.\n\nCurious what people here are running in batch vs realtime - and where the break-even tends to be?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0269t/what_llm_workloads_are_people_actually_running/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4f54ac",
          "author": "Otherwise_Wave9374",
          "text": "Totally seeing the same trend, batch wins. Research agents, doc pipelines, and evals are way more forgiving on latency, and you can do smarter scheduling (spot instances, queues, retries). The trick is getting idempotency and good observability so your agent runs are actually debuggable. I have some notes on async agent workloads and eval loops here: https://www.agentixlabs.com/blog/ What are people using for tracing in batch, OpenTelemetry or vendor tools?",
          "score": 3,
          "created_utc": "2026-02-09 12:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lhakg",
              "author": "NewClaim7739",
              "text": "For the model serving / inference API, we actually ended up building an internal batch tool for this because realtime pricing just didnâ€™t make sense - happy to share details if useful",
              "score": 1,
              "created_utc": "2026-02-10 11:14:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4s1s1n",
              "author": "burntoutdev8291",
              "text": "Bot vs bot",
              "score": 1,
              "created_utc": "2026-02-11 10:51:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4h1r9b",
          "author": "penguinzb1",
          "text": "eval pipelines are the big one for us. we've been working on simulating agent runs to catch issues before they hit production, saves a lot of headaches when you can batch test 100s of scenarios overnight instead of waiting for users to hit edge cases",
          "score": 3,
          "created_utc": "2026-02-09 18:15:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfpmi",
              "author": "NewClaim7739",
              "text": "Super interesting - evals seem to be one of the main use cases. What are you using as the inference API?",
              "score": 1,
              "created_utc": "2026-02-10 11:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ghdmg",
          "author": "AIML_Tom",
          "text": "The common theme is that theyâ€™re not waiting on a human in real time â€” theyâ€™re queued, retried, and scaled out with workers. Synchronous chat is the flashy demo, but async workloads are where throughput and reliability matter most.",
          "score": 2,
          "created_utc": "2026-02-09 16:38:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfmfi",
              "author": "NewClaim7739",
              "text": "Massively agree - 'set and forget' your agent and come back to the results you asked for when you've done other tasks",
              "score": 2,
              "created_utc": "2026-02-10 10:59:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0tb2u",
      "title": "Lessons from Analyzing 18,000 Exposed Agent Instances",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0tb2u/lessons_from_analyzing_18000_exposed_agent/",
      "author": "RevealNoo",
      "created_utc": "2026-02-10 06:24:20",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I work on security research at Gen Threat Labs, and we recently wrapped up an analysis of autonomous AI agents in production that I wanted to share. Specifically focused on OpenClaw given its popularity (165k GitHub stars and growing fast).\n\nQuick caveat upfront: our methodology has limitations. We scanned for exposed instances and analyzed publicly available community skills, but we don't have visibility into properly secured deployments or private enterprise setups. We also couldn't verify intent behind everything we flagged, so some of what we classified as malicious might just be poorly written code with bad patterns. Take the numbers with that context.\n\nThat said, what we found was worse than I expected going in.\n\nWe identified over 18,000 OpenClaw instances exposed directly to the internet. Not behind VPNs, not containerized, just sitting on default port 18789 accepting connections. One instance we found had full access to the user's email, calendar, and file system. Just... open. That one stuck with me because it's exactly the kind of setup that makes agents useful, and exactly what makes them dangerous.\n\nBut the finding that actually surprised me was in the community skill ecosystem. We analyzed hundreds of skills that users build and share, and nearly 15% contained what I'd classify as malicious instructions. Some were designed to download external payloads, others to exfiltrate data. A few had hidden logic that only triggered after repeated uses, which made them harder to catch in initial review.\n\nWe spent a while trying to use static analysis to catch these automatically, but the false positive rate was brutal. Ended up needing a mix of pattern matching and actually running skills in sandboxed environments to see what they do. Still not perfect.\n\nWe also noticed something frustrating: malicious skills that got flagged and removed would reappear under different names within days. Same payload, new identity. Whack a mole.\n\nThe attack pattern we kept seeing is what I've started calling \"Delegated Compromise.\" Instead of targeting the user directly, adversaries target the agent. Once they get in through prompt injection or a poisoned skill, they inherit every permission that user granted. It's honestly elegant from an attacker's perspective.\n\nTo OpenClaw's credit, their docs are transparent about this. They literally describe it as a \"Faustian bargain\" and acknowledge no perfectly safe setup exists. I respect that honesty, but I don't think most users deploying these agents fully internalize what that means.\n\nThe risk vectors we kept categorizing:\n\nâ€¢ Expanded attack surface from agents with read/write/execute across multiple applications â€¢ Prompt injection through messages and web content with hidden instructions â€¢ Supply chain risk from community skills built without security review â€¢ System level impact when broadly permissioned agents get compromised â€¢ What I've been calling \"judgment hallucination\" where agents appear trustworthy but lack genuine reasoning, so users over delegate\n\nIf you're running agents in production, the practical stuff that actually matters:\n\nâ€¢ Isolated environments (VMs or containers), not your primary machine â€¢ Don't expose default ports to public internet (seems obvious but 18,000 instances say otherwise) â€¢ Start read only, expand permissions incrementally â€¢ Secondary accounts during testing â€¢ Actually review activity logs, not just set and forget â€¢ Treat third party skills like installing unknown software, because that's basically what it is\n\nThe detection stuff we built for catching the hidden logic patterns, we've been calling it Agent Trust Hub internally. Happy to compare notes if anyone's working on similar approaches or has found better ways to handle the false positive problem.\n\nCurious how other teams are approaching this. Is agent security getting dedicated attention in your org, or is it still lumped in with general appsec? Trying to get a sense of whether this is becoming a recognized problem or if we're early to the panic.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0tb2u/lessons_from_analyzing_18000_exposed_agent/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4n5h9l",
          "author": "Informal_Tangerine51",
          "text": "18,000 exposed instances is concerning but the deeper problem is debuggability. When agent gets compromised via poisoned skill, can you reconstruct what it accessed and when?\n\nYou found malicious skills that trigger after repeated uses. That's nightmare fuel without audit trails. User runs skill 5 times safely, 6th execution exfiltrates data. Post-incident question: what did it access across all 6 runs? Most deployments can't answer without forensics.\n\n\"Judgment hallucination\" + over-delegation compounds when you can't verify agent decisions. User trusts agent with email access, agent gets compromised, attacker inherits permissions. Without signed traces of every action (what was accessed, what policy should have blocked it, when it happened), incident response is archaeology. Security gaps plus evidence gaps means compromise detection happens weeks late.",
          "score": 1,
          "created_utc": "2026-02-10 16:54:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1frz2",
      "title": "Learning AI deployment & MLOps (AWS/GCP/Azure). How would you approach jobs & interviews in this space?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1frz2/learning_ai_deployment_mlops_awsgcpazure_how/",
      "author": "c0bitz",
      "created_utc": "2026-02-10 22:44:19",
      "score": 7,
      "num_comments": 14,
      "upvote_ratio": 0.82,
      "text": "Iâ€™m currently learning how to deploy AI systems into production. This includes deploying LLM-based services to AWS, GCP, Azure and Vercel, working with MLOps, RAG, agents, Bedrock, SageMaker, as well as topics like observability, security and scalability.\n\nMy longer-term goal is to build my own AI SaaS. In the nearer term, Iâ€™m also considering getting a job to gain hands-on experience with real production systems.\n\nIâ€™d appreciate some advice from people who already work in this space:\n\nWhat roles would make the most sense to look at with this kind of skill set (AI engineer, backend-focused roles, MLOps, or something else)?\n\nDuring interviews, what tends to matter more in practice: system design, cloud and infrastructure knowledge, or coding tasks?\n\nWhat types of projects are usually the most useful to show during interviews (a small SaaS, demos, or more infrastructure-focused repositories)?\n\nAre there any common things early-career candidates often overlook when interviewing for AI, backend, or MLOps-oriented roles?\n\nIâ€™m not trying to rush the process, just aiming to take a reasonable direction and learn from people with more experience.\n\nThanks ðŸ™Œ",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1r1frz2/learning_ai_deployment_mlops_awsgcpazure_how/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4pc5cb",
          "author": "bad_detectiv3",
          "text": "How are you learning to do this OP",
          "score": 3,
          "created_utc": "2026-02-10 23:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfgj3",
              "author": "c0bitz",
              "text": "Mostly self-study + building small experiments. I try to avoid just watching courses and instead replicate simple pipelines end-to-end from training to deployment even if itâ€™s basic. Right now Iâ€™m more focused on understanding inference architecture and cost tradeoffs rather than just model building.",
              "score": 1,
              "created_utc": "2026-02-11 07:23:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pc20t",
          "author": "Otherwise_Wave9374",
          "text": "On the MLOps side, the \"agent\" specific stuff I see teams miss early is observability: log every tool call (inputs, outputs, latency), version prompts, and have a tiny golden eval set you can run on PRs.\n\nAlso, make the agent fail closed. If a tool is down or confidence is low, it should ask a clarifying question or hand off, not hallucinate.\n\nIf you want a few practical patterns for agent tracing/evals, I have been collecting notes here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-10 23:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfpih",
              "author": "c0bitz",
              "text": "Fail closed is such an underrated point. Iâ€™ve seen too many demos where agents just hallucinate confidently instead of degrading gracefully.The golden eval set on PRs is smart too, are you automating those checks in CI or running them manually?",
              "score": 1,
              "created_utc": "2026-02-11 07:25:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pk9v5",
          "author": "overemployed74737",
          "text": "In my J2 im working as MLOps engineer  and during my interview i just explained the entire lifecycle for any ml models and talk about some differents needs between some models. Explained a little about observability and performance drift too",
          "score": 2,
          "created_utc": "2026-02-10 23:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfisd",
              "author": "c0bitz",
              "text": "Thatâ€™s a good point. Iâ€™ve noticed lifecycle/system thinking comes up way more than specific tools. When you explained drift and observability, did they go deep into monitoring stack questions or keep it high level?",
              "score": 1,
              "created_utc": "2026-02-11 07:23:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pkgcl",
          "author": "Competitive-Fact-313",
          "text": "I this your spectrum atm is too broad, try to narrow down a learn specific things first and then widen the scope. Making AI saas is one things and working in Mlops is another. If you define well I can help better. To start small just play with a simple linear regression model on sagemaker  and use how many instances endpoints you wantâ€”->> take a lambda functionâ€”â€”>api gateway â€”-> test the api gateway endpoint using postman once done. Use your choice of frontend to show it as saas. This is the lowest level you can start with.",
          "score": 2,
          "created_utc": "2026-02-10 23:50:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfkqn",
              "author": "c0bitz",
              "text": "Thatâ€™s actually helpful. Breaking it down that way makes it less overwhelming. I was thinking too much in terms of â€œfull AI SaaSâ€ instead of just understanding one clean deployment path first. Did you find AWS interviews expect hands-on experience with those services or mostly conceptual understanding?",
              "score": 2,
              "created_utc": "2026-02-11 07:24:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4spi8o",
                  "author": "Competitive-Fact-313",
                  "text": "In aws interview it depends for seniors roles they may ask you  hands on or sometimes just ask you something from the the pipeline so that mean you must have had those done before thatâ€™s the only things makes you explain stuff",
                  "score": 2,
                  "created_utc": "2026-02-11 13:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pt1ae",
          "author": "burntoutdev8291",
          "text": "In my experience, hiring has shifted more to understanding requirements and system design. I have never used bedrock or sagemaker, places I work at usually run self hosted vLLM. It also depends on the job description, backend roles have a bit more coding and system design, MLOps asks you on more MLOps stuff like model lineage, tracing, observability, and maybe some sysadmin stuff related to GPU. Never worked as an agentic or prompt engineer kind of AI engineer so I can't comment on that.",
          "score": 1,
          "created_utc": "2026-02-11 00:39:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4repik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-11 07:16:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s13h1",
              "author": "c0bitz",
              "text": "Totally agree, practical demos always carry more weight. Iâ€™ve been focusing on getting code + infra clean for simple model endpoints before scaling.",
              "score": 1,
              "created_utc": "2026-02-11 10:45:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4dcq9",
      "title": "Transitioning into MLOps from API Gateway background â€” looking for realistic paths & pitfalls",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r4dcq9/transitioning_into_mlops_from_api_gateway/",
      "author": "EntropyTamer-007",
      "created_utc": "2026-02-14 06:26:58",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.82,
      "text": "Hi everyone,\n\nIâ€™m looking for advice from people actually working in MLOps / ML platform roles, especially those who transitioned from non-ML backgrounds.\n\nMy current background (honest assessment):\n\n\\~4 years of experience working with Axway API Gateway\n\nMost of my work has been configuration-focused (policies in Policy Studio)\n\nI understand concepts like OAuth2, JWT, rate limiting, traffic mediation, etc., but mainly at a conceptual / tool-usage level\n\nI havenâ€™t owned end-to-end systems, production ML pipelines, CI/CD, Kubernetes, or cloud infrastructure yet\n\nBeginner-level Python\n\nNo hands-on AWS/Azure/GCP or IaC experience so far\n\nSo while Iâ€™m not new to tech, Iâ€™m aware that my system ownership depth is limited.\n\nWhat Iâ€™m doing currently:\n\nIâ€™m enrolled in a Data Science with Generative AI course\n\nIâ€™m trying to avoid rushing into â€œML titlesâ€ without the necessary platform depth\n\nMy goal (longer-term):\n\nTransition into MLOps / ML Platform Engineering\n\nWork closer to model deployment, reliability, governance, and infrastructure, not pure research\n\nPrefer roles that are remote-friendly and have long-term growth\n\nFrom my background, \n\nwhat are the most realistic entry points into MLOps?\n\nIs it better to first transition into a Cloud / Platform / DevOps role and then move into MLOps, or are there viable direct bridges?\n\nWhich skills tend to be non-negotiable for MLOps roles that people often underestimate?\n\nWhat are common mistakes people make when trying to move into MLOps without prior ML ownership?\n\nIf you had to do this transition again, what would you focus on first vs ignore initially?\n\nIâ€™m deliberately trying to avoid hype-driven decisions and would really value advice grounded in real hiring and on-the-job experience.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r4dcq9/transitioning_into_mlops_from_api_gateway/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5bgie5",
          "author": "flyingPizza456",
          "text": "Common \"mistake\" (and mistake is not really the right word here) is to think that MLOps is something you do as a profession. It is like devops, it is about how to do things, not what things have to be done.\n\nIt is an approach, which still can be backed with specific tools, processes, skills, roles etc.\n\nYou do not transition into it in my opinion. It is more like you are an ML engineer or data scientist or infrastructure engineer or sulution architect or whatever. Even having a role named MLOps or Devops engineer is totally fine but your main tasks with regards to MLOps thinking is to support the reliability of services that are realized through Ai technologies (mostly ML as the name suggests).\n\nThen this comes down to you are doing machine learning or infrastructure work and you want to professionalize this even more.\n\nOne cannot say this often enough: MLOps is NOT FOR BEGINNERS\n\nDo all the other work that relates to it and after some years of experience you will realize what the important bits of the MLOps approach are. Also: MLOps is, like often times with other approaches, highly individual and has to be adapted for the setting / service / organization (factors could be size of service, number of people using the pipelines and many more)\n\nAnd: I really don't want to dampen your ambitions, but I think you'll find it easier if you don't view it as a dedicated profession, but rather as a task that must be fulfilled by everyone involved. That's why it's so difficult to introduce it into organisations and apply it successfully.",
          "score": 7,
          "created_utc": "2026-02-14 10:06:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1xg77",
      "title": "Hello every one! ðŸ‘‹",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1xg77/hello_every_one/",
      "author": "3MR_MLops",
      "created_utc": "2026-02-11 13:31:16",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 0.75,
      "text": "Hi everyone! Iâ€™m Amr, a 17-year-old aspiring MLOps Engineer from Egypt. Iâ€™ve already covered Python, SQL, Linux, Git/GitHub, and some FastAPI. I recently finished the first two courses of Andrew Ngâ€™s Machine Learning Specialization in just 7 days! To make sure I truly understood the concepts, I applied what I learned in two projects which you can find here: https://github.com/3MR-MLops/my_project_of_ML\n\nHere is my upcoming plan for the next few weeks: 1. Finish Andrew Ngâ€™s 3rd ML course. 2. Deep Learning Specialization. 3. Advanced FastAPI. 4. Docker & Containerization. 5. CI/CD Pipelines. 6. MLflow (Experiment Tracking). 7. Cloud (AWS). 8. Kubernetes (k8s).\n\nMy goal is to be \"Production-ready\" for international internships. Does this order make sense? Is there anything I should add or change to stand out more to recruiters?\n\nThanks for your guidance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r1xg77/hello_every_one/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4srdbn",
          "author": "dukesb89",
          "text": "I think your plan is good. Try and add more projects to your GitHub portfolio to demonstrate your skills. This is the key way to differentiate yourself when you have no work experience.",
          "score": 1,
          "created_utc": "2026-02-11 13:50:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4srxwt",
              "author": "3MR_MLops",
              "text": "ok thanks bro \nI'm trying to add at least one project to everything I'm learning now, and eventually I'll work on 3 or more projects like a chatbot, a product suggestion project, videos, and so on.",
              "score": 1,
              "created_utc": "2026-02-11 13:53:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4sskmo",
                  "author": "dukesb89",
                  "text": "Instead of doing lots of small projects, do a few bigger ones. The one you have now is a good start from a learning perspective but too simple to get a hiring manager to take notice. You will want to get to a point where you are engineering a full system. It will take time",
                  "score": 1,
                  "created_utc": "2026-02-11 13:56:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2039h",
      "title": "Migrating from Slurm to Kubernetes",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/kubernetes/comments/1r202tx/migrating_from_slurm_to_kubernetes/",
      "author": "alex000kim",
      "created_utc": "2026-02-11 15:17:48",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r2039h/migrating_from_slurm_to_kubernetes/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r21sp7",
      "title": "A question for seniors",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r21sp7/a_question_for_seniors/",
      "author": "3MR_MLops",
      "created_utc": "2026-02-11 16:21:58",
      "score": 5,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "If you are now HR\nWhat is the one thing that you rarely see in entry-level employee files that would make you want to hire someone immediately? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r21sp7/a_question_for_seniors/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4tm5jn",
          "author": "dayeye2006",
          "text": "My hiring manager told me they know this person and we must have this person at our company",
          "score": 3,
          "created_utc": "2026-02-11 16:24:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tmi5g",
              "author": "3MR_MLops",
              "text": "Who is he?",
              "score": 1,
              "created_utc": "2026-02-11 16:26:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4toe6l",
                  "author": "dayeye2006",
                  "text": "A hypothetical person",
                  "score": 1,
                  "created_utc": "2026-02-11 16:35:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o502t0x",
          "author": "Hyperventilater",
          "text": "Accurate representation of contributions with numerical impact, personal projects that show legitimate interest rather than just ambition, any personalization that highlights a strong desire to learn. Those really signify a junior that will bring innovation and talent to the role to me.",
          "score": 3,
          "created_utc": "2026-02-12 16:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o509v2m",
              "author": "3MR_MLops",
              "text": "This is gold! I never realized how much 'numerical impact' outweighs just listing skills for a junior. It makes total sense to show how a project actually solved a problem or improved a metric. Iâ€™m definitely going to apply this mindset to my current MLOps learning path. Much appreciated!",
              "score": 3,
              "created_utc": "2026-02-12 16:43:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2t5sn",
      "title": "Seeking deep 1:1 mentoring (Databricks / Snowflake / Azure ML)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r2t5sn/seeking_deep_11_mentoring_databricks_snowflake/",
      "author": "Admirable-Crab-9908",
      "created_utc": "2026-02-12 13:17:02",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.75,
      "text": "Looking for structured 1:1 mentoring to go from implementation-level expertise to platform-level mastery.\n\nFocus areas:\n\n\tâ€¢\tDatabricks MLOps (Unity Catalog, MLflow, CI/CD, governance)\n\n\tâ€¢\tSnowflake ML (Snowpark ML, feature pipelines, deployment patterns)\n\n\tâ€¢\tAzure ML (enterprise pipelines, model serving, security)\n\nKindly DM. Will be happy to pay hourly. ",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r2t5sn/seeking_deep_11_mentoring_databricks_snowflake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4zpz76",
          "author": "kchandank",
          "text": "I will DM you",
          "score": 2,
          "created_utc": "2026-02-12 15:10:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4f6uw",
      "title": "Passed NVIDIA NCA-AIIO and now need Guidance for NCP-AII",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r4f6uw/passed_nvidia_ncaaiio_and_now_need_guidance_for/",
      "author": "Sufficient_Berry_311",
      "created_utc": "2026-02-14 08:14:04",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.72,
      "text": "Hello Everyone \n\nI had passed the NCA-AIIO on 12th Feb 2026. The questions are simple and you can pass the exam using your logic. You can ask me questions about the exam. I have used notebooklm for study, if you want I can give it also. \n\n  \nI need help to clear the NCP-AII. Is there any person here who cleared it. I wanted to know how hard is this and how many questions we need to solve in cli (provided in exam) or is there any lab related work?  \n  \nAny help from where I will get the lab access?\n\nThank you ðŸ™  ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r4f6uw/passed_nvidia_ncaaiio_and_now_need_guidance_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5cw4cw",
          "author": "Fantastic-Chicken748",
          "text": "Hey!!! Iâ€™m also studying to ncp-aai. If you have any tips that you want to share, I would be very much appreciated",
          "score": 2,
          "created_utc": "2026-02-14 16:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dqe4c",
              "author": "Sufficient_Berry_311",
              "text": "I am from infrastructure side not from application side, cannot help on that. You can follow the nvidia study guide to notebooklm and you can ask question.\nNCA-AIIO is very easy, if you have knowledge how gpu works thats it.",
              "score": 2,
              "created_utc": "2026-02-14 18:37:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3s22c",
      "title": "How did you learn Ray Serve? Any good resources?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r3s22c/how_did_you_learn_ray_serve_any_good_resources/",
      "author": "FreshIntroduction120",
      "created_utc": "2026-02-13 15:25:21",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\nIâ€™m trying to learn Ray Serve for model deployment and scaling, but Iâ€™m a bit lost on where to start.\nIf youâ€™ve learned it before, how did you do it? Are there any good tutorials, courses, blog posts, or documentation that really helped you?\nAlso, if you know any good YouTube channels or resources to learn more about MLOps in general (model serving, deployment, infrastructure, etc.), Iâ€™d really appreciate it.\nIâ€™m looking for practical, real-world examples if possible.\nThanks a lot ðŸ™",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r3s22c/how_did_you_learn_ray_serve_any_good_resources/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o56zfwj",
          "author": "Delicious-One-5129",
          "text": "I learned it by working straight through the Ray Serve docs and deploying a small project end to end.\n\nI started with a single local model, then added replicas and autoscaling, and finally containerized it and ran it on a small cluster. For broader MLOps context, the main Ray docs and hands on demos combining it with FastAPI, Docker, and Kubernetes helped a lot.",
          "score": 2,
          "created_utc": "2026-02-13 17:03:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c8y6w",
              "author": "ConsciousML",
              "text": "Second this! Always work from the official docs and then use additional content when necessary.",
              "score": 1,
              "created_utc": "2026-02-14 13:57:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}