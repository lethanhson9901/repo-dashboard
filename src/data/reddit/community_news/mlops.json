{
  "metadata": {
    "last_updated": "2026-01-02 00:59:52",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 7,
    "total_comments": 15,
    "file_size_bytes": 32597
  },
  "items": [
    {
      "id": "1pvygot",
      "title": "Feature Stores: why the MVP always works and that's the trap (6 years of lessons)",
      "subreddit": "mlops",
      "url": "https://mikamu.substack.com/p/feature-store-the-sprawl",
      "author": "Valuable-Cause-6925",
      "created_utc": "2025-12-26 07:24:13",
      "score": 24,
      "num_comments": 9,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pvygot/feature_stores_why_the_mvp_always_works_and_thats/",
      "domain": "mikamu.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw0uk45",
          "author": "stratguitar577",
          "text": "Sounds like the requirements were not defined at all before building. If you approach a feature store without planning on what it needs to solve, it‚Äôs pretty obvious you‚Äôll run into these problems. Point in time training data is a key requirement so not sure why it‚Äôs talked about as an afterthought.¬†",
          "score": 8,
          "created_utc": "2025-12-26 13:50:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0xm2i",
              "author": "Valuable-Cause-6925",
              "text": "I totally agree that dataset construction is a must-have capability for a Feature Store.  \n  \nIn practice, though, the teams care much more about \"getting to production\" first. Figuring out how to make sure that training is consistent with serving is a secondary concern that some teams solve and others leave to their DS counterparts.",
              "score": 1,
              "created_utc": "2025-12-26 14:10:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw13wxq",
                  "author": "stratguitar577",
                  "text": "That‚Äôs where the feature ‚Äúplatform‚Äù takes over vs just the storage piece feature stores deal with. It should abstract the compute to allow writing the transformation once and running across both training and serving contexts for eliminating skew.¬†",
                  "score": 1,
                  "created_utc": "2025-12-26 14:50:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1qynv",
          "author": "DangKilla",
          "text": "An acquaintance is implementing too many features before launching, instead of focusing on the problem being solved",
          "score": 2,
          "created_utc": "2025-12-26 16:57:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1t7jw",
              "author": "Valuable-Cause-6925",
              "text": "Feature Store related?",
              "score": 1,
              "created_utc": "2025-12-26 17:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwitrbv",
          "author": "seunosewa",
          "text": "This might as well be an advertisement for storing everything in standard relational databases like PostgreSQL and MySQL.",
          "score": 1,
          "created_utc": "2025-12-29 10:13:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwivbr9",
              "author": "Valuable-Cause-6925",
              "text": "By everything you mean what exactly?",
              "score": 1,
              "created_utc": "2025-12-29 10:27:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwjces7",
                  "author": "seunosewa",
                  "text": "Data.",
                  "score": 1,
                  "created_utc": "2025-12-29 12:50:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg54z",
      "title": "Production ML Serving Boilerplate - Skip the Infrastructure Setup",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pyg54z/production_ml_serving_boilerplate_skip_the/",
      "author": "Extension_Key_5970",
      "created_utc": "2025-12-29 07:39:01",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "MLOps engineer here. Built this after setting up the same stack for the 5th time.\n\nWhat it is:\n\nInfrastructure boilerplate for MODEL SERVING (not training). Handles everything between \"trained model\" and \"production API.\"\n\nStack:\n\n\\- MLflow (model registry)\n\n\\- FastAPI (inference API)\n\n\\- PostgreSQL + Redis + MinIO\n\n\\- Prometheus + Grafana\n\n\\- Kubernetes (tested on Docker Desktop K8s)\n\nWhat works NOW:\n\nFull stack via \\`docker-compose up -d\\`\n\nK8s deployment with HPA (2-10 replicas)\n\nEnsemble predictions built-in\n\nHot model reloading (zero downtime)\n\nE2E validation script\n\nProduction-grade health probes\n\nKey features for MLOps:\n\n\\- Stage-based deployment (None ‚Üí Staging ‚Üí Production)\n\n\\- Model versioning via MLflow\n\n\\- Prometheus ServiceMonitor for auto-discovery\n\n\\- Rolling updates (maxUnavailable: 0)\n\n\\- Resource limits configured\n\n\\- Non-root containers\n\n5-minute setup:\n\n\\`\\`\\`bash\n\ndocker-compose up -d\n\npython3 scripts/demo-e2e-workflow.py  # Creates model, registers, serves\n\n\\`\\`\\`\n\nProduction deploy:\n\n\\`\\`\\`bash\n\n./scripts/k8s-bootstrap.sh  # One-command K8s setup\n\n./scripts/validate-deployment.sh --env k8s\n\n\\`\\`\\`\n\nHonest question: What's the most significant pain point in your ML deployment workflow that this doesn't solve?\n\nGitHub: [https://github.com/var1914/mlops-boilerplate](https://github.com/var1914/mlops-boilerplate)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pyg54z/production_ml_serving_boilerplate_skip_the/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwz0cbl",
          "author": "pvatokahu",
          "text": "Nice stack choices. We've been running something similar at Okahu but ended up ditching MLflow for model registry after hitting some weird edge cases with versioning conflicts when multiple teams were pushing models. The prometheus + grafana setup looks solid though - that's one thing I wish more boilerplate repos would include by default.\n\nYour hot model reloading approach is interesting.. we tried implementing that but ran into memory leaks with certain model types (especially transformer-based ones). Had to build a whole sidecar pattern just to manage the lifecycle properly. How are you handling memory cleanup during the reload process? Also curious if you've tested this with models larger than a few GB - that's where things usually start breaking in my experience.",
          "score": 1,
          "created_utc": "2025-12-31 20:24:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1v6yi",
              "author": "Extension_Key_5970",
              "text": "These are a great set of realistic queries I was expecting. \n\nFor now, the boilerplate is relatively standard base infrastructure, which usually startups struggle to implement, or if they don't want to spend time on infra, but I will take these as an enhancement, of course, to make things scalable, even if with larger models, and test memory leak scenarios specifically. \n\nAlso, I am interested in the edge cases that led you to ditch MLflow for a model registry.",
              "score": 1,
              "created_utc": "2026-01-01 08:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0xybe",
      "title": "Finally released my guide on deploying ML to Edge Devices: \"Ultimate ONNX for Deep Learning Optimization\"",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q0xybe/finally_released_my_guide_on_deploying_ml_to_edge/",
      "author": "meet_minimalist",
      "created_utc": "2026-01-01 06:49:59",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI‚Äôm excited to share that I‚Äôve just published a new book titled¬†**\"Ultimate ONNX for Deep Learning Optimization\"**.\n\nAs many of you know, taking a model from a research notebook to a production environment‚Äîespecially on resource-constrained edge devices‚Äîis a massive challenge. ONNX (Open Neural Network Exchange) has become the de-facto standard for this, but finding a structured, end-to-end guide that covers the entire ecosystem (not just the \"hello world\" export) can be tough.\n\nI wrote this book to bridge that gap. It‚Äôs designed for ML Engineers and Embedded Developers who need to optimize models for speed and efficiency without losing significant accuracy.\n\n**What‚Äôs inside the book?**¬†It covers the full workflow from export to deployment:\n\n* **Foundations:**¬†Deep dive into ONNX graphs, operators, and integrating with PyTorch/TensorFlow/Scikit-Learn.\n* **Optimization:**¬†Practical guides on Quantization, Pruning, and Knowledge Distillation.\n* **Tools:**¬†Using ONNX Runtime and ONNX Simplifier effectively.\n* **Real-World Case Studies:**¬†We go through end-to-end execution of modern models including¬†**YOLOv12**¬†(Object Detection),¬†**Whisper**¬†(Speech Recognition), and¬†**SmolLM**¬†(Compact Language Models).\n* **Edge Deployment:**¬†How to actually get these running efficiently on hardware like the Raspberry Pi.\n* **Advanced:**¬†Building custom operators and security best practices.\n\n**Who is this for?**¬†If you are a Data Scientist, AI Engineer, or Embedded Developer looking to move models from \"it works on my GPU\" to \"it works on the device,\" this is for you.\n\n**Where to find it:**¬†You can check it out on Amazon here:[https://www.amazon.in/dp/9349887207](https://www.amazon.in/dp/9349887207)\n\nI‚Äôve poured a lot of experience regarding the pain points of deployment into this. I‚Äôd love to hear your thoughts or answer any questions you have about ONNX workflows or the book content!\n\nThanks!\n\n[Book cover](https://preview.redd.it/q5wo0ag5poag1.jpg?width=970&format=pjpg&auto=webp&s=b76d4d84c9fa70e6630d1b776389623a75397afe)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q0xybe/finally_released_my_guide_on_deploying_ml_to_edge/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pxpzeu",
      "title": "Built a small production-style MLOps platform while learning FastAPI, Docker, and CI/CD ‚Äì looking for feedback",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pxpzeu/built_a_small_productionstyle_mlops_platform/",
      "author": "Melodic_Struggle_95",
      "created_utc": "2025-12-28 12:14:33",
      "score": 9,
      "num_comments": 9,
      "upvote_ratio": 0.8,
      "text": "I‚Äôve been learning MLOps and wanted to move beyond notebooks, so I built a small production-style setup from scratch.\n\n\n\nWhat it includes:\n\n\\- Training pipeline with evaluation gate\n\n\\- FastAPI inference service with Pydantic validation\n\n\\- Dockerized API\n\n\\- GitHub Actions CI pipeline\n\n\\- Swagger UI for testing predictions\n\n\n\nThis was mainly a learning project to understand how models move from training to deployment and what can break along the way.\n\n\n\nI ran into a few real-world issues (model loading inside Docker, environment constraints on Ubuntu, CI failures) and documented fixes in the README.\n\n\n\nI‚Äôd really appreciate feedback on:\n\n\\- Project structure\n\n\\- Anything missing for a ‚Äúreal‚Äù MLOps setup\n\n\\- What you‚Äôd add next if this were production\n\n\n\nRepo: [https://github.com/faizalbagwan786/mlops-production-platform](https://github.com/faizalbagwan786/mlops-production-platform)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pxpzeu/built_a_small_productionstyle_mlops_platform/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwcrjyb",
          "author": "raiffuvar",
          "text": "Not gonna put it in production. Split training and platform into different repos.\nGrab a real training repo, stick it here, and see what happens.",
          "score": 4,
          "created_utc": "2025-12-28 12:32:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcs8hh",
              "author": "Melodic_Struggle_95",
              "text": "Fair point. This repo isn‚Äôt meant to represent a final production setup, but a learning-focused MLOps platform where I can iterate end to end.\nIn a real production environment, I agree that training and serving would typically live in separate repos or at least separate deployment units, often owned by different teams.\nFor now, I kept them together to understand the full lifecycle and the interfaces between training, evaluation, and inference. My next step is to split training and serving and treat the trained model as an external artifact to the platform.\nAppreciate the feedback.",
              "score": 2,
              "created_utc": "2025-12-28 12:38:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh2vk2",
          "author": "BackgroundLow3793",
          "text": "Oh that's nice. Thank you. I'm learning MLOps recently too. I think next thing is MLFlow, understand why people use MLFlow. I mean it doesn't have to be MLFLow, but the core idea is tracking and model versioning I guess. \n\nThere is also a good article here: [https://docs.databricks.com/aws/en/machine-learning/mlops/mlops-workflow](https://docs.databricks.com/aws/en/machine-learning/mlops/mlops-workflow)",
          "score": 2,
          "created_utc": "2025-12-29 02:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjldtq",
              "author": "Melodic_Struggle_95",
              "text": "Thanks! Yeah, I‚Äôm on the same page. The main value is tracking and versioning, not MLflow itself. I‚Äôm planning to add that next, and the Databricks article looks solid. Appreciate you sharing it.",
              "score": 1,
              "created_utc": "2025-12-29 13:48:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgfa6",
          "author": "wallesis",
          "text": "Where's the \"platform\" part?",
          "score": 1,
          "created_utc": "2025-12-28 15:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdl5gc",
              "author": "Melodic_Struggle_95",
              "text": "Right now the ‚Äúplatform‚Äù part is still small by design. At this stage I‚Äôm focusing on building the core pieces first a clean training pipeline, an evaluation gate, a consistent model loading layer, and a serving API with clear contracts.The idea is to treat this as the foundation, and then gradually add real platform features like CI/CD, model registry, monitoring, and automated retraining. This repo shows the early platform core, not the final version.",
              "score": 1,
              "created_utc": "2025-12-28 15:40:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwozj3m",
          "author": "Significant-Fig-3933",
          "text": "How do you handle data lineage, code tracking, orchestration, and data/model monitoring?\nLike what exact data and what exact code (model design, features, etc) was the model trained with?\n\nAnd how do you coordinate/track retraining etc (ie orchestration)?",
          "score": 1,
          "created_utc": "2025-12-30 07:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoztqm",
              "author": "Melodic_Struggle_95",
              "text": "Right now the project tracks data and code through the training pipeline and Git commits, with retraining handled manually, and the next planned step is adding MLflow and orchestration to properly manage lineage, versioning, monitoring, and automated retraining.",
              "score": 2,
              "created_utc": "2025-12-30 07:27:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqwnyh",
          "author": "bad_detectiv3",
          "text": "Can you provide resource where you built up on MLOps material? I want to give this a try beyond using LLM in application use case",
          "score": 1,
          "created_utc": "2025-12-30 15:49:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyuobm",
      "title": "I got tired of burning money on idle H100s, so I wrote a script to kill them",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pyuobm/i_got_tired_of_burning_money_on_idle_h100s_so_i/",
      "author": "jordiferrero",
      "created_utc": "2025-12-29 18:54:21",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "[https://github.com/jordiferrero/gpu-auto-shutdown](https://github.com/jordiferrero/gpu-auto-shutdown)\n\nGet it running on your ec2 instances now forever:\n\n    git clone https://github.com/jordiferrero/gpu-auto-shutdown.git\n    cd gpu-auto-shutdown\n    sudo ./install.sh\n\n\n\n\n\nYou  \nknow  \nthe feeling in ML research. You spin up an H100 instance to train a model, go to sleep expecting it to finish at 3 AM, and then wake up at¬†9 AM. Congratulations, you just paid for 6 hours of the world's most¬†expensive space heater.\n\nI did this way too many times. I must run my own EC2 instances for research, there's no other way.\n\nSo I wrote a simple daemon that watches¬†nvidia-smi.\n\nIt‚Äôs not rocket science, but it‚Äôs effective:\n\n1. It monitors GPU usage every minute.\n2. If your training job finishes (usage drops compared to high), it starts a countdown.\n3. If it stays idle for 20 minutes (configurable), it kills¬†the instance.\n\n**The Math:**\n\nAn on-demand H100 typically costs around¬†$5.00/hour.\n\nIf you leave it idle for just¬†10 hours a day¬†(overnight + forgotten weekends + \"I'll check it after lunch\"), that is:\n\n* $50 wasted daily\n* up to $18,250 wasted per year per GPU\n\nThis script stops that bleeding. It works on AWS, GCP, Azure, and pretty much any Linux box with systemd. It even¬†checks if it's running on a cloud instance before shutting down so it doesn't accidentally¬†kill your local rig.\n\nCode is open source, MIT licensed. Roast my¬†bash scripting if you want, but it saved me a fortune.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pyuobm/i_got_tired_of_burning_money_on_idle_h100s_so_i/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwo5pms",
          "author": "kamelkev",
          "text": "Good budget solution, but limited. You can do better analysis by leveraging cloudwatch.\n\nYou have to configure collection of ‚Äúnvidia_gpu,‚Äù but once done you can do run historic analytics in a way you can‚Äôt really do with a script.\n\nFor example you could leverage a combination of cpu, network and gpu activity to truly understand if a job was done, or if it was simply a pipeline stage that had completed.\n\nWhich AMI are you using?",
          "score": 2,
          "created_utc": "2025-12-30 03:47:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9l4g",
          "author": "Dramatic_Hair_3705",
          "text": "Why don't you use apache airflow for task scheduling?",
          "score": 2,
          "created_utc": "2025-12-30 04:10:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpa3pl",
              "author": "Popular-Direction984",
              "text": "Because he can use a simple script. Why use some ‚Äúsolution‚Äù, where three lines of shell will do?:)",
              "score": 1,
              "created_utc": "2025-12-30 09:02:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpbjln",
                  "author": "Dramatic_Hair_3705",
                  "text": "You are a very smart guy.",
                  "score": 2,
                  "created_utc": "2025-12-30 09:15:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwmjedp",
          "author": "KeyIsNull",
          "text": "mmmm you might kill the instance in the middle of something (e.g. uploading the model somewhere), wouldn't it better to wrap your training script with a shell script that automatically shuts down the instance when the script is done?\n\n    #!/bin/bash\n    python my_training.py\n    shutdown\n\n  \nedit: code block",
          "score": 2,
          "created_utc": "2025-12-29 22:27:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmndid",
              "author": "jordiferrero",
              "text": "Absolutely. But I often use UI/fromtends that are always running so there are no scripts per se (e.g. ComfyUI)",
              "score": 2,
              "created_utc": "2025-12-29 22:48:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzupvx",
      "title": "Built spot instance orchestration for batch ML jobs‚Äîfeedback wanted",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pzupvx/built_spot_instance_orchestration_for_batch_ml/",
      "author": "HelpingForDoughnuts",
      "created_utc": "2025-12-30 21:44:57",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "Got tired of building the same spot instance handling code at work, so I made it a product. Submit a job, it runs on Azure spot VMs, handles preemption/retry automatically, scales down when idle.\nThe pitch is simplicity‚Äîmulti-GPU jobs without configuring distributed training yourself, no infrastructure knowledge needed. Upload your container, pick how many GPUs, click run, get results back.\nEarly beta. Looking for people who‚Äôve built this stuff themselves and can tell me what I‚Äôm missing. Free compute credits for useful feedback.\nRoast my architecture if you want, I can take it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pzupvx/built_spot_instance_orchestration_for_batch_ml/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwv2jdk",
          "author": "qwertying23",
          "text": "Have you tried ray ? It does this quite well",
          "score": 3,
          "created_utc": "2025-12-31 04:45:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy0692",
              "author": "HelpingForDoughnuts",
              "text": "Yeah, Ray is solid for distributed ML workloads, and Anyscale makes it more accessible.\n\nThe main difference is that Ray still requires learning the Ray framework - you‚Äôre writing Ray-specific code with decorators, clusters, etc. We‚Äôre targeting the layer above that: ‚ÄúI want to train a PPO agent to play Breakout‚Äù ‚Üí it just works, without learning new APIs.\n\nRay is great if you want that level of control and don‚Äôt mind the learning curve. We‚Äôre going after people who just want their training job to run without becoming Ray experts first.\n\nDifferent markets really - Ray for ML engineers, us for researchers/beginners who want to skip the infrastructure parts entirely.\n\nHave you used Anyscale? Curious how you found the setup experience.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2025-12-31 17:19:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwy4459",
          "author": "qwertying23",
          "text": "Yes I have used anyscale and their workspaces concept is pretty neat. I do agree with your points but in my experience we faced the same issue and got stuck in the future we had to redesign our entire stack again. The beauty of ray is the same code can run on your laptop a single gpu cluster or on 1000‚Äôs of gpu I would rather build this with ray from the get go rather than the way we did this which made distributed computing an after thought. But happy to chat and give feedback on your product.",
          "score": 1,
          "created_utc": "2025-12-31 17:38:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy4yff",
              "author": "HelpingForDoughnuts",
              "text": "That‚Äôs a really thoughtful point about distributed-first architecture. Your experience with having to redesign the entire stack later is exactly the kind of lesson that‚Äôs expensive to learn the hard way.\n\nYou‚Äôre absolutely right that Ray‚Äôs abstraction is powerful - write once, run anywhere from laptop to 1000 GPUs. And if we‚Äôre building orchestration that needs to scale, starting with Ray as the foundation makes way more sense than bolting on distributed later.\n\nThe differentiation would be more in the layer above Ray - instead of users learning Ray APIs and cluster management, they get the natural language interface that routes to Ray workloads under the hood. But you‚Äôre right that the underlying execution should be distributed-native from day one.\n\nI‚Äôd genuinely love to chat more about this. Your experience with both the technical implementation and the business realities is exactly what we need to hear. Happy to jump on a call if you‚Äôre interested - would love to get your perspective on where the real pain points are and how a Ray-based approach might solve them better.\n\nThanks for offering feedback - that kind of input from someone who‚Äôs actually built and scaled these systems is invaluable.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2025-12-31 17:43:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzfdgi",
      "title": "need guidance regarding mlops",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pzfdgi/need_guidance_regarding_mlops/",
      "author": "not_popular_to_know",
      "created_utc": "2025-12-30 11:05:17",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hello everyone,  \nI‚Äôm an engineering student with a physics background. For a long time, I wasn‚Äôt sure about my future plans, but recently I‚Äôve started feeling that machine learning is a great field for me. I find it fascinating because of the strong mathematics involved and its wide applications, even in physics.\n\nNow, I want to build a career in MLOps. So far, I‚Äôve studied machine learning and DSA and have built a few basic projects. I have a decent grasp of ML fundamentals and I‚Äôm currently learning more about AI algorithms.\n\nIf there‚Äôs anyone who can guide me on how to approach advanced concepts and build more valuable, real-world projects, I‚Äôd really appreciate your help.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1pzfdgi/need_guidance_regarding_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwqvfdn",
          "author": "Valeria_Xenakis",
          "text": "This is an answer I gave earlier for a different post. There maybe some irrelevant sections for you so please weed out the required points.  \n\n\nTo be honest, there are effectively zero MLOps jobs for freshers in the industry right now. It is a relatively new field and it is almost entirely dominated by experienced DevOps engineers or data engineers who have pivoted.\n\nAs a fresher, breaking in as a backend/software engineer is a much more realistic path. You should focus on building full stack projects to get your foot in the door first.\n\nOnce you are hired, learn MLflow since it is the industry standard, or just master whatever tools your company is specifically using. The best way to get into MLOps is to slowly take on those responsibilities by showing initiative within a backend role.\n\nAs for cloud providers, there is no point in trying to learn all three. Most companies use AWS, so just focus on that. You can look at the AWS MLOps certification later if you want less friction while working in the field, but get the foundational experience first.\n\nYou need to be a Devops engineer first and then only you can be an Mlops engineer. And Devops is not a fresher's task, usually you do those tasks after about 1.5 years of experience in the same project.",
          "score": 3,
          "created_utc": "2025-12-30 15:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr10h9",
              "author": "not_popular_to_know",
              "text": "\nSo can u tell me where to start and roadmap\nAs I have given very much time learning maths and all\nCan u tell me if I can build a carrier in ai/ml\nBtw thanks for your advice",
              "score": 1,
              "created_utc": "2025-12-30 16:10:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwr8q0q",
                  "author": "Valeria_Xenakis",
                  "text": "Hey, I don't want to give bad advice so I will keep it simple. That way you will still do more research from your side and not take whatever I say at face value.\n\nIf you are a student who has no industry experience, please don't constraint yourself to MLOps. IMO MLOps is a skill and DevOps is the job. Now you said you liked the mathematics part, sorry to say if that is what interests you, you would find DevOps really boring.\n\nThe kind of roles you are looking for are ML engineer, AI engineer etc. In  these roles you would be using models created by people in research segment to solve problems for your organization. These roles require maths and ml/ai but not in the foundational sense. They require your applied  maths and ai chops to use those techniques for the company and not come up with new models or techniques as such. These roles will 100 percent require MLOps knowledge but you would be expected to handle these only after you have 2-3 years of exp.\n\nAs a green horn right of the block, focus on full stack dev and create projects related to full stack dev of AI based projects. Basically aim for a backend/full stack engineer in any company that uses a lot of AI for their core product offering (Not the one which use AI as tools, the ones that use AI as their main product offering like Amazon, AirBnB, Walmart etc).\n\nAs you gain exp you will 100 percent need to take on MLOps responsibilities, but right now you are not cut out for that.\n\nPlease keep asking your doubts. I like to keep individual replies short, they are more engaging.",
                  "score": 3,
                  "created_utc": "2025-12-30 16:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}