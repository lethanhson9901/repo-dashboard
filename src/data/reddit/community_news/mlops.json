{
  "metadata": {
    "last_updated": "2026-02-06 02:55:57",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 18,
    "total_comments": 32,
    "file_size_bytes": 72261
  },
  "items": [
    {
      "id": "1qvhmjc",
      "title": "The weird mismatch in MLOps hiring that nobody talks about",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qvhmjc/the_weird_mismatch_in_mlops_hiring_that_nobody/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-04 06:58:18",
      "score": 58,
      "num_comments": 16,
      "upvote_ratio": 0.93,
      "text": "Something I've noticed after being in this space for a while, and mentioned in past weeks' posts as well.\n\nMLOps roles need strong infrastructure skills. Everyone agrees on that. The job descriptions are full of Kubernetes, CI/CD, cloud, distributed systems, monitoring, etc.\n\nBut the people interviewing you? Mostly data scientists, ML engineers, and PhD researchers.\n\nSo you end up in a strange situation where the job requires you to be good at production engineering, but the interview asks you to speak ML. And these are two very different conversations.\n\nI've seen really solid DevOps engineers, people running massive clusters, handling serious scale, get passed over because they couldn't explain what model drift is or why you'd choose one evaluation metric over another. Not because they couldn't learn it, but because they didn't realise that's what the interview would test.\n\nAnd on the flip side, I've seen ML folks get hired into MLOps roles and MAY struggle because they've never dealt with real production systems at scale.\n\nThe root cause I think is that most companies are still early in their ML maturity. They haven't separated MLOps as its own discipline yet. The ML team owns hiring for it, so naturally, they filter for what they understand: ML knowledge, not infra expertise.\n\nThis isn't a complaint, just an observation. And practically speaking, if you're coming from the infra/DevOps side, it means you kinda have to meet them where they are. Learn enough ML to hold the conversation. You don't need to derive backpropagation on a whiteboard, but you should be able to talk about the model lifecycle, failure modes, why monitoring ML systems is different from monitoring regular services, etc.\n\nThe good news is the bar isn't that high. A few weeks of genuine study go a long way. And once you bridge that language gap, your infrastructure background becomes a massive advantage, because most ML teams are honestly struggling with production engineering.\n\nCurious if others have experienced this same thing? Either as candidates or on the hiring side?\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:Â [topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qvhmjc/the_weird_mismatch_in_mlops_hiring_that_nobody/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3hxttg",
          "author": "BeatTheMarket30",
          "text": "I'm not surprised as often people doing the hiring - hiring managers and recruiters are also clueless. They are hiring people with skills they themselves don't have, so have no clue what questions to ask. Data scientists will of course ask questions on ML that nobody besides themselves needs to know much about.\n\nMost open positions are also junior ones, while this field could benefit from solid DevOps experience. They don't seem to grasp that transition from DevOps to MLOps is perhaps just a few weekends of study as these people don't need low level details/maths. They will not be implementing KServe, Dynamo, vLLM, Tensor-RT, they will be deploying these solutions and monitoring them.\n\nFor a data scientist, MLOps will not be interesting and they will likely leave.",
          "score": 20,
          "created_utc": "2026-02-04 08:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ka9em",
              "author": "teucros_telamonid",
              "text": "If you really want to understand the root problem, you need to think in terms of the business life cycle.\n\nStartups are just happy to secure some ML engineer, data scientist or any other similar title. The focus is to build something at least working, MLOps is not a priority yet. Even if they hire you, it will get ugly soon.\n\nScale-ups are at least starting to get in place there scale and quality becomes important, but they probably still too focused on their competitive advantage. Some AI feature that is, not something smooth and well running from day to day.\n\nI think the best places are way more mature companies which have experience with AI and want to turn it from a successful experiment to really stable service. But these are already quite successful, so not a lot of them in this field...",
              "score": 2,
              "created_utc": "2026-02-04 17:20:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ie23t",
          "author": "proof_required",
          "text": "ML + anything is pretty much a big umbrella of things. Same goes for dedicated ML engineers. They might be asked DevOps question and won't be hired since there might be someone out there in this market who knows it. It's just that the companies can be super picky! So yeah messing up 1-2 questions can reduce your chances of hiring in this market.",
          "score": 7,
          "created_utc": "2026-02-04 11:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k8ffi",
              "author": "eled_",
              "text": "In my experience it's more to do with the fact that a lot of companies, even in the \"AI\" space, have yet to understand what MLeng is really about. Probably even more so than MLOps which can have a more \"direct\" impact on operations when they're not around.\n\nOften MLE are just mapped to \"DS with a few SE chops\", when it's not \"DS without an explicit aversion for SE\". And it's plain wrong, and it has an opportunity cost, but it's what you tend to get when data-scientists select and manage them.",
              "score": 2,
              "created_utc": "2026-02-04 17:12:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i7oja",
          "author": "karthikjusme",
          "text": "This is my biggest worry when applying new jobs. I am good taking models to production, setting pipelines for training and evaluation and dataset creation but I lack many concepts inside LLM's. Hard to keep track of them as well as there is a new thing every week.",
          "score": 6,
          "created_utc": "2026-02-04 10:06:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3izf35",
          "author": "Beneficial_Aioli_797",
          "text": "The idea i have about MLOps is that no one wants to do it. Everyone wants to do data science or machine learning engineering because thats whats most attractive. As result, these jobs are much more competitive and MLOps or infra roles adjacent to ML are left In the shadows.\n\n\nThey are still incredibly well paid, highly specialized skills and imo are currently at a discount in the market. And the market favours you to be ML engineer first and DevOps second (for some reason).\n\n\nIn my opinion, MLOps and DE focused on features stores/online learning etc are even better than ML engineering.",
          "score": 4,
          "created_utc": "2026-02-04 13:32:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3new0k",
              "author": "weeyummy1",
              "text": "\\> They are still incredibly well paid, highly specialized skills and imo are currently at a discount in the market\n\nWhat do you mean by this? Are you saying it should be higher paid than it is?",
              "score": 1,
              "created_utc": "2026-02-05 02:49:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3p0k1x",
                  "author": "Beneficial_Aioli_797",
                  "text": "No. When i mean discount its Im the economical sense.\n\n\nML engineering and MLOps are similar in terms of pay, but competition for ML engineering is much more fierce. So you end up \"paying\" more to get a ML engineering job than MLOps and they both have the same face value.",
                  "score": 1,
                  "created_utc": "2026-02-05 10:29:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hz8nt",
          "author": "No_Mongoose6172",
          "text": "In my experience, deployment is one of the things less taken into account in ML, yet I find it to be one of the most interesting and demanding",
          "score": 2,
          "created_utc": "2026-02-04 08:46:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j8p2f",
          "author": "TheRealStepBot",
          "text": "Maybe you donâ€™t understand what mlops is? Itâ€™s not just running kubernetes for the ml team thatâ€™s for certain. \n\nItâ€™s responsible for managing the overall model lifecycle. The physical infrastructure to do that is just a necessary evil you have to deal with. The actual skills are related to understanding all the assumptions being made in the model and the data to make sure that that the performance at inference time matches performance at train time. \n\nThis involves a ton of work on how data is collected for the training process and then debugging at inference time to determine if the model is performing as intended and dig through all the layers of the stack to figure out why it may not be performing correctly. \n\nYou can have a perfectly good model trained on great data that is undeployable because the inference process has slightly different consistency guarantees from the offline training data. MLops is the final stop on the make it all work train. Itâ€™s not possible to do the job in any non trivial setting without a deep understanding of the math probability and statistics of what is going on.",
          "score": 2,
          "created_utc": "2026-02-04 14:22:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3izx7o",
          "author": "NotSoGenius00",
          "text": "Hot take devops is least mandatory thing for MLOps ! Do you need to know infra ? Yes. Should you setup all the infra yourselves? Fuck no ! There is a devops team for that. Yes data scientists will ask you traditional ML questions but you should know traditional ML because you will be deploying those models so you should be worried about things like latency, memory requirements etc. an MLOps engineer is more like ML + software rather than devops.  \n\nSo devops is the least important thing in my opinion. You need to know modelling well enough to deploy those models ! Tooling is not all you need",
          "score": 3,
          "created_utc": "2026-02-04 13:35:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hp7bv",
          "author": "mystery_biscotti",
          "text": "Literally my plan. Decided to look at an AI professional certification because I need to speak the language. I miss my prod Ops jobs, monitoring systems, proactively fixing stuff, rolling my eyes at the adorable things vendors do during maintenance windows...",
          "score": 2,
          "created_utc": "2026-02-04 07:14:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k8289",
          "author": "klipseracer",
          "text": "How is this weird? They are looking for some who understands BOTH: deep learning models, how to package and distribute them, your experience with tools like DVC, the challenges around versioning models/weights within a data scientists typical workflow without disrupting it or making iterating painful, yada yada.\n\nPlus, they want to know if you have the infra skills they don't have. These two things allows you to meet them in the middle and understand their needs and help them understand what they don't know and would be absolutely expected. The fact you don't realize this is one reason you're not getting the job.\n\nReference: \nI used to work at a deep learning computer vision startup and wrote the cicd that deployed our models to thousands of physical locations across the USA.",
          "score": 1,
          "created_utc": "2026-02-04 17:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5axq",
          "author": "simpleharmonicmotion",
          "text": "+1 This is my life :/ I've flat out said that we're lying to candidates by requiring ML skills for what turns out to be an infra job.",
          "score": 1,
          "created_utc": "2026-02-05 05:43:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3si54q",
          "author": "d1ddydoit",
          "text": "What different teams are hiring for will depend on where that team is in terms of the maturity of its ML system as you rightly say OP. If they are at low levels of maturity, it is as you say often teams of scientists who are hiring and they will often just look for an engineer who can scale up and automate a lot of what they have been crafting - that means at interview they will want an engineer that talks their language and shows that they understand them. If the hiring org is mature, they have employed a system / platform that clearly dictates the engineering skills that need to be brought in.\n\nEvery business and even within a large business, the level of maturity varies dramatically. My advice to applicants looking for roles is to find the role that matches your skills and experience given that the responsibilities are so varied from company to company for the same job title. If you are looking to learn, a firm that is mature is the easiest way to understand scale but if you want a challenge, want to shape something that could be exciting, take a chance on a firm that doesnâ€™t know what it wants if you think you can really help, you like their culture and the remuneration is agreeable. It could still be the happiest place youâ€™ve ever worked (and some of the most fun data scientists I have ever worked with were the most clueless at what was needed outside of their IDE to make it all work).\n\nThat saidâ€¦. If you arenâ€™t able to explain concepts like model drift then you are unable to explain a key metric that helps determine the end of a modelâ€™s lifecycle (by monitoring its declining quality and kicking off a new build of the model). It is still never going to be the same as a DevOps or platform engineer and there will have to be language learnt that is employed by the data scientists if the engineers have not been/worked closely with data scientists previously.",
          "score": 1,
          "created_utc": "2026-02-05 21:37:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k19e8",
          "author": "fiddysix_k",
          "text": "Mlops is just devops except everyone around you pretends you're not just doing standard devops work until you actually just gaslight yourself into believing it.",
          "score": 1,
          "created_utc": "2026-02-04 16:39:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu1p9q",
      "title": "Transformer Lab is an Open-source Control Plane for Modern AI Workflows",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qu1p9q/transformer_lab_is_an_opensource_control_plane/",
      "author": "aliasaria",
      "created_utc": "2026-02-02 17:36:23",
      "score": 28,
      "num_comments": 0,
      "upvote_ratio": 0.98,
      "text": "Just released our latest open source project: **Transformer Lab for Teams** after the past year talking with research labs about friction in their daily workflows. It works with Slurm and SkyPilot to build a unified experience for ML researchers.\n\nTrends we observed:\n\n* The frontier labs invest a ton to build and maintain their own proprietary tooling.\n* Most other AI/ML research teams work with a fragmented landscape of legacy scripts, manual workflows which gets more complicated as you grow your team and run more experiments\n* Researchers spend as much as half their time dealing with environment and experiment logistics. For example, results get lost or rerun because jobs fail before finishing and artifacts arenâ€™t tracked consistently.\n\nHow Transformer Lab for Teams is helpful:\n\n* **Unified Interface:** A single dashboard to manage data ingestion, model fine-tuning, and evaluation.\n* **Seamless Scaling:** The platform is architected to run locally on personal hardware (Apple Silicon, NVIDIA/AMD GPUs) and seamlessly scale to high-performance computing clusters using orchestrators like Slurm and SkyPilot.\n* **Extensibility:** A flexible plugin system allows researchers to add custom training loops, evaluation metrics, and model architectures without leaving the platform.\n* **Privacy-First:** The platform processes data within the user's infrastructure, whether on-premise or in a private cloud, ensuring sensitive research data never leaves the lab's control.\n* **Simplifying workflows:** Capabilities that used to require complex engineering are now built-in.\n   * Capturing checkpoints (with auto-restart)\n   * One-line to add hyperparameter sweeps\n   * Storing artifacts in a global object store accessible even after ephemeral nodes terminate.\n\nThe project is **open source and free to use** with a goal to advance the tools used by any research team big and small.\n\nWould something like this be useful? Welcome feedback for us to make it better. Iâ€™m one of the maintainers and can answer any questions.\n\nTry it here: [https://lab.cloud/](https://lab.cloud/)\n\nAsk any questions below -- really excited to keep working on this with the community!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qu1p9q/transformer_lab_is_an_opensource_control_plane/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qt9i0j",
      "title": "Can someone explain MLOps steps and infrastructure setup? Feeling lost",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qt9i0j/can_someone_explain_mlops_steps_and/",
      "author": "FreshIntroduction120",
      "created_utc": "2026-02-01 20:18:16",
      "score": 14,
      "num_comments": 13,
      "upvote_ratio": 0.9,
      "text": "Hey folks,\n\nI'm trying to wrap my head around MLOps and honestly feeling a bit overwhelmed with all the different info out there.\n\nWould love to hear from people who actually work with this stuff - what are the main steps you go through in an MLOps pipeline? Like from when you start building a model to getting it running in production and keeping it alive?\n\nAlso, how do you even set up the infrastructure for this? What tools do you use and how does it all connect together?\n\nI've been reading articles but they all seem kinda high-level or vendor-specific. Just want to understand how this works in the real world.\n\nAny advice or pointers would be awesome, thanks!",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qt9i0j/can_someone_explain_mlops_steps_and/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o32vesj",
          "author": "Competitive-Fact-313",
          "text": "MLOps in the real world is basically about taking a model from a notebook to something that can run reliably like normal software. At a high level, you clean and prepare data in a repeatable way, train models using scripts while tracking experiments and artifacts with tools like MLflow, package the final model into a service (usually FastAPI) and a Docker image, and then deploy that image on infrastructure like Kubernetes (for example EKS) so it can scale and stay up. CI/CD tools such as GitHub Actions are mainly used to test code, build and push images (to ECR), and trigger deployments, not to train large models directly heavy training usually runs on dedicated compute (EKS jobs, Batch, SageMaker, etc.) and just reports results back to MLflow. The key idea is separating concerns: training is compute-heavy and controlled, deployment is automated, and monitoring closes the loop so you know when to retrain or update the model.",
          "score": 7,
          "created_utc": "2026-02-02 01:41:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bnb22",
              "author": "Glad_Appearance_8190",
              "text": "this is a solid summary tbh. the part people miss is that once itâ€™s in prod, it behaves like any other system. data drifts, inputs go missing, edge cases pile up. ops work is mostly about knowing what version ran, on what data, and why it made a call. infra matters less than having repeatability, logs, and some way to explain failures when something goes weird at 2am.",
              "score": 2,
              "created_utc": "2026-02-03 11:12:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3c5fnu",
                  "author": "Competitive-Fact-313",
                  "text": "Thanks ðŸ™ i am happy to help.",
                  "score": 1,
                  "created_utc": "2026-02-03 13:21:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31clz9",
          "author": "HahaHarmonica",
          "text": "Think of MLOps as just DevOps+Data. \n\nPeople write code to train models on some data. MLOps is the automation of that training to show data lineage of what data was used to train the models. Making the model accessible (e.g. hosting the inferencing), and then monitoring said model. Adjust code, rinse repeat.",
          "score": 6,
          "created_utc": "2026-02-01 20:51:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31d7gq",
              "author": "EviliestBuckle",
              "text": "Can you please point to some structured course plz",
              "score": 2,
              "created_utc": "2026-02-01 20:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o33xqrx",
                  "author": "apexvice88",
                  "text": "No",
                  "score": 2,
                  "created_utc": "2026-02-02 05:40:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o323cym",
          "author": "MyBossIsOnReddit",
          "text": "Won't really straight up answer your question but MLOps is broad and needs pervasive support in tooling, storytelling and permissions. For that you need buy-in with management and other teams. So it's a lot of pitching and powerpointing until you get there.\n\nThen comes a bit of architecture and system design. Any given design won't meet every requirement but you don't need to roll out everything at once. So work iteratively. Try to close the model lifecycle loop (the etl-train-eval-deploy-monitor-retrain) first. Then add the other features depending on need. The problem is the quality of the platform largely depends on the sum of the parts. It will be painful before it gets better.\n\nYou also need to figure out what you need and where it lives. Models, api keys, networking, dashboards, storage, vector databases, infra as code, ci/cd, git? Infra as code, templating, containers, what metrics do you track? p99, latency?\n\nYou can tell the biggest pain point for me has always been getting everyone on the same page. None of the folks I work with get it and think they can either do it themselves over the weekend or that it's complex and we need another 4 consultants.\n\nPersonally I've always used some Infra as code (CDK, terraform), some form of managed services for serving (no way you're going to handle everything without), mlflow, tensorboard. Start with terraform and build infra and templates first. I've had good experiences with AWS sagemaker and Bedrock, and GCP's Vertex.",
          "score": 2,
          "created_utc": "2026-02-01 23:05:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o341k6o",
          "author": "Extension_Key_5970",
          "text": "Not being sure about your background, whether you are a fresh graduate or have some experience with software engineering, here are a few general pointers that are a must  \n  \n\\- MLOps is more of solving Data scientists, ML researchers' problems, rather than pure Infra. Companies usually have a DevOps team to handle Infra problems, what they don't have is someone who can make the model for the production infrastructure, that's where MLOps comes in\n\n\\- So think like an ML engineer, having an Infra experience is a must, that's what I think, skillset needed to become an MLOps, and that's what companies are trying to analyse in an interview\n\n\\- So, start with understanding ML Foundations, good with Python, hands-on, must  \n\\- Try to look for scenarios where DS/ML engineers want to push their model into production, or convert their prototypes from notebooks to a pipeline\n\n\\- From Infra--> Think of exposing models to end users in a scalable, reliable fashion, what metrics are needed to evaluate model performace",
          "score": 2,
          "created_utc": "2026-02-02 06:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37agpg",
          "author": "thelonious_stonk",
          "text": "MLOps has a tough learning curve but is basically: data -> model -> train -> deploy -> monitor. Learn by doing. For tools to help you get started, check out Kubeflow, MLflow and Transformer Lab for model work.",
          "score": 1,
          "created_utc": "2026-02-02 18:45:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i8cb6",
          "author": "llamacoded",
          "text": "Honestly, it's a mess out there with all the info. In the real world, it boils down to a few core things:  \n  \n1. Dev & Tracking:Build your model, track experiments with something like MLflow (code, data, model artifacts). Version everything; code with Git, data on S3.  \n2. CI/CD: Automate training (e.g., Kubeflow Pipelines on EKS for us) and model deployment. Test the hell out of your model \\*before\\* it goes live.  \n3. Serving: Deploy models via FastAPI on K8s (EKS) or AWS SageMaker endpoints if you want to skip some infra setup.  \n4. Monitoring: Crucial. Track model accuracy, latency, data drift, and infra cost. Set up alerts. We use custom Python scripts hooked into Prometheus/Grafana for this.  \n  \nIt's not about fancy tools; it's about reliable pipelines and knowing your numbers. Last time, a model's accuracy tanked because we didn't monitor data quality properly post-deployment. Keep it simple at first.",
          "score": 1,
          "created_utc": "2026-02-04 10:12:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33y1dw",
          "author": "apexvice88",
          "text": "OP I suggest you stop asking reddit and start from the bottom. MLOps is not for beginners and clearly you're not ready. I doubt anyone has time to hand hold you the entire way. Either go out there and learn more and then come back and ask good questions or don't get into this field at all. Sorry if this sounds harsh, but it will be much harsher if you cannot be self sufficient and except things to be handed to you on a silver platter.",
          "score": 1,
          "created_utc": "2026-02-02 05:43:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aq4qw",
              "author": "BackgroundLow3793",
              "text": "ðŸ˜³ðŸ˜³",
              "score": 1,
              "created_utc": "2026-02-03 06:05:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qshhhd",
      "title": "Deployed an ML Model on GCP with Full CI/CD Automation (Cloud Run + GitHub Actions)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qshhhd/deployed_an_ml_model_on_gcp_with_full_cicd/",
      "author": "gringobrsa",
      "created_utc": "2026-01-31 23:14:50",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "# Hey folks\n\nI just published Part 2 of a tutorial showing how to deploy an ML model on GCP using Cloud Run and then evolve it from manual deployment to full CI/CD automation with GitHub Actions.\n\nOnce set up, deployment is as simple as:\n\n    git tag v1.1.0\n    git push origin v1.1.0\n\nFull post:  \n[https://medium.com/@rasvihostings/deploy-your-ml-model-on-gc-part-2-evolving-from-manual-deployments-to-ci-cd-399b0843c582](https://medium.com/@rasvihostings/deploy-your-ml-model-on-gc-part-2-evolving-from-manual-deployments-to-ci-cd-399b0843c582)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qshhhd/deployed_an_ml_model_on_gcp_with_full_cicd/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o369o2r",
          "author": "Informal_Tangerine51",
          "text": "Nice setup. We're doing something similar for our document processing agent but hit a wall CI/CD doesn't solve: behavioral regression.\n\nYour pipeline deploys the model, runs tests, everything green. But when we update the underlying LLM (say GPT-4 to 4.5) or tweak retrieval logic, tests pass but production behavior changes on 15-20% of edge cases we never wrote tests for.\n\nCI catches code regressions because unit tests are deterministic. Agent behavior isn't. We have 80 synthetic test cases that pass every time, but production processes 5,000 document types with edge cases we didn't imagine. Model update ships, extraction accuracy drifts, customers notice before we do.\n\nThe gap: CI needs fixtures from production reality, not synthetic imagination. When agent breaks in prod, that failure should automatically become a regression test. Currently we fix the bug, merge, deploy, and six months later it happens again because we never captured the actual failure as a permanent fixture.\n\nYour tag-based deployment is clean, but how do you prevent model updates from silently changing behavior on real production patterns? Are you running evals against production traces, or just synthetic test suites?",
          "score": 1,
          "created_utc": "2026-02-02 15:57:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38j1zk",
              "author": "gringobrsa",
              "text": "Youâ€™re absolutely right: CI/CD guarantees **deployment** correctness, but it cannot guarantee **semantic** correctness. When you swap GPT-4 for 4.5, or tweak a retrieval embedding, the prompt's 'meaning' might stay the same, but the output distribution shifts.\n\nHere is how you can bridge that gap:\n\n* **CI is the Plumbing, Evals are the Gate:** GitHub Actions shouldn't run your 5,000-case regression suite itâ€™s too heavy and non-deterministic. Instead, CI triggers an external **Evaluation Pipeline**.\n* **Production-to-Fixture Pipeline:**  treat production failures as the highest-value data. When a user flags a hallucination or an extraction error, that trace is automatically sanitized and added to a 'Golden Dataset.' Your test suite is no longer 'synthetic imagination'; itâ€™s a reflection of reality.\n* **Semantic Versioning for Models:**  treat a model swap like a major database schema migration. We run the new model in 'Shadow Mode' against those production fixtures. We don't look for 1:1 string matches (which always break), but use **LLM-as-a-Judge** to ensure the extraction logic hasn't drifted.\n* **Drift Analytics:** The pipeline looks for 'Regression Sensitivity.' If the new model improves 5% of cases but breaks 2% of previously 'solved' edge cases, the CI/CD pipeline blocks the merge.\n\nHaven't tried though you can try like this or have you implemenetd differently ?",
              "score": 1,
              "created_utc": "2026-02-02 22:15:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qri84h",
      "title": "MLOps for LLM prompts - versioning, testing, portability",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qri84h/mlops_for_llm_prompts_versioning_testing/",
      "author": "gogeta1202",
      "created_utc": "2026-01-30 21:13:29",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 0.89,
      "text": "MLOps has mature tooling for models. What about prompts?\n\nTraditional MLOps:  \nâ€¢ Model versioning âœ“  \nâ€¢ Experiment tracking âœ“  \nâ€¢ A/B testing âœ“  \nâ€¢ Rollback âœ“\n\nPrompt management:  \nâ€¢ Versioning: Git?  \nâ€¢ Testing: Manual?  \nâ€¢ A/B across providers: Rebuild everything?  \nâ€¢ Rollback: Hope you saved it?\n\nWhat I built with MLOps principles:\n\nVersioning:  \nâ€¢ Checkpoint system for prompt states  \nâ€¢ SHA256 integrity verification  \nâ€¢ Version history tracking\n\nTesting:  \nâ€¢ Quality validation using embeddings  \nâ€¢ 9 metrics per conversion  \nâ€¢ Round-trip validation (Aâ†’Bâ†’A)\n\nPortability:  \nâ€¢ Convert between OpenAI â†” Anthropic  \nâ€¢ Fidelity scoring  \nâ€¢ Configurable quality thresholds\n\nRollback:  \nâ€¢ One-click restore to previous checkpoint  \nâ€¢ Backup with compression  \nâ€¢ Restore original if needed\n\nQuestions for MLOps practitioners:\n\n1. How do you version prompts today?\n2. What's your testing strategy for LLM outputs?\n3. Would prompt portability fit your pipeline?\n4. What integrations needed? (MLflow? Airflow?)\n\nLooking for MLOps engineers to validate this direction.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qri84h/mlops_for_llm_prompts_versioning_testing/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2oomm5",
          "author": "alexlag64",
          "text": "MLFlow offers a prompt registry and a LLM evaluation framework that works pretty good for our datascience team at our company.\nEasy to load prompts into our workflows using MLFlowâ€™s API, easy to compare the outputs of the LLM using two different prompts versions on the same dataset, I havenâ€™t really looked other solutions since MLFlow works so well for us.",
          "score": 5,
          "created_utc": "2026-01-30 22:07:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xx8dz",
          "author": "Anti-Entropy-Life",
          "text": "I have docs from my lab titled \"Qualitative Prompt Engineering\" with a sub-domain of \"Prompt Discipline\" where functions of various prompts as taxonomically categorized.\n\nWould something like this be useful info to anyone else?",
          "score": 2,
          "created_utc": "2026-02-01 09:01:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30jwes",
              "author": "leveragecubed",
              "text": "Defiantly useful.",
              "score": 2,
              "created_utc": "2026-02-01 18:35:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o329898",
              "author": "gogeta1202",
              "text": "You're hitting on a real gap in the market. We've got tons of tools for latency and cost, but almost nothing for prompt discipline or taxonomy. That's actually the main blocker I'm seeing with multi-model reliability without a shared language for what prompts actually do, moving between models becomes a guessing game.\n\nI'm working on a conversion layer that maps prompts across providers using that kind of framework. Would be curious to see your taxonomy, especially how you handle reasoning granularity vs. output constraints. If you're open to it, I'd love to explore baking some of these principles into the eval loops I'm building.",
              "score": 2,
              "created_utc": "2026-02-01 23:37:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o345f8h",
                  "author": "Anti-Entropy-Life",
                  "text": "This is helpful context, thanks. What youâ€™re describing is exactly the gap I was trying to name with Qualitative Prompt Engineering, especially the separation between what a prompt is doing (control function) and how a model happens to realize it.\n\nThe core of the taxonomy isnâ€™t prompt wording, itâ€™s function: constraint setting, search narrowing, granularity forcing, abstention enforcement, output shaping, etc. Once you label those functions explicitly, moving across models stops being a guessing game.\n\nIâ€™m open to sharing a trimmed version of the taxonomy, particularly the parts around reasoning granularity vs. output constraints and failure modes. If itâ€™s useful, Iâ€™d be interested in exploring how that maps into eval loops rather than staying at the prompt-craft layer.\n\nHappy to continue this in DMs!",
                  "score": 1,
                  "created_utc": "2026-02-02 06:44:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o368xyc",
          "author": "Informal_Tangerine51",
          "text": "We version prompts in Git alongside code, but that only tracks the template text. When an agent breaks in production, Git history shows \"changed system prompt line 3\" but not what retrieval context was injected, which features were stale, or what the final assembled prompt actually was.\n\nThe testing gap is bigger. We run evals with synthetic cases, maybe 50-100 scenarios. Production hits 5,000 edge cases we never imagined. Model update passes all tests, then 15% of real document extractions change behavior. Your embedding-based validation catches synthetic drift but wouldn't catch this.\n\nPortability is interesting but seems secondary to the core problem: when an LLM call breaks, can you replay what it saw? We had an incident where Legal asked \"what documents informed this decision\" and we had the prompt template from Git, request logs with timing, but zero proof of what docs were actually retrieved or how fresh they were. Took 4 hours to say \"we don't know.\"\n\nCheckpoints help with version control but unless they capture retrieval lineage (what was fetched, when, why), you're still debugging with incomplete information. Same with rollback - rolling back the prompt template doesn't rollback the stale cache that caused the bad output.\n\nHow does your checkpoint system handle dynamic context - retrieval, features, function outputs - that changes per request? Or is this focused on static prompt templates only?",
          "score": 2,
          "created_utc": "2026-02-02 15:53:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35jbcg",
          "author": "Competitive-Fact-313",
          "text": "Run mlflow on port 5000 and do some experiments you will finds it useful.",
          "score": 1,
          "created_utc": "2026-02-02 13:42:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quzaun",
      "title": "Orchestrating Two-Tower retrieval: Managing the training-to-serving loop",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1quzaun/orchestrating_twotower_retrieval_managing_the/",
      "author": "skeltzyboiii",
      "created_utc": "2026-02-03 18:05:58",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "The deployment of Two-Tower models for retrieval usually involves significant infrastructure overhead. Beyond just training the user and item encoders, the production pipeline typically requires:\n\n1. Index Orchestration:Â Triggering embedding updates whenever item metadata changes to prevent drift.\n2. Vector DB Synchronization:Â Managing the handoff between the feature store and the ANN index (e.g Pinecone, Milvus, or Weaviate).\n3. Hybrid Querying:Â Implementing a way to combine vector similarity with hard business logic (e.g filtering out \"out of stock\" items) without incurring significant latency penalties.\n\nThe code required to keep these systems in sync often becomes more complex than the model architecture itself.\n\nWeâ€™ve been working on a more declarative approach that treats the training, indexing, and retrieval as a single layer. By using a SQL-based interface, you can query the model directly, the system handles the embedding updates and indexing in the background, allowing for standardÂ WHEREÂ clauses to be applied to the similarity results.\n\nWe put together a technical breakdown of this architecture using a fashion marketplace as the case study. It covers:\n\n* Connecting Postgres/data warehouses directly to the training pipeline.\n* Configuring Two-Tower schemas via YAML.\n* Sub-50ms retrieval benchmarks when combining neural search with SQL filters.\n\nIf youâ€™re interested in the implementation details or the pipeline design:  \n[https://www.shaped.ai/blog/how-to-deploy-a-production-two-tower-model-in-less-than-a-day](https://www.shaped.ai/blog/how-to-deploy-a-production-two-tower-model-in-less-than-a-day)\n\n*Full disclosure: Iâ€™m with the team at Shaped and authored this technical guide.*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1quzaun/orchestrating_twotower_retrieval_managing_the/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qsn0d7",
      "title": "Non sucking, easy tool to convert websites to LLM ready data, Mojo",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qsn0d7/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "author": "malvads",
      "created_utc": "2026-02-01 03:16:12",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "Hey all! After running into *only paid tools or overly complicated setups* for turning web pages into structured data for LLMs, I built **Mojo,** a **simple, free, open-source tool** that does exactly that. Itâ€™s designed to be easy to use and integrate into real workflows.\n\nIf youâ€™ve ever needed to prepare site content for an AI workflow without shelling out for paid services or wrestling with complex scrapers, this might help. Would love feedback, issues, contributions, use cases, etc. <3\n\n[https://github.com/malvads/mojo](https://github.com/malvads/mojo) (and it's MIT licensed)\n\n*Cheers!*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qsn0d7/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3g4zo8",
          "author": "burntoutdev8291",
          "text": "Nice work! But isn't the common problem with scrapers more of the rate limit? Would it be better to combine a crawler with your tool for parsing? Like HTTrack.",
          "score": 1,
          "created_utc": "2026-02-04 01:05:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3g62el",
              "author": "malvads",
              "text": "Hey, thanks for your comment, mojo does support proxy rotation (socks4, socks5, http), so technically, it can bypass rate-limits (429 HTTP) but just dont be evil (hehe)...",
              "score": 1,
              "created_utc": "2026-02-04 01:11:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xzawh",
          "author": "Anti-Entropy-Life",
          "text": "This looks genuinely useful, especially the focus on being simple, free, and MIT licensed without trying to be a full â€œAI platform\" = super cool :D\n\nWhat I particularly like is that it tackles a very real and annoying part of the stack: getting web content into a reasonably clean, LLM-ready form without paying per page or maintaining a complex scraping pipeline. Thatâ€™s a big win for early experiments, internal tools, and small teams.\n\nA few questions that would help me understand how far this can go in real workflows:\n\nâ€¢ Is the extraction deterministic, meaning the same page always produces the same output?  \n  \nâ€¢ How do you think about drift and updates, for example re-ingesting pages that change over time?  \n  \nâ€¢ When things go wrong, like odd markup, partial loads, or missing content, where is the best place to debug? Raw HTML, Mojoâ€™s intermediate output, or the final chunks?\n\nOne thing that could make this even stronger is being explicit about failure modes and contracts in the README. In other words, what Mojo guarantees versus what it intentionally does not. Even a short section in the README about what Mojo will not handle would build a lot of trust.\n\nThanks for building and open-sourcing this, though! Tools that remove friction without over-claiming are rare and genuinely appreciated! :D",
          "score": 0,
          "created_utc": "2026-02-01 09:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ydh60",
              "author": "malvads",
              "text": "Hi, thanks for your question. My idea with Mojo is to provide fast conversion between pages and LLM data.\n\nAnswering your questions:\n\n**â€¢ Is the extraction deterministic, meaning the same page always produces the same output?**  \nâ†’ The same HTML provides the same output. Right now there are two options to get data: with the `--render` flag and without it.\n\nThe first one uses pure HTTP requests (ideal for static web pages). With the `--render` flag, it connects to Chrome using CDP (so no extra dependencies are downloaded, just your existing setup). -- This is still not released, planned for 0.1.0, but you can build Mojo and test this feature\n\nSo it depends on the setup you are using and how the web page is programmed.\n\n**â€¢ How do you think about drift and updates, for example re-ingesting pages that change over time?**  \nâ†’ In my opinion, the best way to handle this is via CI pipelines (Jenkins/GitHub Actions), but you can always set up cron jobs as well (macOS/Linux).\n\n**â€¢ When things go wrong, like odd markup, partial loads, or missing content, where is the best place to debug? Raw HTML, Mojoâ€™s intermediate output, or the final chunks?**  \nâ†’ In my opinion, the best way to debug is to fetch the page via `curl` and use a converter. For example:\n\n    curl -X GET your-web -o file.html\n\nThen later you can use Mojo to fetch the static source using:\n\n    ./mojo -d 0 file://your_file -o ./debug\n\nand see the output it generates (always with depth 0).\n\n**â€œOne thing that could make this even stronger is being explicit about failure modes and contracts in the README. In other words, what Mojo guarantees versus what it intentionally does not. Even a short section in the README about what Mojo will not handle would build a lot of trust.â€**\n\nThanks for your suggestion, I will include it after I finish the render crawler.\n\nThanks :)",
              "score": 1,
              "created_utc": "2026-02-01 11:30:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2yv58y",
                  "author": "Anti-Entropy-Life",
                  "text": "Thanks so much for taking the time to write such a detailed and awesome response! I think this is really a spectacular project my friend! :)\n\nThe deterministic model of â€œsame HTML gives the same outputâ€ makes sense, and the split between pure HTTP fetching and the `--render` path is clear. Using CDP for rendering without pulling in extra dependencies seems like the right call for JS-heavy pages.\n\nThe CI / cron approach for handling drift also feels pragmatic. Treating ingestion as an explicit job instead of something implicit or magical is exactly how Iâ€™d expect this to be used in real pipelines! This is fantastic!\n\nSo, just to be sure I am understanding you properly, the optimal debugging workflow you described is:\n\n0. Fetch HTML with  `curl`\n\n1. Validate or convert it independently\n2. Run Mojo at depth 0 against a static source\n\n(Edit: sorry, I don't know what's wrong with this list, the Reddit formatting refuses to not render it with this weird spacing/gap for some reason I can't quite determine at the moment)  \n  \nIf thatâ€™s right, thatâ€™s a great, clean debugging protocol. Itâ€™s one of the most realistic and sane ways Iâ€™ve seen to isolate issues and understand where things are going wrong.\n\nI feel honored you found my simple README suggestion helpful at all. Being explicit about guarantees and non-goals is something I really appreciate in tools like this.\n\nThank you absolutely supremely for being a clear-minded thinker and building such beautiful tools that increase the coherence of the world :)\n\nLooking forward to seeing and testing out the render crawler once it's out. Thanks again for the thoughtful explanation and for building such a well-reasoned project. :)",
                  "score": 0,
                  "created_utc": "2026-02-01 13:40:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qu4ehl",
      "title": "\"What data trained this model?\" shouldn't require archeology â€” EU AI Act Article 10 compliance with versioned training data",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qu4ehl/what_data_trained_this_model_shouldnt_require/",
      "author": "DoltHub_Official",
      "created_utc": "2026-02-02 19:09:17",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.85,
      "text": "We build Dolt (database with Git-style version control), and we've been writing about how it applies to EU AI Act compliance. Article 10 requires audit trails for training data and reproducible datasets.\n\nHere's a pattern from Flock Safety (computer vision for law enforcement â€” definitely high-risk):\n\n# How It Works\n\nEvery training data change is a commit. Model training = tag that commit. `model-2026-01-28` maps to an immutable snapshot.\n\nWhen a biased record shows up later:\n\nhttps://preview.redd.it/6injhhn4r4hg1.png?width=2182&format=png&auto=webp&s=1ea975d0f08a21025c98cd84644ac43420d582a0\n\nThat's the difference between \"we believe it was clean\" and \"here's the proof.\"\n\n\n\nMore detail: [https://www.dolthub.com/blog/2026-02-02-eu-ai-act/](https://www.dolthub.com/blog/2026-02-02-eu-ai-act/)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qu4ehl/what_data_trained_this_model_shouldnt_require/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3q1djl",
          "author": "NewClaim7739",
          "text": "This feels even more important once synthetic data enters the mix ",
          "score": 1,
          "created_utc": "2026-02-05 14:38:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qux21q",
      "title": "[For Hire] Senior Data & MLOps Engineer | 9+ Years Experience | Azure, Spark, Palantir Foundry | Available IST â€“ 9 PM IST",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qux21q/for_hire_senior_data_mlops_engineer_9_years/",
      "author": "mcheetirala2510",
      "created_utc": "2026-02-03 16:46:17",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.67,
      "text": "\nHi everyone! I am a Senior Data Engineer and MLOps Specialist with over 9 years of experience building scalable data architectures and productionizing machine learning models for global leaders like Microsoft, EPAM, and HCL.  \nI specialize in migrating legacy systems to modern cloud stacks and implementing \"Data Contracts\" to ensure long-term business continuity and data integrity.  \nWhy Hire Me?\nProven Cost Savings: Saved clients $250K USD by migrating bespoke datasets to Palantir Foundry and optimizing refresh rates.  \nArchitectural Leadership: Successfully influenced key architectural pivots that protected 300+ datasets from downstream failures.  \nEnd-to-End MLOps: Experienced in deploying models using Docker, AWS SageMaker, Azure Kubernetes (AKS), and MLflow for both real-time and batch inferencing.  \nInfrastructure & DevOps: Proficient in CI/CD (GitHub Actions, Azure DevOps) and Infrastructure as Code (Terraform).  \nHighly Certified: 6x Azure Certified, 2x Databricks Certified, and 1x AWS Certified.  \nTechnical Toolkit\nLanguages & Frameworks: SQL, Python, PySpark, Scala, Spark.  \nData Engineering: Azure Data Factory (ADF), Palantir Foundry, Databricks, Azure Data Lake.  \nMLOps & AI: Scikit-Learn, XGBoost, MLflow, Azure ML, AWS SageMaker.  \nDatabases: MongoDB, MS SQL Server.  \nVisualization: Power BI, Seaborn, Bokeh.  \nAvailability & Location\nTarget Region: EMEA (Open to remote roles).\nHours: Available from IST until 9 PM IST, providing excellent overlap with UK and European business hours.\nRole Type: Full-time.\nExperience Highlights\nEPAM (Senior Software Engineer): Currently migrating a 30-year legacy PL/SQL Data Warehouse to Spark and Palantir Foundry.  \nMicrosoft (Data Engineer): Built scalable ETL pipelines and handled real-time event processing with Azure Event Hubs.  \nYash Technologies (Data Scientist): Led a team of 6 to build MLOps solutions and successfully onboarded insurance clients through technical presales.  \nLooking for a seasoned engineer to bridge the gap between Data Engineering and Machine Learning?\nPlease DM me or reach out at mcheetirala@gmail.com to discuss how I can help your team!\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qux21q/for_hire_senior_data_mlops_engineer_9_years/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3u1wps",
          "author": "apexvice88",
          "text": "If someone as skilled as you has to look for jobs on Reddit, oh man, economy is doing pretty bad lol. Sucks for the graduates (american term) or freshers (international term)",
          "score": 1,
          "created_utc": "2026-02-06 02:47:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrh2ns",
      "title": "Streaming feature transformations",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qrh2ns/streaming_feature_transformations/",
      "author": "Spirited-Bit9693",
      "created_utc": "2026-01-30 20:30:18",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "What are the popular approaches to do feature transformations on streaming data?\n\nRequirements:\n\nLow latency computations on data from kafka streams\n\npopulate the computed features in online feature store",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1qrh2ns/streaming_feature_transformations/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2o9q9h",
          "author": "Scared_Astronaut9377",
          "text": "What kind of transformations are we talking about? Just extract things from one kafka message, apply a function, put in store? Or like \"use the kafka stream to keep the count of user actions during the current activity session\"? Very different requirements. \n\nAnd what is your existing stack?",
          "score": 1,
          "created_utc": "2026-01-30 20:56:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oa4sa",
              "author": "Spirited-Bit9693",
              "text": "We currently only have a batch framework. We need both : applying simple functions and also stateful transformations",
              "score": 1,
              "created_utc": "2026-01-30 20:58:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ohbwc",
                  "author": "Scared_Astronaut9377",
                  "text": "Apache flink or similar streaming engines.",
                  "score": 3,
                  "created_utc": "2026-01-30 21:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oa7jp",
          "author": "Spirited-Bit9693",
          "text": "We primarily use spark to compute features",
          "score": 1,
          "created_utc": "2026-01-30 20:58:51",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwdbtk",
      "title": "My fraud model didnâ€™t crash  it quietly dropped from F1 0.79 â†’ 0.18. Hereâ€™s how I caught it.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qwdbtk/my_fraud_model_didnt_crash_it_quietly_dropped/",
      "author": "AffectionateSir8341",
      "created_utc": "2026-02-05 06:00:22",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I had a fraud detection demo where nothing â€œbrokeâ€ in production.\n\nNo errors, no crashes, no deploys.\n\n\n\nBut the modelâ€™s F1 score quietly dropped from 0.79 to 0.18 â€” purely due to data drift.\n\n\n\nThatâ€™s what scares me most about production ML: models donâ€™t fail loudly,\n\nthey slowly start lying.\n\n\n\nTo explore this properly, I built ModelGuard â€” a small reference implementation\n\nfocused on what happens \\*after\\* deployment:\n\n\n\n\\- detects data & prediction drift using multiple statistical tests\n\n\\- scores severity (not just yes/no drift)\n\n\\- recommends actions (ignore / monitor / retrain / rollback)\n\n\\- gates retraining with human approval\n\n\\- exposes everything via a CLI + lightweight REST API\n\n\n\nThis is intentionally not a framework or SaaS â€” just a learning artifact for\n\napplied ML / MLOps.\n\n\n\nThe demo uses the Kaggle credit card fraud dataset with a simulated fraud-ring\n\nattack. The original dataset is extremely imbalanced (0.17% fraud), so I use\n\nbalanced resampling for faster iteration. Drift is synthetically injected, but\n\nall drift statistics and severity scores are real calculations.\n\n\n\nIn the demo, ModelGuard:\n\n\\- detects drift in 19 / 30 features (63%)\n\n\\- assigns HIGH severity (0.62)\n\n\\- flags a 77% drop in fraud-class F1\n\n\\- recommends retraining and creates a pending alert for human review\n\n\n\nRepo: [https://github.com/Aagam-Bothara/ModelGuard](https://github.com/Aagam-Bothara/ModelGuard)\n\n\n\nIâ€™d really appreciate feedback from folks running models in prod:\n\nâ€“ does this feel realistic or over-engineered?\n\nâ€“ what would you simplify or remove?\n\nâ€“ whatâ€™s the first thing youâ€™d add?\n\n",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwdbtk/my_fraud_model_didnt_crash_it_quietly_dropped/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qv0dzk",
      "title": "Setting up production monitoring for LLMs without evaluating every single request",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qv0dzk/setting_up_production_monitoring_for_llms_without/",
      "author": "llamacoded",
      "created_utc": "2026-02-03 18:44:20",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "We needed observability for our LLM app but evaluating every production request would cost more than the actual inference. Here's what we implemented.\n\nDistributed tracing: Every request gets traced through its full execution path - retrieval, tool calls, LLM generation. When something breaks, we can see exactly which step failed and what data it received.\n\nSampled quality evaluation: Instead of running evaluators on 100% of traffic, we sample a percentage and run automated checks for hallucinations, instruction adherence, and factual accuracy. The sampling rate is configurable based on your cost tolerance.\n\nAlert thresholds: Set up Slack alerts for latency spikes, cost anomalies, and quality degradation. We track multiple severity levels - critical for safety violations, high for SLA breaches, medium for cost issues.\n\nDrift detection: Production inputs shift over time. We monitor for data drift, model drift from provider updates, and changes in external tool behavior.\n\nThe setup took about an hour using Maxim's SDK. We instrument traces, attach metadata for filtering, and let the platform handle aggregation.\n\nDocs: [https://www.getmaxim.ai/docs/tracing/overview](https://www.getmaxim.ai/docs/tracing/overview)\n\nHow are others handling production monitoring without breaking the bank on evals?",
      "is_original_content": false,
      "link_flair_text": "Tools: paid ðŸ’¸",
      "permalink": "https://reddit.com/r/mlops/comments/1qv0dzk/setting_up_production_monitoring_for_llms_without/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qr5fye",
      "title": "UPDATE: sklearn-diagnose now has an Interactive Chatbot!",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qr5fye/update_sklearndiagnose_now_has_an_interactive/",
      "author": "lc19-",
      "created_utc": "2026-01-30 13:27:44",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm excited to share a major update to sklearn-diagnose - the open-source Python library that acts as an \"MRI scanner\" for your ML models (https://www.reddit.com/r/mlops/s/3HKkXzMbxZ)\n\nWhen I first released sklearn-diagnose, users could generate diagnostic reports to understand why their models were failing. But I kept thinking - what if you could talk to your diagnosis? What if you could ask follow-up questions and drill down into specific issues?\n\nNow you can! ðŸš€\n\nðŸ†• What's New: Interactive Diagnostic Chatbot\n\nInstead of just receiving a static report, you can now launch a local chatbot web app to have back-and-forth conversations with an LLM about your model's diagnostic results:\n\nðŸ’¬ Conversational Diagnosis - Ask questions like \"Why is my model overfitting?\" or \"How do I implement your first recommendation?\"\n\nðŸ” Full Context Awareness - The chatbot has complete knowledge of your hypotheses, recommendations, and model signals\n\nðŸ“ Code Examples On-Demand - Request specific implementation guidance and get tailored code snippets\n\nðŸ§  Conversation Memory - Build on previous questions within your session for deeper exploration\n\nðŸ–¥ï¸ React App for Frontend - Modern, responsive interface that runs locally in your browser\n\nGitHub: https://github.com/leockl/sklearn-diagnose\n\nPlease give my GitHub repo a star if this was helpful â­",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qr5fye/update_sklearndiagnose_now_has_an_interactive/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvw8p3",
      "title": "Traditional OCR vs AI OCR vs GenAI OCR. When does this become a systems problem?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qvw8p3/traditional_ocr_vs_ai_ocr_vs_genai_ocr_when_does/",
      "author": "Tricky_Reveal_5951",
      "created_utc": "2026-02-04 18:17:32",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Early OCR conversations often focus on models and accuracy benchmarks.\n\nIn production, the harder problems show up elsewhere.\n\nTraditional OCR fails quietly when layouts drift.\n\n AI based OCR improves coverage but needs stronger guardrails.\n\n GenAI works on complex documents, but requires careful controls to avoid unreliable outputs.\n\nAt scale, OCR becomes less about choosing a model and more about designing a system that knows when to trust automation and when to stop.\n\nMost production pipelines rely on layered approaches, confidence thresholds, fallback strategies, and human review for edge cases.\n\nFor teams running document extraction in production, when did choosing an OCR approach turn into an MLOps and systems decision for you?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qvw8p3/traditional_ocr_vs_ai_ocr_vs_genai_ocr_when_does/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3m1p2f",
          "author": "Informal_Tangerine51",
          "text": "OCR accuracy matters less than extraction accountability. When GenAI extracts wrong field from invoice, can you prove which text it saw and why it chose that interpretation?\n\nWe layer OCR methods too - traditional for structured forms, AI for complex layouts, GenAI for unstructured docs. Works until Legal asks \"what text informed this customer data extraction\" and we have confidence scores but not the actual OCR output the model operated on.\n\nThe systems problem isn't fallback strategies, it's decision evidence. Confidence threshold says \"trust this\" but doesn't capture what text was extracted, whether it was from correct page region, or why boundary detection chose those coordinates.\n\nYour layered approach handles accuracy. The gap: when extraction breaks, can you replay the exact OCR output and layout interpretation the model saw? Or just thresholds and final results?",
          "score": 3,
          "created_utc": "2026-02-04 22:17:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o4nph",
          "author": "Zoekielshane",
          "text": "From what I have seen, production OCR success depends more on system design and trade offs than picking the most accurate models choice. I have recently tested Traditional OCR (Tesseract), Deep Learning OCR (PaddleOCR), and GenAI OCR (VLM-based) on 10K+ financial documents. We shared real production examples and lessons learned here, in this technical writeup if it is useful context:\n https://visionparser.com/blog/traditional-ocr-vs-ai-ocr-vs-genai-ocr",
          "score": 1,
          "created_utc": "2026-02-05 05:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oziai",
              "author": "letsTalkDude",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-05 10:19:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ozl2y",
                  "author": "Zoekielshane",
                  "text": "welcome",
                  "score": 1,
                  "created_utc": "2026-02-05 10:20:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvx8lj",
      "title": "The AI Analyst Hype Cycle",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/the-ai-analyst-hype-cycle",
      "author": "growth_man",
      "created_utc": "2026-02-04 18:52:19",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qvx8lj/the_ai_analyst_hype_cycle/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwz1y7",
      "title": "Open sourced an AI for debugging production incidents - works for ML infra too",
      "subreddit": "mlops",
      "url": "https://v.redd.it/jmn587p30rhg1",
      "author": "Useful-Process9033",
      "created_utc": "2026-02-05 21:59:17",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwz1y7/open_sourced_an_ai_for_debugging_production/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qws06r",
      "title": "CI quality gatekeeper for AI agents",
      "subreddit": "mlops",
      "url": "https://github.com/marketplace/actions/maos-agentgate-ci-quality-gatekeeper-for-ai-agents",
      "author": "TranslatorSalt1668",
      "created_utc": "2026-02-05 17:44:21",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qws06r/ci_quality_gatekeeper_for_ai_agents/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwrdn9",
      "title": "What happens when you outgrow the wrappers?",
      "subreddit": "mlops",
      "url": "/r/LocalLLaMA/comments/1qwrd6z/what_happens_when_you_outgrow_the_wrappers/",
      "author": "Left-Reflection-8508",
      "created_utc": "2026-02-05 17:21:51",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwrdn9/what_happens_when_you_outgrow_the_wrappers/",
      "domain": "",
      "is_self": false,
      "comments": []
    }
  ]
}