{
  "metadata": {
    "last_updated": "2026-02-11 17:29:57",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 20,
    "total_comments": 55,
    "file_size_bytes": 85380
  },
  "items": [
    {
      "id": "1qzaeeu",
      "title": "Best resource to learn modular code for MLOPs",
      "subreddit": "mlops",
      "url": "https://i.redd.it/bsf6sm1c7aig1.jpeg",
      "author": "Deep-Blue-Sea-645",
      "created_utc": "2026-02-08 14:32:30",
      "score": 33,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qzaeeu/best_resource_to_learn_modular_code_for_mlops/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o49arex",
          "author": "MattA2930",
          "text": "Check out ArjanCodes on YouTube. Great channel on code design in Python, and should help you re-write your notebook functionality with Python best practices.\n\nThere is no single right way though. I usually advise to do whatever you think makes it easiest for someone else to come in and make changes to your codebase.",
          "score": 10,
          "created_utc": "2026-02-08 14:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ebvxk",
              "author": "JayRathod3497",
              "text": "Yes I have followed him for FastAPI modulation",
              "score": 1,
              "created_utc": "2026-02-09 07:31:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o49au2g",
          "author": "MindlessYesterday459",
          "text": "Cookiecutter data science could be relevant here.\n\nhttps://cookiecutter-data-science.drivendata.org/",
          "score": 3,
          "created_utc": "2026-02-08 14:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49ke8m",
          "author": "alex_0528",
          "text": "Marvelous MLOps combines both modular code and notebooks in Databricks so you've got the utility of both: https://www.marvelousmlops.io/\n\nThey also cover ditching the notebooks altogether for paramterised scripts. \n\nYes they use Databricks as the platform to deliver this but the principal is pretty universal and could be applied elsewhere, especially once you've started using the scripts to run your modular, testable code.",
          "score": 3,
          "created_utc": "2026-02-08 15:31:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f66bv",
              "author": "Moist-Matter5777",
              "text": "Databricks is great for that! If you're looking for more variety, check out the MLOps Specialization on Coursera. It dives into modular code practices across different platforms and tools. Also, the book \"Building Machine Learning Powered Applications\" has some solid insights on structuring your code.",
              "score": 2,
              "created_utc": "2026-02-09 12:15:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o49ao42",
          "author": "_caramel_popcorn",
          "text": "Artifacts should be stored remotely right?",
          "score": 1,
          "created_utc": "2026-02-08 14:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49lvzf",
          "author": "Standard-Distance-92",
          "text": "How about Asset bundles MLOps stacks?",
          "score": 1,
          "created_utc": "2026-02-08 15:39:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49nv22",
          "author": "Krekken24",
          "text": "Check my comment which I did on some other post - [link](https://www.reddit.com/r/learnmachinelearning/s/RLENZH0ZuD)",
          "score": 1,
          "created_utc": "2026-02-08 15:48:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4comai",
          "author": "Just_Deal6122",
          "text": "The feature/inference/training design pattern described in the LLM Engineer Handbook is a useful reference. The authors apply this pattern to LLM engineering, but it was originally used for MLOps folder structure.",
          "score": 1,
          "created_utc": "2026-02-09 01:02:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hgq4l",
          "author": "Joker_420_69",
          "text": "Vikas Das MLOps. (If hindi)",
          "score": 1,
          "created_utc": "2026-02-09 19:25:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0tdmj",
      "title": "If you're struggling with ML foundations for MLOps, there's another path, the inference & serving side",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0tdmj/if_youre_struggling_with_ml_foundations_for_mlops/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-10 06:28:20",
      "score": 19,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "In my last post, I discussed the importance of ML foundations and Python as key aspects of MLOps. But I realised I left out the other side of the coin, one that's equally valid and may be a better fit for many of you.\n\nIf math and stats aren't your thing and you dread memorising gradient descent variants or probability distributions, hear me out: there's a whole side of MLOps where that's not the focus.\n\nThis side focuses on **model serving, inference optimisation, and production scaling**. \n\nCompanies need people who can:\n\n* Expose models via FastAPI\n* Optimise inference latency and throughput using vLLM, TensorRT, or Triton\n* Manage serving infrastructure with KServe, Seldon, or Ray Serve\n* Handle autoscaling, batching strategies, A/B deployments, and canary rollouts\n* Build observability, monitoring drift, tracking latency p99s, and managing GPU utilisation\n\nNone of this requires you to derive backpropagation from scratch. What it *does* require is strong production engineering instincts, the kind you already have if you've been in DevOps, SRE, or platform engineering.\n\nSo if you're coming from an infrastructure background and feel overwhelmed trying to learn ML theory just to break into MLOps, know that there's a legit path that maps directly to your existing skills. Inference at scale is genuinely hard engineering, and most ML teams desperately need people who can do it well.\n\nThe ML foundations will come naturally over time through exposure. You don't need to master them before you start contributing meaningfully.\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:¬†[topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0tdmj/if_youre_struggling_with_ml_foundations_for_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4kodi4",
          "author": "Extension_Key_5970",
          "text": "For those who are thinking, on how to start and where to explore, read my detailed blog post: [https://medium.com/@thevarunfreelance/you-dont-need-to-master-ml-theory-to-break-into-mlops-here-s-the-other-path-no-one-talks-about-56bc6fb45319](https://medium.com/@thevarunfreelance/you-dont-need-to-master-ml-theory-to-break-into-mlops-here-s-the-other-path-no-one-talks-about-56bc6fb45319)",
          "score": 2,
          "created_utc": "2026-02-10 06:42:30",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4kvk18",
          "author": "--Thunder",
          "text": "Thank you for sharing, I was also looking to dive into mlOps stuff & come from a strong infra background.\n\nCan you recommend any book or some videos which might have helped you as well. I have read the doc on medium, it‚Äôs precise & awesome.\n\nThank you üôèüèº",
          "score": 1,
          "created_utc": "2026-02-10 07:48:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1hcgr",
      "title": "What's your Production ML infrastructure in 2026?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1hcgr/whats_your_production_ml_infrastructure_in_2026/",
      "author": "Repulsive_Ad_9950",
      "created_utc": "2026-02-10 23:46:35",
      "score": 14,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "I'm currently studying the tools generally associated with MLOps. Some stuff seem to be non-negotiable: Cloud provides like AWS, GCP and Azure, Kubernetes, Docker, CI/CD and monitoring/observability. I'd like to hear about the tooling your company use to handle ML workflows, so I can have some direction in my studies. Here are my questions. \n\n**CI/CD**  \nGithub Actions, GitLab or other? Do you use different CI/CD tools depending for training and deployment?\n\n**Orchestration for training models**  \nWhat actually runs your training jobs? Airflow, Prefect, Kubeflow Pipelines, Argo, or something else?  \nHow does the flow work? For example, GitHub Actions -> Airflow DAG -> SageMaker job, or the pipeline occur integrated within Kubeflow?  \n\n\n**Serving**  \nAre your inference endpoints deployed with FastAPI, KServe or other (like Lambda)? I heard that KServe has the advantage of batching requests, which is more compute-efficient for fetching data from database, feature engineering, and making predictions, and as well as some automated A/B and canary deployments, which seems a big advantage to me. \n\n**Monitoring and Observability**  \nCloud-nativa services like CloudWatch or Prometheus+Grafana?\n\n**Integrated or Scattered?**  \nAll-in on one platform (Kubeflow end-to-end, or everything in SageMaker), or scattered (Airflow/Prefect, Kubernetes, etc)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r1hcgr/whats_your_production_ml_infrastructure_in_2026/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4qvk1q",
          "author": "pmv143",
          "text": "For serving LLMs at scale, the stack starts to diverge from traditional ML serving pretty quickly.\n\nFastAPI or KServe work fine for classic models, but large LLM inference introduces different constraints:\n\n‚Äì GPU residency and memory fragmentation\n‚Äì cold start latency\n‚Äì multi model scheduling on shared GPUs\n‚Äì batching vs interactive latency tradeoffs\n\nIn our case we ended up building a snapshot based runtime specifically for inference. Instead of treating endpoints as long lived pods, we snapshot the fully initialized CUDA graph and restore directly into GPU memory. That lets us scale to zero without warm pools and still keep cold start under ~2 seconds for 70B class models.\n\nMonitoring wise we still rely on Prometheus style metrics and GPU level telemetry, but the core efficiency gains come from how the runtime manages GPU state rather than from orchestration layers.\n\nThe training stack can stay fairly conventional. Inference is where the architecture really changes.",
          "score": 9,
          "created_utc": "2026-02-11 04:39:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rrcin",
              "author": "elsatan666",
              "text": "That snapshot approach sounds very cool, was that something you built yourselves or is it off the shelf?",
              "score": 1,
              "created_utc": "2026-02-11 09:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4s8275",
                  "author": "pmv143",
                  "text": "We built it ourselves.\n\nThere isn‚Äôt really an off-the-shelf solution that snapshots a fully initialized CUDA graph and restores it directly into GPU memory for large LLMs.\n\nContainer images snapshot disk state.\nCheckpointing saves weights.\nBut neither preserves the full runtime state required for fast GPU restore.\n\nThis took several years of work around GPU memory management, CUDA initialization, and state restoration. It‚Äôs very inference-specific. We didn‚Äôt touch the training stack much. The real complexity was managing GPU residency and multi-model state transitions safely.\n\nStill early, but it‚Äôs been working well for 30B‚Äì300B class models so far.",
                  "score": 1,
                  "created_utc": "2026-02-11 11:44:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ppj8q",
          "author": "ixrequalv",
          "text": "GitHub, harness for cicd\nSagemaker for everything it can, also bedrock and lambdas / other AWS services \nPrometheus cloud watch and grafana",
          "score": 2,
          "created_utc": "2026-02-11 00:19:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qu00i",
          "author": "Scared_Astronaut9377",
          "text": "\nLet me better describe what makes sense and what makes less sense. \n\n> Some stuff seem to be non-negotiable: Cloud provides\n\nI mean, there are many jobs that are MLOps in nature that require Spark, Flink, etc. instead of clouds. But they typically hire Data Engineers to do that lol.\n\n> Github Actions, GitLab or other? \n\nBoth work + many other building tools including those in major clouds.\n\n> For example, GitHub Actions -> Airflow DAG -> SageMaker job, or the pipeline occur integrated within Kubeflow? \n\nBoth work, action -> airflow -> training job is a very solid pattern for many use cases.\n\n> Are your inference endpoints deployed with FastAPI, KServe\n\nDepends on the requirements and the framework. Sometimes just throwing things in FastAPI is ok, sometimes you want, say, tf serving. I wouldn't touch lambda, always make your own containers. KServe is indeed solid for many use cases.\n\n> Cloud-nativa services like CloudWatch or Prometheus+Grafana?\n\nMostly what the company is already using lol. If I am designing from scratch, yeah, prometheus+grafana. \n\n> Kubeflow end-to-end\n\nKubeflow pipelines are good. Kubeflow as a platform is garbage. \n\n> everything in SageMaker\n\nWhen I was new to MLOps in cloud. Never again. First-class sitizen cloud services + k8s.",
          "score": 1,
          "created_utc": "2026-02-11 04:28:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyk81r",
      "title": "What course to take?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyk81r/what_course_to_take/",
      "author": "Berlibur",
      "created_utc": "2026-02-07 17:45:37",
      "score": 11,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "I'm a data scientist in a not too data scientisty company. I want to learn MLOps in a prod-ready way, and there might be budget for me to take a course.\n\nAny recommendations?\n\na colleague did a data bricks course on AI with a lecturer (online) and it was basically reading slides and meaningless notebooks. so trying to avoid that",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qyk81r/what_course_to_take/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o45uxfd",
          "author": "Competitive-Fact-313",
          "text": "take a simple iris dataset and then train the model and deploy the model using fastapi+UI(react or streamlit)- create the docker file and piush them to registry+also add mlflow for tracking+ once image is publish then create a CI/CD pipeline. Now take the image and publish using ecs + farget or eks. (you can also chose minikube or kind). Once done edit the dataset and trigger the pipeline. with every edit (data edit or model edit) your workflow should trigger and you will find how model perform. this is a typical mlops (traditional project). You will learn a lot using this. \n\nTo help you get started [https://github.com/amit-chaubey/mlops-docker-k8s-fastapi](https://github.com/amit-chaubey/mlops-docker-k8s-fastapi)  \nyou can check out this repo. edit as per your need. try not to do everthing by yourself (you are not a data scientist) so focus more on deployment and production part. ",
          "score": 4,
          "created_utc": "2026-02-07 23:14:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o445wpr",
          "author": "Commercial-Fly-6296",
          "text": "Datatalks club - not sure if it is prod ready though",
          "score": 3,
          "created_utc": "2026-02-07 17:51:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4629k3",
          "author": "NoobZik",
          "text": "Kedro + MLFlow is the minimum for production\nIf you want to dig deeper, Airflow + DVC + NannyML",
          "score": 1,
          "created_utc": "2026-02-08 00:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44i4t8",
          "author": "apexvice88",
          "text": "But why?",
          "score": 0,
          "created_utc": "2026-02-07 18:51:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44iimu",
              "author": "Berlibur",
              "text": "To really make the value from models come to life. \nOne offs or some analysis etc is one thing, having a properly deployed model + a process how to monitor / upgrade / etc is a whole other thing",
              "score": 1,
              "created_utc": "2026-02-07 18:53:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44sqrq",
                  "author": "apexvice88",
                  "text": "Very good reason",
                  "score": 1,
                  "created_utc": "2026-02-07 19:45:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45o1pz",
          "author": "Anti-Entropy-Life",
          "text": "I strongly recommend you don't take any courses. There is no need.\n\nIf you want to learn about LLMs, you can literally just ask the LLM.\n\nIf you need a proper gated workflow, I have a Dual Window Learning system you can use to teach yourself anything.\n\nI have found this to work better than any of the courses I ever tried out.\n\nYour mileage may vary, of course, but this seems to also be fairly common amongst people truly building with LLMs, so you may not get a lot of good course recommendations from this particular sub.\n\nSorry I can't be more helpful in this specific case, but would be happy to send you the doc on the Dual Window Learning system I use if you decide to go that route :)",
          "score": -1,
          "created_utc": "2026-02-07 22:34:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45ri6e",
              "author": "Berlibur",
              "text": "What do you mean by that system",
              "score": 2,
              "created_utc": "2026-02-07 22:54:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o463vqm",
                  "author": "Anti-Entropy-Life",
                  "text": "I'm sorry, I don't know if this a question, or what it's referring to, would it be possible to be more specific?",
                  "score": 0,
                  "created_utc": "2026-02-08 00:10:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qx9ieg",
      "title": "Do you still need MLOps if you're just orchestrating APIS and RAG?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/",
      "author": "polyber42",
      "created_utc": "2026-02-06 05:52:28",
      "score": 10,
      "num_comments": 21,
      "upvote_ratio": 0.75,
      "text": "I‚Äôm starting to dive into MLOps, but I‚Äôve hit a bit of a skeptical patch.\n\nIt feels like the \"heavy\" MLOps stack‚Äîexperiment tracking, distributed training, GPU cluster management, and model versioning‚Äîis really only meant for FAANG-scale companies or those fine-tuning their own proprietary models.\n\n  \nIf a compnay uses APIs(openai/anthropic), the model is a black box behind an endpoint. \n\nIn this case:  \n1. is there a real need for a dedicated MLOps role?\n\n2. does this fall under standard software engineering + data pipelines?\n\n3. If you're in this situation, what does your \"Ops\" actually look like? Are you mostly just doing prompt versioning and vector DB maintenance?\n\n  \nI'm curious if I should still spend time learning the heavy infra stuff",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3v5g2t",
          "author": "UnreasonableEconomy",
          "text": "Proompting isn't machine learning...\n\nEven RAG isn't machine learning. What are you learning? \n\nIf you're at least finetuning, then the need becomes obvious. But the ML field is significantly bigger than than just language models...",
          "score": 16,
          "created_utc": "2026-02-06 07:35:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v7jnr",
          "author": "Scared_Astronaut9377",
          "text": "You are very confused.",
          "score": 10,
          "created_utc": "2026-02-06 07:55:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v81df",
              "author": "polyber42",
              "text": "That's why I'm here - to clear up that confusion.  \nMaybe you can help me understand.  \nI'm genuinely trying to learn where the line is drawn.",
              "score": 5,
              "created_utc": "2026-02-06 07:59:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3y66ro",
                  "author": "Scared_Astronaut9377",
                  "text": "Read about how companies use ML. I cannot explain where the line between things you don't understand is.",
                  "score": -2,
                  "created_utc": "2026-02-06 18:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vme2o",
          "author": "Glad_Appearance_8190",
          "text": "i‚Äôve seen this land closer to ‚Äúops for behavior‚Äù than classic mlops. even with api models you still have prompt drift, data freshness, weird edge cases, and no idea why something changed last week. logs, traces, versioned prompts, and clear rollback end up mattering more than GPUs. heavy infra maybe not, but zero ops usually hurts later....",
          "score": 2,
          "created_utc": "2026-02-06 10:16:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ytuk0",
          "author": "Anti-Entropy-Life",
          "text": "If you are not training or serving your own models, you can ignore a lot of ‚Äúclassic MLOps‚Äù (distributed training, GPU fleet, checkpoint lineage). But you still need ops, because you are still shipping a probabilistic system whose behavior changes when any of these move: model endpoint, prompt/tooling, retrieval data, embeddings, index parameters, and guardrails.\n\nHow I usually frame it:\n\n1. Dedicated MLOps role?\n\n* Early stage: usually no. It is a backend or platform engineer plus a data engineer wearing an ‚ÄúLLM platform‚Äù hat.\n* You want a dedicated role when you have multiple teams shipping LLM features, regulated data, strict uptime, or you are doing frequent prompt and retrieval changes that need disciplined releases.\n\n1. Is it ‚Äújust software engineering + data pipelines‚Äù?\n\n* Mostly yes, but with extra failure modes: non-determinism, silent quality regressions, prompt injection, data leakage, vendor model updates, and evaluation that is not a simple unit test.\n* So, the missing piece is not GPU infra, it is evaluation, observability, and safety controls designed for LLM behavior.\n\n1. What does ‚ÄúOps‚Äù look like in API + RAG land?\n\n* Data and retrieval ops: ingestion, parsing, chunking, embedding generation, reindexing, backfills, access control, and index versioning/rollbacks.\n* Release management: prompt and config versioning, model version pinning, canary releases, fallbacks (smaller model, ‚Äúno answer‚Äù mode), and feature flags.\n* Evals: a regression suite with golden queries, retrieval quality checks (did we fetch the right docs), answer quality checks, and red team cases. Run it in CI before merges and continuously in production.\n* Observability: tracing across app ‚Üí retriever ‚Üí model call, token and latency budgets, cost tracking, citation coverage, refusal rates, and user feedback loops.\n* Security and compliance: prompt injection defenses, tool permissioning, PII filtering, and audit logs.\n\nSo yes, you still ‚Äúneed MLOps,‚Äù but it is closer to SRE + data engineering + QA for an LLM system. If you are choosing what to learn, prioritize: evaluation harnesses, observability, data/retrieval pipelines, and safe rollout patterns. Learn the heavy GPU stuff when you have a clear reason to own training or serving.\n\nI hope this helps! :)",
          "score": 2,
          "created_utc": "2026-02-06 20:47:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zcdel",
              "author": "Cat_Carrot",
              "text": "THIS.\n\nCall it LMOps if you want, OP, but these skills are a differentiator for standout engineers and definitely worth learning.\n\nIt‚Äôs not just ‚Äúlearning infra‚Äù; it‚Äôs learning to think about and work with tools and processes the same way you learned to work with simple functions.",
              "score": 3,
              "created_utc": "2026-02-06 22:19:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zf8s5",
                  "author": "Anti-Entropy-Life",
                  "text": "Thanks. I would argue that is what makes it so much fun! :D",
                  "score": 1,
                  "created_utc": "2026-02-06 22:34:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o423r00",
                  "author": "polyber42",
                  "text": "Got long way to go.",
                  "score": 1,
                  "created_utc": "2026-02-07 10:11:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o423mx9",
              "author": "polyber42",
              "text": "Thank you!",
              "score": 2,
              "created_utc": "2026-02-07 10:09:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o42mkfa",
                  "author": "Anti-Entropy-Life",
                  "text": "I'm glad it was helpful \\^-\\^",
                  "score": 1,
                  "created_utc": "2026-02-07 12:56:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3v4ucb",
          "author": "raiffuvar",
          "text": "Its probably need at least 1 mlops per 4 DS. \nAnd api is not everything. Even with API compani4s need to set up experiment tracking etc. \nThe issue is that companies do not understand the need of mlops.",
          "score": 3,
          "created_utc": "2026-02-06 07:30:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vfmp3",
          "author": "Classic_Swimming_844",
          "text": "RemindMe! -30 day",
          "score": 1,
          "created_utc": "2026-02-06 09:12:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vfqel",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 month on [**2026-03-08 09:12:08 UTC**](http://www.wolframalpha.com/input/?i=2026-03-08%2009:12:08%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/o3vfmp3/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qx9ieg%2Fdo_you_still_need_mlops_if_youre_just%2Fo3vfmp3%2F%5D%0A%0ARemindMe%21%202026-03-08%2009%3A12%3A08%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qx9ieg)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-06 09:13:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wbwm5",
          "author": "dan994",
          "text": "I think your confusion stems from thinking those things are just reserved for those training LLMs. There are many many companies training their own models that aren't LLMs, and all of those will need MLOps.\n\nIf you're not training and deploying models then your MLOps will likely just become DevOps",
          "score": 1,
          "created_utc": "2026-02-06 13:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40452t",
          "author": "Simple_Ad_9944",
          "text": "Yes you still need ‚Äúops,‚Äù but it looks different. If you‚Äôre calling black-box APIs, you‚Äôre not versioning weights, you‚Äôre versioning **inputs, policies, and failure handling**: prompt/config change control, eval suites, rollback, audit logs, escalation paths, and monitoring for drift in behavior even when the provider model changes under you. The hard part becomes governance + reliability, not training infra.\n\n  \nCurious how others are doing rollback / auditability for prompt+tool changes today.",
          "score": 1,
          "created_utc": "2026-02-07 00:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4310we",
          "author": "Driver_Octa",
          "text": "Even with API models, you still need ops for evals, prompt/version control, data quality, observability, latency/cost, and rollback when outputs drift. It‚Äôs closer to platform/SRE + data engineering than GPUcluster MLOps, but it‚Äôs still real. Tools like LangSmith or Traycer AI help with traceability (runs, prompts, diffs) so you can debug changes instead of guessing...",
          "score": 1,
          "created_utc": "2026-02-07 14:24:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xxly7",
          "author": "SpiritedChoice3706",
          "text": "1. It seems like your definition of \"ML\" is quite narrow and only includes GenAi. There are many, many kinds of ML algorithms in the industry - recommendation and ranking (search, suggestions), forecasting, etc. It's not getting the same kind of hype, but those models are definitely not black box APIs and occasionally actually require a lot of work to connect with the data properly, feature engineering, etc.\n\n2. Even in GenAI, Ops of some sort is often required for anything more than a pretty simple setup. It might end up being devops, but you need things like guardrails, rate limiting, and secure handling of API keys, not to mention appropriate data pipelining, in order to actually have these things return business value. And a lot of ML still involved in things like handling tokens efficiently, etc. So maybe this role isn't 'traditional' MLOps, but the use-case is certainly there. Maybe it's called \"AI engineering\". but it's certainly not that far off from trad MLOps, if a little different.\n\nIn my opinion, learning heavy infra  is one of the more valuable skills these days. Anyone can build now. Getting something scalable and secure into production? That is definitely a more secure skillset, even if the title shifts a bit.",
          "score": 0,
          "created_utc": "2026-02-06 18:10:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyi6ca",
      "title": "Why I chose Pulumi, SkyPilot, and Tailscale for a multi-tenant / multi-region ML platform and open-sourced it",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyi6ca/why_i_chose_pulumi_skypilot_and_tailscale_for_a/",
      "author": "DifficultDifficulty",
      "created_utc": "2026-02-07 16:27:06",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "As an MLOps Dev, I've stood up enough ML platforms to know the drill: VPC, EKS with GPU node pools, a dozen addons, an abstraction layer like Airflow, multi-tenancy, and maybe repeat it all in another region. The stack was usually Terraform, AWS Client VPN, Kubeflow or Airflow, and an external IdP like Okta.\n\nEvery time I'd finish, the same thought would creep up: \"If I started from scratch with fewer constraints, what would I actually pick?\"\n\nI finally worked through that question and open-sourced the result: \n\n**link**: [https://github.com/Roulbac/pulumi-eks-ml](https://github.com/Roulbac/pulumi-eks-ml)\n\n**The repo**\n\nIt's a Python library (named `pulumi-eks-ml`) of composable Pulumi components: VPC, EKS cluster, GPU node pools with Karpenter, networking topologies, etc. You import what you need and wire up your own topology rather than forking a monolithic template. The repo includes three reference architectures that go from simple to complex:\n\n- **Starter** : single VPC, single EKS cluster, recommended addons. Basically a \"hello world\" for ML on EKS.\n\n- **Multi-Region** : full-mesh VPC peering across regions, each with its own cluster. Useful if you need compute close to data in different geographies.\n\n- **SkyPilot Multi-Tenant** : the main one. Hub-and-spoke network, multi-region EKS clusters, a SkyPilot API server in the hub, isolated data planes (namespaces + IRSA) per team, Cognito auth, and Tailscale for VPN access.\n\n**Why SkyPilot?**\n\nI looked at a few options for the \"ML platform layer\" on top of Kubernetes and kept coming back to SkyPilot. It's fully open-source (no vendor lock beyond your cloud provider), it has a clean API server mode that supports workspaces with RBAC out of the box, and it handles the annoying parts of submitting jobs/services to Kubernetes, GPU scheduling, spot instance preemption, etc. It was a natural fit for a multi-tenant setup where you want different teams to have isolated environments but still share the underlying compute. It's not the only option, but for a reference architecture like this, its flexibility made it nice to build around.\n\n**Why Pulumi over Terraform?**\n\nHonestly, this mostly comes down to the fact that writing actual Python is nicer than HCL when your infrastructure has real logic in it. When you're looping over regions, conditionally peering VPCs, creating dynamic numbers of namespaces per cluster based on config, that stuff gets painful in Terraform. Pulumi lets you use normal language constructs, real classes, type hints, tests with pytest. The component model also maps well to building a library that others import, which is harder to do cleanly with Terraform modules. It's not that Terraform can't do this, it's just that the ergonomics of \"infrastructure as an actual library\" fit Pulumi better.\n\n**Why Tailscale?**\n\nThe whole network is designed around private subnets, no public endpoint for the SkyPilot API. You need some way to reach things, and Tailscale makes that trivially easy. You deploy a subnet router pod in the hub cluster, and suddenly your laptop can reach any private IP across all the peered VPCs through your Tailnet. No bastion hosts, no SSH tunnels, no client VPN endpoint billing surprises. It just works and it's basically a lot less config compared to the alternatives.\n\n**What this is and is not:**\n\n- This is not production-hardened. It's a reference/starting point, not a turnkey platform.\n- This is not multi-cloud. It's AWS-only (EKS specifically).\n- This is opinionated by design: the addon choices, networking topology, and SkyPilot integration reflect a specific yet limited set of use cases. Your needs might call for different designs.\n\nIf you're setting up ML infrastructure on AWS and want a place to start, or if you're curious about how these pieces fit together, take a look. Happy to answer questions or take feedback.",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qyi6ca/why_i_chose_pulumi_skypilot_and_tailscale_for_a/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r0tb2u",
      "title": "Lessons from Analyzing 18,000 Exposed Agent Instances",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0tb2u/lessons_from_analyzing_18000_exposed_agent/",
      "author": "RevealNoo",
      "created_utc": "2026-02-10 06:24:20",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "I work on security research at Gen Threat Labs, and we recently wrapped up an analysis of autonomous AI agents in production that I wanted to share. Specifically focused on OpenClaw given its popularity (165k GitHub stars and growing fast).\n\nQuick caveat upfront: our methodology has limitations. We scanned for exposed instances and analyzed publicly available community skills, but we don't have visibility into properly secured deployments or private enterprise setups. We also couldn't verify intent behind everything we flagged, so some of what we classified as malicious might just be poorly written code with bad patterns. Take the numbers with that context.\n\nThat said, what we found was worse than I expected going in.\n\nWe identified over 18,000 OpenClaw instances exposed directly to the internet. Not behind VPNs, not containerized, just sitting on default port 18789 accepting connections. One instance we found had full access to the user's email, calendar, and file system. Just... open. That one stuck with me because it's exactly the kind of setup that makes agents useful, and exactly what makes them dangerous.\n\nBut the finding that actually surprised me was in the community skill ecosystem. We analyzed hundreds of skills that users build and share, and nearly 15% contained what I'd classify as malicious instructions. Some were designed to download external payloads, others to exfiltrate data. A few had hidden logic that only triggered after repeated uses, which made them harder to catch in initial review.\n\nWe spent a while trying to use static analysis to catch these automatically, but the false positive rate was brutal. Ended up needing a mix of pattern matching and actually running skills in sandboxed environments to see what they do. Still not perfect.\n\nWe also noticed something frustrating: malicious skills that got flagged and removed would reappear under different names within days. Same payload, new identity. Whack a mole.\n\nThe attack pattern we kept seeing is what I've started calling \"Delegated Compromise.\" Instead of targeting the user directly, adversaries target the agent. Once they get in through prompt injection or a poisoned skill, they inherit every permission that user granted. It's honestly elegant from an attacker's perspective.\n\nTo OpenClaw's credit, their docs are transparent about this. They literally describe it as a \"Faustian bargain\" and acknowledge no perfectly safe setup exists. I respect that honesty, but I don't think most users deploying these agents fully internalize what that means.\n\nThe risk vectors we kept categorizing:\n\n‚Ä¢ Expanded attack surface from agents with read/write/execute across multiple applications ‚Ä¢ Prompt injection through messages and web content with hidden instructions ‚Ä¢ Supply chain risk from community skills built without security review ‚Ä¢ System level impact when broadly permissioned agents get compromised ‚Ä¢ What I've been calling \"judgment hallucination\" where agents appear trustworthy but lack genuine reasoning, so users over delegate\n\nIf you're running agents in production, the practical stuff that actually matters:\n\n‚Ä¢ Isolated environments (VMs or containers), not your primary machine ‚Ä¢ Don't expose default ports to public internet (seems obvious but 18,000 instances say otherwise) ‚Ä¢ Start read only, expand permissions incrementally ‚Ä¢ Secondary accounts during testing ‚Ä¢ Actually review activity logs, not just set and forget ‚Ä¢ Treat third party skills like installing unknown software, because that's basically what it is\n\nThe detection stuff we built for catching the hidden logic patterns, we've been calling it Agent Trust Hub internally. Happy to compare notes if anyone's working on similar approaches or has found better ways to handle the false positive problem.\n\nCurious how other teams are approaching this. Is agent security getting dedicated attention in your org, or is it still lumped in with general appsec? Trying to get a sense of whether this is becoming a recognized problem or if we're early to the panic.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0tb2u/lessons_from_analyzing_18000_exposed_agent/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4n5h9l",
          "author": "Informal_Tangerine51",
          "text": "18,000 exposed instances is concerning but the deeper problem is debuggability. When agent gets compromised via poisoned skill, can you reconstruct what it accessed and when?\n\nYou found malicious skills that trigger after repeated uses. That's nightmare fuel without audit trails. User runs skill 5 times safely, 6th execution exfiltrates data. Post-incident question: what did it access across all 6 runs? Most deployments can't answer without forensics.\n\n\"Judgment hallucination\" + over-delegation compounds when you can't verify agent decisions. User trusts agent with email access, agent gets compromised, attacker inherits permissions. Without signed traces of every action (what was accessed, what policy should have blocked it, when it happened), incident response is archaeology. Security gaps plus evidence gaps means compromise detection happens weeks late.",
          "score": 1,
          "created_utc": "2026-02-10 16:54:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0269t",
      "title": "What LLM workloads are people actually running asynchronously?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0269t/what_llm_workloads_are_people_actually_running/",
      "author": "NewClaim7739",
      "created_utc": "2026-02-09 11:50:20",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "Feels like most AI infra is still obsessed with latency when it isn't always the thing that moves the needle. The highest-volume workloads we're seeing are offline:\n\n‚Ä¢ eval pipelines  \n‚Ä¢ dataset labeling  \n‚Ä¢ synthetic data  \n‚Ä¢ document processing  \n‚Ä¢ research agents\n\nOnce you stop caring about milliseconds, the economics change completely.\n\nCurious what people here are running in batch vs realtime - and where the break-even tends to be?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0269t/what_llm_workloads_are_people_actually_running/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4f54ac",
          "author": "Otherwise_Wave9374",
          "text": "Totally seeing the same trend, batch wins. Research agents, doc pipelines, and evals are way more forgiving on latency, and you can do smarter scheduling (spot instances, queues, retries). The trick is getting idempotency and good observability so your agent runs are actually debuggable. I have some notes on async agent workloads and eval loops here: https://www.agentixlabs.com/blog/ What are people using for tracing in batch, OpenTelemetry or vendor tools?",
          "score": 3,
          "created_utc": "2026-02-09 12:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lhakg",
              "author": "NewClaim7739",
              "text": "For the model serving / inference API, we actually ended up building an internal batch tool for this because realtime pricing just didn‚Äôt make sense - happy to share details if useful",
              "score": 1,
              "created_utc": "2026-02-10 11:14:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4s1s1n",
              "author": "burntoutdev8291",
              "text": "Bot vs bot",
              "score": 1,
              "created_utc": "2026-02-11 10:51:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4h1r9b",
          "author": "penguinzb1",
          "text": "eval pipelines are the big one for us. we've been working on simulating agent runs to catch issues before they hit production, saves a lot of headaches when you can batch test 100s of scenarios overnight instead of waiting for users to hit edge cases",
          "score": 3,
          "created_utc": "2026-02-09 18:15:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfpmi",
              "author": "NewClaim7739",
              "text": "Super interesting - evals seem to be one of the main use cases. What are you using as the inference API?",
              "score": 1,
              "created_utc": "2026-02-10 11:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ghdmg",
          "author": "AIML_Tom",
          "text": "The common theme is that they‚Äôre not waiting on a human in real time ‚Äî they‚Äôre queued, retried, and scaled out with workers. Synchronous chat is the flashy demo, but async workloads are where throughput and reliability matter most.",
          "score": 2,
          "created_utc": "2026-02-09 16:38:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfmfi",
              "author": "NewClaim7739",
              "text": "Massively agree - 'set and forget' your agent and come back to the results you asked for when you've done other tasks",
              "score": 2,
              "created_utc": "2026-02-10 10:59:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1frz2",
      "title": "Learning AI deployment & MLOps (AWS/GCP/Azure). How would you approach jobs & interviews in this space?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1frz2/learning_ai_deployment_mlops_awsgcpazure_how/",
      "author": "c0bitz",
      "created_utc": "2026-02-10 22:44:19",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 0.89,
      "text": "I‚Äôm currently learning how to deploy AI systems into production. This includes deploying LLM-based services to AWS, GCP, Azure and Vercel, working with MLOps, RAG, agents, Bedrock, SageMaker, as well as topics like observability, security and scalability.\n\nMy longer-term goal is to build my own AI SaaS. In the nearer term, I‚Äôm also considering getting a job to gain hands-on experience with real production systems.\n\nI‚Äôd appreciate some advice from people who already work in this space:\n\nWhat roles would make the most sense to look at with this kind of skill set (AI engineer, backend-focused roles, MLOps, or something else)?\n\nDuring interviews, what tends to matter more in practice: system design, cloud and infrastructure knowledge, or coding tasks?\n\nWhat types of projects are usually the most useful to show during interviews (a small SaaS, demos, or more infrastructure-focused repositories)?\n\nAre there any common things early-career candidates often overlook when interviewing for AI, backend, or MLOps-oriented roles?\n\nI‚Äôm not trying to rush the process, just aiming to take a reasonable direction and learn from people with more experience.\n\nThanks üôå",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1r1frz2/learning_ai_deployment_mlops_awsgcpazure_how/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4pc5cb",
          "author": "bad_detectiv3",
          "text": "How are you learning to do this OP",
          "score": 3,
          "created_utc": "2026-02-10 23:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfgj3",
              "author": "c0bitz",
              "text": "Mostly self-study + building small experiments. I try to avoid just watching courses and instead replicate simple pipelines end-to-end from training to deployment even if it‚Äôs basic. Right now I‚Äôm more focused on understanding inference architecture and cost tradeoffs rather than just model building.",
              "score": 1,
              "created_utc": "2026-02-11 07:23:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pc20t",
          "author": "Otherwise_Wave9374",
          "text": "On the MLOps side, the \"agent\" specific stuff I see teams miss early is observability: log every tool call (inputs, outputs, latency), version prompts, and have a tiny golden eval set you can run on PRs.\n\nAlso, make the agent fail closed. If a tool is down or confidence is low, it should ask a clarifying question or hand off, not hallucinate.\n\nIf you want a few practical patterns for agent tracing/evals, I have been collecting notes here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-10 23:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfpih",
              "author": "c0bitz",
              "text": "Fail closed is such an underrated point. I‚Äôve seen too many demos where agents just hallucinate confidently instead of degrading gracefully.The golden eval set on PRs is smart too, are you automating those checks in CI or running them manually?",
              "score": 1,
              "created_utc": "2026-02-11 07:25:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pk9v5",
          "author": "overemployed74737",
          "text": "In my J2 im working as MLOps engineer  and during my interview i just explained the entire lifecycle for any ml models and talk about some differents needs between some models. Explained a little about observability and performance drift too",
          "score": 2,
          "created_utc": "2026-02-10 23:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfisd",
              "author": "c0bitz",
              "text": "That‚Äôs a good point. I‚Äôve noticed lifecycle/system thinking comes up way more than specific tools. When you explained drift and observability, did they go deep into monitoring stack questions or keep it high level?",
              "score": 1,
              "created_utc": "2026-02-11 07:23:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pkgcl",
          "author": "Competitive-Fact-313",
          "text": "I this your spectrum atm is too broad, try to narrow down a learn specific things first and then widen the scope. Making AI saas is one things and working in Mlops is another. If you define well I can help better. To start small just play with a simple linear regression model on sagemaker  and use how many instances endpoints you want‚Äî->> take a lambda function‚Äî‚Äî>api gateway ‚Äî-> test the api gateway endpoint using postman once done. Use your choice of frontend to show it as saas. This is the lowest level you can start with.",
          "score": 2,
          "created_utc": "2026-02-10 23:50:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfkqn",
              "author": "c0bitz",
              "text": "That‚Äôs actually helpful. Breaking it down that way makes it less overwhelming. I was thinking too much in terms of ‚Äúfull AI SaaS‚Äù instead of just understanding one clean deployment path first. Did you find AWS interviews expect hands-on experience with those services or mostly conceptual understanding?",
              "score": 2,
              "created_utc": "2026-02-11 07:24:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4spi8o",
                  "author": "Competitive-Fact-313",
                  "text": "In aws interview it depends for seniors roles they may ask you  hands on or sometimes just ask you something from the the pipeline so that mean you must have had those done before that‚Äôs the only things makes you explain stuff",
                  "score": 1,
                  "created_utc": "2026-02-11 13:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pt1ae",
          "author": "burntoutdev8291",
          "text": "In my experience, hiring has shifted more to understanding requirements and system design. I have never used bedrock or sagemaker, places I work at usually run self hosted vLLM. It also depends on the job description, backend roles have a bit more coding and system design, MLOps asks you on more MLOps stuff like model lineage, tracing, observability, and maybe some sysadmin stuff related to GPU. Never worked as an agentic or prompt engineer kind of AI engineer so I can't comment on that.",
          "score": 1,
          "created_utc": "2026-02-11 00:39:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4repik",
          "author": "caschir_",
          "text": "start with small projects deploying models on AWS or GCP, show code and infra. focus interviews on system design, cloud skills, and troubleshooting. practical demos impress more than theory alone.",
          "score": 1,
          "created_utc": "2026-02-11 07:16:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s13h1",
              "author": "c0bitz",
              "text": "Totally agree, practical demos always carry more weight. I‚Äôve been focusing on getting code + infra clean for simple model endpoints before scaling.",
              "score": 1,
              "created_utc": "2026-02-11 10:45:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyg78x",
      "title": "Best books/resources for production ML & MLOps?",
      "subreddit": "mlops",
      "url": "/r/learnmachinelearning/comments/1qyby8i/best_booksresources_for_production_ml_mlops/",
      "author": "Giux99",
      "created_utc": "2026-02-07 15:10:04",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qyg78x/best_booksresources_for_production_ml_mlops/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o4lxzaf",
          "author": "Big-Cockroach4492",
          "text": "[https://github.com/harvard-edge/cs249r\\_book?tab=readme-ov-file](https://github.com/harvard-edge/cs249r_book?tab=readme-ov-file)\n\n\n\n\n\n[https://github.com/harvard-edge/cs249r\\_book/blob/dev/book/README.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/README.md)\n\n  \n",
          "score": 1,
          "created_utc": "2026-02-10 13:14:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz7jpc",
      "title": "Every team wants \"MLOps\", until they face the brutal truth of DevOps under the hood",
      "subreddit": "mlops",
      "url": "/r/devops/comments/1qz7e1r/every_team_wants_mlops_until_they_face_the_brutal/",
      "author": "pm19191",
      "created_utc": "2026-02-08 12:21:10",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qz7jpc/every_team_wants_mlops_until_they_face_the_brutal/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r1tlnt",
      "title": "Need some suggestions on using Open-source MLops Tool",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1tlnt/need_some_suggestions_on_using_opensource_mlops/",
      "author": "NetFew2299",
      "created_utc": "2026-02-11 10:13:22",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "I am a Data scientist by Profession. For a project, I need to setup a ML Infrastructure in a local VM. I  am working on  A daily prediction /timeseries analysis. In the case of Open-Source, I have heard good things about ClearML (there are others, such as ZenML/MLrun), to my [knowledge.It](http://knowledge.It) is simply because it offers a complete MLops solution\n\nApart from this, I know I can use a combination of Mlflow, Prefect, Evidently AI, Feast, Grafana, as well. I want suggestions in case of ClearML, if any, on ease of use. Most of the Softwares claim, but I need your feedback.\n\nI am open to using paid solutions as well. My major concerns:\n\n1. Infrastructure cannot run on the cloud\n2. Data versioning\n3. Reproducible Experiment\n4. Tracking of the experiment\n5. Visualisation of experiment\n6. Shadow deployment\n7. Data drift",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r1tlnt/need_some_suggestions_on_using_opensource_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4s4xi3",
          "author": "kayhai",
          "text": "It sounds like you are keen on ClearML. I‚Äôve tried ML Flow (model registry and experiment tracking) + a scheduling tool of your choice (prefect, airflow or dagster etc). I‚Äôm not familiar with ClearML, may I ask which features of ClearML stands out to you?",
          "score": 1,
          "created_utc": "2026-02-11 11:19:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sb0a9",
              "author": "NetFew2299",
              "text": "It is simply because it offers a complete MLops solution.\n\n",
              "score": 2,
              "created_utc": "2026-02-11 12:07:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sbcu1",
          "author": "Fritos121",
          "text": "This is almost exactly what I came here looking for. A lot of focus on Cloud, but it‚Äôs been a bit harder for me to find resources on how best to deploy locally. Thanks for asking the question!",
          "score": 1,
          "created_utc": "2026-02-11 12:09:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sqchp",
          "author": "Garbatronix",
          "text": "I have had positive experiences using LakeFS in conjunction with MinIO. It enables you to version data in a similar way to Git. With an MLFlow server, I can log all the relevant parameters, such as branch and ref. MLFlow enables models to be versioned and stored. An MLFlow Docker image can then be generated and easily deployed on a Docker host or Kubernetes. \n\nDrift detection and data visualisation can be implemented in Python scripts prior to training and stored as artefacts in MLFlow. I have created a custom Python model in MLFlow by generating my own Prometheus metrics. These can then be collected via Prometheus and visualised in Grafana.",
          "score": 1,
          "created_utc": "2026-02-11 13:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sru98",
          "author": "DifficultDifficulty",
          "text": "\"I need to setup a ML infrastructure in a local VM\" -> is this infra mostly for your own VM-local experiments, and is there no need to distribute workloads in the cloud where the infra would be shared by multiple team members?",
          "score": 1,
          "created_utc": "2026-02-11 13:52:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwz1y7",
      "title": "Open sourced an AI for debugging production incidents - works for ML infra too",
      "subreddit": "mlops",
      "url": "https://v.redd.it/jmn587p30rhg1",
      "author": "Useful-Process9033",
      "created_utc": "2026-02-05 21:59:17",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwz1y7/open_sourced_an_ai_for_debugging_production/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3yvxt7",
          "author": "Anti-Entropy-Life",
          "text": "This looks pretty great! A few things that stand out as especially well done:\n\nThe framing is correct. Root cause analysis in prod is mostly about context stitching across logs, metrics, and deploys, not about clever ML. The fact that the agent explicitly gathers and correlates across the stack is the real value here.\n\nI like that it reads the system on setup. Most tools fail because they stay generic and never internalize how a specific system is wired. Treating that as a first-class step is the right call.\n\nPosting findings back into Slack is underrated but critical. Debugging lives where humans already are. Anything that requires a separate UI to be useful usually dies.\n\nOne thing I would watch carefully as this evolves is trust calibration. Engineers need to know when the AI is confident versus when it is exploring. Clear signals around evidence strength, uncertainty, and ‚Äúthis is my best hypothesis‚Äù vs ‚Äúthis is confirmed‚Äù will matter a lot for adoption.\n\nAnother thought is historical learning. If logs expire, the real long-term value may be in the incident summaries it generates and how those feed future investigations. That is where this could compound.\n\nOverall, this looks like real infrastructure, not a demo. If you keep it grounded in observability first and avoid overclaiming intelligence, this could be something teams actually depend on in the future!",
          "score": 1,
          "created_utc": "2026-02-06 20:57:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwdbtk",
      "title": "My fraud model didn‚Äôt crash  it quietly dropped from F1 0.79 ‚Üí 0.18. Here‚Äôs how I caught it.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qwdbtk/my_fraud_model_didnt_crash_it_quietly_dropped/",
      "author": "AffectionateSir8341",
      "created_utc": "2026-02-05 06:00:22",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I had a fraud detection demo where nothing ‚Äúbroke‚Äù in production.\n\nNo errors, no crashes, no deploys.\n\n\n\nBut the model‚Äôs F1 score quietly dropped from 0.79 to 0.18 ‚Äî purely due to data drift.\n\n\n\nThat‚Äôs what scares me most about production ML: models don‚Äôt fail loudly,\n\nthey slowly start lying.\n\n\n\nTo explore this properly, I built ModelGuard ‚Äî a small reference implementation\n\nfocused on what happens \\*after\\* deployment:\n\n\n\n\\- detects data & prediction drift using multiple statistical tests\n\n\\- scores severity (not just yes/no drift)\n\n\\- recommends actions (ignore / monitor / retrain / rollback)\n\n\\- gates retraining with human approval\n\n\\- exposes everything via a CLI + lightweight REST API\n\n\n\nThis is intentionally not a framework or SaaS ‚Äî just a learning artifact for\n\napplied ML / MLOps.\n\n\n\nThe demo uses the Kaggle credit card fraud dataset with a simulated fraud-ring\n\nattack. The original dataset is extremely imbalanced (0.17% fraud), so I use\n\nbalanced resampling for faster iteration. Drift is synthetically injected, but\n\nall drift statistics and severity scores are real calculations.\n\n\n\nIn the demo, ModelGuard:\n\n\\- detects drift in 19 / 30 features (63%)\n\n\\- assigns HIGH severity (0.62)\n\n\\- flags a 77% drop in fraud-class F1\n\n\\- recommends retraining and creates a pending alert for human review\n\n\n\nRepo: [https://github.com/Aagam-Bothara/ModelGuard](https://github.com/Aagam-Bothara/ModelGuard)\n\n\n\nI‚Äôd really appreciate feedback from folks running models in prod:\n\n‚Äì does this feel realistic or over-engineered?\n\n‚Äì what would you simplify or remove?\n\n‚Äì what‚Äôs the first thing you‚Äôd add?\n\n",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwdbtk/my_fraud_model_didnt_crash_it_quietly_dropped/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvx8lj",
      "title": "The AI Analyst Hype Cycle",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/the-ai-analyst-hype-cycle",
      "author": "growth_man",
      "created_utc": "2026-02-04 18:52:19",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qvx8lj/the_ai_analyst_hype_cycle/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3z8txn",
          "author": "Anti-Entropy-Life",
          "text": "This is so real, and makes me so happy about the work I am doing in my lab, I can't wait to unveil our product that solves all of these issues :D",
          "score": 1,
          "created_utc": "2026-02-06 22:01:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxy8lw",
      "title": "Jupyter Notebook Validator Operator for automated validation in MLOps pipelines",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qxy8lw/jupyter_notebook_validator_operator_for_automated/",
      "author": "millionmade03",
      "created_utc": "2026-02-06 23:58:01",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.72,
      "text": "\\- üìä Built-in observability: Expose Prometheus metrics and structured logs so you can wire dashboards and alerts quickly.\n\n\n\nHow you can contribute\n\n\n\n\\- Smart error messages (Issue #9)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/9)): Make notebook failures understandable and actionable for data scientists.\n\n\n\n\\- Community observability dashboards (Issue #8)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/8)): Build Grafana dashboards or integrations with tools like Datadog and Splunk.\n\n\n\n\\- OpenShift-native dashboards (Issue #7)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/7)): Help build a native dashboard experience for OpenShift users.\n\n\n\n\\- Documentation: Improve guides, add more examples, and create tutorials for common MLOps workflows.\n\n\n\n\n\nGitHub: [https://github.com/tosin2013/jupyter-notebook-validator-operator](https://github.com/tosin2013/jupyter-notebook-validator-operator)\n\n\n\n\n\nDev guide (local env in under 2 minutes): [https://github.com/tosin2013/jupyter-notebook-validator-operator/blob/main/docs/DEVELOPMENT.md](https://github.com/tosin2013/jupyter-notebook-validator-operator/blob/main/docs/DEVELOPMENT.md)\n\n\n\n\n\nWe're at an early stage and looking for contributors of all skill levels. Whether you're a Go developer, a Kubernetes enthusiast, an MLOps practitioner, or a technical writer, there are plenty of ways to get involved. Feedback, issues, and PRs are very welcome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qxy8lw/jupyter_notebook_validator_operator_for_automated/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o42ytqi",
          "author": "Anti-Entropy-Life",
          "text": "Very cool! Turning notebooks into an *observable* pipeline step is overdue. Any plans for env/image pinning + artifact capture for reproducibility, and timeouts/resource limits for safety? Smart errors that point to the failing cell would be üî•",
          "score": 1,
          "created_utc": "2026-02-07 14:12:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46jjrh",
              "author": "millionmade03",
              "text": "Some of those features should be there I would need to look at adding the other features.",
              "score": 1,
              "created_utc": "2026-02-08 01:46:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyp1t2",
      "title": "Tech job search : how to get an entry level positions in tech.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyp1t2/tech_job_search_how_to_get_an_entry_level/",
      "author": "Traditional-War-9554",
      "created_utc": "2026-02-07 20:51:19",
      "score": 3,
      "num_comments": 10,
      "upvote_ratio": 0.8,
      "text": "recent graduate and no prior work experience[](https://www.reddit.com/submit/?source_id=t3_1qyp0i3)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qyp1t2/tech_job_search_how_to_get_an_entry_level/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o45s73h",
          "author": "denim_duck",
          "text": "mlops is not an entry level role",
          "score": 8,
          "created_utc": "2026-02-07 22:58:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ja8f",
              "author": "Traditional-War-9554",
              "text": "ok, can you suggest other roles to start with ?",
              "score": 0,
              "created_utc": "2026-02-08 11:26:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48x7xs",
                  "author": "denim_duck",
                  "text": "What was your plan for after graduation when you started school?",
                  "score": 1,
                  "created_utc": "2026-02-08 13:17:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o455ihd",
          "author": "randoomkiller",
          "text": "what are your qualifications?",
          "score": 3,
          "created_utc": "2026-02-07 20:54:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45zl3u",
          "author": "SpiritedChoice3706",
          "text": "The first step is probably posting in a relevant subreddit.",
          "score": 2,
          "created_utc": "2026-02-07 23:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46edob",
          "author": "ApprehensiveFroyo94",
          "text": "I really don‚Äôt want to be the bearer of bad news, but this is not an entry level field. \n\nI know sometimes this sub might feel like it‚Äôs gatekeeping the mlops role, but we say this for a reason - there‚Äôs just so many tools you need to understand, build, and monitor, which is just not feasible for someone with no work experience.\n\nTry going for analytics / entry ds role if you have a sufficient math background and move from there.",
          "score": 2,
          "created_utc": "2026-02-08 01:13:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48jc06",
              "author": "Traditional-War-9554",
              "text": "i dont like analytics",
              "score": 0,
              "created_utc": "2026-02-08 11:27:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48wp5z",
                  "author": "denim_duck",
                  "text": "You don‚Äôt have to like it. You have to do it.",
                  "score": 3,
                  "created_utc": "2026-02-08 13:13:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47fxkz",
          "author": "Boognish28",
          "text": "Mlops is a mixture of a ton of specializations. Traditional engineering, delivery, infra, sre, etc. \n\nLearn one. Then learn another. Give it five or ten years, then you might be good.",
          "score": 2,
          "created_utc": "2026-02-08 05:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45ocdu",
          "author": "Anti-Entropy-Life",
          "text": "Build something end-to-end, make it public on GitHub, then find jobs looking for people to build similar things at scale.",
          "score": 1,
          "created_utc": "2026-02-07 22:36:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvw8p3",
      "title": "Traditional OCR vs AI OCR vs GenAI OCR. When does this become a systems problem?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qvw8p3/traditional_ocr_vs_ai_ocr_vs_genai_ocr_when_does/",
      "author": "Tricky_Reveal_5951",
      "created_utc": "2026-02-04 18:17:32",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Early OCR conversations often focus on models and accuracy benchmarks.\n\nIn production, the harder problems show up elsewhere.\n\nTraditional OCR fails quietly when layouts drift.\n\n AI based OCR improves coverage but needs stronger guardrails.\n\n GenAI works on complex documents, but requires careful controls to avoid unreliable outputs.\n\nAt scale, OCR becomes less about choosing a model and more about designing a system that knows when to trust automation and when to stop.\n\nMost production pipelines rely on layered approaches, confidence thresholds, fallback strategies, and human review for edge cases.\n\nFor teams running document extraction in production, when did choosing an OCR approach turn into an MLOps and systems decision for you?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qvw8p3/traditional_ocr_vs_ai_ocr_vs_genai_ocr_when_does/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3m1p2f",
          "author": "Informal_Tangerine51",
          "text": "OCR accuracy matters less than extraction accountability. When GenAI extracts wrong field from invoice, can you prove which text it saw and why it chose that interpretation?\n\nWe layer OCR methods too - traditional for structured forms, AI for complex layouts, GenAI for unstructured docs. Works until Legal asks \"what text informed this customer data extraction\" and we have confidence scores but not the actual OCR output the model operated on.\n\nThe systems problem isn't fallback strategies, it's decision evidence. Confidence threshold says \"trust this\" but doesn't capture what text was extracted, whether it was from correct page region, or why boundary detection chose those coordinates.\n\nYour layered approach handles accuracy. The gap: when extraction breaks, can you replay the exact OCR output and layout interpretation the model saw? Or just thresholds and final results?",
          "score": 5,
          "created_utc": "2026-02-04 22:17:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o4nph",
          "author": "Zoekielshane",
          "text": "From what I have seen, production OCR success depends more on system design and trade offs than picking the most accurate models choice. I have recently tested Traditional OCR (Tesseract), Deep Learning OCR (PaddleOCR), and GenAI OCR (VLM-based) on 10K+ financial documents. We shared real production examples and lessons learned here, in this technical writeup if it is useful context:\n https://visionparser.com/blog/traditional-ocr-vs-ai-ocr-vs-genai-ocr",
          "score": 1,
          "created_utc": "2026-02-05 05:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oziai",
              "author": "letsTalkDude",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-05 10:19:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ozl2y",
                  "author": "Zoekielshane",
                  "text": "welcome",
                  "score": 1,
                  "created_utc": "2026-02-05 10:20:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qws06r",
      "title": "CI quality gatekeeper for AI agents",
      "subreddit": "mlops",
      "url": "https://github.com/marketplace/actions/maos-agentgate-ci-quality-gatekeeper-for-ai-agents",
      "author": "TranslatorSalt1668",
      "created_utc": "2026-02-05 17:44:21",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qws06r/ci_quality_gatekeeper_for_ai_agents/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwrdn9",
      "title": "What happens when you outgrow the wrappers?",
      "subreddit": "mlops",
      "url": "/r/LocalLLaMA/comments/1qwrd6z/what_happens_when_you_outgrow_the_wrappers/",
      "author": "Left-Reflection-8508",
      "created_utc": "2026-02-05 17:21:51",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwrdn9/what_happens_when_you_outgrow_the_wrappers/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o3z0mom",
          "author": "Anti-Entropy-Life",
          "text": "Neocloud vs AWS:\n\nNeoclouds can be great for raw GPU cost, but you usually trade money for operational burden:\n\n* Spiky capacity\n* Weaker networking or storage primitives\n* Slower support and incident response\n* More SRE work on your side\n\nA common compromise:\n\n* Run the control plane and observability where reliability is high (often AWS or a stable cloud)\n* Run GPU execution where it is cheapest, with fallback\n\n\n\nThe Middle Ground Most People Never See:\n\nYou do not need AWS or a wrapper if you separate control from execution.\n\nRun a small control plane yourself on cheap, boring infra (Hetzner, OVH, or a single VM):\n\n* routing\n* versioning\n* observability\n* rollouts\n* cost visibility\n\nThen attach execution underneath it:\n\n* cheaper GPU providers\n* on prem GPUs\n* spot instances\n* even wrappers as one backend\n\nAt that point, AWS and wrappers stop being architectural commitments and become interchangeable execution targets.\n\nSimple Rule Of Thumb:\n\n* If the wrapper is cheaper than the engineering time to replace it, keep it.\n* If it blocks reliability, observability, or unit economics, build a thin control plane first.\n* Move execution only when cost or reliability forces you.\n\nThis path gives leverage without overbuilding.",
          "score": 1,
          "created_utc": "2026-02-06 21:21:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z507a",
              "author": "Left-Reflection-8508",
              "text": "Appreciate the thoughtful reply, thank you.\n\nIt sounds like you've done it, is that so?",
              "score": 1,
              "created_utc": "2026-02-06 21:42:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3z77e6",
                  "author": "Anti-Entropy-Life",
                  "text": "Yeah, I have done a smaller version of it. We ran FastAPI as a thin control layer in front of GPUs on Lambda so the interface, routing, and logging were ours, and compute was just an execution backend.\n\nThe separation worked well conceptually, but at the scale we were at it ended up costing more than I expected once you factor in always-on capacity and lower utilization. That pushed me to stay inside wrapper constraints a bit longer while being more deliberate about versioning and observability.\n\nThe main thing I took away is that owning even a minimal control plane is the key inflection point. You do not need to go all the way to full self hosting immediately, but once you own the interface, switching execution backends becomes a business decision instead of a rewrite.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:53:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}