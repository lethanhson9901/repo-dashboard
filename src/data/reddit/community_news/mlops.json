{
  "metadata": {
    "last_updated": "2026-02-10 17:16:23",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 20,
    "total_comments": 54,
    "file_size_bytes": 88610
  },
  "items": [
    {
      "id": "1qvhmjc",
      "title": "The weird mismatch in MLOps hiring that nobody talks about",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qvhmjc/the_weird_mismatch_in_mlops_hiring_that_nobody/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-04 06:58:18",
      "score": 64,
      "num_comments": 18,
      "upvote_ratio": 0.93,
      "text": "Something I've noticed after being in this space for a while, and mentioned in past weeks' posts as well.\n\nMLOps roles need strong infrastructure skills. Everyone agrees on that. The job descriptions are full of Kubernetes, CI/CD, cloud, distributed systems, monitoring, etc.\n\nBut the people interviewing you? Mostly data scientists, ML engineers, and PhD researchers.\n\nSo you end up in a strange situation where the job requires you to be good at production engineering, but the interview asks you to speak ML. And these are two very different conversations.\n\nI've seen really solid DevOps engineers, people running massive clusters, handling serious scale, get passed over because they couldn't explain what model drift is or why you'd choose one evaluation metric over another. Not because they couldn't learn it, but because they didn't realise that's what the interview would test.\n\nAnd on the flip side, I've seen ML folks get hired into MLOps roles and MAY struggle because they've never dealt with real production systems at scale.\n\nThe root cause I think is that most companies are still early in their ML maturity. They haven't separated MLOps as its own discipline yet. The ML team owns hiring for it, so naturally, they filter for what they understand: ML knowledge, not infra expertise.\n\nThis isn't a complaint, just an observation. And practically speaking, if you're coming from the infra/DevOps side, it means you kinda have to meet them where they are. Learn enough ML to hold the conversation. You don't need to derive backpropagation on a whiteboard, but you should be able to talk about the model lifecycle, failure modes, why monitoring ML systems is different from monitoring regular services, etc.\n\nThe good news is the bar isn't that high. A few weeks of genuine study go a long way. And once you bridge that language gap, your infrastructure background becomes a massive advantage, because most ML teams are honestly struggling with production engineering.\n\nCurious if others have experienced this same thing? Either as candidates or on the hiring side?\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:¬†[topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qvhmjc/the_weird_mismatch_in_mlops_hiring_that_nobody/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3hxttg",
          "author": "BeatTheMarket30",
          "text": "I'm not surprised as often people doing the hiring - hiring managers and recruiters are also clueless. They are hiring people with skills they themselves don't have, so have no clue what questions to ask. Data scientists will of course ask questions on ML that nobody besides themselves needs to know much about.\n\nMost open positions are also junior ones, while this field could benefit from solid DevOps experience. They don't seem to grasp that transition from DevOps to MLOps is perhaps just a few weekends of study as these people don't need low level details/maths. They will not be implementing KServe, Dynamo, vLLM, Tensor-RT, they will be deploying these solutions and monitoring them.\n\nFor a data scientist, MLOps will not be interesting and they will likely leave.",
          "score": 20,
          "created_utc": "2026-02-04 08:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ka9em",
              "author": "teucros_telamonid",
              "text": "If you really want to understand the root problem, you need to think in terms of the business life cycle.\n\nStartups are just happy to secure some ML engineer, data scientist or any other similar title. The focus is to build something at least working, MLOps is not a priority yet. Even if they hire you, it will get ugly soon.\n\nScale-ups are at least starting to get in place there scale and quality becomes important, but they probably still too focused on their competitive advantage. Some AI feature that is, not something smooth and well running from day to day.\n\nI think the best places are way more mature companies which have experience with AI and want to turn it from a successful experiment to really stable service. But these are already quite successful, so not a lot of them in this field...",
              "score": 2,
              "created_utc": "2026-02-04 17:20:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ie23t",
          "author": "proof_required",
          "text": "ML + anything is pretty much a big umbrella of things. Same goes for dedicated ML engineers. They might be asked DevOps question and won't be hired since there might be someone out there in this market who knows it. It's just that the companies can be super picky! So yeah messing up 1-2 questions can reduce your chances of hiring in this market.",
          "score": 7,
          "created_utc": "2026-02-04 11:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k8ffi",
              "author": "eled_",
              "text": "In my experience it's more to do with the fact that a lot of companies, even in the \"AI\" space, have yet to understand what MLeng is really about. Probably even more so than MLOps which can have a more \"direct\" impact on operations when they're not around.\n\nOften MLE are just mapped to \"DS with a few SE chops\", when it's not \"DS without an explicit aversion for SE\". And it's plain wrong, and it has an opportunity cost, but it's what you tend to get when data-scientists select and manage them.",
              "score": 2,
              "created_utc": "2026-02-04 17:12:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i7oja",
          "author": "karthikjusme",
          "text": "This is my biggest worry when applying new jobs. I am good taking models to production, setting pipelines for training and evaluation and dataset creation but I lack many concepts inside LLM's. Hard to keep track of them as well as there is a new thing every week.",
          "score": 6,
          "created_utc": "2026-02-04 10:06:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j8p2f",
          "author": "TheRealStepBot",
          "text": "Maybe you don‚Äôt understand what mlops is? It‚Äôs not just running kubernetes for the ml team that‚Äôs for certain. \n\nIt‚Äôs responsible for managing the overall model lifecycle. The physical infrastructure to do that is just a necessary evil you have to deal with. The actual skills are related to understanding all the assumptions being made in the model and the data to make sure that that the performance at inference time matches performance at train time. \n\nThis involves a ton of work on how data is collected for the training process and then debugging at inference time to determine if the model is performing as intended and dig through all the layers of the stack to figure out why it may not be performing correctly. \n\nYou can have a perfectly good model trained on great data that is undeployable because the inference process has slightly different consistency guarantees from the offline training data. MLops is the final stop on the make it all work train. It‚Äôs not possible to do the job in any non trivial setting without a deep understanding of the math probability and statistics of what is going on.",
          "score": 4,
          "created_utc": "2026-02-04 14:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xzg1j",
              "author": "SpiritedChoice3706",
              "text": "Scrolled down way too far before I got to this. I agree, lack of engineering skills is a big problem in hiring MLOps folks. And not just because you need a knowledge of DS (though you definitely do). The fact is, ML applications are different than plain Software applications, and there are some specific software skillsets that need to be applied to effectively serve ML.\n\nOn my last project, I was the bridge between a DS team and a Platform team. We had extreme troubles, because the Platform team didn't understand how serving an ML model is different than serving a normal API. For example, the ML artifacts we needed (or the size of them). They were really upset that we wanted to put a large model file in our deployment, but also told us we shouldn't need a volume mount - LOL. How else you gonna serve a model? I have dozens of examples like this. But also, in order to serve my DS' models, I need some understanding of how it works. I don't need the nitty gritty, but I need a solid working knowledge. How I pipeline and retrain a supervised vs unsupervised model is different. How I evaluate the model for drift depends on the algorithm.\n\nMy boss also once was short on MLOps engineers so he had to borrow some DevOps engineers. He said that two of them couldn't do what one engineer from our team can. Not because they weren't skilled - but because the lack of context for what they were engineering was a huge part.",
              "score": 3,
              "created_utc": "2026-02-06 18:18:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3y7hd7",
                  "author": "TheRealStepBot",
                  "text": "Its really all about managing reproducibility. Perfect reproducibility is expensive. So knowing what you can relax and still hit operational targets is the name of the game.",
                  "score": 1,
                  "created_utc": "2026-02-06 18:56:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3izf35",
          "author": "Beneficial_Aioli_797",
          "text": "The idea i have about MLOps is that no one wants to do it. Everyone wants to do data science or machine learning engineering because thats whats most attractive. As result, these jobs are much more competitive and MLOps or infra roles adjacent to ML are left In the shadows.\n\n\nThey are still incredibly well paid, highly specialized skills and imo are currently at a discount in the market. And the market favours you to be ML engineer first and DevOps second (for some reason).\n\n\nIn my opinion, MLOps and DE focused on features stores/online learning etc are even better than ML engineering.",
          "score": 5,
          "created_utc": "2026-02-04 13:32:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3new0k",
              "author": "weeyummy1",
              "text": "\\> They are still incredibly well paid, highly specialized skills and imo are currently at a discount in the market\n\nWhat do you mean by this? Are you saying it should be higher paid than it is?",
              "score": 1,
              "created_utc": "2026-02-05 02:49:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3p0k1x",
                  "author": "Beneficial_Aioli_797",
                  "text": "No. When i mean discount its Im the economical sense.\n\n\nML engineering and MLOps are similar in terms of pay, but competition for ML engineering is much more fierce. So you end up \"paying\" more to get a ML engineering job than MLOps and they both have the same face value.",
                  "score": 1,
                  "created_utc": "2026-02-05 10:29:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hz8nt",
          "author": "No_Mongoose6172",
          "text": "In my experience, deployment is one of the things less taken into account in ML, yet I find it to be one of the most interesting and demanding",
          "score": 2,
          "created_utc": "2026-02-04 08:46:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3si54q",
          "author": "d1ddydoit",
          "text": "What different teams are hiring for will depend on where that team is in terms of the maturity of its ML system as you rightly say OP. If they are at low levels of maturity, it is as you say often teams of scientists who are hiring and they will often just look for an engineer who can scale up and automate a lot of what they have been crafting - that means at interview they will want an engineer that talks their language and shows that they understand them. If the hiring org is mature, they have employed a system / platform that clearly dictates the engineering skills that need to be brought in.\n\nEvery business and even within a large business, the level of maturity varies dramatically. My advice to applicants looking for roles is to find the role that matches your skills and experience given that the responsibilities are so varied from company to company for the same job title. If you are looking to learn, a firm that is mature is the easiest way to understand scale but if you want a challenge, want to shape something that could be exciting, take a chance on a firm that doesn‚Äôt know what it wants if you think you can really help, you like their culture and the remuneration is agreeable. It could still be the happiest place you‚Äôve ever worked (and some of the most fun data scientists I have ever worked with were the most clueless at what was needed outside of their IDE to make it all work).\n\nThat said‚Ä¶. If you aren‚Äôt able to explain concepts like model drift then you are unable to explain a key metric that helps determine the end of a model‚Äôs lifecycle (by monitoring its declining quality and kicking off a new build of the model). It is still never going to be the same as a DevOps or platform engineer and there will have to be language learnt that is employed by the data scientists if the engineers have not been/worked closely with data scientists previously.",
          "score": 2,
          "created_utc": "2026-02-05 21:37:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3izx7o",
          "author": "NotSoGenius00",
          "text": "Hot take devops is least mandatory thing for MLOps ! Do you need to know infra ? Yes. Should you setup all the infra yourselves? Fuck no ! There is a devops team for that. Yes data scientists will ask you traditional ML questions but you should know traditional ML because you will be deploying those models so you should be worried about things like latency, memory requirements etc. an MLOps engineer is more like ML + software rather than devops.  \n\nSo devops is the least important thing in my opinion. You need to know modelling well enough to deploy those models ! Tooling is not all you need",
          "score": 3,
          "created_utc": "2026-02-04 13:35:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hp7bv",
          "author": "mystery_biscotti",
          "text": "Literally my plan. Decided to look at an AI professional certification because I need to speak the language. I miss my prod Ops jobs, monitoring systems, proactively fixing stuff, rolling my eyes at the adorable things vendors do during maintenance windows...",
          "score": 2,
          "created_utc": "2026-02-04 07:14:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k8289",
          "author": "klipseracer",
          "text": "How is this weird? They are looking for some who understands BOTH: deep learning models, how to package and distribute them, your experience with tools like DVC, the challenges around versioning models/weights within a data scientists typical workflow without disrupting it or making iterating painful, yada yada.\n\nPlus, they want to know if you have the infra skills they don't have. These two things allows you to meet them in the middle and understand their needs and help them understand what they don't know and would be absolutely expected. The fact you don't realize this is one reason you're not getting the job.\n\nReference: \nI used to work at a deep learning computer vision startup and wrote the cicd that deployed our models to thousands of physical locations across the USA.",
          "score": 1,
          "created_utc": "2026-02-04 17:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5axq",
          "author": "simpleharmonicmotion",
          "text": "+1 This is my life :/ I've flat out said that we're lying to candidates by requiring ML skills for what turns out to be an infra job.",
          "score": 1,
          "created_utc": "2026-02-05 05:43:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k19e8",
          "author": "fiddysix_k",
          "text": "Mlops is just devops except everyone around you pretends you're not just doing standard devops work until you actually just gaslight yourself into believing it.",
          "score": 1,
          "created_utc": "2026-02-04 16:39:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzaeeu",
      "title": "Best resource to learn modular code for MLOPs",
      "subreddit": "mlops",
      "url": "https://i.redd.it/bsf6sm1c7aig1.jpeg",
      "author": "Deep-Blue-Sea-645",
      "created_utc": "2026-02-08 14:32:30",
      "score": 32,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qzaeeu/best_resource_to_learn_modular_code_for_mlops/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o49arex",
          "author": "MattA2930",
          "text": "Check out ArjanCodes on YouTube. Great channel on code design in Python, and should help you re-write your notebook functionality with Python best practices.\n\nThere is no single right way though. I usually advise to do whatever you think makes it easiest for someone else to come in and make changes to your codebase.",
          "score": 10,
          "created_utc": "2026-02-08 14:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ebvxk",
              "author": "JayRathod3497",
              "text": "Yes I have followed him for FastAPI modulation",
              "score": 1,
              "created_utc": "2026-02-09 07:31:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o49au2g",
          "author": "MindlessYesterday459",
          "text": "Cookiecutter data science could be relevant here.\n\nhttps://cookiecutter-data-science.drivendata.org/",
          "score": 3,
          "created_utc": "2026-02-08 14:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49ke8m",
          "author": "alex_0528",
          "text": "Marvelous MLOps combines both modular code and notebooks in Databricks so you've got the utility of both: https://www.marvelousmlops.io/\n\nThey also cover ditching the notebooks altogether for paramterised scripts. \n\nYes they use Databricks as the platform to deliver this but the principal is pretty universal and could be applied elsewhere, especially once you've started using the scripts to run your modular, testable code.",
          "score": 3,
          "created_utc": "2026-02-08 15:31:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f66bv",
              "author": "Moist-Matter5777",
              "text": "Databricks is great for that! If you're looking for more variety, check out the MLOps Specialization on Coursera. It dives into modular code practices across different platforms and tools. Also, the book \"Building Machine Learning Powered Applications\" has some solid insights on structuring your code.",
              "score": 2,
              "created_utc": "2026-02-09 12:15:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o49ao42",
          "author": "_caramel_popcorn",
          "text": "Artifacts should be stored remotely right?",
          "score": 1,
          "created_utc": "2026-02-08 14:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49lvzf",
          "author": "Standard-Distance-92",
          "text": "How about Asset bundles MLOps stacks?",
          "score": 1,
          "created_utc": "2026-02-08 15:39:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49nv22",
          "author": "Krekken24",
          "text": "Check my comment which I did on some other post - [link](https://www.reddit.com/r/learnmachinelearning/s/RLENZH0ZuD)",
          "score": 1,
          "created_utc": "2026-02-08 15:48:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4comai",
          "author": "Just_Deal6122",
          "text": "The feature/inference/training design pattern described in the LLM Engineer Handbook is a useful reference. The authors apply this pattern to LLM engineering, but it was originally used for MLOps folder structure.",
          "score": 1,
          "created_utc": "2026-02-09 01:02:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hgq4l",
          "author": "Joker_420_69",
          "text": "Vikas Das MLOps. (If hindi)",
          "score": 1,
          "created_utc": "2026-02-09 19:25:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0tdmj",
      "title": "If you're struggling with ML foundations for MLOps, there's another path, the inference & serving side",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0tdmj/if_youre_struggling_with_ml_foundations_for_mlops/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-10 06:28:20",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.76,
      "text": "In my last post, I discussed the importance of ML foundations and Python as key aspects of MLOps. But I realised I left out the other side of the coin, one that's equally valid and may be a better fit for many of you.\n\nIf math and stats aren't your thing and you dread memorising gradient descent variants or probability distributions, hear me out: there's a whole side of MLOps where that's not the focus.\n\nThis side focuses on **model serving, inference optimisation, and production scaling**. \n\nCompanies need people who can:\n\n* Expose models via FastAPI\n* Optimise inference latency and throughput using vLLM, TensorRT, or Triton\n* Manage serving infrastructure with KServe, Seldon, or Ray Serve\n* Handle autoscaling, batching strategies, A/B deployments, and canary rollouts\n* Build observability, monitoring drift, tracking latency p99s, and managing GPU utilisation\n\nNone of this requires you to derive backpropagation from scratch. What it *does* require is strong production engineering instincts, the kind you already have if you've been in DevOps, SRE, or platform engineering.\n\nSo if you're coming from an infrastructure background and feel overwhelmed trying to learn ML theory just to break into MLOps, know that there's a legit path that maps directly to your existing skills. Inference at scale is genuinely hard engineering, and most ML teams desperately need people who can do it well.\n\nThe ML foundations will come naturally over time through exposure. You don't need to master them before you start contributing meaningfully.\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:¬†[topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0tdmj/if_youre_struggling_with_ml_foundations_for_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4kodi4",
          "author": "Extension_Key_5970",
          "text": "For those who are thinking, on how to start and where to explore, read my detailed blog post: [https://medium.com/@thevarunfreelance/you-dont-need-to-master-ml-theory-to-break-into-mlops-here-s-the-other-path-no-one-talks-about-56bc6fb45319](https://medium.com/@thevarunfreelance/you-dont-need-to-master-ml-theory-to-break-into-mlops-here-s-the-other-path-no-one-talks-about-56bc6fb45319)",
          "score": 2,
          "created_utc": "2026-02-10 06:42:30",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4kvk18",
          "author": "--Thunder",
          "text": "Thank you for sharing, I was also looking to dive into mlOps stuff & come from a strong infra background.\n\nCan you recommend any book or some videos which might have helped you as well. I have read the doc on medium, it‚Äôs precise & awesome.\n\nThank you üôèüèº",
          "score": 1,
          "created_utc": "2026-02-10 07:48:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyk81r",
      "title": "What course to take?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyk81r/what_course_to_take/",
      "author": "Berlibur",
      "created_utc": "2026-02-07 17:45:37",
      "score": 9,
      "num_comments": 12,
      "upvote_ratio": 0.85,
      "text": "I'm a data scientist in a not too data scientisty company. I want to learn MLOps in a prod-ready way, and there might be budget for me to take a course.\n\nAny recommendations?\n\na colleague did a data bricks course on AI with a lecturer (online) and it was basically reading slides and meaningless notebooks. so trying to avoid that",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qyk81r/what_course_to_take/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o45uxfd",
          "author": "Competitive-Fact-313",
          "text": "take a simple iris dataset and then train the model and deploy the model using fastapi+UI(react or streamlit)- create the docker file and piush them to registry+also add mlflow for tracking+ once image is publish then create a CI/CD pipeline. Now take the image and publish using ecs + farget or eks. (you can also chose minikube or kind). Once done edit the dataset and trigger the pipeline. with every edit (data edit or model edit) your workflow should trigger and you will find how model perform. this is a typical mlops (traditional project). You will learn a lot using this. \n\nTo help you get started [https://github.com/amit-chaubey/mlops-docker-k8s-fastapi](https://github.com/amit-chaubey/mlops-docker-k8s-fastapi)  \nyou can check out this repo. edit as per your need. try not to do everthing by yourself (you are not a data scientist) so focus more on deployment and production part. ",
          "score": 4,
          "created_utc": "2026-02-07 23:14:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o445wpr",
          "author": "Commercial-Fly-6296",
          "text": "Datatalks club - not sure if it is prod ready though",
          "score": 3,
          "created_utc": "2026-02-07 17:51:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4629k3",
          "author": "NoobZik",
          "text": "Kedro + MLFlow is the minimum for production\nIf you want to dig deeper, Airflow + DVC + NannyML",
          "score": 1,
          "created_utc": "2026-02-08 00:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44i4t8",
          "author": "apexvice88",
          "text": "But why?",
          "score": 0,
          "created_utc": "2026-02-07 18:51:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44iimu",
              "author": "Berlibur",
              "text": "To really make the value from models come to life. \nOne offs or some analysis etc is one thing, having a properly deployed model + a process how to monitor / upgrade / etc is a whole other thing",
              "score": 1,
              "created_utc": "2026-02-07 18:53:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44sqrq",
                  "author": "apexvice88",
                  "text": "Very good reason",
                  "score": 1,
                  "created_utc": "2026-02-07 19:45:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45o1pz",
          "author": "Anti-Entropy-Life",
          "text": "I strongly recommend you don't take any courses. There is no need.\n\nIf you want to learn about LLMs, you can literally just ask the LLM.\n\nIf you need a proper gated workflow, I have a Dual Window Learning system you can use to teach yourself anything.\n\nI have found this to work better than any of the courses I ever tried out.\n\nYour mileage may vary, of course, but this seems to also be fairly common amongst people truly building with LLMs, so you may not get a lot of good course recommendations from this particular sub.\n\nSorry I can't be more helpful in this specific case, but would be happy to send you the doc on the Dual Window Learning system I use if you decide to go that route :)",
          "score": -1,
          "created_utc": "2026-02-07 22:34:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45ri6e",
              "author": "Berlibur",
              "text": "What do you mean by that system",
              "score": 2,
              "created_utc": "2026-02-07 22:54:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o463vqm",
                  "author": "Anti-Entropy-Life",
                  "text": "I'm sorry, I don't know if this a question, or what it's referring to, would it be possible to be more specific?",
                  "score": 0,
                  "created_utc": "2026-02-08 00:10:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qx9ieg",
      "title": "Do you still need MLOps if you're just orchestrating APIS and RAG?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/",
      "author": "polyber42",
      "created_utc": "2026-02-06 05:52:28",
      "score": 8,
      "num_comments": 21,
      "upvote_ratio": 0.71,
      "text": "I‚Äôm starting to dive into MLOps, but I‚Äôve hit a bit of a skeptical patch.\n\nIt feels like the \"heavy\" MLOps stack‚Äîexperiment tracking, distributed training, GPU cluster management, and model versioning‚Äîis really only meant for FAANG-scale companies or those fine-tuning their own proprietary models.\n\n  \nIf a compnay uses APIs(openai/anthropic), the model is a black box behind an endpoint. \n\nIn this case:  \n1. is there a real need for a dedicated MLOps role?\n\n2. does this fall under standard software engineering + data pipelines?\n\n3. If you're in this situation, what does your \"Ops\" actually look like? Are you mostly just doing prompt versioning and vector DB maintenance?\n\n  \nI'm curious if I should still spend time learning the heavy infra stuff",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3v5g2t",
          "author": "UnreasonableEconomy",
          "text": "Proompting isn't machine learning...\n\nEven RAG isn't machine learning. What are you learning? \n\nIf you're at least finetuning, then the need becomes obvious. But the ML field is significantly bigger than than just language models...",
          "score": 16,
          "created_utc": "2026-02-06 07:35:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v7jnr",
          "author": "Scared_Astronaut9377",
          "text": "You are very confused.",
          "score": 10,
          "created_utc": "2026-02-06 07:55:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v81df",
              "author": "polyber42",
              "text": "That's why I'm here - to clear up that confusion.  \nMaybe you can help me understand.  \nI'm genuinely trying to learn where the line is drawn.",
              "score": 4,
              "created_utc": "2026-02-06 07:59:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3y66ro",
                  "author": "Scared_Astronaut9377",
                  "text": "Read about how companies use ML. I cannot explain where the line between things you don't understand is.",
                  "score": -2,
                  "created_utc": "2026-02-06 18:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vme2o",
          "author": "Glad_Appearance_8190",
          "text": "i‚Äôve seen this land closer to ‚Äúops for behavior‚Äù than classic mlops. even with api models you still have prompt drift, data freshness, weird edge cases, and no idea why something changed last week. logs, traces, versioned prompts, and clear rollback end up mattering more than GPUs. heavy infra maybe not, but zero ops usually hurts later....",
          "score": 2,
          "created_utc": "2026-02-06 10:16:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ytuk0",
          "author": "Anti-Entropy-Life",
          "text": "If you are not training or serving your own models, you can ignore a lot of ‚Äúclassic MLOps‚Äù (distributed training, GPU fleet, checkpoint lineage). But you still need ops, because you are still shipping a probabilistic system whose behavior changes when any of these move: model endpoint, prompt/tooling, retrieval data, embeddings, index parameters, and guardrails.\n\nHow I usually frame it:\n\n1. Dedicated MLOps role?\n\n* Early stage: usually no. It is a backend or platform engineer plus a data engineer wearing an ‚ÄúLLM platform‚Äù hat.\n* You want a dedicated role when you have multiple teams shipping LLM features, regulated data, strict uptime, or you are doing frequent prompt and retrieval changes that need disciplined releases.\n\n1. Is it ‚Äújust software engineering + data pipelines‚Äù?\n\n* Mostly yes, but with extra failure modes: non-determinism, silent quality regressions, prompt injection, data leakage, vendor model updates, and evaluation that is not a simple unit test.\n* So, the missing piece is not GPU infra, it is evaluation, observability, and safety controls designed for LLM behavior.\n\n1. What does ‚ÄúOps‚Äù look like in API + RAG land?\n\n* Data and retrieval ops: ingestion, parsing, chunking, embedding generation, reindexing, backfills, access control, and index versioning/rollbacks.\n* Release management: prompt and config versioning, model version pinning, canary releases, fallbacks (smaller model, ‚Äúno answer‚Äù mode), and feature flags.\n* Evals: a regression suite with golden queries, retrieval quality checks (did we fetch the right docs), answer quality checks, and red team cases. Run it in CI before merges and continuously in production.\n* Observability: tracing across app ‚Üí retriever ‚Üí model call, token and latency budgets, cost tracking, citation coverage, refusal rates, and user feedback loops.\n* Security and compliance: prompt injection defenses, tool permissioning, PII filtering, and audit logs.\n\nSo yes, you still ‚Äúneed MLOps,‚Äù but it is closer to SRE + data engineering + QA for an LLM system. If you are choosing what to learn, prioritize: evaluation harnesses, observability, data/retrieval pipelines, and safe rollout patterns. Learn the heavy GPU stuff when you have a clear reason to own training or serving.\n\nI hope this helps! :)",
          "score": 2,
          "created_utc": "2026-02-06 20:47:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zcdel",
              "author": "Cat_Carrot",
              "text": "THIS.\n\nCall it LMOps if you want, OP, but these skills are a differentiator for standout engineers and definitely worth learning.\n\nIt‚Äôs not just ‚Äúlearning infra‚Äù; it‚Äôs learning to think about and work with tools and processes the same way you learned to work with simple functions.",
              "score": 3,
              "created_utc": "2026-02-06 22:19:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zf8s5",
                  "author": "Anti-Entropy-Life",
                  "text": "Thanks. I would argue that is what makes it so much fun! :D",
                  "score": 1,
                  "created_utc": "2026-02-06 22:34:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o423r00",
                  "author": "polyber42",
                  "text": "Got long way to go.",
                  "score": 1,
                  "created_utc": "2026-02-07 10:11:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o423mx9",
              "author": "polyber42",
              "text": "Thank you!",
              "score": 2,
              "created_utc": "2026-02-07 10:09:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o42mkfa",
                  "author": "Anti-Entropy-Life",
                  "text": "I'm glad it was helpful \\^-\\^",
                  "score": 1,
                  "created_utc": "2026-02-07 12:56:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3v4ucb",
          "author": "raiffuvar",
          "text": "Its probably need at least 1 mlops per 4 DS. \nAnd api is not everything. Even with API compani4s need to set up experiment tracking etc. \nThe issue is that companies do not understand the need of mlops.",
          "score": 3,
          "created_utc": "2026-02-06 07:30:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vfmp3",
          "author": "Classic_Swimming_844",
          "text": "RemindMe! -30 day",
          "score": 1,
          "created_utc": "2026-02-06 09:12:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vfqel",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 month on [**2026-03-08 09:12:08 UTC**](http://www.wolframalpha.com/input/?i=2026-03-08%2009:12:08%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/o3vfmp3/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qx9ieg%2Fdo_you_still_need_mlops_if_youre_just%2Fo3vfmp3%2F%5D%0A%0ARemindMe%21%202026-03-08%2009%3A12%3A08%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qx9ieg)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-06 09:13:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wbwm5",
          "author": "dan994",
          "text": "I think your confusion stems from thinking those things are just reserved for those training LLMs. There are many many companies training their own models that aren't LLMs, and all of those will need MLOps.\n\nIf you're not training and deploying models then your MLOps will likely just become DevOps",
          "score": 1,
          "created_utc": "2026-02-06 13:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40452t",
          "author": "Simple_Ad_9944",
          "text": "Yes you still need ‚Äúops,‚Äù but it looks different. If you‚Äôre calling black-box APIs, you‚Äôre not versioning weights, you‚Äôre versioning **inputs, policies, and failure handling**: prompt/config change control, eval suites, rollback, audit logs, escalation paths, and monitoring for drift in behavior even when the provider model changes under you. The hard part becomes governance + reliability, not training infra.\n\n  \nCurious how others are doing rollback / auditability for prompt+tool changes today.",
          "score": 1,
          "created_utc": "2026-02-07 00:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4310we",
          "author": "Driver_Octa",
          "text": "Even with API models, you still need ops for evals, prompt/version control, data quality, observability, latency/cost, and rollback when outputs drift. It‚Äôs closer to platform/SRE + data engineering than GPUcluster MLOps, but it‚Äôs still real. Tools like LangSmith or Traycer AI help with traceability (runs, prompts, diffs) so you can debug changes instead of guessing...",
          "score": 1,
          "created_utc": "2026-02-07 14:24:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xxly7",
          "author": "SpiritedChoice3706",
          "text": "1. It seems like your definition of \"ML\" is quite narrow and only includes GenAi. There are many, many kinds of ML algorithms in the industry - recommendation and ranking (search, suggestions), forecasting, etc. It's not getting the same kind of hype, but those models are definitely not black box APIs and occasionally actually require a lot of work to connect with the data properly, feature engineering, etc.\n\n2. Even in GenAI, Ops of some sort is often required for anything more than a pretty simple setup. It might end up being devops, but you need things like guardrails, rate limiting, and secure handling of API keys, not to mention appropriate data pipelining, in order to actually have these things return business value. And a lot of ML still involved in things like handling tokens efficiently, etc. So maybe this role isn't 'traditional' MLOps, but the use-case is certainly there. Maybe it's called \"AI engineering\". but it's certainly not that far off from trad MLOps, if a little different.\n\nIn my opinion, learning heavy infra  is one of the more valuable skills these days. Anyone can build now. Getting something scalable and secure into production? That is definitely a more secure skillset, even if the title shifts a bit.",
          "score": 0,
          "created_utc": "2026-02-06 18:10:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0tb2u",
      "title": "Lessons from Analyzing 18,000 Exposed Agent Instances",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0tb2u/lessons_from_analyzing_18000_exposed_agent/",
      "author": "RevealNoo",
      "created_utc": "2026-02-10 06:24:20",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I work on security research at Gen Threat Labs, and we recently wrapped up an analysis of autonomous AI agents in production that I wanted to share. Specifically focused on OpenClaw given its popularity (165k GitHub stars and growing fast).\n\nQuick caveat upfront: our methodology has limitations. We scanned for exposed instances and analyzed publicly available community skills, but we don't have visibility into properly secured deployments or private enterprise setups. We also couldn't verify intent behind everything we flagged, so some of what we classified as malicious might just be poorly written code with bad patterns. Take the numbers with that context.\n\nThat said, what we found was worse than I expected going in.\n\nWe identified over 18,000 OpenClaw instances exposed directly to the internet. Not behind VPNs, not containerized, just sitting on default port 18789 accepting connections. One instance we found had full access to the user's email, calendar, and file system. Just... open. That one stuck with me because it's exactly the kind of setup that makes agents useful, and exactly what makes them dangerous.\n\nBut the finding that actually surprised me was in the community skill ecosystem. We analyzed hundreds of skills that users build and share, and nearly 15% contained what I'd classify as malicious instructions. Some were designed to download external payloads, others to exfiltrate data. A few had hidden logic that only triggered after repeated uses, which made them harder to catch in initial review.\n\nWe spent a while trying to use static analysis to catch these automatically, but the false positive rate was brutal. Ended up needing a mix of pattern matching and actually running skills in sandboxed environments to see what they do. Still not perfect.\n\nWe also noticed something frustrating: malicious skills that got flagged and removed would reappear under different names within days. Same payload, new identity. Whack a mole.\n\nThe attack pattern we kept seeing is what I've started calling \"Delegated Compromise.\" Instead of targeting the user directly, adversaries target the agent. Once they get in through prompt injection or a poisoned skill, they inherit every permission that user granted. It's honestly elegant from an attacker's perspective.\n\nTo OpenClaw's credit, their docs are transparent about this. They literally describe it as a \"Faustian bargain\" and acknowledge no perfectly safe setup exists. I respect that honesty, but I don't think most users deploying these agents fully internalize what that means.\n\nThe risk vectors we kept categorizing:\n\n‚Ä¢ Expanded attack surface from agents with read/write/execute across multiple applications ‚Ä¢ Prompt injection through messages and web content with hidden instructions ‚Ä¢ Supply chain risk from community skills built without security review ‚Ä¢ System level impact when broadly permissioned agents get compromised ‚Ä¢ What I've been calling \"judgment hallucination\" where agents appear trustworthy but lack genuine reasoning, so users over delegate\n\nIf you're running agents in production, the practical stuff that actually matters:\n\n‚Ä¢ Isolated environments (VMs or containers), not your primary machine ‚Ä¢ Don't expose default ports to public internet (seems obvious but 18,000 instances say otherwise) ‚Ä¢ Start read only, expand permissions incrementally ‚Ä¢ Secondary accounts during testing ‚Ä¢ Actually review activity logs, not just set and forget ‚Ä¢ Treat third party skills like installing unknown software, because that's basically what it is\n\nThe detection stuff we built for catching the hidden logic patterns, we've been calling it Agent Trust Hub internally. Happy to compare notes if anyone's working on similar approaches or has found better ways to handle the false positive problem.\n\nCurious how other teams are approaching this. Is agent security getting dedicated attention in your org, or is it still lumped in with general appsec? Trying to get a sense of whether this is becoming a recognized problem or if we're early to the panic.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0tb2u/lessons_from_analyzing_18000_exposed_agent/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4n5h9l",
          "author": "Informal_Tangerine51",
          "text": "18,000 exposed instances is concerning but the deeper problem is debuggability. When agent gets compromised via poisoned skill, can you reconstruct what it accessed and when?\n\nYou found malicious skills that trigger after repeated uses. That's nightmare fuel without audit trails. User runs skill 5 times safely, 6th execution exfiltrates data. Post-incident question: what did it access across all 6 runs? Most deployments can't answer without forensics.\n\n\"Judgment hallucination\" + over-delegation compounds when you can't verify agent decisions. User trusts agent with email access, agent gets compromised, attacker inherits permissions. Without signed traces of every action (what was accessed, what policy should have blocked it, when it happened), incident response is archaeology. Security gaps plus evidence gaps means compromise detection happens weeks late.",
          "score": 1,
          "created_utc": "2026-02-10 16:54:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyi6ca",
      "title": "Why I chose Pulumi, SkyPilot, and Tailscale for a multi-tenant / multi-region ML platform and open-sourced it",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyi6ca/why_i_chose_pulumi_skypilot_and_tailscale_for_a/",
      "author": "DifficultDifficulty",
      "created_utc": "2026-02-07 16:27:06",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "As an MLOps Dev, I've stood up enough ML platforms to know the drill: VPC, EKS with GPU node pools, a dozen addons, an abstraction layer like Airflow, multi-tenancy, and maybe repeat it all in another region. The stack was usually Terraform, AWS Client VPN, Kubeflow or Airflow, and an external IdP like Okta.\n\nEvery time I'd finish, the same thought would creep up: \"If I started from scratch with fewer constraints, what would I actually pick?\"\n\nI finally worked through that question and open-sourced the result: \n\n**link**: [https://github.com/Roulbac/pulumi-eks-ml](https://github.com/Roulbac/pulumi-eks-ml)\n\n**The repo**\n\nIt's a Python library (named `pulumi-eks-ml`) of composable Pulumi components: VPC, EKS cluster, GPU node pools with Karpenter, networking topologies, etc. You import what you need and wire up your own topology rather than forking a monolithic template. The repo includes three reference architectures that go from simple to complex:\n\n- **Starter** : single VPC, single EKS cluster, recommended addons. Basically a \"hello world\" for ML on EKS.\n\n- **Multi-Region** : full-mesh VPC peering across regions, each with its own cluster. Useful if you need compute close to data in different geographies.\n\n- **SkyPilot Multi-Tenant** : the main one. Hub-and-spoke network, multi-region EKS clusters, a SkyPilot API server in the hub, isolated data planes (namespaces + IRSA) per team, Cognito auth, and Tailscale for VPN access.\n\n**Why SkyPilot?**\n\nI looked at a few options for the \"ML platform layer\" on top of Kubernetes and kept coming back to SkyPilot. It's fully open-source (no vendor lock beyond your cloud provider), it has a clean API server mode that supports workspaces with RBAC out of the box, and it handles the annoying parts of submitting jobs/services to Kubernetes, GPU scheduling, spot instance preemption, etc. It was a natural fit for a multi-tenant setup where you want different teams to have isolated environments but still share the underlying compute. It's not the only option, but for a reference architecture like this, its flexibility made it nice to build around.\n\n**Why Pulumi over Terraform?**\n\nHonestly, this mostly comes down to the fact that writing actual Python is nicer than HCL when your infrastructure has real logic in it. When you're looping over regions, conditionally peering VPCs, creating dynamic numbers of namespaces per cluster based on config, that stuff gets painful in Terraform. Pulumi lets you use normal language constructs, real classes, type hints, tests with pytest. The component model also maps well to building a library that others import, which is harder to do cleanly with Terraform modules. It's not that Terraform can't do this, it's just that the ergonomics of \"infrastructure as an actual library\" fit Pulumi better.\n\n**Why Tailscale?**\n\nThe whole network is designed around private subnets, no public endpoint for the SkyPilot API. You need some way to reach things, and Tailscale makes that trivially easy. You deploy a subnet router pod in the hub cluster, and suddenly your laptop can reach any private IP across all the peered VPCs through your Tailnet. No bastion hosts, no SSH tunnels, no client VPN endpoint billing surprises. It just works and it's basically a lot less config compared to the alternatives.\n\n**What this is and is not:**\n\n- This is not production-hardened. It's a reference/starting point, not a turnkey platform.\n- This is not multi-cloud. It's AWS-only (EKS specifically).\n- This is opinionated by design: the addon choices, networking topology, and SkyPilot integration reflect a specific yet limited set of use cases. Your needs might call for different designs.\n\nIf you're setting up ML infrastructure on AWS and want a place to start, or if you're curious about how these pieces fit together, take a look. Happy to answer questions or take feedback.",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qyi6ca/why_i_chose_pulumi_skypilot_and_tailscale_for_a/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r0269t",
      "title": "What LLM workloads are people actually running asynchronously?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0269t/what_llm_workloads_are_people_actually_running/",
      "author": "NewClaim7739",
      "created_utc": "2026-02-09 11:50:20",
      "score": 6,
      "num_comments": 6,
      "upvote_ratio": 0.88,
      "text": "Feels like most AI infra is still obsessed with latency when it isn't always the thing that moves the needle. The highest-volume workloads we're seeing are offline:\n\n‚Ä¢ eval pipelines  \n‚Ä¢ dataset labeling  \n‚Ä¢ synthetic data  \n‚Ä¢ document processing  \n‚Ä¢ research agents\n\nOnce you stop caring about milliseconds, the economics change completely.\n\nCurious what people here are running in batch vs realtime - and where the break-even tends to be?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0269t/what_llm_workloads_are_people_actually_running/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4f54ac",
          "author": "Otherwise_Wave9374",
          "text": "Totally seeing the same trend, batch wins. Research agents, doc pipelines, and evals are way more forgiving on latency, and you can do smarter scheduling (spot instances, queues, retries). The trick is getting idempotency and good observability so your agent runs are actually debuggable. I have some notes on async agent workloads and eval loops here: https://www.agentixlabs.com/blog/ What are people using for tracing in batch, OpenTelemetry or vendor tools?",
          "score": 3,
          "created_utc": "2026-02-09 12:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lhakg",
              "author": "NewClaim7739",
              "text": "For the model serving / inference API, we actually ended up building an internal batch tool for this because realtime pricing just didn‚Äôt make sense - happy to share details if useful",
              "score": 1,
              "created_utc": "2026-02-10 11:14:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4h1r9b",
          "author": "penguinzb1",
          "text": "eval pipelines are the big one for us. we've been working on simulating agent runs to catch issues before they hit production, saves a lot of headaches when you can batch test 100s of scenarios overnight instead of waiting for users to hit edge cases",
          "score": 3,
          "created_utc": "2026-02-09 18:15:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfpmi",
              "author": "NewClaim7739",
              "text": "Super interesting - evals seem to be one of the main use cases. What are you using as the inference API?",
              "score": 1,
              "created_utc": "2026-02-10 11:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ghdmg",
          "author": "AIML_Tom",
          "text": "The common theme is that they‚Äôre not waiting on a human in real time ‚Äî they‚Äôre queued, retried, and scaled out with workers. Synchronous chat is the flashy demo, but async workloads are where throughput and reliability matter most.",
          "score": 2,
          "created_utc": "2026-02-09 16:38:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfmfi",
              "author": "NewClaim7739",
              "text": "Massively agree - 'set and forget' your agent and come back to the results you asked for when you've done other tasks",
              "score": 1,
              "created_utc": "2026-02-10 10:59:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quzaun",
      "title": "Orchestrating Two-Tower retrieval: Managing the training-to-serving loop",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1quzaun/orchestrating_twotower_retrieval_managing_the/",
      "author": "skeltzyboiii",
      "created_utc": "2026-02-03 18:05:58",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "The deployment of Two-Tower models for retrieval usually involves significant infrastructure overhead. Beyond just training the user and item encoders, the production pipeline typically requires:\n\n1. Index Orchestration:¬†Triggering embedding updates whenever item metadata changes to prevent drift.\n2. Vector DB Synchronization:¬†Managing the handoff between the feature store and the ANN index (e.g Pinecone, Milvus, or Weaviate).\n3. Hybrid Querying:¬†Implementing a way to combine vector similarity with hard business logic (e.g filtering out \"out of stock\" items) without incurring significant latency penalties.\n\nThe code required to keep these systems in sync often becomes more complex than the model architecture itself.\n\nWe‚Äôve been working on a more declarative approach that treats the training, indexing, and retrieval as a single layer. By using a SQL-based interface, you can query the model directly, the system handles the embedding updates and indexing in the background, allowing for standard¬†WHERE¬†clauses to be applied to the similarity results.\n\nWe put together a technical breakdown of this architecture using a fashion marketplace as the case study. It covers:\n\n* Connecting Postgres/data warehouses directly to the training pipeline.\n* Configuring Two-Tower schemas via YAML.\n* Sub-50ms retrieval benchmarks when combining neural search with SQL filters.\n\nIf you‚Äôre interested in the implementation details or the pipeline design:  \n[https://www.shaped.ai/blog/how-to-deploy-a-production-two-tower-model-in-less-than-a-day](https://www.shaped.ai/blog/how-to-deploy-a-production-two-tower-model-in-less-than-a-day)\n\n*Full disclosure: I‚Äôm with the team at Shaped and authored this technical guide.*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1quzaun/orchestrating_twotower_retrieval_managing_the/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qwz1y7",
      "title": "Open sourced an AI for debugging production incidents - works for ML infra too",
      "subreddit": "mlops",
      "url": "https://v.redd.it/jmn587p30rhg1",
      "author": "Useful-Process9033",
      "created_utc": "2026-02-05 21:59:17",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwz1y7/open_sourced_an_ai_for_debugging_production/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3yvxt7",
          "author": "Anti-Entropy-Life",
          "text": "This looks pretty great! A few things that stand out as especially well done:\n\nThe framing is correct. Root cause analysis in prod is mostly about context stitching across logs, metrics, and deploys, not about clever ML. The fact that the agent explicitly gathers and correlates across the stack is the real value here.\n\nI like that it reads the system on setup. Most tools fail because they stay generic and never internalize how a specific system is wired. Treating that as a first-class step is the right call.\n\nPosting findings back into Slack is underrated but critical. Debugging lives where humans already are. Anything that requires a separate UI to be useful usually dies.\n\nOne thing I would watch carefully as this evolves is trust calibration. Engineers need to know when the AI is confident versus when it is exploring. Clear signals around evidence strength, uncertainty, and ‚Äúthis is my best hypothesis‚Äù vs ‚Äúthis is confirmed‚Äù will matter a lot for adoption.\n\nAnother thought is historical learning. If logs expire, the real long-term value may be in the incident summaries it generates and how those feed future investigations. That is where this could compound.\n\nOverall, this looks like real infrastructure, not a demo. If you keep it grounded in observability first and avoid overclaiming intelligence, this could be something teams actually depend on in the future!",
          "score": 1,
          "created_utc": "2026-02-06 20:57:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz7jpc",
      "title": "Every team wants \"MLOps\", until they face the brutal truth of DevOps under the hood",
      "subreddit": "mlops",
      "url": "/r/devops/comments/1qz7e1r/every_team_wants_mlops_until_they_face_the_brutal/",
      "author": "pm19191",
      "created_utc": "2026-02-08 12:21:10",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qz7jpc/every_team_wants_mlops_until_they_face_the_brutal/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwdbtk",
      "title": "My fraud model didn‚Äôt crash  it quietly dropped from F1 0.79 ‚Üí 0.18. Here‚Äôs how I caught it.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qwdbtk/my_fraud_model_didnt_crash_it_quietly_dropped/",
      "author": "AffectionateSir8341",
      "created_utc": "2026-02-05 06:00:22",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I had a fraud detection demo where nothing ‚Äúbroke‚Äù in production.\n\nNo errors, no crashes, no deploys.\n\n\n\nBut the model‚Äôs F1 score quietly dropped from 0.79 to 0.18 ‚Äî purely due to data drift.\n\n\n\nThat‚Äôs what scares me most about production ML: models don‚Äôt fail loudly,\n\nthey slowly start lying.\n\n\n\nTo explore this properly, I built ModelGuard ‚Äî a small reference implementation\n\nfocused on what happens \\*after\\* deployment:\n\n\n\n\\- detects data & prediction drift using multiple statistical tests\n\n\\- scores severity (not just yes/no drift)\n\n\\- recommends actions (ignore / monitor / retrain / rollback)\n\n\\- gates retraining with human approval\n\n\\- exposes everything via a CLI + lightweight REST API\n\n\n\nThis is intentionally not a framework or SaaS ‚Äî just a learning artifact for\n\napplied ML / MLOps.\n\n\n\nThe demo uses the Kaggle credit card fraud dataset with a simulated fraud-ring\n\nattack. The original dataset is extremely imbalanced (0.17% fraud), so I use\n\nbalanced resampling for faster iteration. Drift is synthetically injected, but\n\nall drift statistics and severity scores are real calculations.\n\n\n\nIn the demo, ModelGuard:\n\n\\- detects drift in 19 / 30 features (63%)\n\n\\- assigns HIGH severity (0.62)\n\n\\- flags a 77% drop in fraud-class F1\n\n\\- recommends retraining and creates a pending alert for human review\n\n\n\nRepo: [https://github.com/Aagam-Bothara/ModelGuard](https://github.com/Aagam-Bothara/ModelGuard)\n\n\n\nI‚Äôd really appreciate feedback from folks running models in prod:\n\n‚Äì does this feel realistic or over-engineered?\n\n‚Äì what would you simplify or remove?\n\n‚Äì what‚Äôs the first thing you‚Äôd add?\n\n",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwdbtk/my_fraud_model_didnt_crash_it_quietly_dropped/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvx8lj",
      "title": "The AI Analyst Hype Cycle",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/the-ai-analyst-hype-cycle",
      "author": "growth_man",
      "created_utc": "2026-02-04 18:52:19",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qvx8lj/the_ai_analyst_hype_cycle/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3z8txn",
          "author": "Anti-Entropy-Life",
          "text": "This is so real, and makes me so happy about the work I am doing in my lab, I can't wait to unveil our product that solves all of these issues :D",
          "score": 1,
          "created_utc": "2026-02-06 22:01:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyg78x",
      "title": "Best books/resources for production ML & MLOps?",
      "subreddit": "mlops",
      "url": "/r/learnmachinelearning/comments/1qyby8i/best_booksresources_for_production_ml_mlops/",
      "author": "Giux99",
      "created_utc": "2026-02-07 15:10:04",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qyg78x/best_booksresources_for_production_ml_mlops/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o4lxzaf",
          "author": "Big-Cockroach4492",
          "text": "[https://github.com/harvard-edge/cs249r\\_book?tab=readme-ov-file](https://github.com/harvard-edge/cs249r_book?tab=readme-ov-file)\n\n\n\n\n\n[https://github.com/harvard-edge/cs249r\\_book/blob/dev/book/README.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/README.md)\n\n  \n",
          "score": 1,
          "created_utc": "2026-02-10 13:14:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv0dzk",
      "title": "Setting up production monitoring for LLMs without evaluating every single request",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qv0dzk/setting_up_production_monitoring_for_llms_without/",
      "author": "llamacoded",
      "created_utc": "2026-02-03 18:44:20",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "We needed observability for our LLM app but evaluating every production request would cost more than the actual inference. Here's what we implemented.\n\nDistributed tracing: Every request gets traced through its full execution path - retrieval, tool calls, LLM generation. When something breaks, we can see exactly which step failed and what data it received.\n\nSampled quality evaluation: Instead of running evaluators on 100% of traffic, we sample a percentage and run automated checks for hallucinations, instruction adherence, and factual accuracy. The sampling rate is configurable based on your cost tolerance.\n\nAlert thresholds: Set up Slack alerts for latency spikes, cost anomalies, and quality degradation. We track multiple severity levels - critical for safety violations, high for SLA breaches, medium for cost issues.\n\nDrift detection: Production inputs shift over time. We monitor for data drift, model drift from provider updates, and changes in external tool behavior.\n\nThe setup took about an hour using Maxim's SDK. We instrument traces, attach metadata for filtering, and let the platform handle aggregation.\n\nDocs: [https://www.getmaxim.ai/docs/tracing/overview](https://www.getmaxim.ai/docs/tracing/overview)\n\nHow are others handling production monitoring without breaking the bank on evals?",
      "is_original_content": false,
      "link_flair_text": "Tools: paid üí∏",
      "permalink": "https://reddit.com/r/mlops/comments/1qv0dzk/setting_up_production_monitoring_for_llms_without/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qyp1t2",
      "title": "Tech job search : how to get an entry level positions in tech.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyp1t2/tech_job_search_how_to_get_an_entry_level/",
      "author": "Traditional-War-9554",
      "created_utc": "2026-02-07 20:51:19",
      "score": 4,
      "num_comments": 10,
      "upvote_ratio": 0.83,
      "text": "recent graduate and no prior work experience[](https://www.reddit.com/submit/?source_id=t3_1qyp0i3)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qyp1t2/tech_job_search_how_to_get_an_entry_level/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o45s73h",
          "author": "denim_duck",
          "text": "mlops is not an entry level role",
          "score": 7,
          "created_utc": "2026-02-07 22:58:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ja8f",
              "author": "Traditional-War-9554",
              "text": "ok, can you suggest other roles to start with ?",
              "score": 0,
              "created_utc": "2026-02-08 11:26:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48x7xs",
                  "author": "denim_duck",
                  "text": "What was your plan for after graduation when you started school?",
                  "score": 1,
                  "created_utc": "2026-02-08 13:17:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o455ihd",
          "author": "randoomkiller",
          "text": "what are your qualifications?",
          "score": 3,
          "created_utc": "2026-02-07 20:54:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45zl3u",
          "author": "SpiritedChoice3706",
          "text": "The first step is probably posting in a relevant subreddit.",
          "score": 2,
          "created_utc": "2026-02-07 23:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46edob",
          "author": "ApprehensiveFroyo94",
          "text": "I really don‚Äôt want to be the bearer of bad news, but this is not an entry level field. \n\nI know sometimes this sub might feel like it‚Äôs gatekeeping the mlops role, but we say this for a reason - there‚Äôs just so many tools you need to understand, build, and monitor, which is just not feasible for someone with no work experience.\n\nTry going for analytics / entry ds role if you have a sufficient math background and move from there.",
          "score": 2,
          "created_utc": "2026-02-08 01:13:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48jc06",
              "author": "Traditional-War-9554",
              "text": "i dont like analytics",
              "score": 0,
              "created_utc": "2026-02-08 11:27:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48wp5z",
                  "author": "denim_duck",
                  "text": "You don‚Äôt have to like it. You have to do it.",
                  "score": 3,
                  "created_utc": "2026-02-08 13:13:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47fxkz",
          "author": "Boognish28",
          "text": "Mlops is a mixture of a ton of specializations. Traditional engineering, delivery, infra, sre, etc. \n\nLearn one. Then learn another. Give it five or ten years, then you might be good.",
          "score": 2,
          "created_utc": "2026-02-08 05:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45ocdu",
          "author": "Anti-Entropy-Life",
          "text": "Build something end-to-end, make it public on GitHub, then find jobs looking for people to build similar things at scale.",
          "score": 1,
          "created_utc": "2026-02-07 22:36:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxy8lw",
      "title": "Jupyter Notebook Validator Operator for automated validation in MLOps pipelines",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qxy8lw/jupyter_notebook_validator_operator_for_automated/",
      "author": "millionmade03",
      "created_utc": "2026-02-06 23:58:01",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.72,
      "text": "\\- üìä Built-in observability: Expose Prometheus metrics and structured logs so you can wire dashboards and alerts quickly.\n\n\n\nHow you can contribute\n\n\n\n\\- Smart error messages (Issue #9)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/9)): Make notebook failures understandable and actionable for data scientists.\n\n\n\n\\- Community observability dashboards (Issue #8)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/8)): Build Grafana dashboards or integrations with tools like Datadog and Splunk.\n\n\n\n\\- OpenShift-native dashboards (Issue #7)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/7)): Help build a native dashboard experience for OpenShift users.\n\n\n\n\\- Documentation: Improve guides, add more examples, and create tutorials for common MLOps workflows.\n\n\n\n\n\nGitHub: [https://github.com/tosin2013/jupyter-notebook-validator-operator](https://github.com/tosin2013/jupyter-notebook-validator-operator)\n\n\n\n\n\nDev guide (local env in under 2 minutes): [https://github.com/tosin2013/jupyter-notebook-validator-operator/blob/main/docs/DEVELOPMENT.md](https://github.com/tosin2013/jupyter-notebook-validator-operator/blob/main/docs/DEVELOPMENT.md)\n\n\n\n\n\nWe're at an early stage and looking for contributors of all skill levels. Whether you're a Go developer, a Kubernetes enthusiast, an MLOps practitioner, or a technical writer, there are plenty of ways to get involved. Feedback, issues, and PRs are very welcome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qxy8lw/jupyter_notebook_validator_operator_for_automated/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o42ytqi",
          "author": "Anti-Entropy-Life",
          "text": "Very cool! Turning notebooks into an *observable* pipeline step is overdue. Any plans for env/image pinning + artifact capture for reproducibility, and timeouts/resource limits for safety? Smart errors that point to the failing cell would be üî•",
          "score": 1,
          "created_utc": "2026-02-07 14:12:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46jjrh",
              "author": "millionmade03",
              "text": "Some of those features should be there I would need to look at adding the other features.",
              "score": 1,
              "created_utc": "2026-02-08 01:46:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvw8p3",
      "title": "Traditional OCR vs AI OCR vs GenAI OCR. When does this become a systems problem?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qvw8p3/traditional_ocr_vs_ai_ocr_vs_genai_ocr_when_does/",
      "author": "Tricky_Reveal_5951",
      "created_utc": "2026-02-04 18:17:32",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Early OCR conversations often focus on models and accuracy benchmarks.\n\nIn production, the harder problems show up elsewhere.\n\nTraditional OCR fails quietly when layouts drift.\n\n AI based OCR improves coverage but needs stronger guardrails.\n\n GenAI works on complex documents, but requires careful controls to avoid unreliable outputs.\n\nAt scale, OCR becomes less about choosing a model and more about designing a system that knows when to trust automation and when to stop.\n\nMost production pipelines rely on layered approaches, confidence thresholds, fallback strategies, and human review for edge cases.\n\nFor teams running document extraction in production, when did choosing an OCR approach turn into an MLOps and systems decision for you?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qvw8p3/traditional_ocr_vs_ai_ocr_vs_genai_ocr_when_does/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3m1p2f",
          "author": "Informal_Tangerine51",
          "text": "OCR accuracy matters less than extraction accountability. When GenAI extracts wrong field from invoice, can you prove which text it saw and why it chose that interpretation?\n\nWe layer OCR methods too - traditional for structured forms, AI for complex layouts, GenAI for unstructured docs. Works until Legal asks \"what text informed this customer data extraction\" and we have confidence scores but not the actual OCR output the model operated on.\n\nThe systems problem isn't fallback strategies, it's decision evidence. Confidence threshold says \"trust this\" but doesn't capture what text was extracted, whether it was from correct page region, or why boundary detection chose those coordinates.\n\nYour layered approach handles accuracy. The gap: when extraction breaks, can you replay the exact OCR output and layout interpretation the model saw? Or just thresholds and final results?",
          "score": 4,
          "created_utc": "2026-02-04 22:17:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o4nph",
          "author": "Zoekielshane",
          "text": "From what I have seen, production OCR success depends more on system design and trade offs than picking the most accurate models choice. I have recently tested Traditional OCR (Tesseract), Deep Learning OCR (PaddleOCR), and GenAI OCR (VLM-based) on 10K+ financial documents. We shared real production examples and lessons learned here, in this technical writeup if it is useful context:\n https://visionparser.com/blog/traditional-ocr-vs-ai-ocr-vs-genai-ocr",
          "score": 1,
          "created_utc": "2026-02-05 05:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oziai",
              "author": "letsTalkDude",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-05 10:19:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ozl2y",
                  "author": "Zoekielshane",
                  "text": "welcome",
                  "score": 1,
                  "created_utc": "2026-02-05 10:20:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qws06r",
      "title": "CI quality gatekeeper for AI agents",
      "subreddit": "mlops",
      "url": "https://github.com/marketplace/actions/maos-agentgate-ci-quality-gatekeeper-for-ai-agents",
      "author": "TranslatorSalt1668",
      "created_utc": "2026-02-05 17:44:21",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qws06r/ci_quality_gatekeeper_for_ai_agents/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwrdn9",
      "title": "What happens when you outgrow the wrappers?",
      "subreddit": "mlops",
      "url": "/r/LocalLLaMA/comments/1qwrd6z/what_happens_when_you_outgrow_the_wrappers/",
      "author": "Left-Reflection-8508",
      "created_utc": "2026-02-05 17:21:51",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwrdn9/what_happens_when_you_outgrow_the_wrappers/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o3z0mom",
          "author": "Anti-Entropy-Life",
          "text": "Neocloud vs AWS:\n\nNeoclouds can be great for raw GPU cost, but you usually trade money for operational burden:\n\n* Spiky capacity\n* Weaker networking or storage primitives\n* Slower support and incident response\n* More SRE work on your side\n\nA common compromise:\n\n* Run the control plane and observability where reliability is high (often AWS or a stable cloud)\n* Run GPU execution where it is cheapest, with fallback\n\n\n\nThe Middle Ground Most People Never See:\n\nYou do not need AWS or a wrapper if you separate control from execution.\n\nRun a small control plane yourself on cheap, boring infra (Hetzner, OVH, or a single VM):\n\n* routing\n* versioning\n* observability\n* rollouts\n* cost visibility\n\nThen attach execution underneath it:\n\n* cheaper GPU providers\n* on prem GPUs\n* spot instances\n* even wrappers as one backend\n\nAt that point, AWS and wrappers stop being architectural commitments and become interchangeable execution targets.\n\nSimple Rule Of Thumb:\n\n* If the wrapper is cheaper than the engineering time to replace it, keep it.\n* If it blocks reliability, observability, or unit economics, build a thin control plane first.\n* Move execution only when cost or reliability forces you.\n\nThis path gives leverage without overbuilding.",
          "score": 1,
          "created_utc": "2026-02-06 21:21:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z507a",
              "author": "Left-Reflection-8508",
              "text": "Appreciate the thoughtful reply, thank you.\n\nIt sounds like you've done it, is that so?",
              "score": 1,
              "created_utc": "2026-02-06 21:42:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3z77e6",
                  "author": "Anti-Entropy-Life",
                  "text": "Yeah, I have done a smaller version of it. We ran FastAPI as a thin control layer in front of GPUs on Lambda so the interface, routing, and logging were ours, and compute was just an execution backend.\n\nThe separation worked well conceptually, but at the scale we were at it ended up costing more than I expected once you factor in always-on capacity and lower utilization. That pushed me to stay inside wrapper constraints a bit longer while being more deliberate about versioning and observability.\n\nThe main thing I took away is that owning even a minimal control plane is the key inflection point. You do not need to go all the way to full self hosting immediately, but once you own the interface, switching execution backends becomes a business decision instead of a rewrite.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:53:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}