{
  "metadata": {
    "last_updated": "2026-01-18 16:49:54",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 8,
    "total_comments": 31,
    "file_size_bytes": 49818
  },
  "items": [
    {
      "id": "1qb5jto",
      "title": "Observability for AI Workloads and GPU Infrencing",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qb5jto/observability_for_ai_workloads_and_gpu_infrencing/",
      "author": "DCGMechanics",
      "created_utc": "2026-01-12 20:03:20",
      "score": 17,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "Hello Folks,\n\nI need some help regarding observability for AI workloads. For those of you working on AI workloads, handling your own ML models, and running your own AI workloads in your own infrastructure, how are you doing the observability for it? I'm specifically interested in the inferencing part, GPU load, VRAM usage, processing, and throughput. How are you achieving this?\n\nWhat tools or stacks are you using? I'm currently working in an AI startup where we process a very high number of images daily. We have observability for CPU and memory, and APM for code, but nothing for the GPU and inferencing part.\n\nWhat kind of tools can I use here to build a full GPU observability solution, or should I go with a SaaS product?\n\nPlease suggest.\n\nThanks",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qb5jto/observability_for_ai_workloads_and_gpu_infrencing/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nz8bfla",
          "author": "dayeye2006",
          "text": "I add metrics emitted to Prometheus in the code. Later you can monitor and visualize using grafana conveniently",
          "score": 8,
          "created_utc": "2026-01-12 20:55:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza54y1",
          "author": "Easy_Appointment_413",
          "text": "You want end-to-end GPU + inference visibility, not just â€œis the box alive?â€\n\n\n\nBaseline stack a lot of teams use: dcgm-exporter on each node to expose GPU metrics (util, memory, ECC, power, temps) into Prometheus, then Grafana dashboards and alerts. Pair that with nvidia-smi dmon logs for quick CLI debugging. For per-model / per-route latency and throughput, push custom metrics from the inference service (p95 latency, queue depth, batch size, tokens or images/sec) to Prometheus or OpenTelemetry, then join them with GPU metrics in Grafana so you can see â€œthis model = this GPU pressure.â€\n\n\n\nFor deeper profiling, Nsight Systems/Compute for sampling, and Triton Inference Server metrics if youâ€™re using it. Datadog or New Relic can work fine if youâ€™re already paying for them; Iâ€™ve also seen people wire alerts into Slack via PagerDuty, plus use something like Pulse alongside Datadog and OpenTelemetry to watch user feedback on Reddit when latency or quality quietly degrades.\n\n\n\nMain thing: treat GPUs as first-class monitored resources with DCGM + Prometheus, then layer model-level metrics on top.",
          "score": 3,
          "created_utc": "2026-01-13 02:32:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzaj8th",
              "author": "DCGMechanics",
              "text": "And what about infrencing observability? Any idea about this?\n\nCurrently i use nvidia-smi or nvtop for GPU metrics but the real black box is infrencing.",
              "score": 1,
              "created_utc": "2026-01-13 03:49:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzp1dms",
                  "author": "mmphsbl",
                  "text": "Some time ago I have used EvidentlyAI for this.",
                  "score": 1,
                  "created_utc": "2026-01-15 08:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nza92kk",
          "author": "Past_Tangerine_847",
          "text": "This is a real gap most teams hit once models go into production.\n\n\n\nFrom what Iâ€™ve seen, GPU observability and inference observability usually end up being two different layers:\n\n\n\n1. GPU-level metrics\n\nPeople typically use:\n\n\\- NVIDIA DCGM + Prometheus exporters\n\n\\- nvidia-smi / DCGM for VRAM, utilization, throttling\n\n\\- Grafana for visualization\n\n\n\nThis covers GPU load, memory, temps, and throughput reasonably well, but it doesnâ€™t tell you if your model behavior is degrading.\n\n\n\n2. Inference-level observability (often missing)\n\nThis is where things usually break silently:\n\n\\- prediction drift\n\n\\- entropy spikes\n\n\\- unstable outputs even though GPU + latency look fine\n\n\n\nAPM and infra metrics wonâ€™t catch this.\n\n\n\nI ran into this problem myself, so I built a small open-source middleware that sits in front of the inference layer and tracks prediction-level signals (drift, instability) without logging raw inputs.\n\n\n\nItâ€™s intentionally lightweight and complements GPU observability rather than replacing it.\n\nRepo is here if useful: [https://github.com/swamy18/prediction-guard--Lightweight-ML-inference-drift-failure-middleware](https://github.com/swamy18/prediction-guard--Lightweight-ML-inference-drift-failure-middleware)\n\n\n\nCurious how others are correlating GPU metrics with actual model behavior in production.",
          "score": 2,
          "created_utc": "2026-01-13 02:53:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzajbdy",
              "author": "DCGMechanics",
              "text": "Thanks, will sure check it out!",
              "score": 1,
              "created_utc": "2026-01-13 03:49:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbyus0",
          "author": "latent_signalcraft",
          "text": "if you already have CPU and APM id frame GPU inference the same way resource metrics plus request level signals tied together with good labels. most teams ive seen use NVIDIA DCGM with Prometheus and Grafana for GPU load memory power and thermals then add inference metrics like latency queue time batch size and errors via app instrumentation or OpenTelemetry. GPU graphs alone wont tell you where youâ€™re stuck so you need both layers. SaaS can help with polish but without consistent tagging by model version and input characteristics you still miss regressions and bottlenecks.",
          "score": 1,
          "created_utc": "2026-01-13 10:52:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhtun0",
          "author": "pvatokahu",
          "text": "Try monocle2ai for inference from Linux foundation.",
          "score": 1,
          "created_utc": "2026-01-14 06:09:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlmt9t",
          "author": "ClearML",
          "text": "If you already have CPU/mem + APM, youâ€™re most of the way there since GPU observability just needs an extra layer.\n\nMost start with DCGM or NVML exporters â†’ Prometheus â†’ Grafana to get GPU utilization, VRAM usage, temps, and throughput alongside existing infra metrics. Where it usually breaks down is context. For inference, raw GPU charts arenâ€™t that useful unless you can tie them back to which model/version, batch size, request rate, and deployment caused the spike. Treat inference like a pipeline, not just a process.\n\nThe setups that work best keep infra metrics OSS, then layer in model and workload metadata (via logging or tracing) so you can correlate latency spike â†’ model X â†’ deployment Y â†’ GPU pressure. Thatâ€™s far more actionable than just watching utilization go up. SaaS can speed things up, but many still prefer owning the core metrics and adding higher-level inference context on top so they donâ€™t lose visibility or control.\n\nIâ€™d start simple: GPU exporters + Prom/Grafana, then focus on correlation before adding more tooling.",
          "score": 1,
          "created_utc": "2026-01-14 20:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03bwib",
          "author": "traceml-ai",
          "text": "I am working on a Pytorch observability tool for training. The tools provide basic GPU and CPU observability. I think it might be particularly interesting for your use case as you get dataloader fetch time and training step time (could be inference time for each batch) and similarly training step memory. The code right now works for a single GPU and I am working on extending to DDP with more distributed observability. \n\nIf you like we can discuss it for your specific use-case.",
          "score": 1,
          "created_utc": "2026-01-17 12:01:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcl7f4",
      "title": "Verticalizing my career/Seeking to become an MLOps specialist.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qcl7f4/verticalizing_my_careerseeking_to_become_an_mlops/",
      "author": "an4k1nskyw4lk3r",
      "created_utc": "2026-01-14 11:45:43",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 0.92,
      "text": "I'm looking to re-enter the job market. I'm a Machine Learning Engineer and I lost my last job due to a layoff. This time, I'm aiming for a position that offers more exposure to MLOps than experimentation with models. Something platform-level. Any tips on how to attract this type of job? Any certifications for MLOps?",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1qcl7f4/verticalizing_my_careerseeking_to_become_an_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzkofmh",
          "author": "d1ddydoit",
          "text": "Becoming confident writing infra as code and architecting secure cloud solutions for ML workloads and development is something youâ€™re probably best off learning away from an MLOps pathway - ML engineering and MLOps already covers too many disciplines to cover well under a single route.\n\nI would take a look at something like a (insert cloud provider) solutions architect and devops pathway courses/exams before focusing further on ML system patterns and services  (e.g feature stores, model registries, experiment tracking, model monitoring, serving managed development environments).\n\nBecoming confident in using and administering managed cloud platform services like SageMaker, Vertex, Snowflake etc also will boost your chances as lots of companies use these off the shelf rather than maintain their own custom in-house platforms (like Uber for example).",
          "score": 3,
          "created_utc": "2026-01-14 17:32:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmeh8y",
          "author": "denim_duck",
          "text": "MLOps is as saturated as ML. There arenâ€™t more opportunities. You should pivot harder- consider robotics or something",
          "score": 1,
          "created_utc": "2026-01-14 22:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp1qs7",
              "author": "EviliestBuckle",
              "text": "Need further clarification on this",
              "score": 1,
              "created_utc": "2026-01-15 08:21:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp1sic",
                  "author": "EviliestBuckle",
                  "text": "Why does everyone feels their field is saturated",
                  "score": 1,
                  "created_utc": "2026-01-15 08:21:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzwnxlt",
              "author": "an4k1nskyw4lk3r",
              "text": "Itâ€™s a good point from eviliestâ€¦ can you explain?",
              "score": 1,
              "created_utc": "2026-01-16 12:11:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qebb90",
      "title": "hosted open source neptune.ai alternative?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qebb90/hosted_open_source_neptuneai_alternative/",
      "author": "Says_Watt",
      "created_utc": "2026-01-16 09:14:59",
      "score": 9,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I would gladly pay for a hosted open source [neptune.ai](http://neptune.ai) alternative that's a drop in replacement for wandb / neptune experiment tracking. The OpenAI acquisition + shutdown of [neptune.ai](http://neptune.ai) is stupid. We as a community need a proper drop in replacement for the purposes of experiment tracking that has a performant UI. I just want to visualize my loss curve without paying w&b unacceptable pricing ($1 per gpu hour is absurd).\n\nThere's no way doing this is that hard. I would do it myself but am working on a different project right now.\n\nAlso aim is an open source alternative but it's not a drop in replacement and it's not hosted. I want to easily switch from wandb and neptune without losing quality UI, without hosting it myself, and without having to do a bunch of gymnastics to fit someone else's design patterns. It needs to be MIT license so that if you decide to sell out someone else can pick up where you left off. Please for the love of god can someone please create a mobile app so I can view my runs while on the go?\n\nedit: also there's [minfx.ai](http://minfx.ai) but their ui is terrible, why is it so hard to just clone wandb / neptune, the spec is there, someone please vibe code it lol",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qebb90/hosted_open_source_neptuneai_alternative/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzw9b7n",
          "author": "MarcelLecture",
          "text": "ML engineers and MLOps has been going with MLflow for at least 5 years.\nIts UI my not be the best and it tries to be a model registry IMHO badly (kitops ftw)\nBuuuut, it is reliable, open-source, has a great community, has managed solution, easy af to deploy and can integrate easily:\n- in your code\n- on your infra\n\nYou are not obliged to use its model registry btw\n\nI never had any big issues with 3 years of using it in production in multiple companies.",
          "score": 3,
          "created_utc": "2026-01-16 10:11:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwa2lh",
              "author": "Says_Watt",
              "text": "I think aim integrate with mlflow so I'll give it a shot, but it seems it's not as easy to use as wandb. So a UI that mimics wandb / [neptune.ai](http://neptune.ai) that uses mlflow and kitops would be great. Hosted of course.",
              "score": 1,
              "created_utc": "2026-01-16 10:18:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzw6y57",
          "author": "extreme4all",
          "text": "Mlflow is OSS mlops tracking, there is probably some managed mlflow services around",
          "score": 2,
          "created_utc": "2026-01-16 09:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw7cvu",
              "author": "Says_Watt",
              "text": "It's doing too much, can I use it as easily as wandb to track my experiments and visualize my training? If not then I'm not interested. I don't need the rest of the bloat. I want a sleak UI that I can visualize and track my experiment and a mobile app that has feature parity with wandb / [neptune.ai](http://neptune.ai) in terms of tracking the experiment. It needs to be open source because [neptune.ai](http://neptune.ai) has proven people are incapable of being reliable and wandb has proven that they're unwilling to solve the problem without gouging their customers.",
              "score": 1,
              "created_utc": "2026-01-16 09:53:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzw8dto",
                  "author": "d_lowl",
                  "text": "Yep, that's the intended usage of MLflow. You record your runs. You record your parameters and metrics (it does support metrics at steps, so you can plot your loss). You don't have to use other features if you don't want to (it's not really that bloated to be fair, it's mostly just an experiment tracker + model registry). It works locally, can be easily deployed too.\n\n\\>mobile app\n\nThat it doesn't have. But you can open it in your browser still.",
                  "score": 1,
                  "created_utc": "2026-01-16 10:03:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzw5c6l",
          "author": "mutlu_simsek",
          "text": "Check perpetual ml. It is not hosted but very cost effective.",
          "score": 1,
          "created_utc": "2026-01-16 09:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw6bts",
              "author": "Says_Watt",
              "text": "I don't understand why every company needs to be \"batteries included\" all in one solution. I just want to track my experiments. I'll deploy it myself by running a few terminal commands and pushing a container to some repository. It's not that difficult.",
              "score": 2,
              "created_utc": "2026-01-16 09:44:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwiw14",
          "author": "latent_signalcraft",
          "text": "its true that many experiment tracking tools dont offer the seamless integration and UI performance youre looking for especially when it comes to being hosted and open source. but beyond just swapping tools its crucial to consider how the underlying infrastructure and data flow will handle scaling with AI workloads. a well integrated experiment tracking system is only as effective as the data and governance practices supporting it. having solid data pipelines and robust MLOps practices in place can significantly improve the user experience especially as experimentation complexity grows. if you're looking for a drop in solution focusing on tools that also prioritize data quality and traceability might be key to a more stable and reliable solution long term.",
          "score": 1,
          "created_utc": "2026-01-16 11:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwjmzf",
              "author": "Says_Watt",
              "text": "data quality? I'm pretty new to mlops in general. I just manage my data myself, store in s3 and train. So in my case I just want a nice UI to visualize the training part.",
              "score": 1,
              "created_utc": "2026-01-16 11:39:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwsqjs",
          "author": "clever_entrepreneur",
          "text": "Why they don't share a docker compose file?",
          "score": 1,
          "created_utc": "2026-01-16 12:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxi8a9",
              "author": "Says_Watt",
              "text": "what?",
              "score": 1,
              "created_utc": "2026-01-16 15:01:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwxdzz",
          "author": "alderteeter",
          "text": "Maybe this will work for you. Seems more appropriate for an individual user than production service, but maybe Iâ€™m misreading it. \n\nhttps://huggingface.co/docs/trackio/en/index",
          "score": 1,
          "created_utc": "2026-01-16 13:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07yg1c",
          "author": "burntoutdev8291",
          "text": "I recently had this issue, moving to mlflow from wandb. The paid services are really so much better, but mlflow has been here for very long.",
          "score": 1,
          "created_utc": "2026-01-18 02:26:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbte3c",
      "title": "Seeking a lightweight orchestrator for Docker Compose (Migration path to k3s)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qbte3c/seeking_a_lightweight_orchestrator_for_docker/",
      "author": "m_gijon",
      "created_utc": "2026-01-13 14:54:14",
      "score": 6,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nIâ€™m currently building an MVP for a platform using **Docker Compose**. The goal is to keep the infrastructure footprint minimal for now, with a planned migration to **k3s** once we scale.\n\nI need to schedule several ETL processes. While Iâ€™m familiar with **Airflow** and **Kestra**, they feel like overkill for our current **resource constraints** and would introduce unnecessary operational overhead **at this stage**.\n\n**What I've looked at so far:**\n\n* **Ofelia:** I love the footprint, but I have concerns regarding robust log management and audit trails for failed jobs.\n* **Supervisord:** Good for process management, but lacks the sophisticated scheduling and observability I'd prefer for ETL.\n\n**My Requirements:**\n\n1. **Low Overhead:** Needs to run comfortably alongside my services in a single-node Compose setup.\n2. **Observability:** Needs a reliable way to capture and review execution logs (essential for debugging ETL failures).\n3. **Path to k3s:** Ideally something that won't require a total rewrite when we move to Kubernetes.\n\nAre there any \"hidden gems\" or lightweight patterns you've used for this middle ground between \"basic cron\" and \"full-blown Airflow\"?",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1qbte3c/seeking_a_lightweight_orchestrator_for_docker/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzdb3z8",
          "author": "dayeye2006",
          "text": "Maybe dagster?",
          "score": 1,
          "created_utc": "2026-01-13 15:46:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdd5ce",
              "author": "m_gijon",
              "text": "I did not known it! thanks! :)\n\nI think I'm gonna try a bunch of solutions, measure how many resources consume, and share the results here",
              "score": 1,
              "created_utc": "2026-01-13 15:55:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdi30w",
          "author": "proof_required",
          "text": "There is also [prefect](https://github.com/PrefectHQ/prefect)\n\n[Docker compose set-up](https://docs.prefect.io/v3/how-to-guides/self-hosted/docker-compose#how-to-run-the-prefect-server-via-docker-compose)",
          "score": 1,
          "created_utc": "2026-01-13 16:18:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzias9w",
              "author": "m_gijon",
              "text": "Thanks, I wasn't aware of that option.   \n  \nHowever, this isn't a separate process I can launch independently from the ETL execution, right?   \n  \nIâ€™m concerned about mixing responsibilities. Iâ€™d prefer to keep them decoupled: the ETL should only be responsible for processing data, while a separate process/orchestrator handles the execution logic.",
              "score": 1,
              "created_utc": "2026-01-14 08:42:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlnq15",
          "author": "ClearML",
          "text": "This is a pretty common spot to be in, and youâ€™re right to avoid over-engineering this early.\n\nIf you squint a bit, what youâ€™re describing isnâ€™t really â€œETL orchestrationâ€ yet, but rather itâ€™s reliable job scheduling + visibility + a clean migration path. That narrows the field a lot.\n\nA few thoughts, based on what Iâ€™ve seen work: Firstly, Docker Compose isnâ€™t the problem, as cron-like schedulers *inside* Compose usually fail once you care about auditability and debugging. Ofelia is fine until the first â€œwhy did this fail last night?â€ incident. One pattern that works well in this middle ground is using a job-centric orchestrator instead of a DAG-centric one. You define jobs as containers/scripts, run them on demand or on schedules, and get logs + history per run, without standing up a full scheduler stack.\n\nThis is actually where tools like ClearML end up fitting better than people expect:\n\n* It runs comfortably in a single-node / Docker Compose setup.\n* Jobs are just containers or scripts, so it feels closer to cron/supervisord than Airflow.\n* Every run gets logs, status, artifacts, and retries by default (huge for ETL debugging).\n* When you move to k3s, youâ€™re not rewriting logic; youâ€™re just changing where agents run.\n\nThe key difference vs Airflow is that youâ€™re not modeling complex DAGs upfront. Youâ€™re tracking and scheduling *executions*, which matches an MVP phase much better.\n\nIf you want something even lighter, some start with:\n\n* simple cron + structured logging + metadata\n* then graduate to something like ClearML once failures/debugging start to hurt\n\nThat way youâ€™re not locking yourself into Airflow semantics before you actually need them.\n\nTL;DR: youâ€™re right to avoid Airflow right now. Look for something that treats jobs as first-class, gives you observability out of the box, and doesnâ€™t care whether itâ€™s running under Compose or k3s. Thatâ€™s the real middle ground.",
          "score": 1,
          "created_utc": "2026-01-14 20:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03o6ni",
          "author": "lastmonty",
          "text": "Hello, \n\nI have been working on a light weight, non intrusive orchestration for some time. \n\nCheck out [runnable](https://astrazeneca.github.io/runnable/). \n\nIt supports complex workflow or jobs and provides a easy path towards kubernetes based workloads. It gives you complete visibility on execution logs and retry capability on cases of failure.\n\nHappy to answer or expand.",
          "score": 1,
          "created_utc": "2026-01-17 13:29:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdtely",
      "title": "Does anyone else feel like Slurm error logs are not very helpful?\"",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qdtely/does_anyone_else_feel_like_slurm_error_logs_are/",
      "author": "Valeria_Xenakis",
      "created_utc": "2026-01-15 19:35:49",
      "score": 6,
      "num_comments": 21,
      "upvote_ratio": 1.0,
      "text": "I manage a small cluster (64 GPUs) for my lab, and I swear 40% of my week is just figuring out why a job is `Pending` or why NCCL timed out.  \n  \nYesterday, a job sat in queue for 6 hours. Slurm said `Priority`, but it turned out to be a specific partition constraint hidden in the config that wasn't documented.  \n  \nIs it just our setup, or is debugging distributed training a nightmare for everyone? What tools are you guys using to actually see *why* a node is failing? `scontrol show job` gives me nothing.\n\n>",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qdtely/does_anyone_else_feel_like_slurm_error_logs_are/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzsst5b",
          "author": "cipioxx",
          "text": "Its very frustrating.",
          "score": 2,
          "created_utc": "2026-01-15 21:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzsujeu",
              "author": "Valeria_Xenakis",
              "text": "I feel like I spend more time than necessery just grepping logs on random nodes.\n\nI really want to know if there are better ways that are industry standard to track down the root cause and would appreciate any guidance. \n\nOr are you guys stuck doing it manually too?",
              "score": 1,
              "created_utc": "2026-01-15 21:12:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzsux4x",
                  "author": "cipioxx",
                  "text": "Manually and guessing.  I have started using llms to get ideas about some issues that pop up.  14 prolog errors now.  I drained the machines last week for maintenance.  I dont know whats going on",
                  "score": 2,
                  "created_utc": "2026-01-15 21:14:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxcbyk",
          "author": "ConcertTechnical25",
          "text": "Slurm error logs are essentially an Information Black Hole. When a job is stuck on Pending, the \"Priority\" label is often a mask for a hard partition constraint or a hardware mismatch. Distributed training requires more than just job status; it requires real-time monitoring of NCCL state and hardware metrics. If youâ€™re manually grepping slurmd logs on random nodes, youâ€™re playing a game of whack-a-mole that Slurm was never designed to help you win.",
          "score": 2,
          "created_utc": "2026-01-16 14:32:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzy19ku",
              "author": "Valeria_Xenakis",
              "text": "Yes, i agree and this is pretty annoying. I was wondering if this is how people go about fixing issues or if there is any better way.",
              "score": 1,
              "created_utc": "2026-01-16 16:26:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o021zl1",
                  "author": "burntoutdev8291",
                  "text": "The reply felt very AI",
                  "score": 1,
                  "created_utc": "2026-01-17 05:12:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt3lxt",
          "author": "cipioxx",
          "text": "Hmmm.  Ok.  I need to build a machine to test this on.  Thank you",
          "score": 1,
          "created_utc": "2026-01-15 21:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqfmy",
              "author": "cipioxx",
              "text": "Thank you my friend",
              "score": 1,
              "created_utc": "2026-01-15 23:50:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02632x",
          "author": "rishiarora",
          "text": "Nice cluster.",
          "score": 1,
          "created_utc": "2026-01-17 05:43:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd4clh",
      "title": "Do you also struggle with AI agents failing in production despite having full visibility into what went wrong?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qd4clh/do_you_also_struggle_with_ai_agents_failing_in/",
      "author": "HonestAnomaly",
      "created_utc": "2026-01-15 00:20:27",
      "score": 6,
      "num_comments": 18,
      "upvote_ratio": 0.69,
      "text": "I've been building AI agents for last 2 years, and I've noticed a pattern that I think is holding back a lot of builders, at least my team, from confidently shipping to production.\n\nYou build an agent. It works great in testing. You ship it to production. For the first few weeks, it's solid. Then:\n\n* A model or RAG gets updated and behavior shifts\n* Your evaluation scores creep down slowly\n* Costs start climbing because of redundant tool calls\n* Users start giving conflicting feedback and explore the limits of your system by handling it like ChatGPT\n* You need to manually tweak the prompt and tools again\n* Then again\n* Then again\n\nThis cycle is exhausting. Given there are few data science papers written on this topic and all observability platforms keep blogging about self-healing capabilities that can be developed with their products, Iâ€™m feeling it's not just me.\n\nWhat if instead of manually firefighting every drift and miss, your agents could adapt themselves? Not replace engineers, but handle the continuous tuning that burns time without adding value. Or at least club similar incidents and provide one-click recommendations to fix the problems.\n\nI'm exploring this idea of connecting live signals (evaluations, user feedback, costs, latency) directly to agent behavior in different scenarios, to come up with prompt, token, and tool optimization recommendations, so agents continuously improve in production with minimal human intervention.\n\nI'd love to validate if this is actually the blocker I think it is:\n\n* Are you running agents in production right now?\n* How often do you find yourself tweaking prompts or configs to keep them working?\n* What percentage of your time is spent on keeping agents healthy vs. building new features?\n* Would an automated system that handles that continuous adaptation be valuable to you?\n\nDrop your thoughts below. If you want to dig deeper or collaborate to build a product, happy to chat.",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qd4clh/do_you_also_struggle_with_ai_agents_failing_in/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nzn4dri",
          "author": "Effective-Total-2312",
          "text": "Working in GenAI systems, my biggest complain, is the architects don't have an idea of what exactly should be an agent, and what should be a workflow, what should be just a traditionally programmed software, etc.\n\nIn the last 2 years I've been asked to use agent frameworks a lot, and always, they weren't the correct \"tool\" for the task that they wanted to complete.\n\nAlso, I don't really like ***most*** libraries and frameworks out there for the GenAI ecosystem, I feel they only add complexities and noise, to an already uncertain nature in LLM systems. I don't like losing control over what's going on, and I don't like complexity, I like sophisticated solutions with very controllable and testable patterns.\n\nTo your specific question, I don't think the way forward is more complexity, I think it is removing all the noise and coming up with design patterns that allow for sophisticated ways in which agents can truly solve business problems.",
          "score": 10,
          "created_utc": "2026-01-15 00:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzn5l0t",
              "author": "HonestAnomaly",
              "text": "Completely agree with that. Bloated systems and over fitting solutions is a big issue.",
              "score": 3,
              "created_utc": "2026-01-15 00:32:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzoplmv",
              "author": "KeyIsNull",
              "text": "Iâ€™ve built with Pydantic Agents and LangGraph and I spent a big amount of time thinking why do I need such a boilerplate to write a simple task. Iâ€™m positive that the ecosystem will be simpler in the future, because right now is a hot mess",
              "score": 2,
              "created_utc": "2026-01-15 06:32:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzsms5l",
                  "author": "HonestAnomaly",
                  "text": "Yes, for with one specific task and a simpler use-case, this is true. But when there is a requirement for multiple agents and dynamic orchestration that changes with different scenarios, there may not be a simple solution. Anthropic's skills concept is pretty promising though. Would love to learn what you built and how would you build it differently, if you had to start from scratch.",
                  "score": 1,
                  "created_utc": "2026-01-15 20:36:39",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzu6rf0",
                  "author": "laststand1881",
                  "text": "Agreed",
                  "score": 1,
                  "created_utc": "2026-01-16 01:19:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzu6qnd",
              "author": "laststand1881",
              "text": "Agreed",
              "score": 1,
              "created_utc": "2026-01-16 01:19:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzn4tvy",
          "author": "Ok_Revenue9041",
          "text": "Totally relate to the struggle of constantly tweaking agents after every shift in model behavior. Automating feedback loops with real signal data can save a ton of manual effort and improve reliability. If you want to boost how your agents surface across AI platforms and handle these adaptive changes more efficiently, check out MentionDesk. It focuses on optimizing brand presence within LLMs so your work is better recognized.",
          "score": 3,
          "created_utc": "2026-01-15 00:28:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzngzz0",
              "author": "HonestAnomaly",
              "text": "Nice. Didn't know about that. Will check it out.",
              "score": 2,
              "created_utc": "2026-01-15 01:37:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzovvf7",
          "author": "Latter_Court2100",
          "text": "Totally agree. Visibility alone often isnâ€™t the blocker; the harder problem is turning those signals into actionable insights like â€œwhat change improves outcomes and cost with the least risk.â€\n\nIn my experience, tools that capture complete agent traces plus per-step cost, latency, and behavior patterns (not just logs) make it much easier to classify failure modes and cluster similar incidents before touching prompts. Debuging/tracing like vLLora helps here by making every decision path visible end to end.\n\nWhat I find especially interesting is the next step: building an agent that works *on top of those traces*. Instead of reacting manually, it can group recurring failure paths, correlate them with eval drift or cost spikes, and then propose specific deltas such as prompt changes, tool constraints, or routing tweaks. You can validate those changes on a small eval slice before shipping anything.\n\nCurious how you think about automated adaptation in practice. Would you prefer a system that only suggests fixes with human review, or one that can apply small changes autonomously under strict guardrails?",
          "score": 2,
          "created_utc": "2026-01-15 07:27:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzspxx7",
              "author": "HonestAnomaly",
              "text": "Love this take. You're spot on that visibility isn't the bottleneck anymore, it's theÂ actionabilityÂ gap. Most teams I worked with have great traces and metrics, and with manual effort they are able to even translate those into â€œsafe, small deltasâ€ that actually move performance or cost in the right direction without regressions. The real unlock, as you said, seems to be building a layerÂ on topÂ of those traces, metrics, and user behavior, one that detects patterns, clusters recurring failures, and proposes fixes that are both measurable and reversible. Less human dependent analysis and more human reviewed auto-optimization.\n\nOn your question: Iâ€™m leaning toward a hybrid approach for adaptation. To optimize a black box, last thing we want, is another black box. ðŸ˜„ IMO, early-stage systems need a â€œco-pilot modeâ€ thatÂ suggestsÂ deltas with explainability and clear diff previews (like, â€œreduce retrieval top-k from 8 â†’ 5 to cut cost by 12%, confidence 0.8â€), and over time, once confidence thresholds and rollback mechanisms prove reliable, move toward limitedÂ auto-applyÂ under guardrails. I am also getting inspired by the concept of backtesting from algorithmic trading. Whatever optimization the system comes up with should have some proof associated that it will work.\n\nHow have you seen â€œsafe autonomyâ€ handled in similar setups, have you experimented with closed-loop systems that make micro-tuning adjustments live automatically?",
              "score": 1,
              "created_utc": "2026-01-15 20:51:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzx5n0u",
                  "author": "Latter_Court2100",
                  "text": "We aare in a similar direction. Weâ€™ve been working on an agent that optimizes LLM requests by analyziing traces and then running controlled experiments on variations in a guided way. In practice itâ€™s mostly â€œco-pilot/cursor wayâ€ today: it proposes small deltas like prompt trims, tool-call schema changes or retrieval parameter changes, then we validate on a replay set or a small eval slice before rolling anything out. Weâ€™ve been cautious about full closed-loop changes in production, but the goal is to gradually earn autonomy for narrow, reversible optimizations with clear rollback and measurable impact.",
                  "score": 1,
                  "created_utc": "2026-01-16 13:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvghiz",
          "author": "dinkinflika0",
          "text": "This is a real problem. We see it constantly at [Maxim](https://getmax.im/Max1m) working with production teams.\n\nThe drift issue is brutal - model updates, data distribution changes, user behavior evolving. Your agent that worked great in January starts degrading by March.\n\nWe built continuous monitoring with automated evals on production traffic (sampling works fine, don't need to eval everything). Set thresholds, get alerts when quality drops before it becomes a crisis.\n\nFor prompt optimization, we have an automated system that analyzes eval results and generates improved versions. You prioritize which metrics matter, it iterates and shows reasoning. Not fully autonomous but cuts the manual tweaking significantly.\n\nHonest take: full self-healing is hard. You still need human judgment for major changes. But automating the continuous tuning (config tweaks, parameter adjustments, identifying failure patterns) saves tons of time.\n\nTo your questions - most teams we work with spend like 40% of time maintaining agents vs building new stuff. That ratio is broken.\n\nWhat kind of agents are you running? Curious what failure modes you're seeing most frequently.",
          "score": 1,
          "created_utc": "2026-01-16 05:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o044sy6",
          "author": "Sudden_Painting3381",
          "text": "Honestly, youâ€™ve hit on a problem that resonates deeply with many teams running AI agents in production. Continuous firefighting takes time without always yielding lasting improvements.\n\nIn my experience, building systems to monitor metrics like latency, cost creep, and model drift in real time can help set threesholds that alert developers when adjustments are needed not after issues are seen. Sometimes, an automated adaptation system makes a lot of sense here rather than manual checks. If such a tool could aggregate signals like user feedback, evaluation scores, and cost spikes to suggest optimization strategies (versus manual tweaking), it could significantly improve runtime stability but can become costly. \n\nHow do you currently prioritize and triage these issues? Are there frameworks helping you already like LangChain for tool selection or custom monitoring dashboards?",
          "score": 1,
          "created_utc": "2026-01-17 15:01:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn8bls",
          "author": "LoaderD",
          "text": "> chatgpt sloppost\n\n> sales hook \n\nThe patented MLOPs sub duo",
          "score": 1,
          "created_utc": "2026-01-15 00:47:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfy95h",
      "title": "I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "mlops",
      "url": "https://i.redd.it/h7duxwz871eg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-18 03:59:55",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qfy95h/i_built_a_tool_that_forces_5_ais_to_debate_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0alini",
          "author": "functionalfunctional",
          "text": "So your solution to llms being bad at facts is to spend 5x as much $ and power to get a consensus on bad facts?  Maybe should have asked 6 if that was a good product idea.",
          "score": 3,
          "created_utc": "2026-01-18 14:23:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0apsu4",
              "author": "S_Anv",
              "text": "You can enable/disable any model. the minimum is 2 models.Â ",
              "score": 0,
              "created_utc": "2026-01-18 14:46:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qatfet",
      "title": "kubesdk v0.3.0 â€” Generate Kubernetes CRDs programmatically from Python dataclasses",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qatfet/kubesdk_v030_generate_kubernetes_crds/",
      "author": "steplokapet",
      "created_utc": "2026-01-12 12:19:49",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[Puzl Team](https://puzl.cloud) here. We are excited to announce kubesdk v0.3.0. This release introduces automatic generation of Kubernetes Custom Resource Definitions (CRDs) directly from Python dataclasses.\n\n**Key Highlights of the release:**\n\n* **Full IDE support:** Since schemas are standard Python classes, you get native autocomplete and type checking for your custom resources.\n* **Resilience:** Operators work in production safer, because all models handle unknown fields gracefully, preventing crashes when Kubernetes API returns unexpected fields.\n* **Automatic generation of CRDs** directly from Python dataclasses.\n\n**Target Audience**\n\nWrite and maintain Kubernetes operators easier. This tool is for those who need their operators to work in production safer and want to handle Kubernetes API fields more effectively.\n\n**Comparison**\n\nYour Python code is your resource schema: generate CRDs programmatically without writing raw YAMLs. See the usage example.\n\n**Full Changelog:** [https://github.com/puzl-cloud/kubesdk/releases/tag/v0.3.0](https://github.com/puzl-cloud/kubesdk/releases/tag/v0.3.0)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qatfet/kubesdk_v030_generate_kubernetes_crds/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    }
  ]
}