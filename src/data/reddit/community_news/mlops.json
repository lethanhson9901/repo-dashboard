{
  "metadata": {
    "last_updated": "2026-01-03 20:26:05",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 12,
    "total_comments": 42,
    "file_size_bytes": 78209
  },
  "items": [
    {
      "id": "1q0zedx",
      "title": "Please be brutally honest: Will I make it in MLOps?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q0zedx/please_be_brutally_honest_will_i_make_it_in_mlops/",
      "author": "OdinPupil",
      "created_utc": "2026-01-01 08:25:00",
      "score": 25,
      "num_comments": 36,
      "upvote_ratio": 0.84,
      "text": "**Strengths:**\n\n* Bachelors in mathematics from top 10 university in the us\n* PhD in engineering from top 10 also\n* 3 published papers (1 in ML, 1 in applied stats, 1 in optimization) however I will say the 1 ML paper did not impress anyone (only 17 citations)\n* Worked as a data scientist for \\~5 years upon graduation\n\n**Weaknesses:**\n\n* I have been unemployed for the last \\~5 years\n* I have ZERO letters of recommendation from my past job nor academia (I apologize for being vague here. Basically I went through a very dark and self-destructive period in my life, quit my job, and burned all my professional and academic bridges down in the process. Made some of the worst decisions of my life in a very short timespan. If you want more details, I can provide via DM/PM)\n* I have never worked with the cloud, with neural networks/AI, nor with anything related to devops. Only purely machine learning in its state circa 2021\n\n**My 6-12 month full-time study plan:**\n\n*(constructed via chatgpt, very open to critique)*\n\n* Refresher of classical ML (stuff I used to do everyday at work, stuff like kaggle and jupyter on one-time tabular data)\n* Certification 1: AWS Solutions Architect\n* Certification 2: Hashicorp Terraform Associate\n* Portfolio Project 1: Terraform-managed ML in AWS\n* Certification 3: Certified Kubernetes Administrator\n* Portfolio Project 2: Kubernetes-native ML pipeline with Inference-Feedback\n* Certification 4: AWS Data Engineer Associate\n* Portfolio Project 3: Automated Warehousing of Streaming Data with Schema Evolution and Cost-Optimization\n* Certification 5: AWS Machine Learning Engineer Associate\n* Portfolio Project 4: End-to-End MLOps in Production with Automated A/B testing and Drift detection\n* Mock Technical Interview Practice\n* Applying and Interviewing for Jobs\n\n**Please be brutally honest. What are my chances of getting into MLOps?**",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1q0zedx/please_be_brutally_honest_will_i_make_it_in_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nx2dhvj",
          "author": "juicymice",
          "text": "Tell the employers you were self-employed or running a business for five years. Don't tell them you were sitting idle. I know of one case where an applicant said he was running a chicken farm for seven years. He networked and got a C-level role.",
          "score": 20,
          "created_utc": "2026-01-01 11:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx24sut",
          "author": "infinity_magnus",
          "text": "I lead AI R&D at a company, have a PhD, have transitioned from academia to industry, and am pretty old. I would suggest the following.\n\n\\- Since you were unemployed for the last 5 years, the top priority is to get at least into a Junior ML job.  Think of the current situation as a reset of your career.  Find a position at a small/medium-sized, not-so-fancy company, even if it is in the service IT sector.\n\n\\- Focus on getting up-skilled on deep learning and related stuff quickly. You can do that because you have done even harder stuff - your PhD.\n\n\\- Don't worry about the pay, etc., get into a junior position. Given the red flags around you, a junior position is the best bet. You will need to be honest in interviews, and if they ask about the 5-year gap, speak in a way that shows you learned from your mistakes and are moving on to a new path. Companies value EQ more than IQ now, and a positive attitude is needed.   \n  \n Once you are in the job, you will get to know many recent tech stacks/cloud infra. Then learn about MLOps if you want to transition into it. I can tell you MLOps is not practised as a separate department in companies if the development team is not big. You will need to spend some time in the new job and work your way up gradually with the certifications and other training. Let me also tell you that just getting certified on some skills does not guarantee an interview - I would myself not hire a person with just certification and no previous production scale experience. \n\n  \nGood luck.",
          "score": 10,
          "created_utc": "2026-01-01 09:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25hfm",
              "author": "OdinPupil",
              "text": "ok thank you, and it seems to resound closely with what u/LoaderD, u/greysteppenwolf, and u/UnreasonableEconomy said above:\n\ndont aim for a MLops job, aim for a more junior, more ML-adjacent job first",
              "score": 2,
              "created_utc": "2026-01-01 09:58:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3hw8n",
                  "author": "infinity_magnus",
                  "text": "Correct !",
                  "score": 1,
                  "created_utc": "2026-01-01 16:13:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx8gejv",
                  "author": "MathmoKiwi",
                  "text": "Data Analyst is a good \"ML adjacent\" position to target initially.",
                  "score": 1,
                  "created_utc": "2026-01-02 10:52:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx219ji",
          "author": "greysteppenwolf",
          "text": "Genuine question: do you really think you can get all these certifications in 6-12 months without having any knowledge in devops now? Why don‚Äôt you consider just going back to DS/academia/etc?",
          "score": 6,
          "created_utc": "2026-01-01 09:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx225l7",
              "author": "OdinPupil",
              "text": "from the research i've done and also browsing in r/AWSCertifications i didnt think the 12 month time horizon would be the most unrealistic part of this plan\n\nand regarding returning to DS, i thought \"pure vanilla DS\" was a rapidly dying + oversaturated field, whereas MLops is the future",
              "score": 1,
              "created_utc": "2026-01-01 09:22:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx22txc",
                  "author": "greysteppenwolf",
                  "text": "I don‚Äôt know exactly what your previous experience with devops is but I think you are overestimating yourself, especially with Kubernetes admin certificate (I‚Äôm less familiar with AWS and don‚Äôt know how hard it is to get certs there). How proficient are you with k8s as a user, for starters? \n\nI also repeat my question about why you chose MLOps in particular. It‚Äôs a profession that requires broad technical knowledge in many different areas: ML, DevOps, system design, GPU architecture, etc.  If you don‚Äôt have recent knowledge in ANY of these, why would you choose a profession that requires you to be such a T-shaped specialist? Instead of returning to DS where you have the base knowledge and experience.\n\nAfter your edit: can‚Äôt you pivot to LLMs or something?",
                  "score": 3,
                  "created_utc": "2026-01-01 09:29:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2joy7",
          "author": "IronFilm",
          "text": "Brutally honest: Targeting MLOps is a bit of a reach too far for your current situation!\n\nAt the moment you just need \"a job\", *any* job.\n\nA good entry level job to target would be a Data Analyst role.\n\nOnce you get this job, work at it for a couple of years, *then* once you've built up that stability of work history and experience, then that is when you can start to think about targeting MLOps jobs",
          "score": 6,
          "created_utc": "2026-01-01 12:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx60wgh",
              "author": "OdinPupil",
              "text": "thank you and i appreciate it. I'd rather take a harsh truth now and be re-directed towards a goal with a higher cahnce of success",
              "score": 2,
              "created_utc": "2026-01-02 00:07:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3v3ou",
          "author": "Yarafsm",
          "text": "Just FYI - AWS certification is not really useful to what you are looking for and it will take lot of your time as it is hard for someone who has never worked in IT",
          "score": 5,
          "created_utc": "2026-01-01 17:23:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3mxic",
          "author": "Tennis-Affectionate",
          "text": "Proud of you OP, you‚Äôre gonna do great",
          "score": 4,
          "created_utc": "2026-01-01 16:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21a64",
          "author": "LoaderD",
          "text": "You‚Äôve been out of work for 5 years? Focus on getting a job, no one is going to hire you for mlops when there are this many red flags.",
          "score": 6,
          "created_utc": "2026-01-01 09:12:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx235ty",
              "author": "OdinPupil",
              "text": "but that was exactly my thinking. i have too many red flags to be hired now, so i should spend a year demonstrating im serious again through certs and projects",
              "score": 1,
              "created_utc": "2026-01-01 09:33:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2k8j0",
                  "author": "IronFilm",
                  "text": "Certs won't solve the red flags",
                  "score": 3,
                  "created_utc": "2026-01-01 12:28:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx27a8g",
                  "author": "LoaderD",
                  "text": "No, the issue is getting into an MLOPs role that you have no experience in. Work on certs and projects, sure, but try to network and get into a JR role in something more closely related to ML, then get on the job experience in ops, then move to MLOPs.\n\nGetting a job should be your first priority, because as much as it sucks, taking 5 years off is a really difficult pill for companies to swallow, having a tangentially related job shows you‚Äôre still able to hold a job.",
                  "score": 2,
                  "created_utc": "2026-01-01 10:17:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2k1y7",
              "author": "IronFilm",
              "text": "He has a whole Tiananmen Square parade full of red flags!",
              "score": 1,
              "created_utc": "2026-01-01 12:27:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx20a5x",
          "author": "UnreasonableEconomy",
          "text": "The bad news: you're right - there isn't much there, unfortunately. At least not for mlops. Even for ML, you have lots of catchup to do. Pretty much everything is transformers now (not just LLMs). Unless you can find some big corp that does old-school stuff.\n\nThe good news: I think the will to learn can 100% get you there. A lot of these certs are good goals, I think. They won't cover everything, and it's pretty AWS oriented, but it's definitely a start.\n\n> Portfolio Project\n\nWhile it's definitely not a bad idea, I don't know if that will do you all that much good in terms of hireability.\n\nIf you already have all that free time available to you, what *I* would do (and your mileage may vary) would be to attach myself to some startup. For sweat equity, I don't think many will say no. Don't expect the startup to make you tons of money (or any at all), but you'll learn (a little bit) to deal with the messy, dirty environments and constraints that involve other humans, which you won't get designing greenfield stuff.\n\nThe 'Rona did a lot of people dirty - welcome back :)",
          "score": 5,
          "created_utc": "2026-01-01 09:02:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx227g6",
              "author": "OdinPupil",
              "text": "Thank you for the honest feedback and the encouragement as well",
              "score": 1,
              "created_utc": "2026-01-01 09:23:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2fy0f",
          "author": "dukesb89",
          "text": "You should leverage your academic experience (which is very strong) to get a data science / research type role. Why are you trying to get into something you have no experience in?",
          "score": 2,
          "created_utc": "2026-01-01 11:47:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2ha8f",
              "author": "OdinPupil",
              "text": "tbh, it was from a counseling/career advice session with chatgpt. i input all my past stats, and asked what should i do, and it said MLops was a good goal",
              "score": 1,
              "created_utc": "2026-01-01 12:00:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5czet",
                  "author": "MathmoKiwi",
                  "text": "LLMs be hallucinating silly bad advice once again.",
                  "score": 1,
                  "created_utc": "2026-01-01 21:56:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2x69s",
          "author": "rishiarora",
          "text": "You give 10 more certifications but none of them come close to getting an actual job. Cut short the plan to 3 months and start giving interviews for junior roles first.",
          "score": 2,
          "created_utc": "2026-01-01 14:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbonoa",
          "author": "valuat",
          "text": "Personal issues aside, you‚Äôre way overqualified. Though ‚Äúproduction is hard‚Äù, to borrow from Musk and others, it is not a research job; it‚Äôs a production job and typically an IT job (it shouldn‚Äôt be but it is). My concern is that you‚Äôll get bored soon.",
          "score": 2,
          "created_utc": "2026-01-02 21:23:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbs9qa",
              "author": "OdinPupil",
              "text": "oh man... being overqualified/bored bc its too easy is the least of my concerns! haha but thank you for the insight",
              "score": 1,
              "created_utc": "2026-01-02 21:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6qol0",
          "author": "coinclink",
          "text": "Plenty of early-stage startups out there will hire you, you might have to work really hard at a startup though, which isn't for everyone",
          "score": 1,
          "created_utc": "2026-01-02 02:40:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8k3ae",
          "author": "eronlloyd",
          "text": "If you want to get back into the AI tech field and work your way up via some short-term training and long-term growth opportunities, consider becoming an entry-level data center technician in an AI factory. Here‚Äôs a great book to get you started: https://a.co/d/3lcKGqv.\n\nI have an MS in data science (2018) but decided to stay in my current career as a telecom engineer, and now I design AI data centers full time. The pay is as good or better than most data science positions, and you can bridge your academic knowledge with operations in creative ways that help you stay valuable and continue to advance.\n\nHappy to discuss with you further. Best of luck!",
          "score": 2,
          "created_utc": "2026-01-02 11:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxb46bi",
          "author": "Establishment_Unique",
          "text": "Stop planning and start doing. This is one thing that bugs me about AI assistants. They always want to create a 12 month timeline. That's not how it works. You start going in one direction and you learning. You learn more about what you are good at and what's valuable as you do it. Then you adjust your direction. You don't need the perfect plan you need feedback and iteration. I also agree with people saying just get a job. You don't even know of you like Mlops. If you do like it what kind?¬†\n\n\nApply for jobs NOW. If you don't get any interviews that's ok. That's feedback. Add one skill at a time until you DO get interviews. Then use feedback from those interviews (and what you are learning) to adjust your career direction. Keep doing that. Short term plans and iteration beat long term plans every time.",
          "score": 1,
          "created_utc": "2026-01-02 19:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx254eo",
          "author": "Hpanduh",
          "text": "you got this it's never to late to goblin mate",
          "score": 1,
          "created_utc": "2026-01-01 09:54:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvygot",
      "title": "Feature Stores: why the MVP always works and that's the trap (6 years of lessons)",
      "subreddit": "mlops",
      "url": "https://mikamu.substack.com/p/feature-store-the-sprawl",
      "author": "Valuable-Cause-6925",
      "created_utc": "2025-12-26 07:24:13",
      "score": 24,
      "num_comments": 9,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pvygot/feature_stores_why_the_mvp_always_works_and_thats/",
      "domain": "mikamu.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw0uk45",
          "author": "stratguitar577",
          "text": "Sounds like the requirements were not defined at all before building. If you approach a feature store without planning on what it needs to solve, it‚Äôs pretty obvious you‚Äôll run into these problems. Point in time training data is a key requirement so not sure why it‚Äôs talked about as an afterthought.¬†",
          "score": 7,
          "created_utc": "2025-12-26 13:50:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0xm2i",
              "author": "Valuable-Cause-6925",
              "text": "I totally agree that dataset construction is a must-have capability for a Feature Store.  \n  \nIn practice, though, the teams care much more about \"getting to production\" first. Figuring out how to make sure that training is consistent with serving is a secondary concern that some teams solve and others leave to their DS counterparts.",
              "score": 1,
              "created_utc": "2025-12-26 14:10:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw13wxq",
                  "author": "stratguitar577",
                  "text": "That‚Äôs where the feature ‚Äúplatform‚Äù takes over vs just the storage piece feature stores deal with. It should abstract the compute to allow writing the transformation once and running across both training and serving contexts for eliminating skew.¬†",
                  "score": 1,
                  "created_utc": "2025-12-26 14:50:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1qynv",
          "author": "DangKilla",
          "text": "An acquaintance is implementing too many features before launching, instead of focusing on the problem being solved",
          "score": 2,
          "created_utc": "2025-12-26 16:57:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1t7jw",
              "author": "Valuable-Cause-6925",
              "text": "Feature Store related?",
              "score": 1,
              "created_utc": "2025-12-26 17:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwitrbv",
          "author": "seunosewa",
          "text": "This might as well be an advertisement for storing everything in standard relational databases like PostgreSQL and MySQL.",
          "score": 1,
          "created_utc": "2025-12-29 10:13:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwivbr9",
              "author": "Valuable-Cause-6925",
              "text": "By everything you mean what exactly?",
              "score": 1,
              "created_utc": "2025-12-29 10:27:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwjces7",
                  "author": "seunosewa",
                  "text": "Data.",
                  "score": 1,
                  "created_utc": "2025-12-29 12:50:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1nzsp",
      "title": "How to deploy multiple Mlflow models?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q1nzsp/how_to_deploy_multiple_mlflow_models/",
      "author": "Plus_Cardiologist540",
      "created_utc": "2026-01-02 03:35:30",
      "score": 19,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "So, I started a new job as a Jr MLOps. I've just entered a moment where the company is undergoing a major refactoring of its infrastructure, driven by new leadership and a different vision. I'm helping to change how we deploy our models. \n\nThe new bosses want to deploy all models in a single FastAPI server that consumes 7 models from MLflow. This is not in production yet. While I'm new and a Jr, I'm starting to implement some of the old code in this new server (validation, Pydantic, etc). \n\nBefore the changes, they had 7 different servers, corresponding to 7 FastAPI servers. The new boss says there is a lot of duplicated code, so they want a single FastAPI, but I'm not sure.\n\nI asked some of the senior MLOps, and they just told me to do what the boss wants. However, I was wondering whether there is a better way to deploy multiple models without duplicating code and having them all in a single repository? Because when a model needs to be retrained, it must restart the Docker container to download the new version. Also, some models (for some reason) have different dependencies, and obviously, each one has its own retraining cycles.\n\nI had the idea of having each model in its own container and using something like MLFlow Serve to deploy the models. With a single FastAPI, I could just route to the /invocation of each model.\n\nIs this a good approach to suggest to the seniors, or should I simply follow the boss's instructions?",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1q1nzsp/how_to_deploy_multiple_mlflow_models/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nx70bya",
          "author": "Salty_Country6835",
          "text": "\n  What you are reacting to is not wrong, but it helps to separate concerns.\n\n  A single FastAPI serving multiple models does reduce duplicated *interface* code,\n  but it also tightly couples model lifecycles, dependencies, and restart semantics.\n  That tradeoff usually shows up later during retraining, hotfixes, or uneven scaling.\n\n  One common middle ground is:\n  - shared FastAPI (or gateway) for auth, validation, routing\n  - one container per model (or per dependency class)\n  - shared libraries instead of shared runtimes\n\n  That way you remove duplication without forcing all models to restart or align dependencies.\n  Framing it as ‚Äúhere are the risks of full consolidation‚Äù rather than ‚Äúthis is better‚Äù\n  usually lands better with seniors.\n\n   Which failure modes are acceptable today but painful six months from now?\n   Is duplication happening in code, or in runtime responsibility?\n   What is the smallest step that preserves optionality?\n\n  If one model needs an urgent retrain or dependency bump, should all others be forced to restart?",
          "score": 15,
          "created_utc": "2026-01-02 03:40:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8o1wk",
          "author": "guardianz42",
          "text": "The decision to do 7 models in 1 server or 7 individual servers can come down to budget, speed or latency considerations. If there's not much traffic on any individual model and they all fit in memory, you can try a single server. \n\nOtherwise, 7 different servers might be work better depending on the traffic patterns. \n\nRegardless, the best thing you can do is experiment a lot to see what works best and have flexibility to change your approach later. We personally deploy thousands of models using litserve which removes a ton of duplicate code and can work in either setting described here. https://github.com/Lightning-AI/LitServe (built on FastAPI but specialized for AI). \n\nFor senior people, come back with options and a suggestion : \"I tried these various approaches and here are all the trade-offs, and my suggestion based on these results is X\" will land better. And if you can follow with \"but the way it's built allows us the flexibility to easily change the approach later\" then the decision of the approach right now matters less as long as you can change it later. \n\nUltimately, model serving approaches change with your product's maturity and usage scaling. What works for 100 users probably won't work for 10 million.",
          "score": 4,
          "created_utc": "2026-01-02 11:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9ek4y",
              "author": "Letzbluntandbong",
              "text": "Good call on experimenting! If you're considering multiple servers, think about the deployment and maintenance trade-offs. Using something like LitServe sounds solid; it could help simplify your codebase and manage dependencies better. Presenting a few options to your seniors shows initiative and might lead to a more flexible solution.",
              "score": 2,
              "created_utc": "2026-01-02 14:51:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbb0c7",
          "author": "pvatokahu",
          "text": "Your boss is making a classic mistake that I've seen play out badly before. Consolidating 7 services into one sounds great in theory until your first production incident when one model crashes and takes down all the others. Or when you need to scale just one model that's getting hammered but now you have to scale the entire monolith.\n\nThe container-per-model approach you're thinking about is way more sensible. At Microsoft we had similar debates about service boundaries and learned the hard way that coupling unrelated models creates more problems than it solves. Different dependency versions alone should be enough to kill this idea - wait till you hit a numpy version conflict between models and watch the fun begin. i'd document the risks clearly (restart downtime affecting all models, dependency hell, independent scaling issues) and present your alternative. Sometimes new leadership needs to learn these lessons themselves though.",
          "score": 1,
          "created_utc": "2026-01-02 20:16:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1vdh3",
      "title": "DevOps ‚Üí ML Engineering: offering 1:1 calls if you're making the transition",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q1vdh3/devops_ml_engineering_offering_11_calls_if_youre/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-02 10:34:47",
      "score": 18,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "Spent 7 years in DevOps before moving into ML Platform Engineering. Now managing 100+ K8s clusters running ML workloads and building production systems at scale. \n\nThe transition was confusing - lots of conflicting advice about what actually matters. Your infrastructure background is more valuable than you might think, but you need to address specific gaps and position yourself effectively. \n\nSet up a Topmate to help folks going through this: [https://topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)\n\nWe can talk through skill gaps, resume positioning, which certs are worth it, project strategy, or answer whatever you're stuck on. \n\nAlso happy to answer quick questions here.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q1vdh3/devops_ml_engineering_offering_11_calls_if_youre/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nxa09ws",
          "author": "enemadoc",
          "text": "I'm a software developer looking to make this transition. I've worked on a wide variety of areas. I come from Java, and have built dev pipelines, cloud infrastructure, and worked on some bare metal Openshift clusters. I plan on getting the CKA certificate this year. Is that the cert you would go for? I assume that's the most impactful certificate I can get. Currently holding the AWS architect and Machine Learning speciality certs also.",
          "score": 3,
          "created_utc": "2026-01-02 16:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf7tzm",
              "author": "Extension_Key_5970",
              "text": "CKA level skills obviously worth it, not sure though, certification is actually crucial in grabbing a job, for early senior roles, maybe  \nFor MLOps, currently, most of the companies' focus is on inference, how to expose models with very low latency, as per my experience, and can handle ML pipelines with respect to batch and streaming data.\n\nWhere to start --> Python is a must, I would say, day to day, at least 50% learning should be using Python, the rest you can distribute across ML foundations, and System design scenarios wrt Inferencing and ML Pipelines",
              "score": 1,
              "created_utc": "2026-01-03 11:27:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbhk8r",
          "author": "Technical_Rutabaga67",
          "text": "Are you free lancing on this project?",
          "score": 1,
          "created_utc": "2026-01-02 20:48:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf7y6r",
              "author": "Extension_Key_5970",
              "text": "Currently yes, I am Freelancing",
              "score": 1,
              "created_utc": "2026-01-03 11:28:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdaj9p",
          "author": "rajeshThevar",
          "text": "Thanks for the post. \n\nCan we have your inputs on what is most needed skills on MLOps?",
          "score": 1,
          "created_utc": "2026-01-03 02:37:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf87ow",
              "author": "Extension_Key_5970",
              "text": "As said in the above comment, \"Where to start --> Python is a must, I would say, day to day, at least 50% learning should be using Python, the rest you can distribute across ML foundations, and System design scenarios wrt Inferencing and ML Pipelines\"\n\nTech stack --> Python, Kubernetes, Airflow, One ML Framework Pytorch or Tensorflow, MLFlow, Strong ML Foundations, ML Pipelines",
              "score": 1,
              "created_utc": "2026-01-03 11:30:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg54z",
      "title": "Production ML Serving Boilerplate - Skip the Infrastructure Setup",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pyg54z/production_ml_serving_boilerplate_skip_the/",
      "author": "Extension_Key_5970",
      "created_utc": "2025-12-29 07:39:01",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "MLOps engineer here. Built this after setting up the same stack for the 5th time.\n\nWhat it is:\n\nInfrastructure boilerplate for MODEL SERVING (not training). Handles everything between \"trained model\" and \"production API.\"\n\nStack:\n\n\\- MLflow (model registry)\n\n\\- FastAPI (inference API)\n\n\\- PostgreSQL + Redis + MinIO\n\n\\- Prometheus + Grafana\n\n\\- Kubernetes (tested on Docker Desktop K8s)\n\nWhat works NOW:\n\nFull stack via \\`docker-compose up -d\\`\n\nK8s deployment with HPA (2-10 replicas)\n\nEnsemble predictions built-in\n\nHot model reloading (zero downtime)\n\nE2E validation script\n\nProduction-grade health probes\n\nKey features for MLOps:\n\n\\- Stage-based deployment (None ‚Üí Staging ‚Üí Production)\n\n\\- Model versioning via MLflow\n\n\\- Prometheus ServiceMonitor for auto-discovery\n\n\\- Rolling updates (maxUnavailable: 0)\n\n\\- Resource limits configured\n\n\\- Non-root containers\n\n5-minute setup:\n\n\\`\\`\\`bash\n\ndocker-compose up -d\n\npython3 scripts/demo-e2e-workflow.py  # Creates model, registers, serves\n\n\\`\\`\\`\n\nProduction deploy:\n\n\\`\\`\\`bash\n\n./scripts/k8s-bootstrap.sh  # One-command K8s setup\n\n./scripts/validate-deployment.sh --env k8s\n\n\\`\\`\\`\n\nHonest question: What's the most significant pain point in your ML deployment workflow that this doesn't solve?\n\nGitHub: [https://github.com/var1914/mlops-boilerplate](https://github.com/var1914/mlops-boilerplate)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pyg54z/production_ml_serving_boilerplate_skip_the/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwz0cbl",
          "author": "pvatokahu",
          "text": "Nice stack choices. We've been running something similar at Okahu but ended up ditching MLflow for model registry after hitting some weird edge cases with versioning conflicts when multiple teams were pushing models. The prometheus + grafana setup looks solid though - that's one thing I wish more boilerplate repos would include by default.\n\nYour hot model reloading approach is interesting.. we tried implementing that but ran into memory leaks with certain model types (especially transformer-based ones). Had to build a whole sidecar pattern just to manage the lifecycle properly. How are you handling memory cleanup during the reload process? Also curious if you've tested this with models larger than a few GB - that's where things usually start breaking in my experience.",
          "score": 1,
          "created_utc": "2025-12-31 20:24:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1v6yi",
              "author": "Extension_Key_5970",
              "text": "These are a great set of realistic queries I was expecting. \n\nFor now, the boilerplate is relatively standard base infrastructure, which usually startups struggle to implement, or if they don't want to spend time on infra, but I will take these as an enhancement, of course, to make things scalable, even if with larger models, and test memory leak scenarios specifically. \n\nAlso, I am interested in the edge cases that led you to ditch MLflow for a model registry.",
              "score": 1,
              "created_utc": "2026-01-01 08:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0xybe",
      "title": "Finally released my guide on deploying ML to Edge Devices: \"Ultimate ONNX for Deep Learning Optimization\"",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q0xybe/finally_released_my_guide_on_deploying_ml_to_edge/",
      "author": "meet_minimalist",
      "created_utc": "2026-01-01 06:49:59",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI‚Äôm excited to share that I‚Äôve just published a new book titled¬†**\"Ultimate ONNX for Deep Learning Optimization\"**.\n\nAs many of you know, taking a model from a research notebook to a production environment‚Äîespecially on resource-constrained edge devices‚Äîis a massive challenge. ONNX (Open Neural Network Exchange) has become the de-facto standard for this, but finding a structured, end-to-end guide that covers the entire ecosystem (not just the \"hello world\" export) can be tough.\n\nI wrote this book to bridge that gap. It‚Äôs designed for ML Engineers and Embedded Developers who need to optimize models for speed and efficiency without losing significant accuracy.\n\n**What‚Äôs inside the book?**¬†It covers the full workflow from export to deployment:\n\n* **Foundations:**¬†Deep dive into ONNX graphs, operators, and integrating with PyTorch/TensorFlow/Scikit-Learn.\n* **Optimization:**¬†Practical guides on Quantization, Pruning, and Knowledge Distillation.\n* **Tools:**¬†Using ONNX Runtime and ONNX Simplifier effectively.\n* **Real-World Case Studies:**¬†We go through end-to-end execution of modern models including¬†**YOLOv12**¬†(Object Detection),¬†**Whisper**¬†(Speech Recognition), and¬†**SmolLM**¬†(Compact Language Models).\n* **Edge Deployment:**¬†How to actually get these running efficiently on hardware like the Raspberry Pi.\n* **Advanced:**¬†Building custom operators and security best practices.\n\n**Who is this for?**¬†If you are a Data Scientist, AI Engineer, or Embedded Developer looking to move models from \"it works on my GPU\" to \"it works on the device,\" this is for you.\n\n**Where to find it:**¬†You can check it out on Amazon here:[https://www.amazon.in/dp/9349887207](https://www.amazon.in/dp/9349887207)\n\nI‚Äôve poured a lot of experience regarding the pain points of deployment into this. I‚Äôd love to hear your thoughts or answer any questions you have about ONNX workflows or the book content!\n\nThanks!\n\n[Book cover](https://preview.redd.it/q5wo0ag5poag1.jpg?width=970&format=pjpg&auto=webp&s=b76d4d84c9fa70e6630d1b776389623a75397afe)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q0xybe/finally_released_my_guide_on_deploying_ml_to_edge/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pyuobm",
      "title": "I got tired of burning money on idle H100s, so I wrote a script to kill them",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pyuobm/i_got_tired_of_burning_money_on_idle_h100s_so_i/",
      "author": "jordiferrero",
      "created_utc": "2025-12-29 18:54:21",
      "score": 11,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "[https://github.com/jordiferrero/gpu-auto-shutdown](https://github.com/jordiferrero/gpu-auto-shutdown)\n\nGet it running on your ec2 instances now forever:\n\n    git clone https://github.com/jordiferrero/gpu-auto-shutdown.git\n    cd gpu-auto-shutdown\n    sudo ./install.sh\n\n\n\n\n\nYou  \nknow  \nthe feeling in ML research. You spin up an H100 instance to train a model, go to sleep expecting it to finish at 3 AM, and then wake up at¬†9 AM. Congratulations, you just paid for 6 hours of the world's most¬†expensive space heater.\n\nI did this way too many times. I must run my own EC2 instances for research, there's no other way.\n\nSo I wrote a simple daemon that watches¬†nvidia-smi.\n\nIt‚Äôs not rocket science, but it‚Äôs effective:\n\n1. It monitors GPU usage every minute.\n2. If your training job finishes (usage drops compared to high), it starts a countdown.\n3. If it stays idle for 20 minutes (configurable), it kills¬†the instance.\n\n**The Math:**\n\nAn on-demand H100 typically costs around¬†$5.00/hour.\n\nIf you leave it idle for just¬†10 hours a day¬†(overnight + forgotten weekends + \"I'll check it after lunch\"), that is:\n\n* $50 wasted daily\n* up to $18,250 wasted per year per GPU\n\nThis script stops that bleeding. It works on AWS, GCP, Azure, and pretty much any Linux box with systemd. It even¬†checks if it's running on a cloud instance before shutting down so it doesn't accidentally¬†kill your local rig.\n\nCode is open source, MIT licensed. Roast my¬†bash scripting if you want, but it saved me a fortune.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pyuobm/i_got_tired_of_burning_money_on_idle_h100s_so_i/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwo5pms",
          "author": "kamelkev",
          "text": "Good budget solution, but limited. You can do better analysis by leveraging cloudwatch.\n\nYou have to configure collection of ‚Äúnvidia_gpu,‚Äù but once done you can do run historic analytics in a way you can‚Äôt really do with a script.\n\nFor example you could leverage a combination of cpu, network and gpu activity to truly understand if a job was done, or if it was simply a pipeline stage that had completed.\n\nWhich AMI are you using?",
          "score": 2,
          "created_utc": "2025-12-30 03:47:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx92jwu",
              "author": "NoobZik",
              "text": "At least the proposed solution is cloud agnostic",
              "score": 1,
              "created_utc": "2026-01-02 13:42:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo9l4g",
          "author": "Dramatic_Hair_3705",
          "text": "Why don't you use apache airflow for task scheduling?",
          "score": 2,
          "created_utc": "2025-12-30 04:10:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpa3pl",
              "author": "Popular-Direction984",
              "text": "Because he can use a simple script. Why use some ‚Äúsolution‚Äù, where three lines of shell will do?:)",
              "score": 1,
              "created_utc": "2025-12-30 09:02:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpbjln",
                  "author": "Dramatic_Hair_3705",
                  "text": "You are a very smart guy.",
                  "score": 2,
                  "created_utc": "2025-12-30 09:15:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwmjedp",
          "author": "KeyIsNull",
          "text": "mmmm you might kill the instance in the middle of something (e.g. uploading the model somewhere), wouldn't it better to wrap your training script with a shell script that automatically shuts down the instance when the script is done?\n\n    #!/bin/bash\n    python my_training.py\n    shutdown\n\n  \nedit: code block",
          "score": 2,
          "created_utc": "2025-12-29 22:27:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmndid",
              "author": "jordiferrero",
              "text": "Absolutely. But I often use UI/fromtends that are always running so there are no scripts per se (e.g. ComfyUI)",
              "score": 2,
              "created_utc": "2025-12-29 22:48:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q15dgo",
      "title": "How does everyone maintain packages?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q15dgo/how_does_everyone_maintain_packages/",
      "author": "ShakeDue8420",
      "created_utc": "2026-01-01 14:26:17",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "How do you guys source and maintain AI/ML dev packages (e.g., PyTorch/CUDA/transformers), and how do you ensure they‚Äôre safe and secure?\n\nI know there‚Äôs a lot of literature out there on the subject but I‚Äôm wondering what everyone‚Äôs source of truth is, what checks/gates do most people run (scanning/signing/SBOM), and what‚Äôs a typical upgrade + rollout process?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1q15dgo/how_does_everyone_maintain_packages/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nx3c1fd",
          "author": "d1ddydoit",
          "text": "Can use a lockfile to maintain the state of my projects package dependencies, use a tool like renovate to make PRs to upgrade packages in that lockfile and SAST & SCA tools on each PR to scan for vulnerabilities.\n\nCan use an artifact manager to cache binaries and tensors from pypi, hugging face etc instead of going direct (with scanning on all packages loaded into artifact manager).\n\nShould always lag behind a dependency package version release however by a couple of weeks at least to ensure security researchers have time to discover vulnerabilities.\n\nI use safetensors for models where possible over pickle etc but that is not always an option (e.g xgboost models).\n\nIn cloud, run behind tight firewalls and define least privilege for all principals used in a service.\n\nEven if my packages are compromised, the networking design and access controls for all my resources helps form a tight data perimeter that helps me sleep at night.\n\nML services are generally deployed to a network, it is both the service and the network that we deploy to that must be secure in design. I would never take one without the other as an ML engineer even though networking design is not really my job function (tuning, training and deploying models is).",
          "score": 3,
          "created_utc": "2026-01-01 15:42:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3l24w",
              "author": "ShakeDue8420",
              "text": "Thanks! What artifact manager are you using?\n\nYou mentioned cloud, are you using sagemaker or some other similar service? I‚Äôm having a hard time finding where they talk about how they manage packages. I read a doc that made it seem like they just use pypi¬†",
              "score": 1,
              "created_utc": "2026-01-01 16:30:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3pa4r",
                  "author": "d1ddydoit",
                  "text": "If you use SageMaker and are worried about the lifecycle of packages behind your SageMaker workloads you can bring/build your own containers (BYOC) for processing/training/tuning jobs and endpoints. If you want to see how SageMaker maintain their own prebuilt containers, depending on the image you can see it all like [this repo](https://github.com/aws/sagemaker-xgboost-container). I do use SageMaker (but more often I‚Äôm using Ray now). For the SageMaker resources I do have running, it‚Äôs all BYOC. The lifecycle of my service dependencies can‚Äôt depend on that of AWS SageMaker prebuilt images - I appreciate they do offer the prebuilt containers to get up and running fast (and on secure VPCs they are fine for lots of companies).\n\nI use a few artifact managers depending on what I‚Äôm up to but Artifactory is my main one.",
                  "score": 3,
                  "created_utc": "2026-01-01 16:53:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7pss9",
          "author": "latent_signalcraft",
          "text": "most teams i have seen stay sane by treating packages as platform concerns not per-project decisions. they standardize on a small set of base environments upgrade on a regular cadence and test against real workloads before rollout. scanning and SBOMs matter but uncontrolled drift and ad hoc upgrades cause more pain than missing the latest version. stability usually wins over freshness in practice.",
          "score": 2,
          "created_utc": "2026-01-02 06:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx99rg9",
          "author": "diamond143420",
          "text": "I‚Äôd recommend using a third party to keep an eye on all packages. I started using Trace AI from Zerberus to monitor our SBOM. It spotted a bunch of abandoned packages and even a few package typos. If I am really sus'd out by a package, I usually do a manual reviews. Start with sandboxed environments, test the new packages thoroughly, and keep an eye on version mismatches or use a service to monitor for you",
          "score": 2,
          "created_utc": "2026-01-02 14:24:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxpzeu",
      "title": "Built a small production-style MLOps platform while learning FastAPI, Docker, and CI/CD ‚Äì looking for feedback",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pxpzeu/built_a_small_productionstyle_mlops_platform/",
      "author": "Melodic_Struggle_95",
      "created_utc": "2025-12-28 12:14:33",
      "score": 9,
      "num_comments": 12,
      "upvote_ratio": 0.77,
      "text": "I‚Äôve been learning MLOps and wanted to move beyond notebooks, so I built a small production-style setup from scratch.\n\n\n\nWhat it includes:\n\n\\- Training pipeline with evaluation gate\n\n\\- FastAPI inference service with Pydantic validation\n\n\\- Dockerized API\n\n\\- GitHub Actions CI pipeline\n\n\\- Swagger UI for testing predictions\n\n\n\nThis was mainly a learning project to understand how models move from training to deployment and what can break along the way.\n\n\n\nI ran into a few real-world issues (model loading inside Docker, environment constraints on Ubuntu, CI failures) and documented fixes in the README.\n\n\n\nI‚Äôd really appreciate feedback on:\n\n\\- Project structure\n\n\\- Anything missing for a ‚Äúreal‚Äù MLOps setup\n\n\\- What you‚Äôd add next if this were production\n\n\n\nRepo: [https://github.com/faizalbagwan786/mlops-production-platform](https://github.com/faizalbagwan786/mlops-production-platform)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pxpzeu/built_a_small_productionstyle_mlops_platform/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwcrjyb",
          "author": "raiffuvar",
          "text": "Not gonna put it in production. Split training and platform into different repos.\nGrab a real training repo, stick it here, and see what happens.",
          "score": 4,
          "created_utc": "2025-12-28 12:32:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcs8hh",
              "author": "Melodic_Struggle_95",
              "text": "Fair point. This repo isn‚Äôt meant to represent a final production setup, but a learning-focused MLOps platform where I can iterate end to end.\nIn a real production environment, I agree that training and serving would typically live in separate repos or at least separate deployment units, often owned by different teams.\nFor now, I kept them together to understand the full lifecycle and the interfaces between training, evaluation, and inference. My next step is to split training and serving and treat the trained model as an external artifact to the platform.\nAppreciate the feedback.",
              "score": 2,
              "created_utc": "2025-12-28 12:38:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh2vk2",
          "author": "BackgroundLow3793",
          "text": "Oh that's nice. Thank you. I'm learning MLOps recently too. I think next thing is MLFlow, understand why people use MLFlow. I mean it doesn't have to be MLFLow, but the core idea is tracking and model versioning I guess. \n\nThere is also a good article here: [https://docs.databricks.com/aws/en/machine-learning/mlops/mlops-workflow](https://docs.databricks.com/aws/en/machine-learning/mlops/mlops-workflow)",
          "score": 2,
          "created_utc": "2025-12-29 02:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjldtq",
              "author": "Melodic_Struggle_95",
              "text": "Thanks! Yeah, I‚Äôm on the same page. The main value is tracking and versioning, not MLflow itself. I‚Äôm planning to add that next, and the Databricks article looks solid. Appreciate you sharing it.",
              "score": 1,
              "created_utc": "2025-12-29 13:48:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgfa6",
          "author": "wallesis",
          "text": "Where's the \"platform\" part?",
          "score": 1,
          "created_utc": "2025-12-28 15:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdl5gc",
              "author": "Melodic_Struggle_95",
              "text": "Right now the ‚Äúplatform‚Äù part is still small by design. At this stage I‚Äôm focusing on building the core pieces first a clean training pipeline, an evaluation gate, a consistent model loading layer, and a serving API with clear contracts.The idea is to treat this as the foundation, and then gradually add real platform features like CI/CD, model registry, monitoring, and automated retraining. This repo shows the early platform core, not the final version.",
              "score": 1,
              "created_utc": "2025-12-28 15:40:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwozj3m",
          "author": "Significant-Fig-3933",
          "text": "How do you handle data lineage, code tracking, orchestration, and data/model monitoring?\nLike what exact data and what exact code (model design, features, etc) was the model trained with?\n\nAnd how do you coordinate/track retraining etc (ie orchestration)?",
          "score": 1,
          "created_utc": "2025-12-30 07:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoztqm",
              "author": "Melodic_Struggle_95",
              "text": "Right now the project tracks data and code through the training pipeline and Git commits, with retraining handled manually, and the next planned step is adding MLflow and orchestration to properly manage lineage, versioning, monitoring, and automated retraining.",
              "score": 2,
              "created_utc": "2025-12-30 07:27:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqwnyh",
          "author": "bad_detectiv3",
          "text": "Can you provide resource where you built up on MLOps material? I want to give this a try beyond using LLM in application use case",
          "score": 1,
          "created_utc": "2025-12-30 15:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx86yrp",
          "author": "Temporary_Term_5093",
          "text": "Cool project ! Did you use any Command Line AIs ?",
          "score": 1,
          "created_utc": "2026-01-02 09:23:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx87o7r",
              "author": "Melodic_Struggle_95",
              "text": "Thanks! No, I didn‚Äôt use any command-line AI tools. I wanted to build things step by step myself so I could really understand how training, evaluation, serving, and deployment work together in a real setup",
              "score": 2,
              "created_utc": "2026-01-02 09:30:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx88dlb",
                  "author": "Temporary_Term_5093",
                  "text": "Yeah that makes sense. I am also getting started with an end-to-end project myself and was looking for some AI tools to help move things faster. But i like your approach, the goal for doing these projects is to learn the basics and AI probably wouldn't help much with that",
                  "score": 1,
                  "created_utc": "2026-01-02 09:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1sryi",
      "title": "When models fail without ‚Äúdrift‚Äù: what actually breaks in long-running ML systems?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1q1sryi/when_models_fail_without_drift_what_actually/",
      "author": "Salty_Country6835",
      "created_utc": "2026-01-02 07:53:20",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "\nI‚Äôve been thinking about a class of failures that don‚Äôt show up as classic data drift or sudden metric collapse, but still end up being the most expensive to unwind.\n\nIn a few deployments I‚Äôve seen, the model looked fine in notebooks, passed offline eval, and even behaved well in early production. The problems showed up later, once the model had time to interact with the system around it:\n\nDownstream processes quietly adapted to the model‚Äôs outputs\n\nHuman operators learned how to work around it\n\nRetraining pipelines reinforced a proxy that no longer tracked the original goal\n\nMonitoring dashboards stayed green because nothing ‚Äústatistically weird‚Äù was happening\n\n\nBy the time anyone noticed, the model wasn‚Äôt really predictive anymore, it was reshaping the environment it was trained to predict.\n\nA few questions I‚Äôm genuinely curious about from people running long-lived models:\n\nWhat failure modes have you actually seen after deployment, months in, that weren‚Äôt visible in offline eval?\n\nWhat signals have been most useful for catching problems early when it wasn‚Äôt input drift?\n\nHow do you think about models whose outputs feed back into future data, do you treat that as a different class of system?\n\nAre there monitoring practices or evaluation designs that helped, or do you mostly rely on periodic human review and post-mortems?\n\n\nNot looking for tool recommendations so much as lessons learned; what broke, what surprised you, and what you‚Äôd warn a new team about before they ship.\n",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1q1sryi/when_models_fail_without_drift_what_actually/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nxbxyil",
          "author": "bobbruno",
          "text": "My experience is that every model fails after some time. Most drift will be in input, but not all - and not all input drift is that detectable.\n\nYou also get drift on results - the inputs are as previously seen, but the competition adjusted to provide better answers to your recommendations, and you lose. The virus mutates on a gene not in your feature set, and now it's resistant. Your business changes something that the model didn't account or track, and now its predictions don't work so well anymore. There are as many ways things can go wrong as there are scenarios for ML.\n\nI just accept that every model is wrong, some are useful, for some time.",
          "score": 2,
          "created_utc": "2026-01-02 22:08:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxc1ho2",
              "author": "Salty_Country6835",
              "text": "\n\nAgreed. That‚Äôs very much my experience as well.\n\nMost failures eventually present as some form of input or outcome drift, even if the root cause wasn‚Äôt detectable or measurable at the time. By the time it‚Äôs obvious, the model‚Äôs ‚Äúuseful window‚Äù has often already passed.\n\nWhere this post came from for me was noticing that some degradations aren‚Äôt driven by novel inputs so much as the environment adapting around the model; competitors adjusting, operators changing behavior, downstream processes re-optimizing. Eventually that does look like drift, but it can take a while to surface as something you can act on.\n\nIn practice I‚Äôve landed in the same place you describe: assume impermanence, expect degradation, and design processes that plan for retirement or rework rather than indefinite correction.\n\n‚ÄúAll models are wrong, some are useful, for some time‚Äù feels like the only stable stance.",
              "score": 2,
              "created_utc": "2026-01-02 22:26:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8jlcg",
          "author": "UnreasonableEconomy",
          "text": "NGL sounds a bit like either scifi, or more charitably, like bad product management.\n\n> Human operators learned how to work around it\n\nThis has nothing really to do with machine learning. You need to talk with your stakeholdes...",
          "score": 1,
          "created_utc": "2026-01-02 11:20:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8kdeg",
              "author": "Salty_Country6835",
              "text": "\nI don‚Äôt disagree that stakeholder alignment and product design matter here, that‚Äôs kind of the point.\n\nWhere I‚Äôm pushing is that once a model is deployed, those human and organizational adaptations become part of the system the model operates in. From an ops perspective, that affects:\n\nwhat data you collect next\n\nwhat retraining reinforces\n\nwhat metrics remain meaningful over time\n\n\nIf operators learn to work around a model, that‚Äôs not sci-fi, it‚Äôs an observable feedback signal that often isn‚Äôt captured by standard ML monitoring. In practice, it can quietly invalidate offline assumptions while dashboards stay green.\n\nI‚Äôm less interested in whether this is ‚ÄúML vs PM‚Äù and more in how teams operationally detect and manage these effects once a model is in prod. If you‚Äôve seen concrete ways to handle that (or decided it‚Äôs out of scope for MLOps entirely), I‚Äôd genuinely like to hear how you draw that boundary.",
              "score": 2,
              "created_utc": "2026-01-02 11:27:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx8mujc",
                  "author": "UnreasonableEconomy",
                  "text": "> If operators learn to work around a model, that‚Äôs not sci-fi, it‚Äôs an observable feedback signal that often isn‚Äôt captured by standard ML monitoring. In practice, it can quietly invalidate offline assumptions while dashboards stay green.\n\nI don't wanna be rude, but is this your professional experience, or chatgpt agreeing with you?\n\n> and more in how teams operationally detect and manage these effects once a model is in prod\n\ndetect: if what users are saying and what the metrics show don't agree\n\nmanage: by talking to users... \n\n---\n\nwhat type of product are you talking about? what industry, what application? what were you trying to accomplish?\n\nIME it's rare that you go to online leaning straight out of eval or 'early production' unless it's a time series application. Even then, rare. Eventually, the product is dialed in well enough, and the processes are understood well enough that you can go online. The users rarely know the difference. Maybe someone else has a different experience.\n\nAt the end of the day you're developing a process. Yes, ML is doing a lot of heavy lifting 'figuring out' the process, but it can't do everything. If your process fails so catastrophically (as you describe, from a process perspective), someone didn't do a good job of figuring out the process. Blaming the model doesn't really fly as an excuse.\n\nIf a model fails as you describe, someone either didn't do the legwork to understand the requirements or the context, or stopped working the process altogether.\n\nThe reason I find this unrealistic is because this assumes software is 'set it and forget it'. It never is. When development stops, the product typically dies - unless it has been developed to a certain maturity. \n\nWhat you do next is you keep developing. What data you collect next depends on what you're doing. What metrics are meaningful depends on what you're trying to accomplish and where you're struggling.",
                  "score": 1,
                  "created_utc": "2026-01-02 11:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pzfdgi",
      "title": "need guidance regarding mlops",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pzfdgi/need_guidance_regarding_mlops/",
      "author": "not_popular_to_know",
      "created_utc": "2025-12-30 11:05:17",
      "score": 6,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hello everyone,  \nI‚Äôm an engineering student with a physics background. For a long time, I wasn‚Äôt sure about my future plans, but recently I‚Äôve started feeling that machine learning is a great field for me. I find it fascinating because of the strong mathematics involved and its wide applications, even in physics.\n\nNow, I want to build a career in MLOps. So far, I‚Äôve studied machine learning and DSA and have built a few basic projects. I have a decent grasp of ML fundamentals and I‚Äôm currently learning more about AI algorithms.\n\nIf there‚Äôs anyone who can guide me on how to approach advanced concepts and build more valuable, real-world projects, I‚Äôd really appreciate your help.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1pzfdgi/need_guidance_regarding_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwqvfdn",
          "author": "Valeria_Xenakis",
          "text": "This is an answer I gave earlier for a different post. There maybe some irrelevant sections for you so please weed out the required points.  \n\n\nTo be honest, there are effectively zero MLOps jobs for freshers in the industry right now. It is a relatively new field and it is almost entirely dominated by experienced DevOps engineers or data engineers who have pivoted.\n\nAs a fresher, breaking in as a backend/software engineer is a much more realistic path. You should focus on building full stack projects to get your foot in the door first.\n\nOnce you are hired, learn MLflow since it is the industry standard, or just master whatever tools your company is specifically using. The best way to get into MLOps is to slowly take on those responsibilities by showing initiative within a backend role.\n\nAs for cloud providers, there is no point in trying to learn all three. Most companies use AWS, so just focus on that. You can look at the AWS MLOps certification later if you want less friction while working in the field, but get the foundational experience first.\n\nYou need to be a Devops engineer first and then only you can be an Mlops engineer. And Devops is not a fresher's task, usually you do those tasks after about 1.5 years of experience in the same project.",
          "score": 3,
          "created_utc": "2025-12-30 15:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr10h9",
              "author": "not_popular_to_know",
              "text": "\nSo can u tell me where to start and roadmap\nAs I have given very much time learning maths and all\nCan u tell me if I can build a carrier in ai/ml\nBtw thanks for your advice",
              "score": 1,
              "created_utc": "2025-12-30 16:10:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwr8q0q",
                  "author": "Valeria_Xenakis",
                  "text": "Hey, I don't want to give bad advice so I will keep it simple. That way you will still do more research from your side and not take whatever I say at face value.\n\nIf you are a student who has no industry experience, please don't constraint yourself to MLOps. IMO MLOps is a skill and DevOps is the job. Now you said you liked the mathematics part, sorry to say if that is what interests you, you would find DevOps really boring.\n\nThe kind of roles you are looking for are ML engineer, AI engineer etc. In  these roles you would be using models created by people in research segment to solve problems for your organization. These roles require maths and ml/ai but not in the foundational sense. They require your applied  maths and ai chops to use those techniques for the company and not come up with new models or techniques as such. These roles will 100 percent require MLOps knowledge but you would be expected to handle these only after you have 2-3 years of exp.\n\nAs a green horn right of the block, focus on full stack dev and create projects related to full stack dev of AI based projects. Basically aim for a backend/full stack engineer in any company that uses a lot of AI for their core product offering (Not the one which use AI as tools, the ones that use AI as their main product offering like Amazon, AirBnB, Walmart etc).\n\nAs you gain exp you will 100 percent need to take on MLOps responsibilities, but right now you are not cut out for that.\n\nPlease keep asking your doubts. I like to keep individual replies short, they are more engaging.",
                  "score": 3,
                  "created_utc": "2025-12-30 16:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pzupvx",
      "title": "Built spot instance orchestration for batch ML jobs‚Äîfeedback wanted",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1pzupvx/built_spot_instance_orchestration_for_batch_ml/",
      "author": "HelpingForDoughnuts",
      "created_utc": "2025-12-30 21:44:57",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "Got tired of building the same spot instance handling code at work, so I made it a product. Submit a job, it runs on Azure spot VMs, handles preemption/retry automatically, scales down when idle.\nThe pitch is simplicity‚Äîmulti-GPU jobs without configuring distributed training yourself, no infrastructure knowledge needed. Upload your container, pick how many GPUs, click run, get results back.\nEarly beta. Looking for people who‚Äôve built this stuff themselves and can tell me what I‚Äôm missing. Free compute credits for useful feedback.\nRoast my architecture if you want, I can take it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1pzupvx/built_spot_instance_orchestration_for_batch_ml/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "nwv2jdk",
          "author": "qwertying23",
          "text": "Have you tried ray ? It does this quite well",
          "score": 3,
          "created_utc": "2025-12-31 04:45:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy0692",
              "author": "HelpingForDoughnuts",
              "text": "Yeah, Ray is solid for distributed ML workloads, and Anyscale makes it more accessible.\n\nThe main difference is that Ray still requires learning the Ray framework - you‚Äôre writing Ray-specific code with decorators, clusters, etc. We‚Äôre targeting the layer above that: ‚ÄúI want to train a PPO agent to play Breakout‚Äù ‚Üí it just works, without learning new APIs.\n\nRay is great if you want that level of control and don‚Äôt mind the learning curve. We‚Äôre going after people who just want their training job to run without becoming Ray experts first.\n\nDifferent markets really - Ray for ML engineers, us for researchers/beginners who want to skip the infrastructure parts entirely.\n\nHave you used Anyscale? Curious how you found the setup experience.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2025-12-31 17:19:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwy4459",
          "author": "qwertying23",
          "text": "Yes I have used anyscale and their workspaces concept is pretty neat. I do agree with your points but in my experience we faced the same issue and got stuck in the future we had to redesign our entire stack again. The beauty of ray is the same code can run on your laptop a single gpu cluster or on 1000‚Äôs of gpu I would rather build this with ray from the get go rather than the way we did this which made distributed computing an after thought. But happy to chat and give feedback on your product.",
          "score": 2,
          "created_utc": "2025-12-31 17:38:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy4yff",
              "author": "HelpingForDoughnuts",
              "text": "That‚Äôs a really thoughtful point about distributed-first architecture. Your experience with having to redesign the entire stack later is exactly the kind of lesson that‚Äôs expensive to learn the hard way.\n\nYou‚Äôre absolutely right that Ray‚Äôs abstraction is powerful - write once, run anywhere from laptop to 1000 GPUs. And if we‚Äôre building orchestration that needs to scale, starting with Ray as the foundation makes way more sense than bolting on distributed later.\n\nThe differentiation would be more in the layer above Ray - instead of users learning Ray APIs and cluster management, they get the natural language interface that routes to Ray workloads under the hood. But you‚Äôre right that the underlying execution should be distributed-native from day one.\n\nI‚Äôd genuinely love to chat more about this. Your experience with both the technical implementation and the business realities is exactly what we need to hear. Happy to jump on a call if you‚Äôre interested - would love to get your perspective on where the real pain points are and how a Ray-based approach might solve them better.\n\nThanks for offering feedback - that kind of input from someone who‚Äôs actually built and scaled these systems is invaluable.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2025-12-31 17:43:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}