{
  "metadata": {
    "last_updated": "2026-02-01 08:57:56",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 20,
    "total_comments": 37,
    "file_size_bytes": 63591
  },
  "items": [
    {
      "id": "1qnlepn",
      "title": "Static model selection did not work (enough) for us",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qnlepn/static_model_selection_did_not_work_enough_for_us/",
      "author": "tech2biz",
      "created_utc": "2026-01-26 16:49:11",
      "score": 66,
      "num_comments": 17,
      "upvote_ratio": 0.97,
      "text": "We spent a few months now on a solution for dynamic model routing because we tried several things and nothing really solved our problem.\n\nThe core issue / our background: we deployed nodes with SLM and RAG to regulated industry teams (the problem is relevant in any setup though). But users couldn't figure out when to use which model (despite ongoing effort to educate). We tried static routing but the classification of queries upfront didn't really work as it was very unpredictable what the users were doing. Also the \"guessing\" part did not feel right, we iterated really a lot on this. So next we thought hybrid with big models would be the solution but somewhat similar we always had to estimate complexity before we saw output. The estimates missed often enough that we either overspent (like, radically, breaking our unit economics) or quality was bad from routing too aggressively to small models.\n\nWe found a Google publication (happy to share) that approaches this very differently, not routing but cascading. Start generating with the small model, validate quality as you go, escalate only if needed.\n\nWe developed this and open-sourced our implementation: [github.com/lemony-ai/cascadeflow](http://github.com/lemony-ai/cascadeflow)\n\nIt plugs into your existing infrastructure, works with LiteLLM, OpenRouter, n8n, LangChain, or direct API calls. From there you can use whatever models you want: OpenAI, Anthropic, Groq, HuggingFace, local models via Ollama, self-hosted via vLLM.\n\nNot replacing your router or orchestration layer, just adding quality validation that decides when the cheap models output is actually good enough.\n\nSeeing 40-90% cost reduction in first production workloads and we are honestly quite excited. Would love feedback and happy to chat with others working on inference layers.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qnlepn/static_model_selection_did_not_work_enough_for_us/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1unm6l",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 17:10:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uo88c",
              "author": "tech2biz",
              "text": "Totally fair concern. It's not another full LLM call. we use lightweight validation (confidence scores, completion checks, some heuristics depending on task type). Latency add is minimal, usually under 20ms. My co-founder Sascha can go deeper on the technical side if you want. But we are coming from the on-prem world, so latency was actually our initial focus (later figured it also reduces the costs alike).",
              "score": 5,
              "created_utc": "2026-01-26 17:13:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v9nns",
                  "author": "ediblescholarship",
                  "text": "Confidence scores are tricky though.small models are often confidently wrong. How do you calibrate the threshold? Or is it task-specific?",
                  "score": 1,
                  "created_utc": "2026-01-26 18:44:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1zi2jf",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-01-27 09:06:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1utlal",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 17:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uwqf7",
              "author": "tech2biz",
              "text": "Sure! This is the blog: [https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/](https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/)  \nAnd the research paper: [https://openreview.net/pdf?id=vo9t20wsmd](https://openreview.net/pdf?id=vo9t20wsmd)",
              "score": 3,
              "created_utc": "2026-01-26 17:50:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uz339",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 18:00:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v4w0f",
              "author": "tech2biz",
              "text": "Streaming works, we use it ourselves. We've ran various 100k benchmarking queries incl concurrent without issues. But also honestly, we are still learning at scale. Would love to hear if you're planning to push volume.",
              "score": 1,
              "created_utc": "2026-01-26 18:24:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uzqh7",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 18:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6tya",
              "author": "tech2biz",
              "text": "it wraps around your existing calls, you don't need to restructure your chains. Basically a few lines to initialize and then swap your calls. We have a LangChain example in the repo. If you hit any issues just open an issue or ping us, still early days so feedback on integration pain points is super useful.",
              "score": 1,
              "created_utc": "2026-01-26 18:33:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v10l0",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 18:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v5aod",
              "author": "tech2biz",
              "text": "awesome! Every feedback much appreciated. And for agentic, we are also working on clawdbot integration.",
              "score": 6,
              "created_utc": "2026-01-26 18:26:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uuhq4",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-26 17:40:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxbls",
              "author": "tech2biz",
              "text": "Nice to hear!!! 50% is real money. And yes, the edge cases are exactly it, our routers always either led to bad quality or overpaying, it always felt there is no middle ground.",
              "score": 1,
              "created_utc": "2026-01-26 17:53:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpf6f8",
      "title": "To the ML Engineers who didnâ€™t take the \"standard\" path: What was the \"Aha!\" moment where it finally clicked?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qpf6f8/to_the_ml_engineers_who_didnt_take_the_standard/",
      "author": "Effective_Kale3359",
      "created_utc": "2026-01-28 15:56:32",
      "score": 35,
      "num_comments": 12,
      "upvote_ratio": 0.91,
      "text": "Weâ€™ve all seen the \"Masterâ€™s degree + 500 LeetCode problems\" roadmap, but Iâ€™m looking for the real, gritty stories.\n\nâ€‹If you transitioned from a college student to ML engineer or if you are self-taught:\n\nâ€‹The Bridge: What was the first project you built that actually felt \"industrial\" and not like a tutorial-hell toy?\n\nâ€‹The \"Lie\": What is one skill everyone told you was \"mandatory\" that youâ€™ve literally never used in your daily job?\n\nâ€‹The Pivot: How did you convince your first employer to take a chance on an ML \"outsider\"?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qpf6f8/to_the_ml_engineers_who_didnt_take_the_standard/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o29kt7b",
          "author": "Scared_Astronaut9377",
          "text": "The real story behind any such recent anecdote will be \"got lucky to meet a hiring manager who had no clue and he hired me for something completely arbitrary\". Get all the skills and understanding in the world, you are going to be automatically filtered out for 98% of real positions.",
          "score": 21,
          "created_utc": "2026-01-28 18:54:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fv8gt",
              "author": "ummitluyum",
              "text": "You're right about HR filters, ATS are ruthless regarding the lack of a relevant degree, making the \"cold apply\" strategy dead on arrival for self-taught engineers\n\nThe only bypass is networking and Proof of Work. Open-source contributions, tech blogs, or hackathons. You need your resume to land on the Hiring Manager's desk bypassing the HR filter. The manager often doesn't give a damn about the diploma, they need tickets closed. HR needs to close the vacancy based on a checklist",
              "score": 2,
              "created_utc": "2026-01-29 16:55:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fuokd",
          "author": "ummitluyum",
          "text": "The Lie: that you need deep math and the ability to derive backpropagation on a whiteboard. In 99% of cases you'll be debugging YAML configs, fixing Docker containers, and optimizing inference, not inventing new loss architectures\n\nThe Bridge: a simple text classification API, but wrapped in Docker, with a CI/CD pipeline, monitoring (Prometheus/Grafana), and load testing. That's what separates a toy from industry\n\nThe Pivot: I showed that I could not just \"train a model\" but \"deliver a model to the user and keep it alive\". Business needs engineers, not scientists",
          "score": 5,
          "created_utc": "2026-01-29 16:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g17nl",
              "author": "an4k1nskyw4lk3r",
              "text": "Thatâ€™s so true! Iâ€™ve been working as an AI/ML Engineer for almost 2 years and I never train an LM or NLP complex architectures from scratch. Everything is pre trained and fine tuned (thatâ€™s the truth). 99% of the time Iâ€™ve been working in yaml files, ci pipelines and (in my case, that my core actuation is NLP) have been working in a bridge between models and business ruleâ€¦ too much RAG and prompt. Too much redis for context engineering and so it isâ€¦",
              "score": 1,
              "created_utc": "2026-01-29 17:22:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2g1txz",
          "author": "an4k1nskyw4lk3r",
          "text": "Iâ€™ve been working as an AI/ML Engineer for almost 2 years and I never train an LM or NLP complex architectures from scratch. Everything is pre trained and fine tuned (thatâ€™s the truth). 99% of the time Iâ€™ve been working in yaml files, ci pipelines, containers, pods or whatever related to prod like environments and (in my case, that my core actuation is NLP) have been working in a bridge between models and business ruleâ€¦ too much RAG and prompt. Too much redis for context engineering and so it isâ€¦\n\nThe truth, FOR REAL, prepare to engineering (80%) and focus 20% of your time ~> neural networks, classic machine learning algorithms and how to retrain those ones.\n\nMost of interviews are about pure data science but in practice you will find more coding.",
          "score": 3,
          "created_utc": "2026-01-29 17:25:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cy03f",
          "author": "devilwithin305",
          "text": "RemindMe! 1 day",
          "score": 1,
          "created_utc": "2026-01-29 05:13:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iovtj",
          "author": "ImposterExperience",
          "text": "For me, at every job I always applied ML techniques regardless of my position and grew. Also consulting gigs helps for getting opportunities.",
          "score": 1,
          "created_utc": "2026-01-30 01:09:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oy9qd",
          "author": "No-Consequence-1779",
          "text": "If your a ha moment q is specificallyâ€˜understanding how an LLM produced an answer; most say understanding back propagation.Â \n\nIf itâ€™s about useful products or servicesâ€¦ it depends. Â ",
          "score": 1,
          "created_utc": "2026-01-30 22:56:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sxkhz",
          "author": "yes-im-hiring-2025",
          "text": "TLDR: was good at coding but didn't know what interested me. Joined startup -> learnt that I'm a natural problem solver/leader, not necessarily only a coder -> joined FAANG after 7 years at the startup.\n\nDecent at academics but bored. Was going to be a researcher. One of my project profs in engineering Year 3 stressed that I try out startups before deciding on research.\n\nI interned at an analytics/ML startup that was in walking distance from my dorm (hostel), and continued the internship over the sem break. My manager insisted I sit with the full time employees and do some parts of their work with them.\n\nWithin a week I was running small scripts to clean and process data, write simple parsing logic etc. Googling -> doing. Had a lot of fun doing that. Super small team and a helpful manager made sure I was having fun in my internship and asked me to return back in year 4.\n\nMet the CEO. Decided to give the startup life a shot. A few years later I was interacting with the clients and attending meetings as a stand-in for the CTO, and it just clicked - I don't really care about ML; I care about building things that solve problems. ML just happened to be the tool I was most well versed in.\n\nLoved talking to people, looking and designing processes, cost computations, meeting clients, etc., and became a natural leader who knew how to bring the right people together to solve something, and how to solve a problem myself. I continued being good at coding, but it became less important in my last few years. Became the founding engineer and team lead, hired my own team and ran it like a mini startup for a few years.\n\nEventually I moved to FAANG after I had enough of the 0->1 phase. Enjoying it so far.\n\nCoding ability and passion is non negotiable. But if that's the only thing you bring to the table you're gonna get stuck being so good at your job that you can't be promoted or replaced.\n\nBuild systems, not just scripts. Become hard to replace because of how much of a force multiplier you are, but be smart enough to keep in touch with your core competencies so that if/when you need to change your employer you can choose to do so.",
          "score": 1,
          "created_utc": "2026-01-31 15:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28taas",
          "author": "Tough-Public-7206",
          "text": "RemindMe! 1 day",
          "score": 0,
          "created_utc": "2026-01-28 16:55:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29a7p0",
          "author": "JeanLuucGodard",
          "text": "RemindMe! 1 day",
          "score": 0,
          "created_utc": "2026-01-28 18:09:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29aeax",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-29 18:09:05 UTC**](http://www.wolframalpha.com/input/?i=2026-01-29%2018:09:05%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qpf6f8/to_the_ml_engineers_who_didnt_take_the_standard/o29a7p0/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qpf6f8%2Fto_the_ml_engineers_who_didnt_take_the_standard%2Fo29a7p0%2F%5D%0A%0ARemindMe%21%202026-01-29%2018%3A09%3A05%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qpf6f8)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-28 18:09:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmj990",
      "title": "[Passed] NVIDIA Agentic AI Certification (NCP-AAI)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qmj990/passed_nvidia_agentic_ai_certification_ncpaai/",
      "author": "Ranger_1928",
      "created_utc": "2026-01-25 13:34:42",
      "score": 30,
      "num_comments": 25,
      "upvote_ratio": 0.97,
      "text": "Just wanted to share a data point for anyone eyeing the new NVIDIA Agentic AI certification. I sat for the exam this today and passed! ðŸš€  \nI already had experience building agents with LangChain/OpenAI, but I quickly realized this exam requires a mindset shift. Itâ€™s less about generic Python loops and more about the \"NVIDIA Way\" (NIMs, Triton, NeMo).\n\n**The Results (The Good & The Ugly):**  \nI wanted to be transparent about the score breakdown because it tells a story:\n\n* **Platform Implementation:**Â 85% \n* **Deployment & Scaling:**Â 79%\n* **Safety, Ethics & Compliance:**Â ...35% ðŸ˜…\n\n**My Takeaway:**  \nIf you are preparing,Â **do not sleep on the infrastructure**. The reason I passed is that I focused nicely on understandingÂ **NIM microservices, Triton Inference Server, and Kubernetes scaling**. If I had relied only on my generic \"coding agents\" knowledge, I would have failed.\n\nAlso, Don't make my mistakeâ€”study the \"boring\" safety docs of safety, Ethics and Human in Loop Too!\n\n  \n**Rest assured, Ask me Anything about the exam and I will try my best to help** ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qmj990/passed_nvidia_agentic_ai_certification_ncpaai/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1n4fze",
          "author": "aksrao1998",
          "text": "Hi did you take any course to prepare?",
          "score": 5,
          "created_utc": "2026-01-25 16:27:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n5pws",
              "author": "Ranger_1928",
              "text": "Nope, online resources. Got a roadmap and list of online articles / videos with the help of ChatGPT.",
              "score": 1,
              "created_utc": "2026-01-25 16:33:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1nm00r",
                  "author": "dank_coder",
                  "text": "Can you please share your roadmap?",
                  "score": 1,
                  "created_utc": "2026-01-25 17:43:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1n6amv",
              "author": "DenseIncome4394",
              "text": "\\+1",
              "score": 0,
              "created_utc": "2026-01-25 16:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ma4an",
          "author": "proof_required",
          "text": "How long did you prepare it for? How experienced are you?",
          "score": 1,
          "created_utc": "2026-01-25 14:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mbbn2",
              "author": "Ranger_1928",
              "text": "Hey,\n\nI have 1.5 years of indsutry experience working with LLMs and agents. But most of it was using Open Source Libraries. The transition from open source to nvidia tools has its own learning curve and took around 3-4 days (\\~ 1hour/day) for me to understand it.\n\nOverall preparation was around 17-20 days (\\~ 1/1.5 hour/day) with initial sections taking over 4 days. The last 3 sections (around \\~15%) of whole syllabus was done by me in an hour, didn't gave much attention to it, and that became the lowest scoring section for me now. Should've given more time to it too.",
              "score": 3,
              "created_utc": "2026-01-25 14:08:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1o303g",
                  "author": "Competitive-Fact-313",
                  "text": "Can you talk about your experience, whatâ€™s like using open source - on a daily basis how your day look like what are the tools you are most exposed with",
                  "score": 1,
                  "created_utc": "2026-01-25 18:53:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mldp3",
          "author": "Yarafsm",
          "text": "Do you think it makes sense for anyone with no ML experience but quite a bit of cloud/platform architect experience ?\nAlso was it more focussed on engineering or architecture?",
          "score": 1,
          "created_utc": "2026-01-25 15:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mm0lq",
              "author": "Ranger_1928",
              "text": "The exam is more **engineering-focused** than pure architecture. Your cloud background helps with deployment/scaling, but youâ€™ll need extra prep on NVIDIAâ€™s AI/ML stack (NIM, Triton, NeMo, safety modules). Itâ€™s doable if youâ€™re comfortable picking up new tools quickly.",
              "score": 3,
              "created_utc": "2026-01-25 15:04:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mwbsg",
                  "author": "dank_coder",
                  "text": "I am familiar with building AI agents. I have built and deployed agents using Langgraph. How time do you think I need to dedicate for this?",
                  "score": 1,
                  "created_utc": "2026-01-25 15:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1nu8xj",
          "author": "Agreeable-Court602",
          "text": "Congratulations ðŸŽ‰ðŸŽ‰. Also recently passed the same.",
          "score": 1,
          "created_utc": "2026-01-25 18:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o1wki",
              "author": "Ranger_1928",
              "text": "Thanks, and Congratulations to you too ðŸ”¥ðŸ”¥",
              "score": 1,
              "created_utc": "2026-01-25 18:48:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1prxut",
              "author": "ab624",
              "text": "how did you prepare ? any good resources and suggestions",
              "score": 1,
              "created_utc": "2026-01-25 23:21:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rjjht",
                  "author": "Ranger_1928",
                  "text": "Didn't took any course, only online freely available resources. The best I found for AI Agents was IBM videos, and for Nvidia architecture - Nvidia articles and youtube videos. Shared a roadmap for studying in this link:  \n[https://limewire.com/d/krbfj#WgVuKGW8kz](https://limewire.com/d/krbfj#WgVuKGW8kz)",
                  "score": 1,
                  "created_utc": "2026-01-26 04:58:21",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q4jrg",
          "author": "burntoutdev8291",
          "text": "How useful is this cert? Do you have any plans on taking the NCA-AIIO?\n\nAnyway great work, I would think the safety and ethics stuff is very important because most of us know how to serve but don't know much about compliance.",
          "score": 1,
          "created_utc": "2026-01-26 00:22:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rk6gd",
              "author": "Ranger_1928",
              "text": "Good point â€” I think AIIO and AAI target slightly different audiences. AIIO is more about understanding the NVIDIA ecosystem and infra basics, while AAI dives into building and deploying agentic AI apps. I found AAI more handsâ€‘on since itâ€™s closer to real-world workflows, but AIIO gives the foundation to appreciate how the infra side works. They complement each other depending on whether youâ€™re infraâ€‘heavy or appâ€‘heavy. As for plans, I don't have any near future plans for NCA-AIIO.\n\nThanks, and yes, safety, ethics and compliance stuff is important, and it contains a very wide range of issues and solutions than we normally understand. So, its a must go.",
              "score": 1,
              "created_utc": "2026-01-26 05:02:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1si7oj",
          "author": "ConnentingDots",
          "text": "!RemindMe 2 weekd",
          "score": 1,
          "created_utc": "2026-01-26 09:40:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1si9w9",
              "author": "RemindMeBot",
              "text": "**Defaulted to one day.**\n\nI will be messaging you on [**2026-01-27 09:40:46 UTC**](http://www.wolframalpha.com/input/?i=2026-01-27%2009:40:46%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qmj990/passed_nvidia_agentic_ai_certification_ncpaai/o1si7oj/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qmj990%2Fpassed_nvidia_agentic_ai_certification_ncpaai%2Fo1si7oj%2F%5D%0A%0ARemindMe%21%202026-01-27%2009%3A40%3A46%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qmj990)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-26 09:41:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2sv6n8",
          "author": "Pattern-Ashamed",
          "text": "They have a course for agentic AI. is it did you take it? Is it worth it?",
          "score": 1,
          "created_utc": "2026-01-31 15:23:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnazmo",
      "title": "MLOps Roadmap",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qnazmo/mlops_roadmap/",
      "author": "Deep_Priority_2443",
      "created_utc": "2026-01-26 08:58:33",
      "score": 28,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "Hi there, if this is of help to you, [roadmap.sh](http://roadmap.sh) has just launched a revised version of its [MLOps roadmap](https://roadmap.sh/mlops). I want to thank the people in this group who contributed to the review of the roadmap with their feedback. \n\nhttps://preview.redd.it/kolchhwvrnfg1.png?width=1088&format=png&auto=webp&s=151207b5db9b37c170fdbf58c3f39d131a826d90\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qnazmo/mlops_roadmap/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1u68ej",
          "author": "Competitive-Fact-313",
          "text": "Adding Prometheus and Grafana as observability as a wise choice.",
          "score": 2,
          "created_utc": "2026-01-26 15:56:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq5sdm",
      "title": "Advice for those switching to MLOps/ML from other backgrounds: stick with one or two domains",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qq5sdm/advice_for_those_switching_to_mlopsml_from_other/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-29 11:11:59",
      "score": 18,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "If you're transitioning into MLOps or ML Engineering from a different background (DevOps, backend, etc.), here's something I've learned the hard way:\n\n**Pick one or two ML domains and go deep.**\n\nWhy?\n\n1. **Every company has their own unique pipeline and infra.** There's no universal \"MLOps stack\" that everyone uses. What works at one company looks completely different at another.\n2. **Interviews have changed.** People rarely ask general theory questions anymore. Instead, they dig into the details of *your* projects â€” what decisions you made, what tradeoffs you faced, how you solved specific problems.\n3. **Being a generalist dilutes your value.** Applying to 100 places with surface-level knowledge across everything is less effective than targeting roles that match your specific ML or business interest and becoming genuinely expert in that space.\n\n**What do I mean by \"domains\"?**\n\nThink: Computer Vision, NLP, Recommender Systems, Time Series/Forecasting, Speech/Audio, etc.\n\nFor example, if you pick CV, you learn common model architectures (CNNs, Vision Transformers), understand data pipelines (image preprocessing, augmentation), know deployment challenges (model size, latency, GPU serving), and build projects around it. Now, when you apply to companies doing CV work, you're not a generalist; you actually speak their language.\n\nAnd if you're coming from DevOps/infra like me, that's actually a **unique advantage**. Production infrastructure, scaling, reliability â€” these are the real problems ML teams are struggling with right now. Most ML folks can build models. Far fewer can deploy and operate them reliably.\n\nDon't undersell your background. Lean into it.\n\nI've helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here: [topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qq5sdm/advice_for_those_switching_to_mlopsml_from_other/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2ko0cb",
          "author": "Extension_Key_5970",
          "text": "sharing my detailed blog as well [https://medium.com/p/58878bb1cd64](https://medium.com/p/58878bb1cd64), if any one is interested",
          "score": 2,
          "created_utc": "2026-01-30 09:19:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq23vz",
      "title": "Iceberg REST Catalog Alternatives: Top Options & How to Choose The Best One For Your Team",
      "subreddit": "mlops",
      "url": "https://lakefs.io/blog/iceberg-rest-catalog-alternatives/",
      "author": "Comfortable-Site8626",
      "created_utc": "2026-01-29 07:31:51",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qq23vz/iceberg_rest_catalog_alternatives_top_options_how/",
      "domain": "lakefs.io",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qpdar2",
      "title": "[Update] Benchmarking the \"Airflow Tax\": I tested 6 lightweight orchestrators so you don't have to.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qpdar2/update_benchmarking_the_airflow_tax_i_tested_6/",
      "author": "m_gijon",
      "created_utc": "2026-01-28 14:46:28",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "Last week, I asked this sub for advice on finding a lightweight, polyglot-ready orchestrator for a Docker-based MVP ([original post](https://www.reddit.com/r/mlops/comments/1qbte3c/seeking_a_lightweight_orchestrator_for_docker/)). I wanted to avoid the 1GB+ RAM footprint of Airflow while keeping observability. \n\nI finally finished the benchmarks.\n\n**The TL;DR:**\n\n* **Airflow/Kestra:** Both demand 1GB+ just to sit idle.\n* **Cronicle:** The winner my use case. 50MB RAM but gives you a full UI and audit trail.\n* **Ofelia:** The minimalist king at <10MB. Hard to audit.\n\n[A breakdown of the memory â€˜entry feeâ€™ for each orchestrator.](https://preview.redd.it/4ssqks7qr3gg1.png?width=722&format=png&auto=webp&s=61531c5bbdc6b44817171a99af2cfa50c816cf2e)\n\nI documented the full methodology, the Python/Docker setup, and the raw CSV data in this write-up: [Orchestration Without the Bloat: Benchmarking 6 Lightweight Alternatives to Airflow](https://mgijon94.medium.com/orchestration-without-the-bloat-benchmarking-6-lightweight-alternatives-to-airflow-c68413ba699c)\n\nThe whole code can be found here: [Github repo](https://github.com/MGijon/Posts/tree/main/ETL-scheduler-docker-compose)\n\nMassive thanks to everyone here who suggested I look into the 'job-centric' model. It saved my MVP's infrastructure budget!\n\n",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1qpdar2/update_benchmarking_the_airflow_tax_i_tested_6/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qp2mq3",
      "title": "Machine learning Interview",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qp2mq3/machine_learning_interview/",
      "author": "jfhurtado89",
      "created_utc": "2026-01-28 05:29:56",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "I have a ML interview coming up and these are the types of asking.\n\nTechnical / Roleâ€‘Specific Questions (20 minutes):\n\nWeâ€™ll cover topics such as ML modeling, MLOps (deployment), system design, algorithms, GenAI, infrastructure & tooling, and commonly used frameworks.\n\nLive Coding Interview (30 minutes):\n\nA Google Collab notebook will be shared at the start of the interview. Youâ€™ll be asked to share your screenwhile completing the exercises.\n\nCoding will focus on ML algorithms and implementations, transformerâ€‘based GenAI concepts, debugging, and troubleshootingâ€”not LeetCodeâ€‘style problems.\n\nAdditional Note:\n\nYou will have full access to the internet and LLMs during the interview.\n\nWhat do you guys think, I should focus on the live coding part knowing that Iâ€™ll have access to llms?\n\nI do have practical experience in deployment, works as a data scientist and finishing a masters in computer science in Georgia tech.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qp2mq3/machine_learning_interview/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o279zl6",
          "author": "denim_duck",
          "text": "Theyâ€™ll probably give you a dataset, have you clean it and predict on it live. Just be ready to talk through first-year ML basics like train/test/validate, bias/variance, feature engineering, auc/roc",
          "score": 3,
          "created_utc": "2026-01-28 12:19:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o273o1e",
          "author": "DenseUsual5732",
          "text": "What role specifically are you interviewing for",
          "score": 1,
          "created_utc": "2026-01-28 11:33:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2757lj",
              "author": "jfhurtado89",
              "text": "Is for a Machine learning Engineer role",
              "score": 2,
              "created_utc": "2026-01-28 11:45:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpbrpu",
      "title": "MLflow Full Course (MLOps + LLMOps) for beginners| End-to-End Experiments, Tracking & Deployment",
      "subreddit": "mlops",
      "url": "https://youtu.be/_Ox2Ft0xumE",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-01-28 13:46:15",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qpbrpu/mlflow_full_course_mlops_llmops_for_beginners/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qqezfl",
      "title": "Ontologies, Context Graphs, and Semantic Layers: What AI Actually Needs in 2026",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/ontologies-context-graphs-and-semantic",
      "author": "Berserk_l_",
      "created_utc": "2026-01-29 17:28:12",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qqezfl/ontologies_context_graphs_and_semantic_layers/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qri84h",
      "title": "MLOps for LLM prompts - versioning, testing, portability",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qri84h/mlops_for_llm_prompts_versioning_testing/",
      "author": "gogeta1202",
      "created_utc": "2026-01-30 21:13:29",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "MLOps has mature tooling for models. What about prompts?\n\nTraditional MLOps:  \nâ€¢ Model versioning âœ“  \nâ€¢ Experiment tracking âœ“  \nâ€¢ A/B testing âœ“  \nâ€¢ Rollback âœ“\n\nPrompt management:  \nâ€¢ Versioning: Git?  \nâ€¢ Testing: Manual?  \nâ€¢ A/B across providers: Rebuild everything?  \nâ€¢ Rollback: Hope you saved it?\n\nWhat I built with MLOps principles:\n\nVersioning:  \nâ€¢ Checkpoint system for prompt states  \nâ€¢ SHA256 integrity verification  \nâ€¢ Version history tracking\n\nTesting:  \nâ€¢ Quality validation using embeddings  \nâ€¢ 9 metrics per conversion  \nâ€¢ Round-trip validation (Aâ†’Bâ†’A)\n\nPortability:  \nâ€¢ Convert between OpenAI â†” Anthropic  \nâ€¢ Fidelity scoring  \nâ€¢ Configurable quality thresholds\n\nRollback:  \nâ€¢ One-click restore to previous checkpoint  \nâ€¢ Backup with compression  \nâ€¢ Restore original if needed\n\nQuestions for MLOps practitioners:\n\n1. How do you version prompts today?\n2. What's your testing strategy for LLM outputs?\n3. Would prompt portability fit your pipeline?\n4. What integrations needed? (MLflow? Airflow?)\n\nLooking for MLOps engineers to validate this direction.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qri84h/mlops_for_llm_prompts_versioning_testing/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2oomm5",
          "author": "alexlag64",
          "text": "MLFlow offers a prompt registry and a LLM evaluation framework that works pretty good for our datascience team at our company.\nEasy to load prompts into our workflows using MLFlowâ€™s API, easy to compare the outputs of the LLM using two different prompts versions on the same dataset, I havenâ€™t really looked other solutions since MLFlow works so well for us.",
          "score": 3,
          "created_utc": "2026-01-30 22:07:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqeo01",
      "title": "Feast now supports OpenLineage (and dbt imports)!",
      "subreddit": "mlops",
      "url": "https://feast.dev/blog/feast-openlineage-integration/",
      "author": "chaosengineeringdev",
      "created_utc": "2026-01-29 17:16:49",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qqeo01/feast_now_supports_openlineage_and_dbt_imports/",
      "domain": "feast.dev",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qn2ho1",
      "title": "continuous debugging for long running training jobs?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qn2ho1/continuous_debugging_for_long_running_training/",
      "author": "tensorpool_tycho",
      "created_utc": "2026-01-26 01:47:45",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "Are there any OSS agentic tools for debugging long running training jobs? Particularly Xid errors, OOMs, or other errors that pop up deep into training. \n\nor has anyone built tools out in house for this? curious what peoples' experiences have been.",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qn2ho1/continuous_debugging_for_long_running_training/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1qp7a4",
          "author": "tensorpool_tycho",
          "text": "might just build this one myself but am curious if something exists alr. tbh if i cant debug an infra issue and i feed my whole context into claude, it usually gets it first or second try",
          "score": 1,
          "created_utc": "2026-01-26 02:05:59",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1rxi0u",
          "author": "flyingPizza456",
          "text": "What do you mean by long running jobs? So you mean debugging during training? This is more a question of monitoring. Tensorboard, Mlflow etc. do help here.\n\nAnd why does ist need to be agentic? Feels like a buzzy question without more context.",
          "score": 1,
          "created_utc": "2026-01-26 06:40:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ryf2h",
              "author": "tensorpool_tycho",
              "text": "Sorry ur right in retrospect that was kinda vague lol. I moreso mean if a run crashes from a Xid error, or an OOM issue, or something like that late into a training run. Feel like there have been a ton of times a job will crash and then my compute is just idle before I have to manually fix it in the morning",
              "score": 1,
              "created_utc": "2026-01-26 06:47:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1rygln",
              "author": "tensorpool_tycho",
              "text": "Gonna update my post",
              "score": 1,
              "created_utc": "2026-01-26 06:47:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zlmgl",
          "author": "Glad_Appearance_8190",
          "text": "havent seen a clean OSS silver bullet tbh. most teams i know end up stitching together logs, metrics, and checkpoints so you can rewind what state the job was actually in when it died. xid and oom stuff is usually more about visibility than fixing the error itself. if you cant trace what changed over time, debugging turns into guessing real fast.",
          "score": 1,
          "created_utc": "2026-01-27 09:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o256ssq",
              "author": "tensorpool_tycho",
              "text": "Hmm thatâ€™s interesting - so itâ€™s really a problem about proper observability? Iâ€™ve been thinking of how to holistically attack this so I can spend less time debugging training runs for customers. \n\nIâ€™m thinking rn Iâ€™ll properly track logs, metrics, etc, and just give CC access to my k8s cluster and have it go ham. Thoughts?",
              "score": 1,
              "created_utc": "2026-01-28 02:51:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2sl617",
                  "author": "Glad_Appearance_8190",
                  "text": "ohhh, that tracks. Most of the pain is not the crash itself but not knowing what changed before it happened. Solid observability cuts the guesswork way down. Letting CC correlate logs and metrics could help, as long as the signals are clean and consistent.",
                  "score": 1,
                  "created_utc": "2026-01-31 14:29:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22wu8t",
          "author": "traceml-ai",
          "text": "I donâ€™t know of a solid OSS â€œagenticâ€ solution for this yet.\n\nBut I have been working on a lightweight tool for continuous observability of long-running training jobs, mostly focused on surfacing ground-truth signals over time (step time drift, worst-rank vs median in DDP, memory evolution, dataloader stalls). \n\nIf this problem is something you are actively dealing with, happy to chat. I am still learning what signals actually matter in real systems.",
          "score": 1,
          "created_utc": "2026-01-27 20:09:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrh2ns",
      "title": "Streaming feature transformations",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qrh2ns/streaming_feature_transformations/",
      "author": "Spirited-Bit9693",
      "created_utc": "2026-01-30 20:30:18",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "What are the popular approaches to do feature transformations on streaming data?\n\nRequirements:\n\nLow latency computations on data from kafka streams\n\npopulate the computed features in online feature store",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1qrh2ns/streaming_feature_transformations/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2o9q9h",
          "author": "Scared_Astronaut9377",
          "text": "What kind of transformations are we talking about? Just extract things from one kafka message, apply a function, put in store? Or like \"use the kafka stream to keep the count of user actions during the current activity session\"? Very different requirements. \n\nAnd what is your existing stack?",
          "score": 1,
          "created_utc": "2026-01-30 20:56:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oa4sa",
              "author": "Spirited-Bit9693",
              "text": "We currently only have a batch framework. We need both : applying simple functions and also stateful transformations",
              "score": 1,
              "created_utc": "2026-01-30 20:58:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ohbwc",
                  "author": "Scared_Astronaut9377",
                  "text": "Apache flink or similar streaming engines.",
                  "score": 3,
                  "created_utc": "2026-01-30 21:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oa7jp",
          "author": "Spirited-Bit9693",
          "text": "We primarily use spark to compute features",
          "score": 1,
          "created_utc": "2026-01-30 20:58:51",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsn0d7",
      "title": "Non sucking, easy tool to convert websites to LLM ready data, Mojo",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qsn0d7/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "author": "malvads",
      "created_utc": "2026-02-01 03:16:12",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey all! After running into *only paid tools or overly complicated setups* for turning web pages into structured data for LLMs, I built **Mojo,** a **simple, free, open-source tool** that does exactly that. Itâ€™s designed to be easy to use and integrate into real workflows.\n\nIf youâ€™ve ever needed to prepare site content for an AI workflow without shelling out for paid services or wrestling with complex scrapers, this might help. Would love feedback, issues, contributions, use cases, etc. <3\n\n[https://github.com/malvads/mojo](https://github.com/malvads/mojo) (and it's MIT licensed)\n\n*Cheers!*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qsn0d7/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qshhhd",
      "title": "Deployed an ML Model on GCP with Full CI/CD Automation (Cloud Run + GitHub Actions)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qshhhd/deployed_an_ml_model_on_gcp_with_full_cicd/",
      "author": "gringobrsa",
      "created_utc": "2026-01-31 23:14:50",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.83,
      "text": "# Hey folks\n\nI just published Part 2 of a tutorial showing how to deploy an ML model on GCP using Cloud Run and then evolve it from manual deployment to full CI/CD automation with GitHub Actions.\n\nOnce set up, deployment is as simple as:\n\n    git tag v1.1.0\n    git push origin v1.1.0\n\nFull post:  \n[https://medium.com/@rasvihostings/deploy-your-ml-model-on-gc-part-2-evolving-from-manual-deployments-to-ci-cd-399b0843c582](https://medium.com/@rasvihostings/deploy-your-ml-model-on-gc-part-2-evolving-from-manual-deployments-to-ci-cd-399b0843c582)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qshhhd/deployed_an_ml_model_on_gcp_with_full_cicd/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qphftl",
      "title": "At what point does inference latency become a deal-breaker for you?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qphftl/at_what_point_does_inference_latency_become_a/",
      "author": "Good-Listen1276",
      "created_utc": "2026-01-28 17:15:19",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI keep hearing about inference \"acceleration,\" but Iâ€™m seeing teams choose smaller, dumber models (SLMs) just to keep the UX snappy.\n\nI want to know: have you ever had to kill a feature because it was too slow to be profitable? I'm gathering insights on three specific \"pain points\" for research:\n\n1. If an agent takes 15 internal \"thought\" steps, and each takes 1.5s, thatâ€™s a 22-second wait. Does your churn spike at 5s? 10s? Or do your users actually wait?\n2. How much time does your team waste trying to refactor layers (like moving PyTorch â†’ TensorRT) only to have the accuracy drop or the conversion fail?\n3. Are you stuck paying for H100s because cheaper hardware (L4s/T4s) just can't hit the TTFT (Time to First Token) you need?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qphftl/at_what_point_does_inference_latency_become_a/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qr5fye",
      "title": "UPDATE: sklearn-diagnose now has an Interactive Chatbot!",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qr5fye/update_sklearndiagnose_now_has_an_interactive/",
      "author": "lc19-",
      "created_utc": "2026-01-30 13:27:44",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm excited to share a major update to sklearn-diagnose - the open-source Python library that acts as an \"MRI scanner\" for your ML models (https://www.reddit.com/r/mlops/s/3HKkXzMbxZ)\n\nWhen I first released sklearn-diagnose, users could generate diagnostic reports to understand why their models were failing. But I kept thinking - what if you could talk to your diagnosis? What if you could ask follow-up questions and drill down into specific issues?\n\nNow you can! ðŸš€\n\nðŸ†• What's New: Interactive Diagnostic Chatbot\n\nInstead of just receiving a static report, you can now launch a local chatbot web app to have back-and-forth conversations with an LLM about your model's diagnostic results:\n\nðŸ’¬ Conversational Diagnosis - Ask questions like \"Why is my model overfitting?\" or \"How do I implement your first recommendation?\"\n\nðŸ” Full Context Awareness - The chatbot has complete knowledge of your hypotheses, recommendations, and model signals\n\nðŸ“ Code Examples On-Demand - Request specific implementation guidance and get tailored code snippets\n\nðŸ§  Conversation Memory - Build on previous questions within your session for deeper exploration\n\nðŸ–¥ï¸ React App for Frontend - Modern, responsive interface that runs locally in your browser\n\nGitHub: https://github.com/leockl/sklearn-diagnose\n\nPlease give my GitHub repo a star if this was helpful â­",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qr5fye/update_sklearndiagnose_now_has_an_interactive/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qqpjx6",
      "title": "A Practical Framework for Designing AI Agent Systems (With Real Production Examples)",
      "subreddit": "mlops",
      "url": "https://youtu.be/CMMlLB01rcE",
      "author": "OnlyProggingForFun",
      "created_utc": "2026-01-30 00:02:07",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qqpjx6/a_practical_framework_for_designing_ai_agent/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qq8j9r",
      "title": "MLOPs jobs",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qq8j9r/mlops_jobs/",
      "author": "Competitive-Fact-313",
      "created_utc": "2026-01-29 13:28:03",
      "score": 2,
      "num_comments": 8,
      "upvote_ratio": 0.58,
      "text": "Brutally honest! Whatâ€™s the bare minimum to get into mlops straightaway. \n\nPlease consider following in order to answer\n\n1. Bachelor degree? \n\n2. MSc degree?\n\n3. Certs? \n\n4. Experience? \n\nI heard people say that you need this or that many year of experience before getting into MLOPs. I mean come on if one has 10+year of experience but no ml tools exposed then he has to work but one exposed themselves to mlops n work for 3-4 year along with some infra tools is well qualified for mlops? \n\nNote: if I have 10+ experience in ml or mlops i would rather contest for CTO lol! ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qq8j9r/mlops_jobs/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2epsyi",
          "author": "ProcessIndependent38",
          "text": "â€œI heard people say that you need this or that many year of experience before getting into MLOPs. I mean come on if one has 10+year of experience but no ml tools exposed then he has to work but one exposed themselves to mlops n work for 3-4 year along with some infra tools is well qualified for mlops?â€\n\nWhat?\n\nYou need a few years of experience in any ops role.",
          "score": 7,
          "created_utc": "2026-01-29 13:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eq2cj",
              "author": "Competitive-Fact-313",
              "text": "Thanks for clarifying that Mlops is not a rocket science.",
              "score": -6,
              "created_utc": "2026-01-29 13:40:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lslu0",
          "author": "Affectionate-Heron90",
          "text": "I'd weigh more on actual ops experience. I've seen DevOps folks easily transition to MLOps role.\nAlso, its easier to get into some ML projects within your current company , gain experience and then look for another role. Vs. having no experience andÂ  trying to switch career.Â \n\n\nIf you cant in your current company, start actively building / contributing in OSS tools.Â ",
          "score": 3,
          "created_utc": "2026-01-30 14:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eso6u",
          "author": "Vpharrish",
          "text": "What's your qualifications? Imo, MLOps itself is looked as more of a SWE people integrating ML (insights from my professors and people) rather than something that one would dedicate and work through.",
          "score": 2,
          "created_utc": "2026-01-29 13:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eu8qg",
              "author": "Competitive-Fact-313",
              "text": "I work as ml platform guy, got few years of ml experience and now integrating infra but I see a lot of stupidity in and around the JD ? I mean shouldnâ€™t people hire someone with some ml Background with some infra hands on ? The same people who develop software now simply moving to ml fields doesnâ€™t make much sense than someone who actually studies ML in university and got some hands on",
              "score": 1,
              "created_utc": "2026-01-29 14:02:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2g875u",
                  "author": "eemamedo",
                  "text": "Are you working full-time as ml infra or internship? Your ML infra experience is exactly what MLOps is at many companies. Yes, you might be missing some â€œoperationalâ€ stuff like helping build custom solutions for DS to connect infra with their workloads but honestly, itâ€™s not that impossible to gain.Â ",
                  "score": 3,
                  "created_utc": "2026-01-29 17:53:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2q8uyf",
          "author": "MathmoKiwi",
          "text": "> Brutally honest! Whatâ€™s the bare minimum to get into mlops straightaway.\n\n\"All of the above\"",
          "score": 2,
          "created_utc": "2026-01-31 03:22:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}