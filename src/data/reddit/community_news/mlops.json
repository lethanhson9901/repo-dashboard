{
  "metadata": {
    "last_updated": "2026-02-02 02:59:57",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 19,
    "total_comments": 34,
    "file_size_bytes": 62979
  },
  "items": [
    {
      "id": "1qnlepn",
      "title": "Static model selection did not work (enough) for us",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qnlepn/static_model_selection_did_not_work_enough_for_us/",
      "author": "tech2biz",
      "created_utc": "2026-01-26 16:49:11",
      "score": 68,
      "num_comments": 17,
      "upvote_ratio": 0.97,
      "text": "We spent a few months now on a solution for dynamic model routing because we tried several things and nothing really solved our problem.\n\nThe core issue / our background: we deployed nodes with SLM and RAG to regulated industry teams (the problem is relevant in any setup though). But users couldn't figure out when to use which model (despite ongoing effort to educate). We tried static routing but the classification of queries upfront didn't really work as it was very unpredictable what the users were doing. Also the \"guessing\" part did not feel right, we iterated really a lot on this. So next we thought hybrid with big models would be the solution but somewhat similar we always had to estimate complexity before we saw output. The estimates missed often enough that we either overspent (like, radically, breaking our unit economics) or quality was bad from routing too aggressively to small models.\n\nWe found a Google publication (happy to share) that approaches this very differently, not routing but cascading. Start generating with the small model, validate quality as you go, escalate only if needed.\n\nWe developed this and open-sourced our implementation: [github.com/lemony-ai/cascadeflow](http://github.com/lemony-ai/cascadeflow)\n\nIt plugs into your existing infrastructure, works with LiteLLM, OpenRouter, n8n, LangChain, or direct API calls. From there you can use whatever models you want: OpenAI, Anthropic, Groq, HuggingFace, local models via Ollama, self-hosted via vLLM.\n\nNot replacing your router or orchestration layer, just adding quality validation that decides when the cheap models output is actually good enough.\n\nSeeing 40-90% cost reduction in first production workloads and we are honestly quite excited. Would love feedback and happy to chat with others working on inference layers.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qnlepn/static_model_selection_did_not_work_enough_for_us/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1unm6l",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 17:10:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uo88c",
              "author": "tech2biz",
              "text": "Totally fair concern. It's not another full LLM call. we use lightweight validation (confidence scores, completion checks, some heuristics depending on task type). Latency add is minimal, usually under 20ms. My co-founder Sascha can go deeper on the technical side if you want. But we are coming from the on-prem world, so latency was actually our initial focus (later figured it also reduces the costs alike).",
              "score": 5,
              "created_utc": "2026-01-26 17:13:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v9nns",
                  "author": "ediblescholarship",
                  "text": "Confidence scores are tricky though.small models are often confidently wrong. How do you calibrate the threshold? Or is it task-specific?",
                  "score": 1,
                  "created_utc": "2026-01-26 18:44:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1zi2jf",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-01-27 09:06:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1utlal",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 17:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uwqf7",
              "author": "tech2biz",
              "text": "Sure! This is the blog: [https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/](https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/)  \nAnd the research paper: [https://openreview.net/pdf?id=vo9t20wsmd](https://openreview.net/pdf?id=vo9t20wsmd)",
              "score": 3,
              "created_utc": "2026-01-26 17:50:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uz339",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 18:00:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v4w0f",
              "author": "tech2biz",
              "text": "Streaming works, we use it ourselves. We've ran various 100k benchmarking queries incl concurrent without issues. But also honestly, we are still learning at scale. Would love to hear if you're planning to push volume.",
              "score": 1,
              "created_utc": "2026-01-26 18:24:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uzqh7",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 18:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6tya",
              "author": "tech2biz",
              "text": "it wraps around your existing calls, you don't need to restructure your chains. Basically a few lines to initialize and then swap your calls. We have a LangChain example in the repo. If you hit any issues just open an issue or ping us, still early days so feedback on integration pain points is super useful.",
              "score": 1,
              "created_utc": "2026-01-26 18:33:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v10l0",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-26 18:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v5aod",
              "author": "tech2biz",
              "text": "awesome! Every feedback much appreciated. And for agentic, we are also working on clawdbot integration.",
              "score": 5,
              "created_utc": "2026-01-26 18:26:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uuhq4",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-26 17:40:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxbls",
              "author": "tech2biz",
              "text": "Nice to hear!!! 50% is real money. And yes, the edge cases are exactly it, our routers always either led to bad quality or overpaying, it always felt there is no middle ground.",
              "score": 1,
              "created_utc": "2026-01-26 17:53:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpf6f8",
      "title": "To the ML Engineers who didn’t take the \"standard\" path: What was the \"Aha!\" moment where it finally clicked?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qpf6f8/to_the_ml_engineers_who_didnt_take_the_standard/",
      "author": "Effective_Kale3359",
      "created_utc": "2026-01-28 15:56:32",
      "score": 34,
      "num_comments": 13,
      "upvote_ratio": 0.88,
      "text": "We’ve all seen the \"Master’s degree + 500 LeetCode problems\" roadmap, but I’m looking for the real, gritty stories.\n\n​If you transitioned from a college student to ML engineer or if you are self-taught:\n\n​The Bridge: What was the first project you built that actually felt \"industrial\" and not like a tutorial-hell toy?\n\n​The \"Lie\": What is one skill everyone told you was \"mandatory\" that you’ve literally never used in your daily job?\n\n​The Pivot: How did you convince your first employer to take a chance on an ML \"outsider\"?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qpf6f8/to_the_ml_engineers_who_didnt_take_the_standard/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o29kt7b",
          "author": "Scared_Astronaut9377",
          "text": "The real story behind any such recent anecdote will be \"got lucky to meet a hiring manager who had no clue and he hired me for something completely arbitrary\". Get all the skills and understanding in the world, you are going to be automatically filtered out for 98% of real positions.",
          "score": 22,
          "created_utc": "2026-01-28 18:54:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fv8gt",
              "author": "ummitluyum",
              "text": "You're right about HR filters, ATS are ruthless regarding the lack of a relevant degree, making the \"cold apply\" strategy dead on arrival for self-taught engineers\n\nThe only bypass is networking and Proof of Work. Open-source contributions, tech blogs, or hackathons. You need your resume to land on the Hiring Manager's desk bypassing the HR filter. The manager often doesn't give a damn about the diploma, they need tickets closed. HR needs to close the vacancy based on a checklist",
              "score": 2,
              "created_utc": "2026-01-29 16:55:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fuokd",
          "author": "ummitluyum",
          "text": "The Lie: that you need deep math and the ability to derive backpropagation on a whiteboard. In 99% of cases you'll be debugging YAML configs, fixing Docker containers, and optimizing inference, not inventing new loss architectures\n\nThe Bridge: a simple text classification API, but wrapped in Docker, with a CI/CD pipeline, monitoring (Prometheus/Grafana), and load testing. That's what separates a toy from industry\n\nThe Pivot: I showed that I could not just \"train a model\" but \"deliver a model to the user and keep it alive\". Business needs engineers, not scientists",
          "score": 7,
          "created_utc": "2026-01-29 16:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g17nl",
              "author": "an4k1nskyw4lk3r",
              "text": "That’s so true! I’ve been working as an AI/ML Engineer for almost 2 years and I never train an LM or NLP complex architectures from scratch. Everything is pre trained and fine tuned (that’s the truth). 99% of the time I’ve been working in yaml files, ci pipelines and (in my case, that my core actuation is NLP) have been working in a bridge between models and business rule… too much RAG and prompt. Too much redis for context engineering and so it is…",
              "score": 1,
              "created_utc": "2026-01-29 17:22:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2g1txz",
          "author": "an4k1nskyw4lk3r",
          "text": "I’ve been working as an AI/ML Engineer for almost 2 years and I never train an LM or NLP complex architectures from scratch. Everything is pre trained and fine tuned (that’s the truth). 99% of the time I’ve been working in yaml files, ci pipelines, containers, pods or whatever related to prod like environments and (in my case, that my core actuation is NLP) have been working in a bridge between models and business rule… too much RAG and prompt. Too much redis for context engineering and so it is…\n\nThe truth, FOR REAL, prepare to engineering (80%) and focus 20% of your time ~> neural networks, classic machine learning algorithms and how to retrain those ones.\n\nMost of interviews are about pure data science but in practice you will find more coding.",
          "score": 4,
          "created_utc": "2026-01-29 17:25:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cy03f",
          "author": "devilwithin305",
          "text": "RemindMe! 1 day",
          "score": 1,
          "created_utc": "2026-01-29 05:13:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iovtj",
          "author": "ImposterExperience",
          "text": "For me, at every job I always applied ML techniques regardless of my position and grew. Also consulting gigs helps for getting opportunities.",
          "score": 1,
          "created_utc": "2026-01-30 01:09:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oy9qd",
          "author": "No-Consequence-1779",
          "text": "If your a ha moment q is specifically‘understanding how an LLM produced an answer; most say understanding back propagation. \n\nIf it’s about useful products or services… it depends.  ",
          "score": 1,
          "created_utc": "2026-01-30 22:56:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sxkhz",
          "author": "yes-im-hiring-2025",
          "text": "TLDR: was good at coding but didn't know what interested me. Joined startup -> learnt that I'm a natural problem solver/leader, not necessarily only a coder -> joined FAANG after 7 years at the startup.\n\nDecent at academics but bored. Was going to be a researcher. One of my project profs in engineering Year 3 stressed that I try out startups before deciding on research.\n\nI interned at an analytics/ML startup that was in walking distance from my dorm (hostel), and continued the internship over the sem break. My manager insisted I sit with the full time employees and do some parts of their work with them.\n\nWithin a week I was running small scripts to clean and process data, write simple parsing logic etc. Googling -> doing. Had a lot of fun doing that. Super small team and a helpful manager made sure I was having fun in my internship and asked me to return back in year 4.\n\nMet the CEO. Decided to give the startup life a shot. A few years later I was interacting with the clients and attending meetings as a stand-in for the CTO, and it just clicked - I don't really care about ML; I care about building things that solve problems. ML just happened to be the tool I was most well versed in.\n\nLoved talking to people, looking and designing processes, cost computations, meeting clients, etc., and became a natural leader who knew how to bring the right people together to solve something, and how to solve a problem myself. I continued being good at coding, but it became less important in my last few years. Became the founding engineer and team lead, hired my own team and ran it like a mini startup for a few years.\n\nEventually I moved to FAANG after I had enough of the 0->1 phase. Enjoying it so far.\n\nCoding ability and passion is non negotiable. But if that's the only thing you bring to the table you're gonna get stuck being so good at your job that you can't be promoted or replaced.\n\nBuild systems, not just scripts. Become hard to replace because of how much of a force multiplier you are, but be smart enough to keep in touch with your core competencies so that if/when you need to change your employer you can choose to do so.",
          "score": 1,
          "created_utc": "2026-01-31 15:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xy7tf",
          "author": "Anti-Entropy-Life",
          "text": "Found LLMs on my own about a year ago, couldn't stop until I got down to CUDA and SASS. Now in love with them, realized SASS basically encodes the thermal envelope information I used to read invisibly as a world top 100 overclocker back in the day hah. I left tech entirely for 20 years because of Nvidia and other bad actors honestly. I just happened to return when LLMs tanked one of my other core businesses, so had to learn what they were in depth.\n\nNow, I love these things, but people forgot a lot of basic design and engineering principles. My life now revolves around cleaning up the mess and actually caring for LLM tech now and in the future in a very serious way. I founded a lab, got it funded, and things are pretty cool.\n\nHowever, I mostly want to tear my eyes out whenever I look at any \"AI news\" of any kind...\n\nIf sticking to the format:\n\n0. Self-taught 100%.\n\n1. I can't actually say as it is undergoing review for patents.\n2. I fortunately never got any outside education so never heard any lies, except from the LLMs themselves. Rather important to figure out how to get them not to do that.\n3. I have been self-employed my entire life, so the employer to be convinced was me, it therefore was pretty easy! :D",
          "score": 1,
          "created_utc": "2026-02-01 09:10:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28taas",
          "author": "Tough-Public-7206",
          "text": "RemindMe! 1 day",
          "score": 0,
          "created_utc": "2026-01-28 16:55:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29a7p0",
          "author": "JeanLuucGodard",
          "text": "RemindMe! 1 day",
          "score": 0,
          "created_utc": "2026-01-28 18:09:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29aeax",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-29 18:09:05 UTC**](http://www.wolframalpha.com/input/?i=2026-01-29%2018:09:05%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qpf6f8/to_the_ml_engineers_who_didnt_take_the_standard/o29a7p0/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qpf6f8%2Fto_the_ml_engineers_who_didnt_take_the_standard%2Fo29a7p0%2F%5D%0A%0ARemindMe%21%202026-01-29%2018%3A09%3A05%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qpf6f8)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-28 18:09:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnazmo",
      "title": "MLOps Roadmap",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qnazmo/mlops_roadmap/",
      "author": "Deep_Priority_2443",
      "created_utc": "2026-01-26 08:58:33",
      "score": 29,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "Hi there, if this is of help to you, [roadmap.sh](http://roadmap.sh) has just launched a revised version of its [MLOps roadmap](https://roadmap.sh/mlops). I want to thank the people in this group who contributed to the review of the roadmap with their feedback. \n\nhttps://preview.redd.it/kolchhwvrnfg1.png?width=1088&format=png&auto=webp&s=151207b5db9b37c170fdbf58c3f39d131a826d90\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qnazmo/mlops_roadmap/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1u68ej",
          "author": "Competitive-Fact-313",
          "text": "Adding Prometheus and Grafana as observability as a wise choice.",
          "score": 3,
          "created_utc": "2026-01-26 15:56:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq5sdm",
      "title": "Advice for those switching to MLOps/ML from other backgrounds: stick with one or two domains",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qq5sdm/advice_for_those_switching_to_mlopsml_from_other/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-29 11:11:59",
      "score": 18,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "If you're transitioning into MLOps or ML Engineering from a different background (DevOps, backend, etc.), here's something I've learned the hard way:\n\n**Pick one or two ML domains and go deep.**\n\nWhy?\n\n1. **Every company has their own unique pipeline and infra.** There's no universal \"MLOps stack\" that everyone uses. What works at one company looks completely different at another.\n2. **Interviews have changed.** People rarely ask general theory questions anymore. Instead, they dig into the details of *your* projects — what decisions you made, what tradeoffs you faced, how you solved specific problems.\n3. **Being a generalist dilutes your value.** Applying to 100 places with surface-level knowledge across everything is less effective than targeting roles that match your specific ML or business interest and becoming genuinely expert in that space.\n\n**What do I mean by \"domains\"?**\n\nThink: Computer Vision, NLP, Recommender Systems, Time Series/Forecasting, Speech/Audio, etc.\n\nFor example, if you pick CV, you learn common model architectures (CNNs, Vision Transformers), understand data pipelines (image preprocessing, augmentation), know deployment challenges (model size, latency, GPU serving), and build projects around it. Now, when you apply to companies doing CV work, you're not a generalist; you actually speak their language.\n\nAnd if you're coming from DevOps/infra like me, that's actually a **unique advantage**. Production infrastructure, scaling, reliability — these are the real problems ML teams are struggling with right now. Most ML folks can build models. Far fewer can deploy and operate them reliably.\n\nDon't undersell your background. Lean into it.\n\nI've helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here: [topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qq5sdm/advice_for_those_switching_to_mlopsml_from_other/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2ko0cb",
          "author": "Extension_Key_5970",
          "text": "sharing my detailed blog as well [https://medium.com/p/58878bb1cd64](https://medium.com/p/58878bb1cd64), if any one is interested",
          "score": 2,
          "created_utc": "2026-01-30 09:19:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq23vz",
      "title": "Iceberg REST Catalog Alternatives: Top Options & How to Choose The Best One For Your Team",
      "subreddit": "mlops",
      "url": "https://lakefs.io/blog/iceberg-rest-catalog-alternatives/",
      "author": "Comfortable-Site8626",
      "created_utc": "2026-01-29 07:31:51",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qq23vz/iceberg_rest_catalog_alternatives_top_options_how/",
      "domain": "lakefs.io",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qpbrpu",
      "title": "MLflow Full Course (MLOps + LLMOps) for beginners| End-to-End Experiments, Tracking & Deployment",
      "subreddit": "mlops",
      "url": "https://youtu.be/_Ox2Ft0xumE",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-01-28 13:46:15",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qpbrpu/mlflow_full_course_mlops_llmops_for_beginners/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qt9i0j",
      "title": "Can someone explain MLOps steps and infrastructure setup? Feeling lost",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qt9i0j/can_someone_explain_mlops_steps_and/",
      "author": "FreshIntroduction120",
      "created_utc": "2026-02-01 20:18:16",
      "score": 10,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey folks,\n\nI'm trying to wrap my head around MLOps and honestly feeling a bit overwhelmed with all the different info out there.\n\nWould love to hear from people who actually work with this stuff - what are the main steps you go through in an MLOps pipeline? Like from when you start building a model to getting it running in production and keeping it alive?\n\nAlso, how do you even set up the infrastructure for this? What tools do you use and how does it all connect together?\n\nI've been reading articles but they all seem kinda high-level or vendor-specific. Just want to understand how this works in the real world.\n\nAny advice or pointers would be awesome, thanks!",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qt9i0j/can_someone_explain_mlops_steps_and/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o31clz9",
          "author": "HahaHarmonica",
          "text": "Think of MLOps as just DevOps+Data. \n\nPeople write code to train models on some data. MLOps is the automation of that training to show data lineage of what data was used to train the models. Making the model accessible (e.g. hosting the inferencing), and then monitoring said model. Adjust code, rinse repeat.",
          "score": 3,
          "created_utc": "2026-02-01 20:51:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31d7gq",
              "author": "EviliestBuckle",
              "text": "Can you please point to some structured course plz",
              "score": 2,
              "created_utc": "2026-02-01 20:54:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o323cym",
          "author": "MyBossIsOnReddit",
          "text": "Won't really straight up answer your question but MLOps is broad and needs pervasive support in tooling, storytelling and permissions. For that you need buy-in with management and other teams. So it's a lot of pitching and powerpointing until you get there.\n\nThen comes a bit of architecture and system design. Any given design won't meet every requirement but you don't need to roll out everything at once. So work iteratively. Try to close the model lifecycle loop (the etl-train-eval-deploy-monitor-retrain) first. Then add the other features depending on need. The problem is the quality of the platform largely depends on the sum of the parts. It will be painful before it gets better.\n\nYou also need to figure out what you need and where it lives. Models, api keys, networking, dashboards, storage, vector databases, infra as code, ci/cd, git? Infra as code, templating, containers, what metrics do you track? p99, latency?\n\nYou can tell the biggest pain point for me has always been getting everyone on the same page. None of the folks I work with get it and think they can either do it themselves over the weekend or that it's complex and we need another 4 consultants.\n\nPersonally I've always used some Infra as code (CDK, terraform), some form of managed services for serving (no way you're going to handle everything without), mlflow, tensorboard. Start with terraform and build infra and templates first. I've had good experiences with AWS sagemaker and Bedrock, and GCP's Vertex.",
          "score": 2,
          "created_utc": "2026-02-01 23:05:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32vesj",
          "author": "Competitive-Fact-313",
          "text": "MLOps in the real world is basically about taking a model from a notebook to something that can run reliably like normal software. At a high level, you clean and prepare data in a repeatable way, train models using scripts while tracking experiments and artifacts with tools like MLflow, package the final model into a service (usually FastAPI) and a Docker image, and then deploy that image on infrastructure like Kubernetes (for example EKS) so it can scale and stay up. CI/CD tools such as GitHub Actions are mainly used to test code, build and push images (to ECR), and trigger deployments, not to train large models directly heavy training usually runs on dedicated compute (EKS jobs, Batch, SageMaker, etc.) and just reports results back to MLflow. The key idea is separating concerns: training is compute-heavy and controlled, deployment is automated, and monitoring closes the loop so you know when to retrain or update the model.",
          "score": 1,
          "created_utc": "2026-02-02 01:41:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpdar2",
      "title": "[Update] Benchmarking the \"Airflow Tax\": I tested 6 lightweight orchestrators so you don't have to.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qpdar2/update_benchmarking_the_airflow_tax_i_tested_6/",
      "author": "m_gijon",
      "created_utc": "2026-01-28 14:46:28",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "Last week, I asked this sub for advice on finding a lightweight, polyglot-ready orchestrator for a Docker-based MVP ([original post](https://www.reddit.com/r/mlops/comments/1qbte3c/seeking_a_lightweight_orchestrator_for_docker/)). I wanted to avoid the 1GB+ RAM footprint of Airflow while keeping observability. \n\nI finally finished the benchmarks.\n\n**The TL;DR:**\n\n* **Airflow/Kestra:** Both demand 1GB+ just to sit idle.\n* **Cronicle:** The winner my use case. 50MB RAM but gives you a full UI and audit trail.\n* **Ofelia:** The minimalist king at <10MB. Hard to audit.\n\n[A breakdown of the memory ‘entry fee’ for each orchestrator.](https://preview.redd.it/4ssqks7qr3gg1.png?width=722&format=png&auto=webp&s=61531c5bbdc6b44817171a99af2cfa50c816cf2e)\n\nI documented the full methodology, the Python/Docker setup, and the raw CSV data in this write-up: [Orchestration Without the Bloat: Benchmarking 6 Lightweight Alternatives to Airflow](https://mgijon94.medium.com/orchestration-without-the-bloat-benchmarking-6-lightweight-alternatives-to-airflow-c68413ba699c)\n\nThe whole code can be found here: [Github repo](https://github.com/MGijon/Posts/tree/main/ETL-scheduler-docker-compose)\n\nMassive thanks to everyone here who suggested I look into the 'job-centric' model. It saved my MVP's infrastructure budget!\n\n",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1qpdar2/update_benchmarking_the_airflow_tax_i_tested_6/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qp2mq3",
      "title": "Machine learning Interview",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qp2mq3/machine_learning_interview/",
      "author": "jfhurtado89",
      "created_utc": "2026-01-28 05:29:56",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.85,
      "text": "I have a ML interview coming up and these are the types of asking.\n\nTechnical / Role‑Specific Questions (20 minutes):\n\nWe’ll cover topics such as ML modeling, MLOps (deployment), system design, algorithms, GenAI, infrastructure & tooling, and commonly used frameworks.\n\nLive Coding Interview (30 minutes):\n\nA Google Collab notebook will be shared at the start of the interview. You’ll be asked to share your screenwhile completing the exercises.\n\nCoding will focus on ML algorithms and implementations, transformer‑based GenAI concepts, debugging, and troubleshooting—not LeetCode‑style problems.\n\nAdditional Note:\n\nYou will have full access to the internet and LLMs during the interview.\n\nWhat do you guys think, I should focus on the live coding part knowing that I’ll have access to llms?\n\nI do have practical experience in deployment, works as a data scientist and finishing a masters in computer science in Georgia tech.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qp2mq3/machine_learning_interview/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o279zl6",
          "author": "denim_duck",
          "text": "They’ll probably give you a dataset, have you clean it and predict on it live. Just be ready to talk through first-year ML basics like train/test/validate, bias/variance, feature engineering, auc/roc",
          "score": 3,
          "created_utc": "2026-01-28 12:19:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o273o1e",
          "author": "DenseUsual5732",
          "text": "What role specifically are you interviewing for",
          "score": 1,
          "created_utc": "2026-01-28 11:33:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2757lj",
              "author": "jfhurtado89",
              "text": "Is for a Machine learning Engineer role",
              "score": 2,
              "created_utc": "2026-01-28 11:45:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qri84h",
      "title": "MLOps for LLM prompts - versioning, testing, portability",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qri84h/mlops_for_llm_prompts_versioning_testing/",
      "author": "gogeta1202",
      "created_utc": "2026-01-30 21:13:29",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "MLOps has mature tooling for models. What about prompts?\n\nTraditional MLOps:  \n• Model versioning ✓  \n• Experiment tracking ✓  \n• A/B testing ✓  \n• Rollback ✓\n\nPrompt management:  \n• Versioning: Git?  \n• Testing: Manual?  \n• A/B across providers: Rebuild everything?  \n• Rollback: Hope you saved it?\n\nWhat I built with MLOps principles:\n\nVersioning:  \n• Checkpoint system for prompt states  \n• SHA256 integrity verification  \n• Version history tracking\n\nTesting:  \n• Quality validation using embeddings  \n• 9 metrics per conversion  \n• Round-trip validation (A→B→A)\n\nPortability:  \n• Convert between OpenAI ↔ Anthropic  \n• Fidelity scoring  \n• Configurable quality thresholds\n\nRollback:  \n• One-click restore to previous checkpoint  \n• Backup with compression  \n• Restore original if needed\n\nQuestions for MLOps practitioners:\n\n1. How do you version prompts today?\n2. What's your testing strategy for LLM outputs?\n3. Would prompt portability fit your pipeline?\n4. What integrations needed? (MLflow? Airflow?)\n\nLooking for MLOps engineers to validate this direction.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qri84h/mlops_for_llm_prompts_versioning_testing/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2oomm5",
          "author": "alexlag64",
          "text": "MLFlow offers a prompt registry and a LLM evaluation framework that works pretty good for our datascience team at our company.\nEasy to load prompts into our workflows using MLFlow’s API, easy to compare the outputs of the LLM using two different prompts versions on the same dataset, I haven’t really looked other solutions since MLFlow works so well for us.",
          "score": 4,
          "created_utc": "2026-01-30 22:07:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xx8dz",
          "author": "Anti-Entropy-Life",
          "text": "I have docs from my lab titled \"Qualitative Prompt Engineering\" with a sub-domain of \"Prompt Discipline\" where functions of various prompts as taxonomically categorized.\n\nWould something like this be useful info to anyone else?",
          "score": 2,
          "created_utc": "2026-02-01 09:01:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30jwes",
              "author": "leveragecubed",
              "text": "Defiantly useful.",
              "score": 2,
              "created_utc": "2026-02-01 18:35:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o329898",
              "author": "gogeta1202",
              "text": "You're hitting on a real gap in the market. We've got tons of tools for latency and cost, but almost nothing for prompt discipline or taxonomy. That's actually the main blocker I'm seeing with multi-model reliability without a shared language for what prompts actually do, moving between models becomes a guessing game.\n\nI'm working on a conversion layer that maps prompts across providers using that kind of framework. Would be curious to see your taxonomy, especially how you handle reasoning granularity vs. output constraints. If you're open to it, I'd love to explore baking some of these principles into the eval loops I'm building.",
              "score": 2,
              "created_utc": "2026-02-01 23:37:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qshhhd",
      "title": "Deployed an ML Model on GCP with Full CI/CD Automation (Cloud Run + GitHub Actions)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qshhhd/deployed_an_ml_model_on_gcp_with_full_cicd/",
      "author": "gringobrsa",
      "created_utc": "2026-01-31 23:14:50",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "# Hey folks\n\nI just published Part 2 of a tutorial showing how to deploy an ML model on GCP using Cloud Run and then evolve it from manual deployment to full CI/CD automation with GitHub Actions.\n\nOnce set up, deployment is as simple as:\n\n    git tag v1.1.0\n    git push origin v1.1.0\n\nFull post:  \n[https://medium.com/@rasvihostings/deploy-your-ml-model-on-gc-part-2-evolving-from-manual-deployments-to-ci-cd-399b0843c582](https://medium.com/@rasvihostings/deploy-your-ml-model-on-gc-part-2-evolving-from-manual-deployments-to-ci-cd-399b0843c582)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qshhhd/deployed_an_ml_model_on_gcp_with_full_cicd/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qqezfl",
      "title": "Ontologies, Context Graphs, and Semantic Layers: What AI Actually Needs in 2026",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/ontologies-context-graphs-and-semantic",
      "author": "Berserk_l_",
      "created_utc": "2026-01-29 17:28:12",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qqezfl/ontologies_context_graphs_and_semantic_layers/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qqeo01",
      "title": "Feast now supports OpenLineage (and dbt imports)!",
      "subreddit": "mlops",
      "url": "https://feast.dev/blog/feast-openlineage-integration/",
      "author": "chaosengineeringdev",
      "created_utc": "2026-01-29 17:16:49",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qqeo01/feast_now_supports_openlineage_and_dbt_imports/",
      "domain": "feast.dev",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qsn0d7",
      "title": "Non sucking, easy tool to convert websites to LLM ready data, Mojo",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qsn0d7/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "author": "malvads",
      "created_utc": "2026-02-01 03:16:12",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.86,
      "text": "Hey all! After running into *only paid tools or overly complicated setups* for turning web pages into structured data for LLMs, I built **Mojo,** a **simple, free, open-source tool** that does exactly that. It’s designed to be easy to use and integrate into real workflows.\n\nIf you’ve ever needed to prepare site content for an AI workflow without shelling out for paid services or wrestling with complex scrapers, this might help. Would love feedback, issues, contributions, use cases, etc. <3\n\n[https://github.com/malvads/mojo](https://github.com/malvads/mojo) (and it's MIT licensed)\n\n*Cheers!*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qsn0d7/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2xzawh",
          "author": "Anti-Entropy-Life",
          "text": "This looks genuinely useful, especially the focus on being simple, free, and MIT licensed without trying to be a full “AI platform\" = super cool :D\n\nWhat I particularly like is that it tackles a very real and annoying part of the stack: getting web content into a reasonably clean, LLM-ready form without paying per page or maintaining a complex scraping pipeline. That’s a big win for early experiments, internal tools, and small teams.\n\nA few questions that would help me understand how far this can go in real workflows:\n\n• Is the extraction deterministic, meaning the same page always produces the same output?  \n  \n• How do you think about drift and updates, for example re-ingesting pages that change over time?  \n  \n• When things go wrong, like odd markup, partial loads, or missing content, where is the best place to debug? Raw HTML, Mojo’s intermediate output, or the final chunks?\n\nOne thing that could make this even stronger is being explicit about failure modes and contracts in the README. In other words, what Mojo guarantees versus what it intentionally does not. Even a short section in the README about what Mojo will not handle would build a lot of trust.\n\nThanks for building and open-sourcing this, though! Tools that remove friction without over-claiming are rare and genuinely appreciated! :D",
          "score": 1,
          "created_utc": "2026-02-01 09:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ydh60",
              "author": "malvads",
              "text": "Hi, thanks for your question. My idea with Mojo is to provide fast conversion between pages and LLM data.\n\nAnswering your questions:\n\n**• Is the extraction deterministic, meaning the same page always produces the same output?**  \n→ The same HTML provides the same output. Right now there are two options to get data: with the `--render` flag and without it.\n\nThe first one uses pure HTTP requests (ideal for static web pages). With the `--render` flag, it connects to Chrome using CDP (so no extra dependencies are downloaded, just your existing setup). -- This is still not released, planned for 0.1.0, but you can build Mojo and test this feature\n\nSo it depends on the setup you are using and how the web page is programmed.\n\n**• How do you think about drift and updates, for example re-ingesting pages that change over time?**  \n→ In my opinion, the best way to handle this is via CI pipelines (Jenkins/GitHub Actions), but you can always set up cron jobs as well (macOS/Linux).\n\n**• When things go wrong, like odd markup, partial loads, or missing content, where is the best place to debug? Raw HTML, Mojo’s intermediate output, or the final chunks?**  \n→ In my opinion, the best way to debug is to fetch the page via `curl` and use a converter. For example:\n\n    curl -X GET your-web -o file.html\n\nThen later you can use Mojo to fetch the static source using:\n\n    ./mojo -d 0 file://your_file -o ./debug\n\nand see the output it generates (always with depth 0).\n\n**“One thing that could make this even stronger is being explicit about failure modes and contracts in the README. In other words, what Mojo guarantees versus what it intentionally does not. Even a short section in the README about what Mojo will not handle would build a lot of trust.”**\n\nThanks for your suggestion, I will include it after I finish the render crawler.\n\nThanks :)",
              "score": 2,
              "created_utc": "2026-02-01 11:30:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2yv58y",
                  "author": "Anti-Entropy-Life",
                  "text": "Thanks so much for taking the time to write such a detailed and awesome response! I think this is really a spectacular project my friend! :)\n\nThe deterministic model of “same HTML gives the same output” makes sense, and the split between pure HTTP fetching and the `--render` path is clear. Using CDP for rendering without pulling in extra dependencies seems like the right call for JS-heavy pages.\n\nThe CI / cron approach for handling drift also feels pragmatic. Treating ingestion as an explicit job instead of something implicit or magical is exactly how I’d expect this to be used in real pipelines! This is fantastic!\n\nSo, just to be sure I am understanding you properly, the optimal debugging workflow you described is:\n\n0. Fetch HTML with  `curl`\n\n1. Validate or convert it independently\n2. Run Mojo at depth 0 against a static source\n\n(Edit: sorry, I don't know what's wrong with this list, the Reddit formatting refuses to not render it with this weird spacing/gap for some reason I can't quite determine at the moment)  \n  \nIf that’s right, that’s a great, clean debugging protocol. It’s one of the most realistic and sane ways I’ve seen to isolate issues and understand where things are going wrong.\n\nI feel honored you found my simple README suggestion helpful at all. Being explicit about guarantees and non-goals is something I really appreciate in tools like this.\n\nThank you absolutely supremely for being a clear-minded thinker and building such beautiful tools that increase the coherence of the world :)\n\nLooking forward to seeing and testing out the render crawler once it's out. Thanks again for the thoughtful explanation and for building such a well-reasoned project. :)",
                  "score": 1,
                  "created_utc": "2026-02-01 13:40:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrh2ns",
      "title": "Streaming feature transformations",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qrh2ns/streaming_feature_transformations/",
      "author": "Spirited-Bit9693",
      "created_utc": "2026-01-30 20:30:18",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "What are the popular approaches to do feature transformations on streaming data?\n\nRequirements:\n\nLow latency computations on data from kafka streams\n\npopulate the computed features in online feature store",
      "is_original_content": false,
      "link_flair_text": "beginner help😓",
      "permalink": "https://reddit.com/r/mlops/comments/1qrh2ns/streaming_feature_transformations/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2o9q9h",
          "author": "Scared_Astronaut9377",
          "text": "What kind of transformations are we talking about? Just extract things from one kafka message, apply a function, put in store? Or like \"use the kafka stream to keep the count of user actions during the current activity session\"? Very different requirements. \n\nAnd what is your existing stack?",
          "score": 1,
          "created_utc": "2026-01-30 20:56:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oa4sa",
              "author": "Spirited-Bit9693",
              "text": "We currently only have a batch framework. We need both : applying simple functions and also stateful transformations",
              "score": 1,
              "created_utc": "2026-01-30 20:58:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ohbwc",
                  "author": "Scared_Astronaut9377",
                  "text": "Apache flink or similar streaming engines.",
                  "score": 3,
                  "created_utc": "2026-01-30 21:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oa7jp",
          "author": "Spirited-Bit9693",
          "text": "We primarily use spark to compute features",
          "score": 1,
          "created_utc": "2026-01-30 20:58:51",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qphftl",
      "title": "At what point does inference latency become a deal-breaker for you?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qphftl/at_what_point_does_inference_latency_become_a/",
      "author": "Good-Listen1276",
      "created_utc": "2026-01-28 17:15:19",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI keep hearing about inference \"acceleration,\" but I’m seeing teams choose smaller, dumber models (SLMs) just to keep the UX snappy.\n\nI want to know: have you ever had to kill a feature because it was too slow to be profitable? I'm gathering insights on three specific \"pain points\" for research:\n\n1. If an agent takes 15 internal \"thought\" steps, and each takes 1.5s, that’s a 22-second wait. Does your churn spike at 5s? 10s? Or do your users actually wait?\n2. How much time does your team waste trying to refactor layers (like moving PyTorch → TensorRT) only to have the accuracy drop or the conversion fail?\n3. Are you stuck paying for H100s because cheaper hardware (L4s/T4s) just can't hit the TTFT (Time to First Token) you need?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qphftl/at_what_point_does_inference_latency_become_a/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2ya3v0",
          "author": "Anti-Entropy-Life",
          "text": "\"Too slow to be profitable\" -> we should be figuring out ways to socially engineer society so this is reversed entirely as it is a corroding principle that can only ever serve to inject entropy into society, destabilizing it on a long enough horizon.\n\nReframe the problem, don't give in to the nonsense. Tell a story that encodes why a slow process is preferable to endlessly dumb-optimized speed for speeds sake.",
          "score": 1,
          "created_utc": "2026-02-01 11:00:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr5fye",
      "title": "UPDATE: sklearn-diagnose now has an Interactive Chatbot!",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qr5fye/update_sklearndiagnose_now_has_an_interactive/",
      "author": "lc19-",
      "created_utc": "2026-01-30 13:27:44",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm excited to share a major update to sklearn-diagnose - the open-source Python library that acts as an \"MRI scanner\" for your ML models (https://www.reddit.com/r/mlops/s/3HKkXzMbxZ)\n\nWhen I first released sklearn-diagnose, users could generate diagnostic reports to understand why their models were failing. But I kept thinking - what if you could talk to your diagnosis? What if you could ask follow-up questions and drill down into specific issues?\n\nNow you can! 🚀\n\n🆕 What's New: Interactive Diagnostic Chatbot\n\nInstead of just receiving a static report, you can now launch a local chatbot web app to have back-and-forth conversations with an LLM about your model's diagnostic results:\n\n💬 Conversational Diagnosis - Ask questions like \"Why is my model overfitting?\" or \"How do I implement your first recommendation?\"\n\n🔍 Full Context Awareness - The chatbot has complete knowledge of your hypotheses, recommendations, and model signals\n\n📝 Code Examples On-Demand - Request specific implementation guidance and get tailored code snippets\n\n🧠 Conversation Memory - Build on previous questions within your session for deeper exploration\n\n🖥️ React App for Frontend - Modern, responsive interface that runs locally in your browser\n\nGitHub: https://github.com/leockl/sklearn-diagnose\n\nPlease give my GitHub repo a star if this was helpful ⭐",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qr5fye/update_sklearndiagnose_now_has_an_interactive/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qqpjx6",
      "title": "A Practical Framework for Designing AI Agent Systems (With Real Production Examples)",
      "subreddit": "mlops",
      "url": "https://youtu.be/CMMlLB01rcE",
      "author": "OnlyProggingForFun",
      "created_utc": "2026-01-30 00:02:07",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qqpjx6/a_practical_framework_for_designing_ai_agent/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qq8j9r",
      "title": "MLOPs jobs",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qq8j9r/mlops_jobs/",
      "author": "Competitive-Fact-313",
      "created_utc": "2026-01-29 13:28:03",
      "score": 3,
      "num_comments": 9,
      "upvote_ratio": 0.62,
      "text": "Brutally honest! What’s the bare minimum to get into mlops straightaway. \n\nPlease consider following in order to answer\n\n1. Bachelor degree? \n\n2. MSc degree?\n\n3. Certs? \n\n4. Experience? \n\nI heard people say that you need this or that many year of experience before getting into MLOPs. I mean come on if one has 10+year of experience but no ml tools exposed then he has to work but one exposed themselves to mlops n work for 3-4 year along with some infra tools is well qualified for mlops? \n\nNote: if I have 10+ experience in ml or mlops i would rather contest for CTO lol! ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qq8j9r/mlops_jobs/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o2epsyi",
          "author": "ProcessIndependent38",
          "text": "“I heard people say that you need this or that many year of experience before getting into MLOPs. I mean come on if one has 10+year of experience but no ml tools exposed then he has to work but one exposed themselves to mlops n work for 3-4 year along with some infra tools is well qualified for mlops?”\n\nWhat?\n\nYou need a few years of experience in any ops role.",
          "score": 6,
          "created_utc": "2026-01-29 13:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eq2cj",
              "author": "Competitive-Fact-313",
              "text": "Thanks for clarifying that Mlops is not a rocket science.",
              "score": -6,
              "created_utc": "2026-01-29 13:40:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lslu0",
          "author": "Affectionate-Heron90",
          "text": "I'd weigh more on actual ops experience. I've seen DevOps folks easily transition to MLOps role.\nAlso, its easier to get into some ML projects within your current company , gain experience and then look for another role. Vs. having no experience and  trying to switch career. \n\n\nIf you cant in your current company, start actively building / contributing in OSS tools. ",
          "score": 3,
          "created_utc": "2026-01-30 14:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eso6u",
          "author": "Vpharrish",
          "text": "What's your qualifications? Imo, MLOps itself is looked as more of a SWE people integrating ML (insights from my professors and people) rather than something that one would dedicate and work through.",
          "score": 2,
          "created_utc": "2026-01-29 13:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eu8qg",
              "author": "Competitive-Fact-313",
              "text": "I work as ml platform guy, got few years of ml experience and now integrating infra but I see a lot of stupidity in and around the JD ? I mean shouldn’t people hire someone with some ml Background with some infra hands on ? The same people who develop software now simply moving to ml fields doesn’t make much sense than someone who actually studies ML in university and got some hands on",
              "score": 1,
              "created_utc": "2026-01-29 14:02:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2g875u",
                  "author": "eemamedo",
                  "text": "Are you working full-time as ml infra or internship? Your ML infra experience is exactly what MLOps is at many companies. Yes, you might be missing some “operational” stuff like helping build custom solutions for DS to connect infra with their workloads but honestly, it’s not that impossible to gain. ",
                  "score": 3,
                  "created_utc": "2026-01-29 17:53:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2q8uyf",
          "author": "MathmoKiwi",
          "text": "> Brutally honest! What’s the bare minimum to get into mlops straightaway.\n\n\"All of the above\"",
          "score": 2,
          "created_utc": "2026-01-31 03:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2y0kpw",
          "author": "Anti-Entropy-Life",
          "text": "I think a lot of answers over-credentialize this, but I admit I am a self-taught lab founder, so your mileage may vary.\n\nIn practice, the real bar for MLOps is simpler than degrees or certs:\n\nHave you shipped and operated at least one non-trivial system end-to-end, seen how it fails in the real world, and done work to either prevent those failures or make them easy to debug and recover from?\n\nIf yes, then you already have the core skill set. The ML-specific parts (model registry, evals, data drift, feature pipelines) are learnable on top of solid systems and ops experience.\n\nDegrees and certs mostly help with HR filters (I know, not trivial as a block these days, but for MLOps should matter less than more generic jobs). Day-to-day MLOps is about operating probabilistic systems under uncertainty, not academic ML theory.",
          "score": 1,
          "created_utc": "2026-02-01 09:33:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}