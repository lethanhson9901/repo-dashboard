{
  "metadata": {
    "last_updated": "2026-02-24 02:58:11",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 15,
    "total_comments": 36,
    "file_size_bytes": 62830
  },
  "items": [
    {
      "id": "1r8v6v5",
      "title": "Friendly advice for infra engineers moving to MLOps: your Python scripting may not enough, here's the gap to close",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r8v6v5/friendly_advice_for_infra_engineers_moving_to/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-19 09:54:09",
      "score": 61,
      "num_comments": 10,
      "upvote_ratio": 0.95,
      "text": "In my last post, I covered ML foundations. This one's about Python, specifically, the gap between \"I know Python\" and the Python you actually need for MLOps.\n\nIf you're from infra/DevOps, your Python probably looks like mine did: boto3 scripts, automation glue, maybe some Ansible helpers. That's scripting. MLOps needs programming, and the difference matters.\n\n**What you're probably missing:**\n\n* **Decorators & closures** â€” ML frameworks live on these. Airflow's \\`@tasks\\`, FastAPI's \\`@app.get()\\`. If you can't write a custom decorator, you'll struggle to read any ML codebase.\n* **Generators** â€” You can't load 10M records into memory. Generators let you stream data lazily. Every ML pipeline uses this.\n* **Context managers** â€” GPU contexts, model loading/unloading, DB connections. The `with` Pattern is everywhere.\n\n**Why memory management suddenly matters:**\n\nIn infra, your script runs for 5 seconds and exits. In ML, you're loading multi-GB models into servers that run for weeks. You need to understand Python's garbage collector, the difference between a Python list and a NumPy array, and the GPU memory lifecycle.\n\n**Async isn't optional:**\n\nFastAPI is async-first. Inference backends require you to understand when to use asyncio, multiprocessing, or threading, and why it matters for ML workloads.\n\n**Best way to learn all this?** Don't read a textbook. Build an inference backend from scratch, load a Hugging Face model, wrap it in FastAPI, add batching, profile memory under load, and make it handle 10K requests. Each step targets the exact Python skills you're missing.\n\nThe uncomfortable truth: you can orchestrate everything with K8s and Helm, but the moment something breaks *inside* the inference service, you're staring at Python you can't debug. That's the gap. Close it.\n\nIf anyone interested in detailed version, with an atual scenarios covering WHYs and code snippets please refer: [https://medium.com/@thevarunfreelance/friendly-advice-for-infra-engineers-moving-to-mlops-your-python-scripting-isnt-enough-here-s-f2f82439c519](https://medium.com/@thevarunfreelance/friendly-advice-for-infra-engineers-moving-to-mlops-your-python-scripting-isnt-enough-here-s-f2f82439c519)\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:Â [topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r8v6v5/friendly_advice_for_infra_engineers_moving_to/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o67u125",
          "author": "pmv143",
          "text": "Totally. The difference shows up fast when youâ€™re running real inference workloads. A five second boto3 script mindset doesnâ€™t translate to managing GPU memory, batching, async request handling, and long-lived model state.",
          "score": 12,
          "created_utc": "2026-02-19 10:02:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68u8a6",
          "author": "Ancient_Canary1148",
          "text": "as in DevOps,im not coding applications or api but helping Dev teams to build,deploy,run and observe. why do i need to learn deep python ml programming to be an MlOps? as infra engineer,im helping ml teams to run models,prepare infra for them (kafka,ml flow,flink) and etc.",
          "score": 6,
          "created_utc": "2026-02-19 14:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d8bdf",
              "author": "Extension_Key_5970",
              "text": "That's a fair point, and honestly, you're not wrong. If you're in a pure infra role, the toolset is completely different, and that work is genuinely valuable. ML teams need someone to set up Kafka, MLflow, Flink, and the K8S layer.\n\nBut here's where MLOps gets tricky, the line is blurred. In traditional DevOps, you don't touch the app code. Clear boundary. In MLOps, that boundary keeps breaking. One day, you're debugging why an inference service is leaking memory, or why a pipeline DAG is failing, and the answer isn't in the infrastructure; it's in the Python running on top of it.\n\nYou don't need to become a developer. But knowing enough Python to read, debug, and make sense of what's running on your infra, that's the difference. Both paths are valid; it just depends on where you want to grow.",
              "score": 3,
              "created_utc": "2026-02-20 04:22:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dz26b",
                  "author": "burntoutdev8291",
                  "text": "Depends on the team, I am the other way, developer / AI engineer to MLOps. Sometimes the lines are abit blurry, but my senior mentioned we need to know when to draw the line because deployment friendly code is on the developer. Otherwise very soon MLOps needs to help deploy jupyter notebooks.\n\nOur job is to reduce toil on the developer and solve infrastructure related problems. Because from what you are saying, I also need to know how to debug rust, go, python, ts, fortran depending on what the team uses. Python is easy enough that most MLOps can learn it but not the rest.",
                  "score": 1,
                  "created_utc": "2026-02-20 08:07:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6hdt76",
              "author": "Useful-Process9033",
              "text": "You are right that you do not need to become a Python expert to do MLOps from the infra side. But understanding how model serving works, how GPU memory behaves, and how to observe inference latency will make you 10x more effective at supporting ML teams than just provisioning Kafka clusters.",
              "score": 2,
              "created_utc": "2026-02-20 20:04:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68klzr",
          "author": "TranslatorSalt1668",
          "text": "Great. Exactly what I was looking for. Thanks",
          "score": 1,
          "created_utc": "2026-02-19 13:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h3lqz",
          "author": "bedel99",
          "text": "It sounds easy !",
          "score": 1,
          "created_utc": "2026-02-20 19:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s7kz1",
          "author": "SpiritedChoice3706",
          "text": "Absolutely. It's going to vary based on the role, but I'm a consultant, and how much pure infra I'm doing depends on the client. Right now I'm mostly in Kubernetes, standing LLMs up on GPUs. But last project, I was helping a rather built-out data platform deploy their first-ever recommendation model. I had to build an API for serving recs with real-time constraints, and also a retraining and monitoring pipeline. My main tools were FastAPI and Airflow, because that's what the client used. Tons of Python, and because we were working with lots of data, some real constraints in how we did things. It's going to vary from role-to-role, but if you're serving a model in production, you gotta know Python, because that's what your data scientists will be writing that model in.",
          "score": 1,
          "created_utc": "2026-02-22 15:19:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcp0ad",
      "title": "Broke down our $3.2k LLM bill - 68% was preventable waste",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "author": "llamacoded",
      "created_utc": "2026-02-23 18:11:09",
      "score": 37,
      "num_comments": 8,
      "upvote_ratio": 0.95,
      "text": "We run ML systems in production. LLM API costs hit $3,200 last month. Actually analyzed where money went.\n\n**68% - Repeat queries hitting API every time** Same questions phrased differently. \"How do I reset password\" vs \"password reset help\" vs \"can't login need reset\". All full API calls. Same answer.\n\nSemantic caching cut this by 65%. Cache similar queries based on embeddings, not exact strings.\n\n**22% - Dev/staging using production keys** QA running test suites against live APIs. One staging loop hit the API 40k times before we caught it. Burned $280.\n\nSeparate API keys per environment with hard budget caps fixed this. Dev capped at $50/day, requests stop when limit hits.\n\n**10% - Oversized context windows** Dumping 2500 tokens of docs into every request when 200 relevant tokens would work. Paying for irrelevant context.\n\nBetter RAG chunking strategy reduced this waste.\n\n**What actually helped:**\n\n* Caching layer for similar queries\n* Budget controls per environment\n* Proper context management in RAG\n\nCost optimization isn't optional at scale. It's infrastructure hygiene.\n\nWhat's your biggest LLM cost leak? Context bloat? Retry loops? Poor caching?",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6zw8xk",
          "author": "Morpheyz",
          "text": "Cut cost by 99%. External consultants sold us Azure Open AI PTUs for 50k/month, claiming we absolutely needed them for our use case. Couple months later convinced leadership to switch to pay-as-you-yo model, now spending 300$/month.\n\nEdit: PTUs, not TPUs",
          "score": 11,
          "created_utc": "2026-02-23 18:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o700ndp",
              "author": "pmv143",
              "text": "Classic overprovisioning trap. Fixed infra before validated demand is expensive. Usage based models are much more forgiving while workloads are still evolving.",
              "score": 5,
              "created_utc": "2026-02-23 18:59:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o704ptq",
              "author": "m98789",
              "text": "Azure doesnâ€™t offer TPUs tho",
              "score": 2,
              "created_utc": "2026-02-23 19:18:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o706ns8",
                  "author": "Morpheyz",
                  "text": "Typo, I meant PTUs.",
                  "score": 2,
                  "created_utc": "2026-02-23 19:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70098v",
          "author": "pmv143",
          "text": " most ppl underestimate how much waste lives above the model. Interesting part is that even after fixing caching and RAG, infrastructure-level inefficiencies still compound at scale.",
          "score": 2,
          "created_utc": "2026-02-23 18:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o712b33",
          "author": "KeyIsNull",
          "text": "Mind to share some details about the semantic cache layer?Â ",
          "score": 1,
          "created_utc": "2026-02-23 21:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70to85",
          "author": "inspectedinspector",
          "text": "How much will it cost you to build embedding-based semantic caching to save $2000?",
          "score": 0,
          "created_utc": "2026-02-23 21:18:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71c9sa",
              "author": "ZestyData",
              "text": "Very little, could do it in an hour and with very low running costs. Embeddings and vector search are basically free.",
              "score": 2,
              "created_utc": "2026-02-23 22:50:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ravnd2",
      "title": "Cleared NVIDIA NCA-AIIO - Next Target: NCP-AII",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ravnd2/cleared_nvidia_ncaaiio_next_target_ncpaii/",
      "author": "TuckerSavannah1",
      "created_utc": "2026-02-21 16:37:26",
      "score": 19,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "Hello Everyone\n\nGlad to share that Iâ€™ve successfully cleared the NVIDIA NCA-AIIO (AI Infrastructure & Operations) exam!\n\nMy journey was focused on building strong fundamentals in GPUs, networking, and AI infrastructure concepts. I avoided rote learning and concentrated on understanding how things actually work. Practice tests from itexamscerts also played a big role, they helped me identify weak areas and improve my confidence before the exam. Overall, if your basics are clear, the exam is very manageable.\n\nNow Iâ€™m preparing for NVIDIA NCP-AII, and I would really appreciate guidance from those who have cleared it.\n\n\\* How tough is it compared to NCA-AIIO?\n\n\\* Is it more hands-on or CLI/lab focused?\n\n\\* Any recommended labs?y\n\nI look forward to your valuable insights. Thank you.",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1ravnd2/cleared_nvidia_ncaaiio_next_target_ncpaii/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6n5oab",
          "author": "RubySera1",
          "text": "Well done and congrats! Could you please share which practice tests you found most useful for NCA-AIIO?",
          "score": 3,
          "created_utc": "2026-02-21 18:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n7xk9",
              "author": "TuckerSavannah1",
              "text": "itexamscerts.",
              "score": 2,
              "created_utc": "2026-02-21 18:52:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zio9a",
          "author": "Sure-Programmer-8462",
          "text": "Nice achievement. Were the NCA-AIIO exam questions more theoretical or based on real world scenarios?",
          "score": 2,
          "created_utc": "2026-02-23 17:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zjnod",
              "author": "TuckerSavannah1",
              "text": "The exam was a mix of both, but many questions were scenario-based. If you understand the core concepts and practical use cases, it becomes much easier to handle.",
              "score": 1,
              "created_utc": "2026-02-23 17:42:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zlaws",
                  "author": "Sure-Programmer-8462",
                  "text": "Did you find the exam objectives closely aligned with the official syllabus or were there some unexpected topics?",
                  "score": 2,
                  "created_utc": "2026-02-23 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nzjbf",
          "author": "Satsuma_Johnson",
          "text": "congrats..",
          "score": 1,
          "created_utc": "2026-02-21 21:15:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh2d5",
              "author": "TuckerSavannah1",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-23 17:29:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tcfen",
          "author": "Wright_Lucy11",
          "text": "congrats.",
          "score": 1,
          "created_utc": "2026-02-22 18:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh3ne",
              "author": "TuckerSavannah1",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-23 17:30:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zidpo",
          "author": "GrapeThompson",
          "text": "Congratulations on clearing the NCA-AIIO exam! How long did you take to prepare for it, and did you already have experience with AI infrastructure before starting?",
          "score": 1,
          "created_utc": "2026-02-23 17:36:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zj98g",
              "author": "TuckerSavannah1",
              "text": "Thank you! It took me around 4â€“5 weeks of consistent study. I had some basic knowledge of networking and virtualization, but I was new to AI infrastructure concepts. I focused on understanding GPU architecture and deployment scenarios.",
              "score": 1,
              "created_utc": "2026-02-23 17:40:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7s2ni",
      "title": "[D] We tested the same INT8 model on 5 Snapdragon chipsets. Accuracy ranged from 93% to 71%. Same weights, same ONNX file.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r7s2ni/d_we_tested_the_same_int8_model_on_5_snapdragon/",
      "author": "NoAdministration6906",
      "created_utc": "2026-02-18 03:30:42",
      "score": 18,
      "num_comments": 4,
      "upvote_ratio": 0.96,
      "text": "We've been doing on-device accuracy testing across multiple Snapdragon SoCs and the results have been eye-opening.\n\nSame model. Same quantization. Same ONNX export. Deployed to 5 different chipsets:\n\n|Device|Accuracy|\n|:-|:-|\n|Snapdragon 8 Gen 3|91.8%|\n|Snapdragon 8 Gen 2|89.1%|\n|Snapdragon 7s Gen 2|84.3%|\n|Snapdragon 6 Gen 1|79.6%|\n|Snapdragon 4 Gen 2|71.2%|\n\nCloud benchmark reported 94.2%.\n\nThe spread comes down to three things we've observed:\n\n1. **NPU precision handling**Â â€” INT8 rounding behavior differs across Hexagon generations. Not all INT8 is created equal.\n2. **Operator fusion differences**Â â€” the QNN runtime optimizes the graph differently per SoC, sometimes trading accuracy for throughput.\n3. **Memory-constrained fallback**Â â€” on lower-tier chips, certain ops fall back from NPU to CPU, changing the execution path entirely.\n\nNone of this shows up in cloud-based benchmarks. You only see it when you run on real hardware.\n\nCurious if others are seeing similar drift across chipsets â€” or if anyone has a good strategy for catching this before shipping. Most CI pipelines we've seen only test on cloud GPUs and call it a day.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r7s2ni/d_we_tested_the_same_int8_model_on_5_snapdragon/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o60owaq",
          "author": "KeyIsNull",
          "text": "Wow that's a huge performance drop, it is always a good idea to measure on device but I'd never expect a 15/20% drop. Have you also tested the model in full precision?",
          "score": 1,
          "created_utc": "2026-02-18 08:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60wvu3",
          "author": "datashri",
          "text": "Did you publish anywhere?",
          "score": 1,
          "created_utc": "2026-02-18 09:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o629f1b",
          "author": "Commercial-Fly-6296",
          "text": "Maybe the chip wear and tear also matters ?",
          "score": 1,
          "created_utc": "2026-02-18 14:56:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7128gb",
          "author": "Outrageous_Hat_9852",
          "text": "This is a great example of why hardware-specific validation needs to be part of your testing pipeline, not just a post-deployment discovery. The accuracy drop suggests quantization is behaving differently across those Snapdragon variants - you'd want to systematically test your model's behavior on each target chipset during development, ideally with the same test scenarios that matter for your use case. If this is an AI agent or conversational system, running conversation simulations across all your deployment targets would catch these consistency issues before users hit them.",
          "score": 1,
          "created_utc": "2026-02-23 21:59:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra4fky",
      "title": "Preparing for ML System Design Round (Fraud Detection / E-commerce Abuse) â€“ Need Guidance (4 Days Left)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ra4fky/preparing_for_ml_system_design_round_fraud/",
      "author": "SuccessfulStorm5342",
      "created_utc": "2026-02-20 19:09:16",
      "score": 9,
      "num_comments": 10,
      "upvote_ratio": 0.8,
      "text": "Hey everyone,\n\nI am a final year [B.Tech](http://B.Tech) student and I have an **ML System Design interview in 4 days** at a startup focused on **e-commerce fraud and return abuse detection**. They use ML for things like:\n\n* Detecting return fraud (e.g., customer buys a real item, returns a fake)\n* Multi-account detection / identity linking across emails, devices, IPs\n* Serial returner risk scoring\n* Coupon / bot abuse\n* Graph-based fraud detection and customer behavior risk scoring\n\nI have solid ML fundamentals but havenâ€™t worked in fraud detection specifically. Iâ€™m trying to prep hard in the time I have.\n\n# What Iâ€™m looking for:\n\n**1. What are the most important topics I absolutely should not miss when preparing for this kind of interview?**  \nPlease prioritize.\n\n**2. Any good resources (blogs, papers, videos, courses)?**\n\n**3. Any advice on how to approach the preparation itself?**  \nAny guidance is appreciated.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1ra4fky/preparing_for_ml_system_design_round_fraud/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6j8thz",
          "author": "DGSPJS",
          "text": "I used to be PM for an MLOps platform for fraud detection models.\n\nSome areas I'd stress are:  \nHandling highly imbalanced datasets - a company being absolutely battered by fraud is still only experiencing maybe a couple % of transactions as fraud and I've seen models deployed for 1:1,000,000 cases.\n\nModel retraining loops in the face of a delayed / irregular feedback loop (false positives might be worked out in minutes, false negatives can take months to be fully reported).\n\nModel optimization and threshold selection based on dollar value of transactions rather than number of transactions, and potentially accounting for the cost of frustrated customers with false positives.\n\nModel explainability techniques for understanding what types of fraud are being experienced and identifying if new types of attacks are emerging.\n\nGood luck.",
          "score": 8,
          "created_utc": "2026-02-21 02:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k7dxr",
              "author": "SuccessfulStorm5342",
              "text": "Thanks a lot for sharing this.",
              "score": 1,
              "created_utc": "2026-02-21 06:19:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kxzy1",
          "author": "Gaussianperson",
          "text": "Since you only have four days, focus on feature engineering and latency. Fraud systems usually live or die by features like velocity, such as how many times an IP appears in a short window, and device fingerprinting. For return abuse, you should talk about the feedback loop since labels often take weeks to arrive after a return happens. Make sure you can explain how to handle the extreme class imbalance since fraud cases are rare compared to normal orders.\n\nOn the architecture side, look into how graph databases help with identity linking. If a person uses multiple emails but the same device ID, a graph helps you find those connections quickly. You should also think about the trade offs between a real time blocking system and a batch based risk scoring system. The interviewer will probably ask how you plan to handle data drift when fraudsters change their patterns to avoid detection.\n\nI write about these kinds of engineering challenges in my newsletter, Machine Learning at Scale. I actually cover specific system design topics and production infrastructure over at [machinelearningatscale.substack.com](http://machinelearningatscale.substack.com) if you want to see some deep dives before your interview. Good luck with the process.",
          "score": 3,
          "created_utc": "2026-02-21 10:37:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m3qg4",
              "author": "SuccessfulStorm5342",
              "text": "Thanks a lot",
              "score": 1,
              "created_utc": "2026-02-21 15:31:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hjgi1",
          "author": "Spare-Builder-355",
          "text": "if you are final year student, how you are supposed to know fraud detection domain if you never worked in one ? This is not public knowledge. There are no books or opensource projects on the topic.",
          "score": 4,
          "created_utc": "2026-02-20 20:32:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k6216",
              "author": "SuccessfulStorm5342",
              "text": "Exactly, I didnâ€™t find many resources. In the first round, which I was able to clear through some reading from ChatGPT and basic ML fundamentals the interviewer told me that the second round would be more in-depth, . Iâ€™m not sure the same approach will work for the upcoming round. They will basically give situations and see how I approach the problem.",
              "score": 1,
              "created_utc": "2026-02-21 06:08:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6kwawf",
                  "author": "Spare-Builder-355",
                  "text": "considering you only have few days left - you have zero chances of learning anything meaningful about fraud detection. \n\nThe only advice I can give: spend this time by practicing the interview. Pick one problem from your list and design ML system to solve that problem. Have a couple of parallel chatgpt sessions to act as friendly expert and as interviewer. Always come up with ideas youself, bounce few times with \"freindly expert\", improve your design. Get interviewer to ask questions about your choices. \n\nSince it's ML System Design I'd maybe focus on broader picture of typical ML system:\n\n- how do you collect historical data\n\n- how do you classify it for training\n\n- how would you extract features from your data\n\n- features engineering for training\n\n- features engineering for inference\n\n- model performance evaluation (a/b testing)\n\n- maybe take a look at industry's tooling like Hopworks Feature Store and MLFlow.\n\n- make sure you understand \"big picture\" of your ML system and maybe can dive deep into details.\n\n- finally, always keep in mind that it is OK for you to say \"sorry I never looked into this aspect in details\". You are student not seasoned pro.",
                  "score": 2,
                  "created_utc": "2026-02-21 10:20:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sdqwo",
          "author": "Most-Bell-5195",
          "text": "If you're looking for targeted mock interviews, feel free to reach out.",
          "score": 2,
          "created_utc": "2026-02-22 15:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k7os9",
          "author": "SuccessfulStorm5342",
          "text": "I would request anyone to cross-post this in r/MachineLearning , don't know why i'm banned there.",
          "score": 1,
          "created_utc": "2026-02-21 06:22:38",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r88f78",
      "title": "From 40-minute builds to seconds: Why we stopped baking model weights into Docker images",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r88f78/from_40minute_builds_to_seconds_why_we_stopped/",
      "author": "No-Pay5841",
      "created_utc": "2026-02-18 16:55:48",
      "score": 8,
      "num_comments": 14,
      "upvote_ratio": 0.68,
      "text": "Weâ€™ve all been there. You spend weeks tweaking hyperparameters, the validation loss finally drops, and you feel like a wizard. You wrap the model in a Docker container, push to the registry, and suddenly youâ€™re just a plumber dealing with a clogged pipe.\n\nWe recently realized that treating ML models like standard microservices was killing our velocity. Specifically, the anti-pattern of baking gigabyte-sized weights directly into the Docker image (`COPY ./model_weights.pt /app/`).\n\nHere is why this destroys your pipeline and how we fixed it:\n\n**The Cache Trap:** Docker builds rely on layer caching. If you bundle code (KB) with weights (GB), you couple two artifacts with vastly different lifecycles.\n\n* Change one line of Python logging?\n* Docker invalidates the cache.\n* The CI runner re-copies, re-compresses, and re-uploads the entire 10GB blob.\n* **Result:** 40+ minute build times and autoscaling that lags so bad users leave before the pod boots.\n\n# Model-as-Artifact with Render\n\nWe decided to stop fighting the infrastructure and moved our stack to Render to implement the \"Model-as-Artifact\" pattern properly. Hereâ€™s how we decoupled the state (weights) from the logic (code):\n\n* **External Storage via Render Disks:** Instead of baking weights into the image, we store them on Render Persistent Disks. These are high-performance SSDs that stay attached to our instances even when the code changes.\n* **Decoupled Logic:** Our container now only holds the API code. When a build triggers on Render, it only has to package the lightweight Python environment, not the 10GB model.\n* **Smart Rollouts:** We used Render Blueprints to declaratively manage our GPU quotas and disk mounts. This ensures that every time we push to Git, the new code mounts the existing weight-filled disk instantly.\n* **Proper Probing:** We configured Renderâ€™s health checks to distinguish between the container starting and the model actually being loaded into VRAM, preventing \"zombie pods\" from hitting production.\n\n**The Results**\n\n* Build time: Dropped from \\~45 mins to <2 minutes.\n* Cold starts: Reduced to seconds using local NVMe caching on GPU nodes.\n* Cost: Stopped paying for idle GPUs while waiting for massive image pulls.\n\nI wrote a deeper dive on the architecture, specifically regarding Kubernetes probes and Docker BuildKit optimizations here: [https://engineersguide.substack.com/p/from-git-push-to-gpu-api-stop-baking](https://engineersguide.substack.com/p/from-git-push-to-gpu-api-stop-baking)",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1r88f78/from_40minute_builds_to_seconds_why_we_stopped/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o63et27",
          "author": "NotSoGenius00",
          "text": "Idk why people copy model weights onto docker files ? ðŸ˜‚ \n\nLike just use PVC on k8s or use s3 to stream your weights. \n\nPeople want instant results imagine running deepseek MOE and expecting your api to respond in second. \n\nPeople need to go back to school who expect such things to happen, it takes time to load the model ! There is IO (most of the people are either in leaderships or PM ) \n\nNo offence but the truth is truth",
          "score": 10,
          "created_utc": "2026-02-18 18:04:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63lw9y",
              "author": "No-Pay5841",
              "text": "Exactly. I think the 'baking weights' habit comes from people wanting that 'one-click' deployment convenience without realizing theyâ€™re killing their autoscaling.\n\nYou're right about the I/O bottleneck: waiting for a 50GB image pull is just a slow way to burn money. Moving to PVCs or S3-streaming isn't just an 'optimization' anymore; for LLMs/MoE, itâ€™s the only way to actually stay operational.",
              "score": 1,
              "created_utc": "2026-02-18 18:35:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o64ky8d",
              "author": "KeyIsNull",
              "text": "I actually froze when I read opâ€™s post: I lost count of the times I scolded my juniors during a code review for a baked model\n\nMaybe itâ€™s me, I get upset even for cuda dependenciesÂ \n",
              "score": 1,
              "created_utc": "2026-02-18 21:16:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68je5p",
                  "author": "No-Pay5841",
                  "text": "I love that youâ€™ve avoided this! To be honest, our team started exactly there**.** In the early days, when models were only 100MB, `COPY ./weights` felt like the clean way to ensure reproducibility.\n\nIt wasn't until our models grew into the multi-gigabyte range and our CI/CD builds started becoming the bottleneck that we realized weâ€™d accidentally built a monster. We had to break our own habits to get back our deployment speed. My post is basically a 'letter to my past self' to save others from that same headache.",
                  "score": 1,
                  "created_utc": "2026-02-19 13:20:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65lxy0",
              "author": "burntoutdev8291",
              "text": "AI generated post, probably AI generated problem then AI generated solution.",
              "score": 1,
              "created_utc": "2026-02-19 00:22:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o649gth",
          "author": "conditiosinequano",
          "text": "I agree, these things should not be in the image. \nThe example of changing a loc and getting a cache bust is a bit forced though:\n\nYou could just put the copy in an early layer.",
          "score": 2,
          "created_utc": "2026-02-18 20:23:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64lg1k",
              "author": "KeyIsNull",
              "text": "Or simply mount as volume the HF, Ollama, whatever cache folder. Works even in ECS and itâ€™s one of the lowest hanging fruits when doing cost and time optimisationÂ ",
              "score": 1,
              "created_utc": "2026-02-18 21:19:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o685h0l",
          "author": "symphonicdev",
          "text": "My first reaction when reading this post is: Why? Why would anyone bake the model weights into their Docker image? I personally haven't seen anyone doing so.\n\nBut thank you for sharing this insight!",
          "score": 2,
          "created_utc": "2026-02-19 11:43:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68jb9r",
              "author": "No-Pay5841",
              "text": "I love that youâ€™ve avoided this! To be honest, our team started exactly there**.** In the early days, when models were only 100MB, `COPY ./weights` felt like the clean way to ensure reproducibility.\n\nIt wasn't until our models grew into the multi-gigabyte range and our CI/CD builds started becoming the bottleneck that we realized weâ€™d accidentally built a monster. We had to break our own habits to get back our deployment speed. My post is basically a 'letter to my past self' to save others from that same headache.",
              "score": 3,
              "created_utc": "2026-02-19 13:19:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6upyse",
              "author": "trailing_zero_count",
              "text": "Reproducibility",
              "score": 1,
              "created_utc": "2026-02-22 22:31:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jz5pm",
          "author": "le-fou",
          "text": "I donâ€™t really understand or resonate with this. I used to use the public MLServer docker image to serve models on k8s â€” but on startup, that image has to install model dependencies at runtime and then start up the model. This took upwards of 5 minutes. Each model has different dependencies (like even slightly different versions of PyTorch). We use MLFlow so the image would have to pull weights at runtime too.\n\nOur solution was to built model-specific containers that still use MLServer as the model runtime. They start up almost instantly because runtime dependencies are pre-installed. Yeah, the build is a little longer, but how often are you rebuilding images vs scaling existing containersâ€¦.? Our pre-built model-specific images take sub 10 seconds to start and serve requests, assuming the image exists on the node already and therefore isnâ€™t being re-pulled from the image registry. This allows for quick auto scaling.\n\nI understand you are suggesting caching weights. But when you say â€œlightweight python environmentâ€â€¦. PyTorch and CUDA arenâ€™t lightweight and models could be trained/packaged with different dependency versions. Your model needs to be served with the same version it was trained with. So how do you make sure the image used to serve the model I has identical runtime dependencies?\n\nYeah. This whole post and all the comments are reading like bad marketing for Render.",
          "score": 1,
          "created_utc": "2026-02-21 05:11:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k2wt0",
              "author": "No-Pay5841",
              "text": "100%.\n\nyou make a reallyyyy good point about dependency hell because honestly i dont think anyone wants to pip install anything at runtime :) that sounds like a nightmare. im definitely not suggesting that.\n\nBut the \"lightweight\" part is just the delta. if you have 5 models on PyTorch 2.2, you can share one 2GB base image. if you bake weights in, you have 5 separate 12GB images and your registry costs start looking like a disaster.\n\nAnd if you use MLFlow to pull weights, youâ€™re already halfway there. the \"model-as-artifact\" thing just means treating that 10GB file like a database entry instead of a code change so you dont have to push gigabytes over the wire every time you fix a typo.",
              "score": 1,
              "created_utc": "2026-02-21 05:41:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s9p0f",
          "author": "SpiritedChoice3706",
          "text": "Great post. Lots of folks in here saying why would anyone do this. I think if you're in the ML space, it wouldn't make sense, but I recently had a long engagement with a data platform that knew nothing about ML. Really skilled software and data engineers, but didn't know anything about models. We weren't baking the image in, but we ended up having to download because of issues with streaming (it was a rec model, not an LLM thankfully). Then they complained that it took so long for our deployment to boot up (warm start was a business requirement). GOD that was a pain. They thought I was the worst engineer ever. But I had NO access to the EKS infra underneath - I couldn't build a PVC. I asked if they could just mount the fucking bucket to the container - nope, we can't support that.\n\nIt was a nightmare. But the reason I bring this up is to say that a lot of the frictions folks might hit in these things are going to be from external stakeholders. It's worth learning how to anticipate misconceptions folks might have about model serving and get ahead of them.",
          "score": 1,
          "created_utc": "2026-02-22 15:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o639rzx",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-18 17:42:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63lisn",
              "author": "No-Pay5841",
              "text": "Multi-stage builds are great for stripping out build dependencies (like compilers or dev headers) to keep images slim, but they don't solve the '10GB weights' problem.\n\nEven with multiple stages, if your final `COPY` command includes a massive model file, you're still stuck with huge registry upload times and slow cold starts on your nodes. The goal here isn't just a clean Dockerfile, itâ€™s getting that 10GB blob out of the deployment artifact entirely so your CI/CD can actually move at the speed of code.",
              "score": 1,
              "created_utc": "2026-02-18 18:33:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8nnui",
      "title": "A 16-mode failure map for LLM / RAG pipelines (open source checklist)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r8nnui/a_16mode_failure_map_for_llm_rag_pipelines_open/",
      "author": "Over-Ad-6085",
      "created_utc": "2026-02-19 02:55:11",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "If you are running LLM / RAG / agent systems in production, this might be relevant. If you mostly work on classic ML training pipelines (tabular, CV etc.), this map probably does not match your day-to-day pain points.\n\nIn the last year I kept getting pulled into the same kind of fire drills: RAG pipelines that pass benchmarks, but behave strangely in real traffic. Agents that look fine in a notebook, then go off the rails in prod. Incidents where everyone says â€œthe model hallucinatedâ€, but nobody can agree what exactly failed.\n\nAfter enough of these, I tried to write down a **failure map** instead of one more checklist. The result is a **16-problem map for AI pipelines** that is now open source and used as my default language when I debug LLM systems.\n\nVery roughly, it is split by layers:\n\n* **Input & Retrieval \\[IN\\]** hallucination & chunk drift, semantic â‰  embedding, debugging is a black box\n* **Reasoning & Planning \\[RE\\]** interpretation collapse, long-chain drift, logic collapse & recovery, creative freeze, symbolic collapse, philosophical recursion\n* **State & Context \\[ST\\]** memory breaks across sessions, entropy collapse, multi-agent chaos\n* **Infra & Deployment \\[OP\\]** bootstrap ordering, deployment deadlock, pre-deploy collapse\n* **Observability / Eval {OBS}** tags that mark â€œthis breaks in ways you cannot see from a single requestâ€\n* **Security / Language / OCR {SEC / LOC}** mainly cross-cutting concerns that show up as weird failure patterns\n\nThe 16 concrete problems look like this, in plain English:\n\n1. **hallucination & chunk drift** â€“ retrieval returns the wrong or irrelevant content\n2. **interpretation collapse** â€“ the chunk is right, but the logic built on top is wrong\n3. **long reasoning chains** â€“ the model drifts across multi-step tasks\n4. **bluffing / overconfidence** â€“ confident tone, unfounded answers\n5. **semantic â‰  embedding** â€“ cosine match is high, true meaning is wrong\n6. **logic collapse & recovery** â€“ reasoning hits a dead end and needs a controlled reset\n7. **memory breaks across sessions** â€“ lost threads, no continuity between runs\n8. **debugging is a black box** â€“ you cannot see the failure path through the pipeline\n9. **entropy collapse** â€“ attention melts into one narrow path, no exploration\n10. **creative freeze** â€“ outputs become flat, literal, repetitive\n11. **symbolic collapse** â€“ abstract / logical / math style prompts break\n12. **philosophical recursion** â€“ self-reference loops and paradox traps\n13. **multi-agent chaos** â€“ agents overwrite or misalign each otherâ€™s roles and memories\n14. **bootstrap ordering** â€“ services fire before their dependencies are ready\n15. **deployment deadlock** â€“ circular waits inside infra or glue code\n16. **pre-deploy collapse** â€“ version skew or missing secret on the very first call\n\nEach item has its own page with:\n\n* how it typically shows up in logs and user reports\n* what people usually *think* is happening\n* what is actually happening under the hood\n* concrete mitigation ideas and test cases\n\nEverything lives in one public repo, under a single page:\n\n* **Full map + docs:** [https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md](https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md)\n\nThere is also a small helper I use when people send me long incident descriptions:\n\n* **â€œDr. WFGYâ€ triage link (ChatGPT share):** [https://chatgpt.com/share/68b9b7ad-51e4-8000-90ee-a25522da01d7](https://chatgpt.com/share/68b9b7ad-51e4-8000-90ee-a25522da01d7)\n\nYou paste your incident or pipeline description, and it tries to:\n\n1. guess which of the 16 modes are most likely involved\n2. point you to the relevant docs in the map\n\nIt is just a text-only helper built on top of the same open docs. No signup, no tracking, MIT license.\n\nOver time this map grew from my own notes into a public resource. The repo is sitting around \\~1.5k stars now, and several **awesome-AI / robustness / RAG** lists have added it as a reference for failure-mode taxonomies. That is nice, but my main goal here is to stress-test the taxonomy with people who actually own production systems.\n\nSo I am curious:\n\n* Which of these 16 do you see the most in your own incidents?\n* Is there a failure mode you hit often that is completely missing here?\n* If you already use some internal taxonomy or external framework for LLM failure modes, how does this compare?\n\nIf you end up trying the map or the triage link in a real postmortem or runbook, I would love to hear where it feels helpful, and where it feels wrong. The whole point is to make the language around â€œwhat brokeâ€ a bit less vague for LLM / RAG pipelines.",
      "is_original_content": false,
      "link_flair_text": "Freemium :snoo_tableflip:",
      "permalink": "https://reddit.com/r/mlops/comments/1r8nnui/a_16mode_failure_map_for_llm_rag_pipelines_open/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r6yai3",
      "title": "How deeply should an SRE understand PyTorch for ML production environments?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r6yai3/how_deeply_should_an_sre_understand_pytorch_for/",
      "author": "Simple-Toe20",
      "created_utc": "2026-02-17 06:10:03",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r6yai3/how_deeply_should_an_sre_understand_pytorch_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r923zw",
      "title": "Deploy ML Models Securely on K8s: KitOps + KServe Integration Guide",
      "subreddit": "mlops",
      "url": "https://youtu.be/0gXe_q458K4",
      "author": "iamjessew",
      "created_utc": "2026-02-19 15:26:42",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r923zw/deploy_ml_models_securely_on_k8s_kitops_kserve/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rbo5pi",
      "title": "Weâ€™re seeing 8â€“10x difference between execution time and billed time on bursty LLM workloads. Is this normal?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rbo5pi/were_seeing_810x_difference_between_execution/",
      "author": "pmv143",
      "created_utc": "2026-02-22 15:07:58",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "We profiled a 25B-equivalent workload recently.\n\n\\~8 minutes actual inference time\n\n\\~100+ minutes billed time under a typical serverless setup\n\nMost of the delta was:\n\nâ€¢ Model reloads\n\nâ€¢ Idle retention between requests\n\nâ€¢ Scaling behavior\n\nFor teams running multi-model or long-tail deployments, \n\nAre you just absorbing this overhead?\n\nOr have you found a way to align billing closer to actual execution time?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rbo5pi/were_seeing_810x_difference_between_execution/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6yoi6p",
          "author": "nebulaidigital",
          "text": "Yes, that 8â€“10x gap can be â€œnormalâ€ in serverless-ish setups, but itâ€™s usually a sign youâ€™re paying for cold starts, model load, and retention policies that donâ€™t match your traffic shape. A few levers that often help: keep-warm pools for the long tail, pin a smaller set of models per endpoint (or use an in-process router), move weights to local NVMe and aggressively cache artifacts, and separate preprocessing/postprocessing from GPU-bound inference so the GPU container stays hot. If you can, measure: cold start time, model load time, queueing, and actual GPU utilization. Whatâ€™s your request interarrival distribution and max tolerated p95 latency?",
          "score": 1,
          "created_utc": "2026-02-23 15:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrblp",
              "author": "pmv143",
              "text": "That makes sense. Especially about  retention not matching traffic shape. In our case the traffic is really bursty with long idle gaps, so the keep-warm strategy feels expensive quickly.\n\nHave you seen setups that avoid warm pools entirely without eating 40â€“60s reload times?",
              "score": 1,
              "created_utc": "2026-02-23 15:29:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o712d83",
          "author": "Outrageous_Hat_9852",
          "text": "That billing/execution gap usually points to queuing delays, connection pooling issues, or the provider's internal batching - especially with bursty traffic patterns. Are you measuring wall-clock time from request start to response end, or just the actual inference time? Proper tracing (OpenTelemetry works well for this) can help you break down where those extra milliseconds are hiding - we've seen teams discover everything from DNS resolution delays to token counting overhead that wasn't obvious in basic logging.",
          "score": 1,
          "created_utc": "2026-02-23 22:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o712xw3",
              "author": "pmv143",
              "text": "Exactly. Most ppl measure model execution time but ignore end to end end wall clock. Queuing, cold starts, connection pooling, and provider batching , all these can easily dwarf the actual forward pass. \n\nThis is also why separating â€˜compute timeâ€™ from â€˜billed timeâ€™ becomes critical in bursty workloads.",
              "score": 1,
              "created_utc": "2026-02-23 22:02:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra6rff",
      "title": "OpenStack vs other entire stacks",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ra6rff/openstack_vs_other_entire_stacks/",
      "author": "No-Fig-8614",
      "created_utc": "2026-02-20 20:37:21",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "I've been looking around for the entire end to end stack for inference providing on hardware. There is OpenStack which gives a good end to end solution. I can't remember but there are others out there that have the entire end to end inference stack solution. Can anyone help me remember other stacks that are similar and opensource (even if they have the closed source add-ons for additional features). ",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1ra6rff/openstack_vs_other_entire_stacks/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6hz043",
          "author": "gscjj",
          "text": "Lllm-d or Kserve?",
          "score": 1,
          "created_utc": "2026-02-20 21:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ng18i",
          "author": "repilicus",
          "text": "Kserve is nice",
          "score": 1,
          "created_utc": "2026-02-21 19:32:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r87p2u",
      "title": "The Human Elements of the AI Foundations",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/the-human-elements-of-the-ai-foundations",
      "author": "growth_man",
      "created_utc": "2026-02-18 16:30:28",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r87p2u/the_human_elements_of_the_ai_foundations/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1raw3l1",
      "title": "I built a small library to version and compare LLM prompts (because Git wasnâ€™t enough)",
      "subreddit": "mlops",
      "url": "/r/LLMDevs/comments/1ravxjq/i_built_a_small_library_to_version_and_compare/",
      "author": "ankursrivas",
      "created_utc": "2026-02-21 16:54:39",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1raw3l1/i_built_a_small_library_to_version_and_compare/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o6nwekq",
          "author": "Internal-Tackle-1322",
          "text": "Interesting problem. In document pipelines Iâ€™ve seen prompt drift caused not only by wording changes but also by upstream dependency shifts (model version updates, temperature defaults, tokenizer changes).\n\nHave you considered versioning execution context separately from prompt text? Thatâ€™s often where reproducibility breaks down.",
          "score": 2,
          "created_utc": "2026-02-21 20:59:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oqf7f",
              "author": "ankursrivas",
              "text": "Thatâ€™s a great point â€” and I completely agree.\n\nRight now the library versions prompt text explicitly, and logs execution metadata per run (model name, latency, tokens, etc.).\n\nBut youâ€™re absolutely right that reproducibility often breaks due to execution context drift:\n\nâ€¢ Model version changes\nâ€¢ Temperature defaults\nâ€¢ Tokenizer differences\nâ€¢ Max token limits\nâ€¢ System-level prompts\n\nAt the moment, those can be logged via metadata in log(), but they arenâ€™t versioned as a first-class â€œexecution context object.â€\n\nSeparating prompt versioning from execution context versioning is something Iâ€™ve been thinking about, especially for more reproducible evaluation workflows.\n\nAppreciate you raising that â€” itâ€™s a very real issue in production pipelines.",
              "score": 2,
              "created_utc": "2026-02-21 23:46:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s6h4h",
          "author": "SpiritedChoice3706",
          "text": "Neat, I'm going to for sure flag this one. About a year ago I was experimenting with MLFlow's abilities. They might have gotten better, but basically it was solving a similar problem but within the existing MLFlow framework. Ie, you had to have an instance, and the experiment tracking format they used could get tricky with anything off HF. Basically you're tied not only to their tools, but their storage and formatting.\n\nI like how lightweight this is - it lets the user decide how they want to track and store this data, but also can be used as a one-off in notebooks. Looking forward to trying this out.",
          "score": 1,
          "created_utc": "2026-02-22 15:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s8l4o",
              "author": "ankursrivas",
              "text": "Appreciate that â€” thank you!",
              "score": 1,
              "created_utc": "2026-02-22 15:23:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9fcau",
      "title": "Need Data for MLFlow Agent",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r9fcau/need_data_for_mlflow_agent/",
      "author": "lauptimus",
      "created_utc": "2026-02-19 23:39:59",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,   \nI'm working on a project involving making an agent that can interact with MLFlow logs and provide analysis and insights into experiment runs. So far, I've been using a bit of dummy data, but it would be great if anyone would help me understand where to get some real data from.  \nI don't have compute to run a lot of DL experiments. If anyone has any logs lying around, or knows where I can find some, I'd be grateful if they can share.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r9fcau/need_data_for_mlflow_agent/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6c495h",
          "author": "data-intel-dev",
          "text": "For this case, maybe, you should use a Pycaret to create experiments easier. What do you think ?",
          "score": 1,
          "created_utc": "2026-02-20 00:12:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfsq7",
      "title": "Deploy HuggingFace Models on Databricks (Custom PyFunc End-to-End Tutorial) | Project.1",
      "subreddit": "mlops",
      "url": "https://youtu.be/m1pVXfD2yYI",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-02-23 12:11:35",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcfsq7/deploy_huggingface_models_on_databricks_custom/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    }
  ]
}