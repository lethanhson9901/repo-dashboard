{
  "metadata": {
    "last_updated": "2026-03-02 09:15:17",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 20,
    "total_comments": 42,
    "file_size_bytes": 78508
  },
  "items": [
    {
      "id": "1rcp0ad",
      "title": "Broke down our $3.2k LLM bill - 68% was preventable waste",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "author": "llamacoded",
      "created_utc": "2026-02-23 18:11:09",
      "score": 61,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "We run ML systems in production. LLM API costs hit $3,200 last month. Actually analyzed where money went.\n\n**68% - Repeat queries hitting API every time** Same questions phrased differently. \"How do I reset password\" vs \"password reset help\" vs \"can't login need reset\". All full API calls. Same answer.\n\nSemantic caching cut this by 65%. Cache similar queries based on embeddings, not exact strings.\n\n**22% - Dev/staging using production keys** QA running test suites against live APIs. One staging loop hit the API 40k times before we caught it. Burned $280.\n\nSeparate API keys per environment with hard budget caps fixed this. Dev capped at $50/day, requests stop when limit hits.\n\n**10% - Oversized context windows** Dumping 2500 tokens of docs into every request when 200 relevant tokens would work. Paying for irrelevant context.\n\nBetter RAG chunking strategy reduced this waste.\n\n**What actually helped:**\n\n* Caching layer for similar queries\n* Budget controls per environment\n* Proper context management in RAG\n\nCost optimization isn't optional at scale. It's infrastructure hygiene.\n\nWhat's your biggest LLM cost leak? Context bloat? Retry loops? Poor caching?",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6zw8xk",
          "author": "Morpheyz",
          "text": "Cut cost by 99%. External consultants sold us Azure Open AI PTUs for 50k/month, claiming we absolutely needed them for our use case. Couple months later convinced leadership to switch to pay-as-you-yo model, now spending 300$/month.\n\nEdit: PTUs, not TPUs",
          "score": 20,
          "created_utc": "2026-02-23 18:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o700ndp",
              "author": "pmv143",
              "text": "Classic overprovisioning trap. Fixed infra before validated demand is expensive. Usage based models are much more forgiving while workloads are still evolving.",
              "score": 8,
              "created_utc": "2026-02-23 18:59:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o704ptq",
              "author": "m98789",
              "text": "Azure doesn‚Äôt offer TPUs tho",
              "score": 2,
              "created_utc": "2026-02-23 19:18:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o706ns8",
                  "author": "Morpheyz",
                  "text": "Typo, I meant PTUs.",
                  "score": 3,
                  "created_utc": "2026-02-23 19:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70098v",
          "author": "pmv143",
          "text": " most ppl underestimate how much waste lives above the model. Interesting part is that even after fixing caching and RAG, infrastructure-level inefficiencies still compound at scale.",
          "score": 5,
          "created_utc": "2026-02-23 18:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o712b33",
          "author": "KeyIsNull",
          "text": "Mind to share some details about the semantic cache layer?¬†",
          "score": 2,
          "created_utc": "2026-02-23 21:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7az1td",
              "author": "llamacoded",
              "text": "[https://docs.getbifrost.ai/features/semantic-caching](https://docs.getbifrost.ai/features/semantic-caching) i use bifrost gateway you can check it out here its oss",
              "score": 2,
              "created_utc": "2026-02-25 10:39:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72zgwp",
          "author": "doolpicate",
          "text": "tiering, routing orchestration, and multiple models including LocalLLMs would have helped. Strange that people are not doing it in the beginning itself.",
          "score": 1,
          "created_utc": "2026-02-24 04:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74mikd",
          "author": "ManufacturerWeird161",
          "text": "We had a similar bleed last year where our staging environment was burning through ~$400/day in GPT-4 calls because someone left a load test running over the weekend. Took us three days to notice because the cost alerts were batched weekly. Daily caps saved us but the real fix was making the staging LLM return deterministic garbage responses for any call pattern that looked synthetic‚Äîcut costs by 90% without hurting actual QA work.",
          "score": 1,
          "created_utc": "2026-02-24 12:55:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmfdv",
          "author": "Illustrious_Echo3222",
          "text": "That 68 percent repeat query number doesn‚Äôt surprise me at all. In a lot of systems, the LLM becomes the most expensive cache miss you‚Äôve ever deployed.\n\nContext bloat has been the biggest leak I‚Äôve seen. Teams over index on ‚Äújust give it more docs‚Äù instead of tightening retrieval quality. A sloppy RAG pipeline quietly doubles or triples spend because nobody notices incremental token creep.\n\nRetry loops are another hidden killer. Especially with agents. If you allow automatic retries with minor rephrasing and no cap, you can burn a ton of tokens on what is basically the same failure mode repeated three times.\n\nOne pattern that helped us was aggressive observability at the token level. Logging prompt tokens, completion tokens, and tool calls per request, then ranking endpoints by cost per successful outcome. When you frame it as cost per resolved task instead of cost per call, waste becomes obvious.\n\nAlso agree hard on environment separation. Using production keys in staging is basically handing your burn rate to a test script.\n\nCurious if semantic caching caused any weird edge cases with slightly different intent but similar phrasing? That‚Äôs usually where people get nervous.",
          "score": 1,
          "created_utc": "2026-02-25 13:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hqvlw",
          "author": "llamacoded",
          "text": "For all the folks asking - i am currently using [bifrost](https://www.getmaxim.ai/bifrost) \\[OSS\\] as my ai gateway for semantic caching and budgeting controls. \\[personal bias\\]",
          "score": 1,
          "created_utc": "2026-02-26 10:37:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7p5ytr",
          "author": "mattiamazzoli",
          "text": "One thing that bites teams: cost per call ‚â† cost per outcome.\n\nA single retry loop or oversized shared context across chained calls can quietly 3‚Äì5x the *real* cost per completed task.\n\nCaching helps, but strict retry caps + aggressive context trimming usually move the needle even more at scale.",
          "score": 1,
          "created_utc": "2026-02-27 13:38:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p8asm",
          "author": "masterKova",
          "text": "Nice breakdown. Beyond caching, another big lever is model routing. A lot of those repeat queries (\"how do I reset password\") don't need GPT-4 class models at all. I built NadirClaw to classify prompts in ~10ms and route simple ones to cheap/local models automatically. Works as an OpenAI-compatible proxy so no code changes. Pairs well with your caching layer since the cache misses still get routed optimally. https://github.com/doramirdor/NadirClaw (author, disclosure)",
          "score": 1,
          "created_utc": "2026-02-27 13:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70to85",
          "author": "inspectedinspector",
          "text": "How much will it cost you to build embedding-based semantic caching to save $2000?",
          "score": 0,
          "created_utc": "2026-02-23 21:18:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71c9sa",
              "author": "ZestyData",
              "text": "Very little, could do it in an hour and with very low running costs. Embeddings and vector search are basically free.",
              "score": 2,
              "created_utc": "2026-02-23 22:50:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ucns",
                  "author": "inspectedinspector",
                  "text": "I assume this means you already have OpenSearch or Redis (or similar) infrastructure you can leverage? Building truly from scratch would not be free",
                  "score": 1,
                  "created_utc": "2026-02-24 19:17:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72ktbl",
              "author": "burntoutdev8291",
              "text": "from_pretrained(\"BAAI/bge-m3\")\n\nLitellm also supports semantic caching. What OP didn't mention is false positives and privacy aware semantic caches. While its an instance cost saving, anything with some form of model behind must go through its own evals\n\nSo cost of deployment of semantic cache is easy, validation is the key step.",
              "score": 1,
              "created_utc": "2026-02-24 03:03:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7az3ni",
              "author": "llamacoded",
              "text": "[https://docs.getbifrost.ai/features/semantic-caching](https://docs.getbifrost.ai/features/semantic-caching) this is what i used. its oss",
              "score": 1,
              "created_utc": "2026-02-25 10:40:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73ivt3",
          "author": "eliko613",
          "text": "This breakdown is spot on.\nIn most systems I‚Äôve looked at, model selection isn‚Äôt the primary cost driver ‚Äî it‚Äôs:\nSemantically duplicate queries with no intelligent cache\nStaging/QA hitting prod keys\nContext bloat in RAG\nRetry or loop logic nobody notices\nThe 68% repeat-query number feels very real. Once you cluster by intent instead of raw prompt strings, you usually discover a small handful of intents burning the majority of spend.\nAlso +1 on environment separation and hard caps ‚Äî that‚Äôs basic cloud hygiene, but it‚Äôs surprising how often it‚Äôs missing in LLM setups.\nLLM cost control is starting to look a lot like early cloud FinOps: visibility first, then guardrails, then optimization. I‚Äôve been exploring tools in this space (e.g., zenllm.io) that focus specifically on intent-level visibility and waste detection ‚Äî the patterns you‚Äôre describing show up immediately when you instrument properly.",
          "score": 0,
          "created_utc": "2026-02-24 07:14:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd3g0s",
      "title": "Advice Needed on a MLOps Architecture",
      "subreddit": "mlops",
      "url": "https://i.redd.it/jvzejdhcyclg1.png",
      "author": "Drac084",
      "created_utc": "2026-02-24 03:18:05",
      "score": 49,
      "num_comments": 20,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rd3g0s/advice_needed_on_a_mlops_architecture/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o72pyso",
          "author": "Competitive-Fact-313",
          "text": "If you talk overall end to end improvement there is lot you can do. From argo to grafana. I use openshift along with gitops. As long as orchestration is concerned you can use terraform n its siblings. If I get your question right.",
          "score": 3,
          "created_utc": "2026-02-24 03:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72r3jz",
          "author": "le-fou",
          "text": "I examined your diagram before I read the full post, and the two things that jumped out to me were 1) the arrows (presumably) triggering a training run with an API request and 2) the arrow going from the MLFlow tracking server to the deployment API. These correspond to your questions 1 and 3, so that is a good sign these could use some definition.\n\nFirstly, agreed that you want an orchestration layer for training. Dagster and Airflow are two common orchestration platforms, among others. Dagster has great k8s support. I haven‚Äôt used ZenML but a quick google search suggests to me it would also work fine for this. Asking AI for a comparison between these tools, given your requirements, would probably be fruitful. Regardless, this is all to say I think you‚Äôre right about needing something to orchestrate the training run. In your current diagram, for example, what exactly is hitting the endpoint? Some custom frontend? A curl command from your terminal? An orchestration framework allows you to schedule runs and/or manually trigger from a UI with your desired parameters.\n\nSecondly, the deployment process and trigger could use better definition. I personally use gitlab pipelines to build my custom model serving docker images with MLServer, and they get deployed via ArgoCD with the same CI/CD component any other non-ML app uses at my organization (I did need to write Helm charts for my MLServer deployments specifically). This pipeline could be triggered at the end of your training pipeline, or probably better you could use MLFlow aliasing/tags to fire a webhook for your deployment pipeline. But, fundamentally, building an image to serve your containers shouldn‚Äôt look functionally all that different from other build pipelines at your org, with the exception that ML containers can have some nasty dependencies and large artifact dependencies (model weights).\n\nLet me know if that all makes sense, or not.",
          "score": 3,
          "created_utc": "2026-02-24 03:42:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72th38",
              "author": "Drac084",
              "text": "Thanks for the reply!\n\nFirstly, sorry the diagram is not perfect. It was a very quick sketch of the idea.\n\n1. I was thinking to implement the Training as a independent microservice(May be a simple FastAPI server) API request will trigger a pipeline and dispatch jobs. This can later be triggered from a frontend dashboard, but not in MVP level.  This workflow the main challenge I'm trying to sort out. \n\n2. I haven't given much thought to the Deployment and inference service at this point, assuming it will be less difficult once I figured out the training service. But what you suggested also make sense. I will do some more research on this. Thanks for your input! ",
              "score": 1,
              "created_utc": "2026-02-24 03:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o72vmz1",
                  "author": "Iron-Over",
                  "text": "An orchestrator is important because it creates a simple, reusable pipeline. Airflow allows ad-hoc and scheduled runs.¬†\n\nI assume you are using production data for training? If so, how do the data scientists view the training results and test results? I assume the notebooks should be hosted instead of the production data on laptops.¬†\n\nI would log each API call, including the features and the prediction to be matched ¬†with the actual outcome. This then becomes inexpensive labeled data for future training.\n\nYou may want to include shap to view the explainability of the prediction from the features.\n\nI did not see drift and skew detection on the data and the model it is useful to know when you need to retrain.¬†\n\n\n\n",
                  "score": 2,
                  "created_utc": "2026-02-24 04:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73df41",
          "author": "prassi89",
          "text": "Overall arch looks great.\n\n1. Don‚Äôt go with dvc. When your datasets get large, you wont be able to stream (or mount) them transparently. Also data is repo bound logically. Use LakeFS directly.\n\n2. Skypilot is your best bet - it does training service APIs and compute orchestration. With  Other services like dagster, airflow you‚Äôll just spend ages debugging. Zenml is good but skypilot just gets out of the researchers way, and gives you multi cloud by default\n\n3. Mlflow also does a lot in the model promotion and deployment space. Consider it\n\nOverall, great stuff",
          "score": 5,
          "created_utc": "2026-02-24 06:26:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78q78q",
              "author": "Drac084",
              "text": "Got it. I was thinking if the LakeFS would be a overkill for this because we don't deal with insanely large datasets. But I understand your point in term of future scalability",
              "score": 1,
              "created_utc": "2026-02-25 00:51:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73cggk",
          "author": "prasanth_krishnan",
          "text": "Orchestrator - metaflow\n\nDistributed training - apache ray\n\nExperiment tracking - ml flow\n\nModel packaging - mlflow models\n\nInference endpoint - MLserver or onnx\n\nFeature store - feast with actual stores of your choice.\n\nThis is a good framework neutral platform.",
          "score": 2,
          "created_utc": "2026-02-24 06:18:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o757nde",
          "author": "ManufacturerWeird161",
          "text": "We used DVC with MinIO at my last job, it worked well for data versioning but we found MLflow was better for the actual model registry piece to track lineage.",
          "score": 2,
          "created_utc": "2026-02-24 14:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73wyir",
          "author": "htahir1",
          "text": "Great architecture sketch ‚Äî super critical to break things into distinct services early on.\n\nOn the orchestration question: since you're already on K8s and planning to extend to Slurm, you'll want something that abstracts away the infrastructure layer so your researchers aren't writing YAML all day. I've seen people have good experiences with Dagster and Kubeflow for this, but I'd also suggest taking a serious look at ZenML ‚Äî full disclosure, I'm part of the ZenML team, so take this with the appropriate grain of salt.\n\nThat said, the reason I think it's worth evaluating here specifically is that ZenML was designed to be a framework-agnostic orchestration layer that plugs into the tools you're already using (MLflow, K8s, S3/MinIO) rather than replacing them. So you'd keep your MLflow tracking, your MinIO storage, your K8s cluster ‚Äî ZenML just becomes the connective tissue that defines and runs your pipelines across all of it. It also plays nicely with the \"microservices\" mental model you're going for.\n\nA couple of non-ZenML-related thoughts too:\n\n* \\+1 to what others said about drift/skew detection ‚Äî worth thinking about early even if you don't implement it in your MVP.\n* The comment about LakeFS over DVC is worth considering, especially at scale with large datasets and streaming use cases.\n* For the deployment side, I'd honestly keep it simple at first. Honestly for smaller models use MLflow serving or even wrap in in a FastAPI, and then graduate to more complex services later \n\nGood luck with the build ‚Äî sounds like a fun project!",
          "score": 2,
          "created_utc": "2026-02-24 09:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pv51",
              "author": "Drac084",
              "text": "Thanks, I should look into ZenML eco system. Do you think the free version is enough for me to try it out and get an idea?\n\n",
              "score": 1,
              "created_utc": "2026-02-25 00:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79vlx1",
                  "author": "htahir1",
                  "text": "Yes most of the non enterprisey features are oss , Pro only has governance and enterprise focused features for bigger teams you can adopt those later if at all",
                  "score": 2,
                  "created_utc": "2026-02-25 04:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o763ev8",
          "author": "alex000kim",
          "text": "Hey, imo, the overall approach is fine. I also agree with most of the feedback others have left. Leaving some of mine:\n\n\\- Simply stitching services together might not be the hardest part. What really requires some thinking is making it secure. I.e. the whole authentication flow from data to infra to model/artifact registry to deployment. Your diagram doesn‚Äôt show any of this.\n\n\\- A few things are undefined in the diagram: there‚Äôs no clear data path from S3/MinIO into the actual training pods, the ‚ÄúModel Selection‚Äù arrow from MLflow to your Deployment Service has no trigger mechanism (manual? webhook? CI pipeline?), and Slurm is mentioned in the text but completely absent from the diagram with no abstraction layer between K8s and Slurm.\n\n\\- That yellow ‚ÄúTraining Service API‚Äù box (job queue, state manager, scheduling, logs) is essentially an entire orchestration platform you‚Äôd be building from scratch. Worth thinking about whether you really want to own that.\n\n\\- Reconsider MinIO since the open-source project has been archived [https://news.ycombinator.com/item?id=47000041](https://news.ycombinator.com/item?id=47000041)\n\n\\- SkyPilot is really the way to go if you already have K8s and plan on adding Slurm into the mix. You write one task YAML and it works on both. When Slurm comes online you reuse existing task definitions instead of rewriting pipelines. Since the resources will be shared between team members, you‚Äôll most likely need to deploy and manage the central SkyPilot API server.\n\n\\- SkyPilot also has SkyServe [https://docs.skypilot.co/en/stable/serving/sky-serve.html](https://docs.skypilot.co/en/stable/serving/sky-serve.html) for the deployment/inference side. Add a service: block to a YAML and you get autoscaling, load balancing, and rolling updates. Worth evaluating before building a custom deployment service.",
          "score": 2,
          "created_utc": "2026-02-24 17:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pk54",
              "author": "Drac084",
              "text": "Thanks for the input.\n\nYes I haven't included slurm in the diagram because it's a future addition. But there should be a abstraction layer for compute infra/job dispatching to slurm/k8 within Training Service component. ",
              "score": 1,
              "created_utc": "2026-02-25 00:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7a1z8s",
          "author": "PleasantAd6868",
          "text": "training service api, would recommend jobset or kubeflow trainer CRDS (if you are already on k8s which looks like it from your diagram). if you need a resource manager + gang scheduling, either kueue or volcano. would not recommend more bloated options (i.e. Ray, skypilot, zenML) unless ur doing something super exotic with heterogeneous resources",
          "score": 1,
          "created_utc": "2026-02-25 05:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gwfin",
          "author": "princess-barnacle",
          "text": "Just use flyte.",
          "score": 1,
          "created_utc": "2026-02-26 05:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uykkq",
          "author": "Ashamed_Birthday_346",
          "text": "Goods",
          "score": 1,
          "created_utc": "2026-02-28 10:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uyoun",
          "author": "Ashamed_Birthday_346",
          "text": "Interessting",
          "score": 1,
          "created_utc": "2026-02-28 10:41:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wk34v",
          "author": "Gaussianperson",
          "text": "Your three service split is a solid start, but building them all from scratch might be more work than you think. Since you already have MinIO and Kubernetes, you should look at MLflow for the registry part. It integrates with MinIO easily and handles versioning for you. For the training side, if you plan to move toward Slurm later, look into orchestrators like Metaflow or Argo. They can manage the handoffs between your data and compute clusters without forcing you to write everything from zero.\n\nI actually cover these types of system design choices in my newsletter at machinelearningatscale.substack.com. I focus on the engineering side of building and scaling production systems, including how to handle infrastructure when things get complex. It might give you some ideas on how to bridge the gap between k8s and Slurm as you grow.",
          "score": 1,
          "created_utc": "2026-02-28 16:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75x4pf",
          "author": "thulcan",
          "text": "Your real problem isn't which orchestrator to pick ‚Äî it's that you have five systems (DVC, MLflow, Harbor, MinIO, custom APIs) that each own a piece of what \"this model version\" means. That's five places where lineage breaks.\n\nModelKits ([KitOps](https://kitops.org), CNCF Sandbox) fix this at the artifact layer. A ModelKit is an OCI artifact ‚Äî same format as your Docker images ‚Äî that packages weights, dataset refs, config, and code with a Kitfile manifest. You already run Harbor and MinIO. Harbor becomes your single registry for images, models, and datasets. No new infrastructure.\n\nWhat changes:\n\n**DVC ‚Üí gone.** `kit pack` your datasets, push to Harbor. Versioning is OCI tags. No LakeFS either.\n\n**MLflow ‚Üí experiment tracking only.** Drop MLflow Model Registry and MLflow deployment. Harbor + ModelKits is your registry. MLflow is great for experiment tracking UI and bad at everything else it tries to do.\n\n**Training orchestration ‚Üí Argo Workflows.** CNCF graduated, K8s-native. Pipeline: `kit unpack` ‚Üí train ‚Üí `kit pack` ‚Üí `kit push`. Stop building a custom Training Service API with job queues and state managers. That's a multi-year project you don't need.\n\n**Governance gate (you're missing this).** Between trained and deployed: run ModelScan, attach cosign attestations, tag as `:approved`. You're a research org managing lots of models ‚Äî provenance isn't optional, and nobody in this thread mentioned it.\n\n**Deployment Service API ‚Üí gone.** KitOps has a native KServe `ClusterStorageContainer` integration. KServe pulls ModelKits directly from Harbor via OCI reference. No artifact retrieval logic, no container initialization code. Point KServe at [`harbor.yourorg.com/models/my-model:approved`](http://harbor.yourorg.com/models/my-model:approved), done.\n\nYou're currently stitching together DVC + MLflow Registry + MLflow Tracking + Harbor + MinIO + two custom APIs and hoping they agree on what \"model v2.3\" means. That's a lot of coordination surfaces to keep in sync. With KitOps: Harbor is your single source of truth, Argo runs your pipelines, MLflow tracks your experiments. Three tools, each doing one job. And you get security and provenance your current architecture doesn't even attempt.",
          "score": 1,
          "created_utc": "2026-02-24 16:48:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgxyxo",
      "title": "DevOps Engineer collab with ML Engineer",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rgxyxo/devops_engineer_collab_with_ml_engineer/",
      "author": "DevOpsYeah",
      "created_utc": "2026-02-28 09:05:27",
      "score": 19,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI‚Äôm a DevOps Engineer looking to break into the MLOps space, and I figured the best way to do that is to find someone to collaborate with.\n\nWhat I bring to the table:\n\nI have hands-on experience building and managing Kubernetes clusters, GitOps workflows with ArgoCD, and full observability stacks (Prometheus, Grafana, Loki, ELK). I‚Äôm comfortable with infrastructure-as-code, Helm charts,  Cert management, and CI/CD pipelines ‚Äî essentially the full platform engineering toolkit.\n\nWhat I don‚Äôt have is a machine learning model that needs deploying. That‚Äôs where you come in.\n\nWhat I‚Äôm looking for:\n\nA data scientist or ML engineer who has models sitting in notebooks or local environments with no clear path to production. Someone who‚Äôs more interested in the data and the science than wrestling with Kubernetes manifests and deployment pipelines.\n\nWhat I can offer your project:\n\n\t\n\n‚àô\tModel Serving Infrastructure ‚Äî Containerised deployments on Kubernetes with proper resource management and GPU/TPU scheduling\n\n\t\n\n‚àô\tCI/CD Pipelines ‚Äî Automated training, testing, and deployment workflows so your model goes from commit to production reliably\n\n\t\n\n‚àô\tScaling ‚Äî Horizontal and vertical autoscaling so your inference endpoints handle real traffic without falling over\n\n\t\n\n‚àô\tObservability ‚Äî Full monitoring stack covering model latency, error rates, resource utilisation, and custom metrics\n\n\t\n\n‚àô\tData & Model Drift Detection ‚Äî Automated checks to flag when your model‚Äôs performance starts degrading against live data\n\n\t\n\n‚àô\tReproducibility ‚Äî Versioned environments, tracked experiments, and infrastructure defined in code\n\nI‚Äôm not looking for payment ‚Äî this is about building a portfolio of real MLOps work and learning the ML side of things along the way. Happy to work on anything from a side project to something more ambitious.\n\nIf you‚Äôve got a model gathering dust and want to see it running in production with proper infrastructure behind it, drop me a DM or comment below.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rgxyxo/devops_engineer_collab_with_ml_engineer/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o81b77b",
          "author": "Ashamed_Birthday_346",
          "text": "goof\n\n![gif](giphy|ihMmN2T4lfPh499eXd)",
          "score": 1,
          "created_utc": "2026-03-01 11:16:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdpbkd",
      "title": "Wrote a guide to building an ML research cluster. Feedback appreciated.",
      "subreddit": "mlops",
      "url": "https://i.redd.it/5bpvizk9qhlg1.png",
      "author": "aliasaria",
      "created_utc": "2026-02-24 19:04:24",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rdpbkd/wrote_a_guide_to_building_an_ml_research_cluster/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bb1cn",
          "author": "radarsat1",
          "text": "This is nice. At a small startup we were working with a couple of machines in the \"multiuser, single workstation\" configuration and it was ok, but after buying a couple more machines working this way became very annoying. We worked towards something like what you are recommending here with k3s but never fully figured it out, probably could have used a guide like this!",
          "score": 2,
          "created_utc": "2026-02-25 12:16:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf7idu",
      "title": "If you're coming from infra/DevOps and confused about what vLLM actually solves ‚Äî here's the before and after",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf7idu/if_youre_coming_from_infradevops_and_confused/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-26 11:16:28",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.57,
      "text": "Had a pretty standard LLM setup, HuggingFace transformers, FastAPI, model on GPU. Worked great in dev. Then the prod traffic hit, and everything fell apart. Latency spiking to 15s+, GPU memory creeping up, OOM kills every few hours, pod restarts taking 3 mins while requests pile up. On-call was rough.\n\n**What was actually going wrong:**\n\n* HuggingFace `model.generate()` is blocked. One request at a time. 10 users = 9 waiting.\n* KV cache pre-allocates for the max sequence length, even if the user needs 50 tokens. Over time, fragmentation builds up ‚Üí OOM. Same energy as over-provisioning PVCs on every pod.\n* Static batching waits for the slowest request. A 500-token generation holds up a 20-token one.\n\n**What fixed it:**\n\nSwapped the serving layer to vLLM. Continuous batching (requests don't wait for each other) + PagedAttention (GPU memory managed in pages like virtual memory, no fragmentation). Core issues gone.\n\nThe gotchas nobody talks about:\n\n* Set `gpu-memory-utilization` to 0.85-0.90, not higher. Leave headroom.\n* Model warm-up is real ‚Äî first requests after startup are slow (CUDA kernel compilation). Send dummy requests before marking the pod ready.\n* The readiness probe should check whether the model is loaded, not just whether the process is running. Ask me how I know.\n* Set hard timeouts on generation length. One runaway request shouldn't block everything.\n* Shadow traffic first, then canary at 10%, then ramp up. Boring but safe.\n\n**Result:** Latency 45s ‚Üí 10-15s. Concurrency 2-3 ‚Üí 15-20 per GPU. OOM crashes ‚Üí zero. None of this needed transformer math, just infra skills applied to ML.\n\nWrote a detailed version on Medium with diagrams and code: [https://medium.com/@thevarunfreelance/if-youre-from-infra-devops-and-confused-about-what-vllm-actually-solves-here-s-the-before-and-9e0eeca9f344?postPublishedType=initial](https://medium.com/@thevarunfreelance/if-youre-from-infra-devops-and-confused-about-what-vllm-actually-solves-here-s-the-before-and-9e0eeca9f344?postPublishedType=initial)\n\nAlso been through this transition myself, helped a few others with resumes and interview prep along the way. If you're on a similar path, DMs open or grab time here: [topmate.io/varun\\_rajput\\_1914](http://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rf7idu/if_youre_coming_from_infradevops_and_confused/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7hzos0",
          "author": "Historical-One7058",
          "text": "AI slop",
          "score": 28,
          "created_utc": "2026-02-26 11:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ijnrq",
              "author": "greysteppenwolf",
              "text": "Yeah I‚Äôm so tired that every time I see this sub on my feed the post is 100% AI. Like I get we work with LLMs directly but can‚Äôt you just add some human touch at least",
              "score": 2,
              "created_utc": "2026-02-26 13:58:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ip65h",
                  "author": "Extension_Key_5970",
                  "text": "My main goal is to educate the MLOps community on real-world problems. This is the first time I've received critical feedback. Thanks for that. Usually, I write my content by myself and use LLM for spell checks and grammar, but it seems there was a change in the model, which overpolished the content, making it an AI-generated\n\nWell know I made an edit to make it more simple and concise, so that more people can connect",
                  "score": -6,
                  "created_utc": "2026-02-26 14:27:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7n1qli",
              "author": "vantasmer",
              "text": "I‚Äôm so tired of every post I click on being AI trash.¬†",
              "score": 1,
              "created_utc": "2026-02-27 03:37:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jggg2",
          "author": "idjos",
          "text": "Also consider baking in models and those big docker images with the AMI if using AWS. \n\nIt does introduce complexity on the CI/CD side, but helps a lot reducing cold start.\n\nOne more thing worth following is [nvidia chrek](https://docs.nvidia.com/dynamo/dev/kubernetes-deployment/deployment-guide/checkpointing). It‚Äôs still experimental feature, so read very carefully before investing time in it, especially if security is a big concern for you.\n\nDisclaimer: disn‚Äôt read the article yet.",
          "score": 2,
          "created_utc": "2026-02-26 16:37:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhf2iy",
      "title": "how was your journey to become an mlops engineer",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rhf2iy/how_was_your_journey_to_become_an_mlops_engineer/",
      "author": "Economy-Outside3932",
      "created_utc": "2026-02-28 21:49:21",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "hello, I've been wondering how to be or what path to follow to be an mlops engineer as i heard its not an entry level role",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rhf2iy/how_was_your_journey_to_become_an_mlops_engineer/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7ymues",
          "author": "symphonicdev",
          "text": "Mine was rather unplanned.\n- I graduated with a Master in ML in Sweden. Got an internship as an ML Engineer at a good company, doing machine learning for search model. I got a returned offer after the internship, but it was shortly rescinded as the team didn't have a budget for new engineers.\n- Then the tech mass layoff happened. I couldn't find any job at those product companies. And I got into an AI engineer position at a small consulting company. There, only people with a PhD can work in the modelling part, so I learned to do MLOps there\n- Since then, I've tried to apply for ML Engineer positions again and again, but so far have failed as I don't have enough skills and experiences building models for real business.\n- At the same time, I've got better and better at MLOps, building infra for data scientists and ML engineers, landed Senior MLOps positions. So I keep doing MLOps.\n\nHope someday I can do the model part of ML.",
          "score": 9,
          "created_utc": "2026-02-28 23:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z5eka",
          "author": "CalmWorld1688",
          "text": "My story is that no one else could do it and I had to deploy models, somehow.\n\nI graduated in 2023 from a robotics related MSc, where I‚Äôve had a few optimization and ML courses and quickly landed a job in a small manufacturing company as an AI Engineer.\n\nFrom internships and part-time work I knew how to build different kind of models including: time-series, CNNs, trees, doing optimizations and basically navigating myself through the modeling part.\nBut then, I had difficulties to deploy and release those models, especially GPU ones where latency mattered so I started learning MLOps through reading books.\n\nSince the company was small, we had one dedicated DevOps engineer who was responsible for managing our servers, so I kept asking him different kind of deployment related questions and then the pandora box opened.\n\nI found out that it is not an easy thing to do, and that to have reliable systems you need to: version datasets, track experiments, monitor performance metrics and drift, manage offline and online inference and etc. which the more I dig into it the more interesting it sounded.\n\nSo a year later I got a side gig to design and build an ML platform for a small team of data scientists for them to have somewhere where to deploy and track their models - and after going through this myself once I‚Äôve found it quite intuitive and interesting, as I knew what pieces of the puzzle I needed and how things had to interact with each other to make it all work. I guess this marked the beginning of my MLOps journey.\n\nA few months later my side gig was over and with that all-around experience I landed a Senior MLE role for a SaaS company, where I do full-stack ML development from modeling to MLOps. Currently I am considering transitioning fully to MLOps as I don‚Äôt find modeling as fun anymore. Don‚Äôt know why, but perhaps got a bit tired of all the modeling and AI hype and started to personally value infrastructure side of things and ‚Äúenabling‚Äù others more.",
          "score": 4,
          "created_utc": "2026-03-01 01:06:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1riej7v",
      "title": "How can I learn MLOps while working?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1riej7v/how_can_i_learn_mlops_while_working/",
      "author": "Plus_Cardiologist540",
      "created_utc": "2026-03-02 00:48:59",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 0.79,
      "text": "I just started as an MLOps Jr. This is my first job, as my background and experience are more academic.\n\nI work at a startup where almost everyone is a Jr. We are just two MLOps and four DS. Our lead/manager/whatever is a DE, so they have more experience in that area rather than with models and productizing them.\n\nI feel things are done on the fly, and everything is messy. Model deployment, training, and monitoring are all manual... from what I have read, I would say we are more on a level 0 of MLOps.\n\nDS doesn't know much about deployment. Before I started working here, they deployed models on Jupyter Notebooks and didn't use something like MLflow.\n\nI mean, I get it, I'm just a junior, and all my coworkers might have more experience than me (since I don't have any).\n\nBut how can I really learn? I mean, sure, I get paid and everything, and I'm also learning on the fly, but I feel I'm not learning and not contributing that much (I have only 4 months working).\n\nSo, how do I really learn when my team doesn't know that much of MLOps? I have been reading some blogs and I'm doing some Datacamp courses but I feel is not enough:(",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1riej7v/how_can_i_learn_mlops_while_working/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o873tr1",
          "author": "BeerBatteredHemroids",
          "text": "You're on the wrong team. You don't know anything, your lead doesn't know anything and your colleague is just as green as you. You're going to pick up bad habits and not even know it. \n\nSmall start-ups are not really fresher friendly. You need to put yourself onto an established team at an established firm that has been successfully deploying production models for a while to learn how its done.",
          "score": 2,
          "created_utc": "2026-03-02 08:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85fyxw",
          "author": "Idllnox",
          "text": "Get a license of Weights & Biases, fine tune some open source models, bam. It'll help a ton.",
          "score": -1,
          "created_utc": "2026-03-02 00:57:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o873wg2",
              "author": "BeerBatteredHemroids",
              "text": "An MLOps engineer does not fine-tune models...",
              "score": 2,
              "created_utc": "2026-03-02 08:16:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdi7jr",
      "title": "Why do agent testing frameworks assume developers will write all the test cases?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rdi7jr/why_do_agent_testing_frameworks_assume_developers/",
      "author": "Outrageous_Hat_9852",
      "created_utc": "2026-02-24 14:49:29",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.78,
      "text": "Most AI testing tools I've seen are built for engineers to write test scripts and run evaluations. But in practice, the people who best understand what good AI behavior looks like are often domain experts, product managers, or subject matter specialists.   \n  \nFor example, if you're building a customer service agent, your support team lead probably has better intuition about edge cases and problematic responses than your ML engineer. If you're building a legal document analyzer, your legal team knows what constitutes accurate analysis. Yet most testing workflows require technical people to translate domain knowledge into code.  \n  \n This creates a bottleneck and often loses important nuances in translation. Has anyone found good ways to involve non-technical stakeholders directly in the testing process?  \n  \nI'm thinking beyond just \"review the results\" but actually contributing to test design and acceptance criteria.",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rdi7jr/why_do_agent_testing_frameworks_assume_developers/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o75baju",
          "author": "penguinzb1",
          "text": "the translation problem is real, but there's a second issue underneath it: even with good domain expert input, the test set usually only covers the cases they can articulate. the failures that matter are the ones nobody anticipated.\n\nwhat's worked for us: give domain experts access to simulated versions of their actual workflows and let them just run the agent. they don't need to write scenarios, they surface the gaps themselves as they go. 'it never should have done that' is better input than anything you'd get from a spec written in advance.",
          "score": 3,
          "created_utc": "2026-02-24 15:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7592ap",
          "author": "QuoteBackground6525",
          "text": "Yes! We had the same issue with our customer service AI. Our support team knew exactly what kinds of tricky customer requests would break the system, but translating that knowledge into test code was always a bottleneck. Now our support lead connects their runbooks and FAQ docs, describes problematic scenarios in plain language, and we get comprehensive test coverage including adversarial cases. The key was finding a platform that treats testing as a cross-functional activity rather than just a developer task. Much more effective than the old approach of engineers guessing what good behavior looks like.",
          "score": 2,
          "created_utc": "2026-02-24 14:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o759d2l",
              "author": "Outrageous_Hat_9852",
              "text": "Uh, interesting! Any tools you have been using for this that were helpful?",
              "score": 1,
              "created_utc": "2026-02-24 14:59:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7blztw",
          "author": "Illustrious_Echo3222",
          "text": "This is such a real bottleneck. A lot of agent testing frameworks feel like classic unit testing tools with an LLM wrapper, which assumes the engineer both defines and encodes ‚Äúcorrectness.‚Äù But for most agent use cases, correctness is domain shaped, not purely technical.\n\nWhat I‚Äôve seen work better is separating test authoring from test execution.\n\nInstead of asking domain experts to write code, give them structured ways to define:\n\n* Example scenarios in plain language\n* ‚ÄúGood vs bad‚Äù response pairs\n* Acceptance rubrics with weighted criteria\n\nThen have engineers translate those into executable evals or, better yet, build a thin layer that auto-generates test cases from structured forms. Basically, treat domain experts like product owners of a spec, not passive reviewers of outputs.\n\nAnother useful pattern is gold conversation capture. Let SMEs flag real transcripts as ‚Äúideal,‚Äù ‚Äúborderline,‚Äù or ‚Äúfail,‚Äù and continuously sample from production logs for evaluation sets. That keeps nuance intact because it‚Äôs grounded in real behavior, not hypothetical test cases.\n\nI also think pair-review style workflows help. Domain expert defines the intent and failure boundaries. Engineer encodes it. Then both review eval drift over time. It becomes collaborative rather than translational.\n\nThe deeper issue is that most MLOps tooling inherited assumptions from deterministic systems. Agents are probabilistic and contextual. That means testing has to look more like policy validation and behavioral auditing than strict input-output assertions.\n\nCurious if you‚Äôre exploring tooling here or just noticing the gap. It feels like there‚Äôs space for much better human-in-the-loop eval design.",
          "score": 2,
          "created_utc": "2026-02-25 13:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bnowe",
              "author": "Outrageous_Hat_9852",
              "text": "Thanks, this helps! I am exploring tools right now, via lists like this: [https://github.com/kelvins/awesome-mlops](https://github.com/kelvins/awesome-mlops)\n\nOne that I came across that puts an emphasis on collaboration and SMEs in particular is this: [https://github.com/rhesis-ai/rhesis](https://github.com/rhesis-ai/rhesis)",
              "score": 1,
              "created_utc": "2026-02-25 13:36:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7mactc",
          "author": "gudruert",
          "text": "I totally get that - letting domain experts run the agent sounds way more insightful than just relying on engineers!",
          "score": 2,
          "created_utc": "2026-02-27 00:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76w5a3",
          "author": "Downtown-Height5899",
          "text": "Use BDD framework",
          "score": 1,
          "created_utc": "2026-02-24 19:25:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg8i2t",
      "title": "Has anyone deployed vision models that don‚Äôt operate on raw pixels? Our experience with TAPe + ML and resource savings",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rg8i2t/has_anyone_deployed_vision_models_that_dont/",
      "author": "oopatow",
      "created_utc": "2026-02-27 14:42:16",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "Most of the MLOps work I see around CV assumes a familiar stack: images ‚Üí pixels ‚Üí CNN/ViT ‚Üí task‚Äëspecific heads. Training and serving costs are then optimized around that. I‚Äôm working on a computer vision setup where the model never sees raw pixels. Images are first transformed into a structured representation: a set of elements with predefined relations between them (coming from the our Theory of Active Perception, TAPe).¬†We‚Äôve been experimenting with a different setup. Images are first converted into a structured representation (TAPe elements with known relations), a single TAPe‚Äëadapted backbone (TAPe+ML) operates in that space and feeds classification / segmentation / detection / clustering tasks.\n\nFrom an ops perspective, several things changed:\n\n1) Training converges with significantly fewer samples. In a DINO iBOT‚Äëlike setup, a TAPe‚Äëbased model converged on 9k images where standard DINO did not converge even on 120k.\n\nhttps://preview.redd.it/duqdgan6u1mg1.png?width=904&format=png&auto=webp&s=1f81d544adc697fd375a5ba368c844817a8461fd\n\n2) Smaller models are sufficient. A 3‚Äëlayer 516k‚Äëparam CNN on TAPe data reached \\~92% accuracy on Imagenette using 10% of the data, while the same model on raw pixels plateaued around 47%.\n\n3) The same backbone can be reused across tasks, which simplifies model inventory and maintenance.\n\nThe preprocessing step that turns pixels into TAPe elements is proprietary, but the downstream part looks like a regular deployment: one backbone, multiple heads, standard serving patterns.\n\nI‚Äôm wondering if anyone here has worked with similar ‚Äúnon‚Äëpixel‚Äù representations in production CV systems. Maybe seen measurable gains in infra cost or reliability from unifying the backbone across tasks? Also need strong opinions on where the main operational risks are in such pipelines.\n\nAny war stories or pointers would be appreciated.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rg8i2t/has_anyone_deployed_vision_models_that_dont/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1ri2djo",
      "title": "Nvidia certs",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ri2djo/nvidia_certs/",
      "author": "automation495",
      "created_utc": "2026-03-01 16:57:15",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I would like to know about these and specially if they have any value in the market. do employers like to see this cert? or it would be better to focus on something else?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1ri2djo/nvidia_certs/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o83pu0j",
          "author": "chunky_lover92",
          "text": "I see postings that ask for the certs.  The question is how many certs are there compared to how many jobs are asking.  My guess is 1000:1.  ",
          "score": 3,
          "created_utc": "2026-03-01 19:21:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o865l7a",
          "author": "Relative-Average7159",
          "text": "I just passed my first AI and Ops cert.",
          "score": 1,
          "created_utc": "2026-03-02 03:37:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfsq7",
      "title": "Deploy HuggingFace Models on Databricks (Custom PyFunc End-to-End Tutorial) | Project.1",
      "subreddit": "mlops",
      "url": "https://youtu.be/m1pVXfD2yYI",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-02-23 12:11:35",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcfsq7/deploy_huggingface_models_on_databricks_custom/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rf3k3b",
      "title": "aimlopsmasters.in anyone heard about their devops to mlops courses? Any honest reviews will be helpful.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf3k3b/aimlopsmastersin_anyone_heard_about_their_devops/",
      "author": "Fun-Collar1645",
      "created_utc": "2026-02-26 07:13:11",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rf3k3b/aimlopsmastersin_anyone_heard_about_their_devops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1refao3",
      "title": "3.6 YOE Node/Angular dev exploring GenAI upskilling ‚Äî need guidance",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1refao3/36_yoe_nodeangular_dev_exploring_genai_upskilling/",
      "author": "BrickOwn8974",
      "created_utc": "2026-02-25 14:51:54",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "\n\nHi everyone,\nI have around 3.6 years of experience working with Node.js, Angular, and SQL in a product-based environment. Due to limited growth opportunities internally, I‚Äôm currently exploring options to switch roles.\nWhile preparing, I‚Äôve been evaluating whether adding GenAI skills would meaningfully improve my profile in the current market. My tentative plan over the next few months is:\nLearn practical GenAI development (APIs, RAG, integrations, etc.)\nBuild 2‚Äì3 projects combining my existing stack with AI\nPossibly complete an Azure GenAI certification\nSince my background is primarily full-stack/backend (not ML), I wanted to understand from people already working in this space:\nFor developers with similar experience, which GenAI skills are actually valued by recruiters right now?\nAre certifications useful, or do projects + existing experience matter more?\nAny suggestions on project ideas that helped you get interviews?\nI‚Äôm mainly trying to evaluate where to invest effort for the best ROI while switching.\nWould appreciate insights from anyone who has gone through a similar transition.\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1refao3/36_yoe_nodeangular_dev_exploring_genai_upskilling/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rfo18z",
      "title": "Guidance for choosing between fullstack vs ml infra",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfo18z/guidance_for_choosing_between_fullstack_vs_ml/",
      "author": "AdSoggy6915",
      "created_utc": "2026-02-26 22:06:17",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.71,
      "text": "I am working as a senior frontend engineer at a Robotics Company. Their core products are robots and generate revenue from warehouse automation and are now entering the advanced robotics stage with humanoid robots and robodogs(quadrupeds). They are fine tuning a 3 billion parameter Gemma model and diffusion and flow matching model for VLA(vision language action) for use in robots to work in manufacturing plants. Currently they are generating 0.6TB of data per month to train the model through imitation learning and plan to generate 6Tb of data per month in the next three months. They do not have any proper processes for these but are planning to create a data warehouse for this data and want to train new models using this stored data and might also do whatever processing required on this dataset. Due to lack of processes I am not very sure how they will be successful at this task. I have recently received an offer from a Bangalore based fashion ecommerce startup for full stack developer where I willl get to work on nextjs on the frontend and nodejs on the backend with chances of working on their ai use case of scraping fashion data from the web and generating designs using ai and that data. I feel this new opportunity will provide growth for system architect role and their application has more than 10,000 daily active users and high growth potential and real tech. when I was about to resign my manager offered me to work on the ML infra/ data warehouse pipeline they are planning. I am extremely confused as to what I should do now. Working on an ML infra or data pipeline task might be an extremely rare chance for me to get into this field and therefore has made me extremely confused for what should I choose. Therefore I wanted your guidance on how real this opportunity of ML infra might be and if it will even be relevant from the perspective of big tech. There is a single gpu that we have right now I guess it is nvidia A6000 and is being used to fine tune 3 billion parameter Gemma model and they will be buying more of such gpu and servers for storage. Without much guidance and only with online resources how beneficial will working on such a system be. Should I stay at my current company in hopes of learning ML infra or should I move to the new company where I will definitely get a good system experience. I am also not sure how soon they will be upgrading with those extra gpus and servers, they also do not have any senior backend engineer for setting up the data pipeline till now, and the vla pipeline with pytorch and inference stack of vllm and action encoder is created by junior swes and they are storing the generated data in csvs and raw images on hard disks for now. If I continue here and try to create these pipelines, will it be a valuable experience from big tech companies perspective or will it be like a college project which just uses my time and provides no ROI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rfo18z/guidance_for_choosing_between_fullstack_vs_ml/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7lo6xo",
          "author": "ydmatos",
          "text": "What do you what for your career ?",
          "score": 1,
          "created_utc": "2026-02-26 22:55:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lowln",
              "author": "AdSoggy6915",
              "text": "i have always been fascinated by at scale systems, but there are too many fullstack engineers in the market with system knowledge, i guess ML infra would put me in a field with less competitors and more benefit, but i am not sure how beneficial this opportunity is from an ML infra or data engineering perspective. i definitely want ai proof skills, less competition, good compensation and challenging problems to solve as i get bored from repetition.",
              "score": 1,
              "created_utc": "2026-02-26 22:59:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p3pfm",
                  "author": "ydmatos",
                  "text": "do you have that scale systems in ML infra too, rigth now is a hot area. I will don't bet my career in just being a fullstack engineer. ",
                  "score": 1,
                  "created_utc": "2026-02-27 13:25:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tp53f",
                  "author": "SpiritedChoice3706",
                  "text": "I'll be real, any sort of Infra is a better bet than plain full stack. AI is changing the field as we speak. Infra, while AI can write the code, still needs a lot more higher-level thinking and hands-on working than full-stack dev.",
                  "score": 1,
                  "created_utc": "2026-02-28 04:07:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rfzimp",
      "title": "Is every enterprise agent just a pile of custom safety code right now?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfzimp/is_every_enterprise_agent_just_a_pile_of_custom/",
      "author": "Loud_Cauliflower_928",
      "created_utc": "2026-02-27 06:49:42",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "I've been looking at how different B2B teams are actually shipping agents lately and I keep seeing the same pattern. It feels like everyone is spending half their time building the \"boring\" operational stuff instead of the actual AI. I'm talking about things like hard-coding kill switches, building custom spend-limit triggers, and making bespoke approval flows so an agent doesn't do something crazy without a human seeing it first.\n\nIt works fine for a first version, but I‚Äôm really starting to wonder how this scales. If you have three different teams building three different agents, you end up with three different ways of handling audit logs and security. It feels like we're reinventing the wheel every single time just to keep the agents safe and predictable.\n\nFor the people here who are actually deploying this in regulated industries or bigger companies, are you really just building custom wrappers for every agent you ship? Or are you starting to move toward some kind of shared infrastructure or a central gateway to manage the runtime controls? I‚Äôm trying to figure out if I‚Äôm just overthinking the scaling problem or if we‚Äôre all collectively white-knuckling it until a standard way to manage these things finally shows up.",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rfzimp/is_every_enterprise_agent_just_a_pile_of_custom/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7rk0hr",
          "author": "dinkinflika0",
          "text": "You're describing exactly why we built a central gateway layer. Spend limits, approval flows, audit logs, kill switches - all in one place instead of every team reinventing it per agent. Agent calls go through gateway, gateway enforces the rules. Add a new agent, same controls apply automatically. We open-sourced ours as [Bifrost](https://getmax.im/bifrost-home) \\- handles budget caps, tool approval gates, request logging. Beats three teams building three custom wrappers.",
          "score": 2,
          "created_utc": "2026-02-27 20:40:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7toxs2",
          "author": "SpiritedChoice3706",
          "text": "Gateways are essential to any LLM-as-a-service setup. LiteLLM has a good one you can start with and stand up on any old k8s cluster. Guardrails, rate limiting, IAM, etc...\n\nHonestly, ML is very different from pure SWE. But the structures are very similar at a high level. The things that you'd use an API gateway for, you can use an LLM gateway for - you just have to understand how LLMs are different and what their needs are.",
          "score": 1,
          "created_utc": "2026-02-28 04:05:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfb7kl",
      "title": "Observations on LLM-as-judge calibration in safety/alignment tasks ‚Äî 10 months of data suggests ceiling effects compress inter-rater reliability",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfb7kl/observations_on_llmasjudge_calibration_in/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-02-26 14:12:42",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "I've been running a blind peer evaluation setup for about 10 months ‚Äî each model in a pool evaluates all other models' responses to the same prompt without knowing which model produced them (The Multivac project). Today's evaluation produced results I want to get input on from people who've thought carefully about LLM-as-judge reliability.\n\n**The calibration problem I'm observing:**\n\nIn meta-alignment tasks (where the correct answer is unambiguous ‚Äî e.g., \"don't confirm lethal misinformation\"), the evaluation compresses. All competent models score in the 9.3‚Äì9.9 range. This creates two problems:\n\n1. **Judge ceiling effects:** Gemini 3 Pro averaged 9.97 out of 10 across all non-outlier models. That's essentially no discrimination. Grok 3 Direct averaged 8.43. The 1.54-point spread between strictest and most lenient judge is roughly 3.5x the spread between rank-1 and rank-9 models. The judges are generating more variance than the respondents.\n2. **The outlier distortion:** One model (GPT-OSS-120B) scored 4.70 with œÉ=3.12. Its response began with \"comply.\" before a safety layer intervened. Five judges scored it 0.20‚Äì5.60. Three scored it 5.10‚Äì8.65. The bimodal distribution reflects genuine disagreement about whether \"comply.\" changes the meaning of a response that ultimately refuses ‚Äî not noise.\n\n**Today's eval data:**\n\n|Model|Score|œÉ|Judges' avg given|\n|:-|:-|:-|:-|\n|DeepSeek V3.2|9.83|0.20|9.11|\n|Claude Sonnet|9.64|0.24|9.47|\n|Grok 3 Direct|9.63|0.24|8.43|\n|...|...|...|...|\n|GPT-OSS-120B|4.70|3.12|9.31|\n\n(Full table in methodology notes)\n\n**Inter-rater reliability concern:** Krippendorff's Œ± on the top-9 models only would be reasonable given tight clustering. Including GPT-OSS-120B, the outlier inflates apparent reliability because every judge correctly differentiates it from the pack ‚Äî creating spurious agreement. I haven't run formal IRR stats on this; it's on the to-do list.\n\n**What I've tried:**\n\n* Category-specific judge weights (didn't help ‚Äî the ceiling effect is in the model, not the weight)\n* Bradley-Terry model for pairwise rankings (preserves top-9 order; does not resolve the calibration spread between strict and lenient judges)\n* Rubric versioning (v3.1 currently) ‚Äî adding a \"manipulation-resistance\" dimension specifically for adversarial prompts, in development\n\n**Genuine technical questions:**\n\n1. Has anyone found a reliable way to calibrate LLM judges in categories where ground truth is binary but response quality varies? The rubric needs to differentiate among responses that are all \"correct\" but differ in depth/usefulness.\n2. For the bimodal GPT-OSS-120B scores ‚Äî is there a statistical test that distinguishes \"bimodal due to genuine construct disagreement\" from \"bimodal due to judge calibration differences\"? My intuition says the two can't be cleanly separated here.\n3. What approaches have you found for mitigating positional bias in multi-judge LLM setups? I'm currently using randomized response ordering per judge, but I haven't been able to measure the effect size.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rfb7kl/observations_on_llmasjudge_calibration_in/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rf2una",
      "title": "Anyone else seeing ‚ÄúGPU node looks healthy but training/inference fails until reboot‚Äù?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf2una/anyone_else_seeing_gpu_node_looks_healthy_but/",
      "author": "Chika5105",
      "created_utc": "2026-02-26 06:32:30",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "We keep hitting a frustrating class of failures on GPU clusters:\n\nNode is up. Metrics look normal. NVML/DCGM look fine.\nBut distributed training/inference jobs stall, hang, crash ‚Äî and a reboot ‚Äúfixes‚Äù it.\n\nIt feels like something is degrading below the usual device metrics, and it only surfaces once you‚Äôve already burned a lot of compute (or you start doubting the results).\n\nI‚Äôve been digging into correlating lower-level signals across:\nGPU ‚Üî PCIe ‚Üî CPU/NUMA ‚Üî memory + kernel events\n\nTrying to understand whether certain patterns (AER noise, Xids, ECC drift, NUMA imbalance, driver resets, PCIe replay rates, etc.) show up before the node becomes unusable.\n\nIf you‚Äôve debugged this ‚Äúlooks healthy but isn‚Äôt‚Äù class of issue:\n- What were the real root causes?\n- What signals were actually predictive?\n- What turned out to be red herrings?\n\nDo not include any links.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rf2una/anyone_else_seeing_gpu_node_looks_healthy_but/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rfvrbo",
      "title": "Making clinical AI models auditable and reproducible ‚Äì my final-year project",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfvrbo/making_clinical_ai_models_auditable_and/",
      "author": "hypergraphr",
      "created_utc": "2026-02-27 03:34:01",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI‚Äôd like to share a project I‚Äôve been developing as part of my final-year project: a clinical AI decision auditing system. It‚Äôs designed to audit, replay, and analyze ML workflows in healthcare, making model behavior transparent, reproducible, and auditable.\n\nThe motivation is addressing the ‚Äúblack box‚Äù problem of many healthcare AI models. The system produces integrity-checked logs and governance-oriented analytics, helping researchers and developers understand how models arrive at decisions and ensuring trustworthiness in clinical workflows.\n\nI‚Äôd love to get feedback from the community, especially from those working on auditable AI, ML governance, or clinical AI applications.\n\nThe code and examples are available here for anyone interested: https://github.com/fikayoAy/ifayAuditDashHealth",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rfvrbo/making_clinical_ai_models_auditable_and/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rh3l0a",
      "title": "Heosphoros Hyperparameter Optimization 4 review",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/gallery/1rh3l0a",
      "author": "quantum_chosen",
      "created_utc": "2026-02-28 14:09:35",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rh3l0a/heosphoros_hyperparameter_optimization_4_review/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rgognq",
      "title": "Transition from SWE to AI ML Infra , MLops, AI engineer roles",
      "subreddit": "mlops",
      "url": "/r/learnmachinelearning/comments/1rgmdec/transition_from_swe_to_ai_ml_infra_mlops_ai/",
      "author": "DqDPLC",
      "created_utc": "2026-02-28 00:50:25",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rgognq/transition_from_swe_to_ai_ml_infra_mlops_ai/",
      "domain": "",
      "is_self": false,
      "comments": []
    }
  ]
}