{
  "metadata": {
    "last_updated": "2026-02-25 17:22:58",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 14,
    "total_comments": 44,
    "file_size_bytes": 73306
  },
  "items": [
    {
      "id": "1r8v6v5",
      "title": "Friendly advice for infra engineers moving to MLOps: your Python scripting may not enough, here's the gap to close",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r8v6v5/friendly_advice_for_infra_engineers_moving_to/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-19 09:54:09",
      "score": 63,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "In my last post, I covered ML foundations. This one's about Python, specifically, the gap between \"I know Python\" and the Python you actually need for MLOps.\n\nIf you're from infra/DevOps, your Python probably looks like mine did: boto3 scripts, automation glue, maybe some Ansible helpers. That's scripting. MLOps needs programming, and the difference matters.\n\n**What you're probably missing:**\n\n* **Decorators & closures** ‚Äî ML frameworks live on these. Airflow's \\`@tasks\\`, FastAPI's \\`@app.get()\\`. If you can't write a custom decorator, you'll struggle to read any ML codebase.\n* **Generators** ‚Äî You can't load 10M records into memory. Generators let you stream data lazily. Every ML pipeline uses this.\n* **Context managers** ‚Äî GPU contexts, model loading/unloading, DB connections. The `with` Pattern is everywhere.\n\n**Why memory management suddenly matters:**\n\nIn infra, your script runs for 5 seconds and exits. In ML, you're loading multi-GB models into servers that run for weeks. You need to understand Python's garbage collector, the difference between a Python list and a NumPy array, and the GPU memory lifecycle.\n\n**Async isn't optional:**\n\nFastAPI is async-first. Inference backends require you to understand when to use asyncio, multiprocessing, or threading, and why it matters for ML workloads.\n\n**Best way to learn all this?** Don't read a textbook. Build an inference backend from scratch, load a Hugging Face model, wrap it in FastAPI, add batching, profile memory under load, and make it handle 10K requests. Each step targets the exact Python skills you're missing.\n\nThe uncomfortable truth: you can orchestrate everything with K8s and Helm, but the moment something breaks *inside* the inference service, you're staring at Python you can't debug. That's the gap. Close it.\n\nIf anyone interested in detailed version, with an atual scenarios covering WHYs and code snippets please refer: [https://medium.com/@thevarunfreelance/friendly-advice-for-infra-engineers-moving-to-mlops-your-python-scripting-isnt-enough-here-s-f2f82439c519](https://medium.com/@thevarunfreelance/friendly-advice-for-infra-engineers-moving-to-mlops-your-python-scripting-isnt-enough-here-s-f2f82439c519)\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:¬†[topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r8v6v5/friendly_advice_for_infra_engineers_moving_to/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o67u125",
          "author": "pmv143",
          "text": "Totally. The difference shows up fast when you‚Äôre running real inference workloads. A five second boto3 script mindset doesn‚Äôt translate to managing GPU memory, batching, async request handling, and long-lived model state.",
          "score": 12,
          "created_utc": "2026-02-19 10:02:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68u8a6",
          "author": "Ancient_Canary1148",
          "text": "as in DevOps,im not coding applications or api but helping Dev teams to build,deploy,run and observe. why do i need to learn deep python ml programming to be an MlOps? as infra engineer,im helping ml teams to run models,prepare infra for them (kafka,ml flow,flink) and etc.",
          "score": 5,
          "created_utc": "2026-02-19 14:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d8bdf",
              "author": "Extension_Key_5970",
              "text": "That's a fair point, and honestly, you're not wrong. If you're in a pure infra role, the toolset is completely different, and that work is genuinely valuable. ML teams need someone to set up Kafka, MLflow, Flink, and the K8S layer.\n\nBut here's where MLOps gets tricky, the line is blurred. In traditional DevOps, you don't touch the app code. Clear boundary. In MLOps, that boundary keeps breaking. One day, you're debugging why an inference service is leaking memory, or why a pipeline DAG is failing, and the answer isn't in the infrastructure; it's in the Python running on top of it.\n\nYou don't need to become a developer. But knowing enough Python to read, debug, and make sense of what's running on your infra, that's the difference. Both paths are valid; it just depends on where you want to grow.",
              "score": 3,
              "created_utc": "2026-02-20 04:22:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dz26b",
                  "author": "burntoutdev8291",
                  "text": "Depends on the team, I am the other way, developer / AI engineer to MLOps. Sometimes the lines are abit blurry, but my senior mentioned we need to know when to draw the line because deployment friendly code is on the developer. Otherwise very soon MLOps needs to help deploy jupyter notebooks.\n\nOur job is to reduce toil on the developer and solve infrastructure related problems. Because from what you are saying, I also need to know how to debug rust, go, python, ts, fortran depending on what the team uses. Python is easy enough that most MLOps can learn it but not the rest.",
                  "score": 1,
                  "created_utc": "2026-02-20 08:07:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6hdt76",
              "author": "Useful-Process9033",
              "text": "You are right that you do not need to become a Python expert to do MLOps from the infra side. But understanding how model serving works, how GPU memory behaves, and how to observe inference latency will make you 10x more effective at supporting ML teams than just provisioning Kafka clusters.",
              "score": 2,
              "created_utc": "2026-02-20 20:04:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68klzr",
          "author": "TranslatorSalt1668",
          "text": "Great. Exactly what I was looking for. Thanks",
          "score": 1,
          "created_utc": "2026-02-19 13:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h3lqz",
          "author": "bedel99",
          "text": "It sounds easy !",
          "score": 1,
          "created_utc": "2026-02-20 19:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s7kz1",
          "author": "SpiritedChoice3706",
          "text": "Absolutely. It's going to vary based on the role, but I'm a consultant, and how much pure infra I'm doing depends on the client. Right now I'm mostly in Kubernetes, standing LLMs up on GPUs. But last project, I was helping a rather built-out data platform deploy their first-ever recommendation model. I had to build an API for serving recs with real-time constraints, and also a retraining and monitoring pipeline. My main tools were FastAPI and Airflow, because that's what the client used. Tons of Python, and because we were working with lots of data, some real constraints in how we did things. It's going to vary from role-to-role, but if you're serving a model in production, you gotta know Python, because that's what your data scientists will be writing that model in.",
          "score": 1,
          "created_utc": "2026-02-22 15:19:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcp0ad",
      "title": "Broke down our $3.2k LLM bill - 68% was preventable waste",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "author": "llamacoded",
      "created_utc": "2026-02-23 18:11:09",
      "score": 57,
      "num_comments": 17,
      "upvote_ratio": 0.96,
      "text": "We run ML systems in production. LLM API costs hit $3,200 last month. Actually analyzed where money went.\n\n**68% - Repeat queries hitting API every time** Same questions phrased differently. \"How do I reset password\" vs \"password reset help\" vs \"can't login need reset\". All full API calls. Same answer.\n\nSemantic caching cut this by 65%. Cache similar queries based on embeddings, not exact strings.\n\n**22% - Dev/staging using production keys** QA running test suites against live APIs. One staging loop hit the API 40k times before we caught it. Burned $280.\n\nSeparate API keys per environment with hard budget caps fixed this. Dev capped at $50/day, requests stop when limit hits.\n\n**10% - Oversized context windows** Dumping 2500 tokens of docs into every request when 200 relevant tokens would work. Paying for irrelevant context.\n\nBetter RAG chunking strategy reduced this waste.\n\n**What actually helped:**\n\n* Caching layer for similar queries\n* Budget controls per environment\n* Proper context management in RAG\n\nCost optimization isn't optional at scale. It's infrastructure hygiene.\n\nWhat's your biggest LLM cost leak? Context bloat? Retry loops? Poor caching?",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6zw8xk",
          "author": "Morpheyz",
          "text": "Cut cost by 99%. External consultants sold us Azure Open AI PTUs for 50k/month, claiming we absolutely needed them for our use case. Couple months later convinced leadership to switch to pay-as-you-yo model, now spending 300$/month.\n\nEdit: PTUs, not TPUs",
          "score": 20,
          "created_utc": "2026-02-23 18:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o700ndp",
              "author": "pmv143",
              "text": "Classic overprovisioning trap. Fixed infra before validated demand is expensive. Usage based models are much more forgiving while workloads are still evolving.",
              "score": 6,
              "created_utc": "2026-02-23 18:59:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o704ptq",
              "author": "m98789",
              "text": "Azure doesn‚Äôt offer TPUs tho",
              "score": 2,
              "created_utc": "2026-02-23 19:18:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o706ns8",
                  "author": "Morpheyz",
                  "text": "Typo, I meant PTUs.",
                  "score": 3,
                  "created_utc": "2026-02-23 19:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70098v",
          "author": "pmv143",
          "text": " most ppl underestimate how much waste lives above the model. Interesting part is that even after fixing caching and RAG, infrastructure-level inefficiencies still compound at scale.",
          "score": 5,
          "created_utc": "2026-02-23 18:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o712b33",
          "author": "KeyIsNull",
          "text": "Mind to share some details about the semantic cache layer?¬†",
          "score": 2,
          "created_utc": "2026-02-23 21:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7az1td",
              "author": "llamacoded",
              "text": "[https://docs.getbifrost.ai/features/semantic-caching](https://docs.getbifrost.ai/features/semantic-caching) i use bifrost gateway you can check it out here its oss",
              "score": 2,
              "created_utc": "2026-02-25 10:39:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72zgwp",
          "author": "doolpicate",
          "text": "tiering, routing orchestration, and multiple models including LocalLLMs would have helped. Strange that people are not doing it in the beginning itself.",
          "score": 1,
          "created_utc": "2026-02-24 04:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ivt3",
          "author": "eliko613",
          "text": "This breakdown is spot on.\nIn most systems I‚Äôve looked at, model selection isn‚Äôt the primary cost driver ‚Äî it‚Äôs:\nSemantically duplicate queries with no intelligent cache\nStaging/QA hitting prod keys\nContext bloat in RAG\nRetry or loop logic nobody notices\nThe 68% repeat-query number feels very real. Once you cluster by intent instead of raw prompt strings, you usually discover a small handful of intents burning the majority of spend.\nAlso +1 on environment separation and hard caps ‚Äî that‚Äôs basic cloud hygiene, but it‚Äôs surprising how often it‚Äôs missing in LLM setups.\nLLM cost control is starting to look a lot like early cloud FinOps: visibility first, then guardrails, then optimization. I‚Äôve been exploring tools in this space (e.g., zenllm.io) that focus specifically on intent-level visibility and waste detection ‚Äî the patterns you‚Äôre describing show up immediately when you instrument properly.",
          "score": 1,
          "created_utc": "2026-02-24 07:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74mikd",
          "author": "ManufacturerWeird161",
          "text": "We had a similar bleed last year where our staging environment was burning through ~$400/day in GPT-4 calls because someone left a load test running over the weekend. Took us three days to notice because the cost alerts were batched weekly. Daily caps saved us but the real fix was making the staging LLM return deterministic garbage responses for any call pattern that looked synthetic‚Äîcut costs by 90% without hurting actual QA work.",
          "score": 1,
          "created_utc": "2026-02-24 12:55:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmfdv",
          "author": "Illustrious_Echo3222",
          "text": "That 68 percent repeat query number doesn‚Äôt surprise me at all. In a lot of systems, the LLM becomes the most expensive cache miss you‚Äôve ever deployed.\n\nContext bloat has been the biggest leak I‚Äôve seen. Teams over index on ‚Äújust give it more docs‚Äù instead of tightening retrieval quality. A sloppy RAG pipeline quietly doubles or triples spend because nobody notices incremental token creep.\n\nRetry loops are another hidden killer. Especially with agents. If you allow automatic retries with minor rephrasing and no cap, you can burn a ton of tokens on what is basically the same failure mode repeated three times.\n\nOne pattern that helped us was aggressive observability at the token level. Logging prompt tokens, completion tokens, and tool calls per request, then ranking endpoints by cost per successful outcome. When you frame it as cost per resolved task instead of cost per call, waste becomes obvious.\n\nAlso agree hard on environment separation. Using production keys in staging is basically handing your burn rate to a test script.\n\nCurious if semantic caching caused any weird edge cases with slightly different intent but similar phrasing? That‚Äôs usually where people get nervous.",
          "score": 1,
          "created_utc": "2026-02-25 13:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70to85",
          "author": "inspectedinspector",
          "text": "How much will it cost you to build embedding-based semantic caching to save $2000?",
          "score": 0,
          "created_utc": "2026-02-23 21:18:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71c9sa",
              "author": "ZestyData",
              "text": "Very little, could do it in an hour and with very low running costs. Embeddings and vector search are basically free.",
              "score": 2,
              "created_utc": "2026-02-23 22:50:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ucns",
                  "author": "inspectedinspector",
                  "text": "I assume this means you already have OpenSearch or Redis (or similar) infrastructure you can leverage? Building truly from scratch would not be free",
                  "score": 1,
                  "created_utc": "2026-02-24 19:17:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72ktbl",
              "author": "burntoutdev8291",
              "text": "from_pretrained(\"BAAI/bge-m3\")\n\nLitellm also supports semantic caching. What OP didn't mention is false positives and privacy aware semantic caches. While its an instance cost saving, anything with some form of model behind must go through its own evals\n\nSo cost of deployment of semantic cache is easy, validation is the key step.",
              "score": 1,
              "created_utc": "2026-02-24 03:03:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7az3ni",
              "author": "llamacoded",
              "text": "[https://docs.getbifrost.ai/features/semantic-caching](https://docs.getbifrost.ai/features/semantic-caching) this is what i used. its oss",
              "score": 1,
              "created_utc": "2026-02-25 10:40:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rd3g0s",
      "title": "Advice Needed on a MLOps Architecture",
      "subreddit": "mlops",
      "url": "https://i.redd.it/jvzejdhcyclg1.png",
      "author": "Drac084",
      "created_utc": "2026-02-24 03:18:05",
      "score": 48,
      "num_comments": 16,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rd3g0s/advice_needed_on_a_mlops_architecture/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o72pyso",
          "author": "Competitive-Fact-313",
          "text": "If you talk overall end to end improvement there is lot you can do. From argo to grafana. I use openshift along with gitops. As long as orchestration is concerned you can use terraform n its siblings. If I get your question right.",
          "score": 3,
          "created_utc": "2026-02-24 03:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72r3jz",
          "author": "le-fou",
          "text": "I examined your diagram before I read the full post, and the two things that jumped out to me were 1) the arrows (presumably) triggering a training run with an API request and 2) the arrow going from the MLFlow tracking server to the deployment API. These correspond to your questions 1 and 3, so that is a good sign these could use some definition.\n\nFirstly, agreed that you want an orchestration layer for training. Dagster and Airflow are two common orchestration platforms, among others. Dagster has great k8s support. I haven‚Äôt used ZenML but a quick google search suggests to me it would also work fine for this. Asking AI for a comparison between these tools, given your requirements, would probably be fruitful. Regardless, this is all to say I think you‚Äôre right about needing something to orchestrate the training run. In your current diagram, for example, what exactly is hitting the endpoint? Some custom frontend? A curl command from your terminal? An orchestration framework allows you to schedule runs and/or manually trigger from a UI with your desired parameters.\n\nSecondly, the deployment process and trigger could use better definition. I personally use gitlab pipelines to build my custom model serving docker images with MLServer, and they get deployed via ArgoCD with the same CI/CD component any other non-ML app uses at my organization (I did need to write Helm charts for my MLServer deployments specifically). This pipeline could be triggered at the end of your training pipeline, or probably better you could use MLFlow aliasing/tags to fire a webhook for your deployment pipeline. But, fundamentally, building an image to serve your containers shouldn‚Äôt look functionally all that different from other build pipelines at your org, with the exception that ML containers can have some nasty dependencies and large artifact dependencies (model weights).\n\nLet me know if that all makes sense, or not.",
          "score": 3,
          "created_utc": "2026-02-24 03:42:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72th38",
              "author": "Drac084",
              "text": "Thanks for the reply!\n\nFirstly, sorry the diagram is not perfect. It was a very quick sketch of the idea.\n\n1. I was thinking to implement the Training as a independent microservice(May be a simple FastAPI server) API request will trigger a pipeline and dispatch jobs. This can later be triggered from a frontend dashboard, but not in MVP level.  This workflow the main challenge I'm trying to sort out. \n\n2. I haven't given much thought to the Deployment and inference service at this point, assuming it will be less difficult once I figured out the training service. But what you suggested also make sense. I will do some more research on this. Thanks for your input! ",
              "score": 1,
              "created_utc": "2026-02-24 03:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o72vmz1",
                  "author": "Iron-Over",
                  "text": "An orchestrator is important because it creates a simple, reusable pipeline. Airflow allows ad-hoc and scheduled runs.¬†\n\nI assume you are using production data for training? If so, how do the data scientists view the training results and test results? I assume the notebooks should be hosted instead of the production data on laptops.¬†\n\nI would log each API call, including the features and the prediction to be matched ¬†with the actual outcome. This then becomes inexpensive labeled data for future training.\n\nYou may want to include shap to view the explainability of the prediction from the features.\n\nI did not see drift and skew detection on the data and the model it is useful to know when you need to retrain.¬†\n\n\n\n",
                  "score": 2,
                  "created_utc": "2026-02-24 04:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73df41",
          "author": "prassi89",
          "text": "Overall arch looks great.\n\n1. Don‚Äôt go with dvc. When your datasets get large, you wont be able to stream (or mount) them transparently. Also data is repo bound logically. Use LakeFS directly.\n\n2. Skypilot is your best bet - it does training service APIs and compute orchestration. With  Other services like dagster, airflow you‚Äôll just spend ages debugging. Zenml is good but skypilot just gets out of the researchers way, and gives you multi cloud by default\n\n3. Mlflow also does a lot in the model promotion and deployment space. Consider it\n\nOverall, great stuff",
          "score": 5,
          "created_utc": "2026-02-24 06:26:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78q78q",
              "author": "Drac084",
              "text": "Got it. I was thinking if the LakeFS would be a overkill for this because we don't deal with insanely large datasets. But I understand your point in term of future scalability",
              "score": 1,
              "created_utc": "2026-02-25 00:51:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73cggk",
          "author": "prasanth_krishnan",
          "text": "Orchestrator - metaflow\n\nDistributed training - apache ray\n\nExperiment tracking - ml flow\n\nModel packaging - mlflow models\n\nInference endpoint - MLserver or onnx\n\nFeature store - feast with actual stores of your choice.\n\nThis is a good framework neutral platform.",
          "score": 2,
          "created_utc": "2026-02-24 06:18:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o757nde",
          "author": "ManufacturerWeird161",
          "text": "We used DVC with MinIO at my last job, it worked well for data versioning but we found MLflow was better for the actual model registry piece to track lineage.",
          "score": 2,
          "created_utc": "2026-02-24 14:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73wyir",
          "author": "htahir1",
          "text": "Great architecture sketch ‚Äî super critical to break things into distinct services early on.\n\nOn the orchestration question: since you're already on K8s and planning to extend to Slurm, you'll want something that abstracts away the infrastructure layer so your researchers aren't writing YAML all day. I've seen people have good experiences with Dagster and Kubeflow for this, but I'd also suggest taking a serious look at ZenML ‚Äî full disclosure, I'm part of the ZenML team, so take this with the appropriate grain of salt.\n\nThat said, the reason I think it's worth evaluating here specifically is that ZenML was designed to be a framework-agnostic orchestration layer that plugs into the tools you're already using (MLflow, K8s, S3/MinIO) rather than replacing them. So you'd keep your MLflow tracking, your MinIO storage, your K8s cluster ‚Äî ZenML just becomes the connective tissue that defines and runs your pipelines across all of it. It also plays nicely with the \"microservices\" mental model you're going for.\n\nA couple of non-ZenML-related thoughts too:\n\n* \\+1 to what others said about drift/skew detection ‚Äî worth thinking about early even if you don't implement it in your MVP.\n* The comment about LakeFS over DVC is worth considering, especially at scale with large datasets and streaming use cases.\n* For the deployment side, I'd honestly keep it simple at first. Honestly for smaller models use MLflow serving or even wrap in in a FastAPI, and then graduate to more complex services later \n\nGood luck with the build ‚Äî sounds like a fun project!",
          "score": 2,
          "created_utc": "2026-02-24 09:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pv51",
              "author": "Drac084",
              "text": "Thanks, I should look into ZenML eco system. Do you think the free version is enough for me to try it out and get an idea?\n\n",
              "score": 1,
              "created_utc": "2026-02-25 00:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79vlx1",
                  "author": "htahir1",
                  "text": "Yes most of the non enterprisey features are oss , Pro only has governance and enterprise focused features for bigger teams you can adopt those later if at all",
                  "score": 2,
                  "created_utc": "2026-02-25 04:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o763ev8",
          "author": "alex000kim",
          "text": "Hey, imo, the overall approach is fine. I also agree with most of the feedback others have left. Leaving some of mine:\n\n\\- Simply stitching services together might not be the hardest part. What really requires some thinking is making it secure. I.e. the whole authentication flow from data to infra to model/artifact registry to deployment. Your diagram doesn‚Äôt show any of this.\n\n\\- A few things are undefined in the diagram: there‚Äôs no clear data path from S3/MinIO into the actual training pods, the ‚ÄúModel Selection‚Äù arrow from MLflow to your Deployment Service has no trigger mechanism (manual? webhook? CI pipeline?), and Slurm is mentioned in the text but completely absent from the diagram with no abstraction layer between K8s and Slurm.\n\n\\- That yellow ‚ÄúTraining Service API‚Äù box (job queue, state manager, scheduling, logs) is essentially an entire orchestration platform you‚Äôd be building from scratch. Worth thinking about whether you really want to own that.\n\n\\- Reconsider MinIO since the open-source project has been archived [https://news.ycombinator.com/item?id=47000041](https://news.ycombinator.com/item?id=47000041)\n\n\\- SkyPilot is really the way to go if you already have K8s and plan on adding Slurm into the mix. You write one task YAML and it works on both. When Slurm comes online you reuse existing task definitions instead of rewriting pipelines. Since the resources will be shared between team members, you‚Äôll most likely need to deploy and manage the central SkyPilot API server.\n\n\\- SkyPilot also has SkyServe [https://docs.skypilot.co/en/stable/serving/sky-serve.html](https://docs.skypilot.co/en/stable/serving/sky-serve.html) for the deployment/inference side. Add a service: block to a YAML and you get autoscaling, load balancing, and rolling updates. Worth evaluating before building a custom deployment service.",
          "score": 2,
          "created_utc": "2026-02-24 17:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pk54",
              "author": "Drac084",
              "text": "Thanks for the input.\n\nYes I haven't included slurm in the diagram because it's a future addition. But there should be a abstraction layer for compute infra/job dispatching to slurm/k8 within Training Service component. ",
              "score": 1,
              "created_utc": "2026-02-25 00:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7a1z8s",
          "author": "PleasantAd6868",
          "text": "training service api, would recommend jobset or kubeflow trainer CRDS (if you are already on k8s which looks like it from your diagram). if you need a resource manager + gang scheduling, either kueue or volcano. would not recommend more bloated options (i.e. Ray, skypilot, zenML) unless ur doing something super exotic with heterogeneous resources",
          "score": 1,
          "created_utc": "2026-02-25 05:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75x4pf",
          "author": "thulcan",
          "text": "Your real problem isn't which orchestrator to pick ‚Äî it's that you have five systems (DVC, MLflow, Harbor, MinIO, custom APIs) that each own a piece of what \"this model version\" means. That's five places where lineage breaks.\n\nModelKits ([KitOps](https://kitops.org), CNCF Sandbox) fix this at the artifact layer. A ModelKit is an OCI artifact ‚Äî same format as your Docker images ‚Äî that packages weights, dataset refs, config, and code with a Kitfile manifest. You already run Harbor and MinIO. Harbor becomes your single registry for images, models, and datasets. No new infrastructure.\n\nWhat changes:\n\n**DVC ‚Üí gone.** `kit pack` your datasets, push to Harbor. Versioning is OCI tags. No LakeFS either.\n\n**MLflow ‚Üí experiment tracking only.** Drop MLflow Model Registry and MLflow deployment. Harbor + ModelKits is your registry. MLflow is great for experiment tracking UI and bad at everything else it tries to do.\n\n**Training orchestration ‚Üí Argo Workflows.** CNCF graduated, K8s-native. Pipeline: `kit unpack` ‚Üí train ‚Üí `kit pack` ‚Üí `kit push`. Stop building a custom Training Service API with job queues and state managers. That's a multi-year project you don't need.\n\n**Governance gate (you're missing this).** Between trained and deployed: run ModelScan, attach cosign attestations, tag as `:approved`. You're a research org managing lots of models ‚Äî provenance isn't optional, and nobody in this thread mentioned it.\n\n**Deployment Service API ‚Üí gone.** KitOps has a native KServe `ClusterStorageContainer` integration. KServe pulls ModelKits directly from Harbor via OCI reference. No artifact retrieval logic, no container initialization code. Point KServe at [`harbor.yourorg.com/models/my-model:approved`](http://harbor.yourorg.com/models/my-model:approved), done.\n\nYou're currently stitching together DVC + MLflow Registry + MLflow Tracking + Harbor + MinIO + two custom APIs and hoping they agree on what \"model v2.3\" means. That's a lot of coordination surfaces to keep in sync. With KitOps: Harbor is your single source of truth, Argo runs your pipelines, MLflow tracks your experiments. Three tools, each doing one job. And you get security and provenance your current architecture doesn't even attempt.",
          "score": 1,
          "created_utc": "2026-02-24 16:48:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ravnd2",
      "title": "Cleared NVIDIA NCA-AIIO - Next Target: NCP-AII",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ravnd2/cleared_nvidia_ncaaiio_next_target_ncpaii/",
      "author": "TuckerSavannah1",
      "created_utc": "2026-02-21 16:37:26",
      "score": 19,
      "num_comments": 15,
      "upvote_ratio": 0.96,
      "text": "Hello Everyone\n\nGlad to share that I‚Äôve successfully cleared the NVIDIA NCA-AIIO (AI Infrastructure & Operations) exam!\n\nMy journey was focused on building strong fundamentals in GPUs, networking, and AI infrastructure concepts. I avoided rote learning and concentrated on understanding how things actually work. Practice tests from itexamscerts also played a big role, they helped me identify weak areas and improve my confidence before the exam. Overall, if your basics are clear, the exam is very manageable.\n\nNow I‚Äôm preparing for NVIDIA NCP-AII, and I would really appreciate guidance from those who have cleared it.\n\n\\* How tough is it compared to NCA-AIIO?\n\n\\* Is it more hands-on or CLI/lab focused?\n\n\\* Any recommended labs?y\n\nI look forward to your valuable insights. Thank you.",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1ravnd2/cleared_nvidia_ncaaiio_next_target_ncpaii/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6n5oab",
          "author": "RubySera1",
          "text": "Well done and congrats! Could you please share which practice tests you found most useful for NCA-AIIO?",
          "score": 3,
          "created_utc": "2026-02-21 18:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n7xk9",
              "author": "TuckerSavannah1",
              "text": "itexamscerts.",
              "score": 2,
              "created_utc": "2026-02-21 18:52:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zio9a",
          "author": "Sure-Programmer-8462",
          "text": "Nice achievement. Were the NCA-AIIO exam questions more theoretical or based on real world scenarios?",
          "score": 2,
          "created_utc": "2026-02-23 17:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zjnod",
              "author": "TuckerSavannah1",
              "text": "The exam was a mix of both, but many questions were scenario-based. If you understand the core concepts and practical use cases, it becomes much easier to handle.",
              "score": 1,
              "created_utc": "2026-02-23 17:42:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zlaws",
                  "author": "Sure-Programmer-8462",
                  "text": "Did you find the exam objectives closely aligned with the official syllabus or were there some unexpected topics?",
                  "score": 2,
                  "created_utc": "2026-02-23 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nzjbf",
          "author": "Satsuma_Johnson",
          "text": "congrats..",
          "score": 1,
          "created_utc": "2026-02-21 21:15:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh2d5",
              "author": "TuckerSavannah1",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-23 17:29:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tcfen",
          "author": "Wright_Lucy11",
          "text": "congrats.",
          "score": 1,
          "created_utc": "2026-02-22 18:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh3ne",
              "author": "TuckerSavannah1",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-23 17:30:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zidpo",
          "author": "GrapeThompson",
          "text": "Congratulations on clearing the NCA-AIIO exam! How long did you take to prepare for it, and did you already have experience with AI infrastructure before starting?",
          "score": 1,
          "created_utc": "2026-02-23 17:36:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zj98g",
              "author": "TuckerSavannah1",
              "text": "Thank you! It took me around 4‚Äì5 weeks of consistent study. I had some basic knowledge of networking and virtualization, but I was new to AI infrastructure concepts. I focused on understanding GPU architecture and deployment scenarios.",
              "score": 1,
              "created_utc": "2026-02-23 17:40:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra4fky",
      "title": "Preparing for ML System Design Round (Fraud Detection / E-commerce Abuse) ‚Äì Need Guidance (4 Days Left)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ra4fky/preparing_for_ml_system_design_round_fraud/",
      "author": "SuccessfulStorm5342",
      "created_utc": "2026-02-20 19:09:16",
      "score": 8,
      "num_comments": 10,
      "upvote_ratio": 0.79,
      "text": "Hey everyone,\n\nI am a final year [B.Tech](http://B.Tech) student and I have an **ML System Design interview in 4 days** at a startup focused on **e-commerce fraud and return abuse detection**. They use ML for things like:\n\n* Detecting return fraud (e.g., customer buys a real item, returns a fake)\n* Multi-account detection / identity linking across emails, devices, IPs\n* Serial returner risk scoring\n* Coupon / bot abuse\n* Graph-based fraud detection and customer behavior risk scoring\n\nI have solid ML fundamentals but haven‚Äôt worked in fraud detection specifically. I‚Äôm trying to prep hard in the time I have.\n\n# What I‚Äôm looking for:\n\n**1. What are the most important topics I absolutely should not miss when preparing for this kind of interview?**  \nPlease prioritize.\n\n**2. Any good resources (blogs, papers, videos, courses)?**\n\n**3. Any advice on how to approach the preparation itself?**  \nAny guidance is appreciated.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1ra4fky/preparing_for_ml_system_design_round_fraud/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6j8thz",
          "author": "DGSPJS",
          "text": "I used to be PM for an MLOps platform for fraud detection models.\n\nSome areas I'd stress are:  \nHandling highly imbalanced datasets - a company being absolutely battered by fraud is still only experiencing maybe a couple % of transactions as fraud and I've seen models deployed for 1:1,000,000 cases.\n\nModel retraining loops in the face of a delayed / irregular feedback loop (false positives might be worked out in minutes, false negatives can take months to be fully reported).\n\nModel optimization and threshold selection based on dollar value of transactions rather than number of transactions, and potentially accounting for the cost of frustrated customers with false positives.\n\nModel explainability techniques for understanding what types of fraud are being experienced and identifying if new types of attacks are emerging.\n\nGood luck.",
          "score": 7,
          "created_utc": "2026-02-21 02:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k7dxr",
              "author": "SuccessfulStorm5342",
              "text": "Thanks a lot for sharing this.",
              "score": 1,
              "created_utc": "2026-02-21 06:19:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kxzy1",
          "author": "Gaussianperson",
          "text": "Since you only have four days, focus on feature engineering and latency. Fraud systems usually live or die by features like velocity, such as how many times an IP appears in a short window, and device fingerprinting. For return abuse, you should talk about the feedback loop since labels often take weeks to arrive after a return happens. Make sure you can explain how to handle the extreme class imbalance since fraud cases are rare compared to normal orders.\n\nOn the architecture side, look into how graph databases help with identity linking. If a person uses multiple emails but the same device ID, a graph helps you find those connections quickly. You should also think about the trade offs between a real time blocking system and a batch based risk scoring system. The interviewer will probably ask how you plan to handle data drift when fraudsters change their patterns to avoid detection.\n\nI write about these kinds of engineering challenges in my newsletter, Machine Learning at Scale. I actually cover specific system design topics and production infrastructure over at [machinelearningatscale.substack.com](http://machinelearningatscale.substack.com) if you want to see some deep dives before your interview. Good luck with the process.",
          "score": 3,
          "created_utc": "2026-02-21 10:37:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m3qg4",
              "author": "SuccessfulStorm5342",
              "text": "Thanks a lot",
              "score": 1,
              "created_utc": "2026-02-21 15:31:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hjgi1",
          "author": "Spare-Builder-355",
          "text": "if you are final year student, how you are supposed to know fraud detection domain if you never worked in one ? This is not public knowledge. There are no books or opensource projects on the topic.",
          "score": 4,
          "created_utc": "2026-02-20 20:32:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k6216",
              "author": "SuccessfulStorm5342",
              "text": "Exactly, I didn‚Äôt find many resources. In the first round, which I was able to clear through some reading from ChatGPT and basic ML fundamentals the interviewer told me that the second round would be more in-depth, . I‚Äôm not sure the same approach will work for the upcoming round. They will basically give situations and see how I approach the problem.",
              "score": 1,
              "created_utc": "2026-02-21 06:08:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6kwawf",
                  "author": "Spare-Builder-355",
                  "text": "considering you only have few days left - you have zero chances of learning anything meaningful about fraud detection. \n\nThe only advice I can give: spend this time by practicing the interview. Pick one problem from your list and design ML system to solve that problem. Have a couple of parallel chatgpt sessions to act as friendly expert and as interviewer. Always come up with ideas youself, bounce few times with \"freindly expert\", improve your design. Get interviewer to ask questions about your choices. \n\nSince it's ML System Design I'd maybe focus on broader picture of typical ML system:\n\n- how do you collect historical data\n\n- how do you classify it for training\n\n- how would you extract features from your data\n\n- features engineering for training\n\n- features engineering for inference\n\n- model performance evaluation (a/b testing)\n\n- maybe take a look at industry's tooling like Hopworks Feature Store and MLFlow.\n\n- make sure you understand \"big picture\" of your ML system and maybe can dive deep into details.\n\n- finally, always keep in mind that it is OK for you to say \"sorry I never looked into this aspect in details\". You are student not seasoned pro.",
                  "score": 2,
                  "created_utc": "2026-02-21 10:20:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sdqwo",
          "author": "Most-Bell-5195",
          "text": "If you're looking for targeted mock interviews, feel free to reach out.",
          "score": 2,
          "created_utc": "2026-02-22 15:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k7os9",
          "author": "SuccessfulStorm5342",
          "text": "I would request anyone to cross-post this in r/MachineLearning , don't know why i'm banned there.",
          "score": 1,
          "created_utc": "2026-02-21 06:22:38",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdpbkd",
      "title": "Wrote a guide to building an ML research cluster. Feedback appreciated.",
      "subreddit": "mlops",
      "url": "https://i.redd.it/5bpvizk9qhlg1.png",
      "author": "aliasaria",
      "created_utc": "2026-02-24 19:04:24",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rdpbkd/wrote_a_guide_to_building_an_ml_research_cluster/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bb1cn",
          "author": "radarsat1",
          "text": "This is nice. At a small startup we were working with a couple of machines in the \"multiuser, single workstation\" configuration and it was ok, but after buying a couple more machines working this way became very annoying. We worked towards something like what you are recommending here with k3s but never fully figured it out, probably could have used a guide like this!",
          "score": 2,
          "created_utc": "2026-02-25 12:16:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcfsq7",
      "title": "Deploy HuggingFace Models on Databricks (Custom PyFunc End-to-End Tutorial) | Project.1",
      "subreddit": "mlops",
      "url": "https://youtu.be/m1pVXfD2yYI",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-02-23 12:11:35",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcfsq7/deploy_huggingface_models_on_databricks_custom/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r8nnui",
      "title": "A 16-mode failure map for LLM / RAG pipelines (open source checklist)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r8nnui/a_16mode_failure_map_for_llm_rag_pipelines_open/",
      "author": "Over-Ad-6085",
      "created_utc": "2026-02-19 02:55:11",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "If you are running LLM / RAG / agent systems in production, this might be relevant. If you mostly work on classic ML training pipelines (tabular, CV etc.), this map probably does not match your day-to-day pain points.\n\nIn the last year I kept getting pulled into the same kind of fire drills: RAG pipelines that pass benchmarks, but behave strangely in real traffic. Agents that look fine in a notebook, then go off the rails in prod. Incidents where everyone says ‚Äúthe model hallucinated‚Äù, but nobody can agree what exactly failed.\n\nAfter enough of these, I tried to write down a **failure map** instead of one more checklist. The result is a **16-problem map for AI pipelines** that is now open source and used as my default language when I debug LLM systems.\n\nVery roughly, it is split by layers:\n\n* **Input & Retrieval \\[IN\\]** hallucination & chunk drift, semantic ‚â† embedding, debugging is a black box\n* **Reasoning & Planning \\[RE\\]** interpretation collapse, long-chain drift, logic collapse & recovery, creative freeze, symbolic collapse, philosophical recursion\n* **State & Context \\[ST\\]** memory breaks across sessions, entropy collapse, multi-agent chaos\n* **Infra & Deployment \\[OP\\]** bootstrap ordering, deployment deadlock, pre-deploy collapse\n* **Observability / Eval {OBS}** tags that mark ‚Äúthis breaks in ways you cannot see from a single request‚Äù\n* **Security / Language / OCR {SEC / LOC}** mainly cross-cutting concerns that show up as weird failure patterns\n\nThe 16 concrete problems look like this, in plain English:\n\n1. **hallucination & chunk drift** ‚Äì retrieval returns the wrong or irrelevant content\n2. **interpretation collapse** ‚Äì the chunk is right, but the logic built on top is wrong\n3. **long reasoning chains** ‚Äì the model drifts across multi-step tasks\n4. **bluffing / overconfidence** ‚Äì confident tone, unfounded answers\n5. **semantic ‚â† embedding** ‚Äì cosine match is high, true meaning is wrong\n6. **logic collapse & recovery** ‚Äì reasoning hits a dead end and needs a controlled reset\n7. **memory breaks across sessions** ‚Äì lost threads, no continuity between runs\n8. **debugging is a black box** ‚Äì you cannot see the failure path through the pipeline\n9. **entropy collapse** ‚Äì attention melts into one narrow path, no exploration\n10. **creative freeze** ‚Äì outputs become flat, literal, repetitive\n11. **symbolic collapse** ‚Äì abstract / logical / math style prompts break\n12. **philosophical recursion** ‚Äì self-reference loops and paradox traps\n13. **multi-agent chaos** ‚Äì agents overwrite or misalign each other‚Äôs roles and memories\n14. **bootstrap ordering** ‚Äì services fire before their dependencies are ready\n15. **deployment deadlock** ‚Äì circular waits inside infra or glue code\n16. **pre-deploy collapse** ‚Äì version skew or missing secret on the very first call\n\nEach item has its own page with:\n\n* how it typically shows up in logs and user reports\n* what people usually *think* is happening\n* what is actually happening under the hood\n* concrete mitigation ideas and test cases\n\nEverything lives in one public repo, under a single page:\n\n* **Full map + docs:** [https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md](https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md)\n\nThere is also a small helper I use when people send me long incident descriptions:\n\n* **‚ÄúDr. WFGY‚Äù triage link (ChatGPT share):** [https://chatgpt.com/share/68b9b7ad-51e4-8000-90ee-a25522da01d7](https://chatgpt.com/share/68b9b7ad-51e4-8000-90ee-a25522da01d7)\n\nYou paste your incident or pipeline description, and it tries to:\n\n1. guess which of the 16 modes are most likely involved\n2. point you to the relevant docs in the map\n\nIt is just a text-only helper built on top of the same open docs. No signup, no tracking, MIT license.\n\nOver time this map grew from my own notes into a public resource. The repo is sitting around \\~1.5k stars now, and several **awesome-AI / robustness / RAG** lists have added it as a reference for failure-mode taxonomies. That is nice, but my main goal here is to stress-test the taxonomy with people who actually own production systems.\n\nSo I am curious:\n\n* Which of these 16 do you see the most in your own incidents?\n* Is there a failure mode you hit often that is completely missing here?\n* If you already use some internal taxonomy or external framework for LLM failure modes, how does this compare?\n\nIf you end up trying the map or the triage link in a real postmortem or runbook, I would love to hear where it feels helpful, and where it feels wrong. The whole point is to make the language around ‚Äúwhat broke‚Äù a bit less vague for LLM / RAG pipelines.",
      "is_original_content": false,
      "link_flair_text": "Freemium :snoo_tableflip:",
      "permalink": "https://reddit.com/r/mlops/comments/1r8nnui/a_16mode_failure_map_for_llm_rag_pipelines_open/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rdi7jr",
      "title": "Why do agent testing frameworks assume developers will write all the test cases?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rdi7jr/why_do_agent_testing_frameworks_assume_developers/",
      "author": "Outrageous_Hat_9852",
      "created_utc": "2026-02-24 14:49:29",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 0.79,
      "text": "Most AI testing tools I've seen are built for engineers to write test scripts and run evaluations. But in practice, the people who best understand what good AI behavior looks like are often domain experts, product managers, or subject matter specialists.   \n  \nFor example, if you're building a customer service agent, your support team lead probably has better intuition about edge cases and problematic responses than your ML engineer. If you're building a legal document analyzer, your legal team knows what constitutes accurate analysis. Yet most testing workflows require technical people to translate domain knowledge into code.  \n  \n This creates a bottleneck and often loses important nuances in translation. Has anyone found good ways to involve non-technical stakeholders directly in the testing process?  \n  \nI'm thinking beyond just \"review the results\" but actually contributing to test design and acceptance criteria.",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rdi7jr/why_do_agent_testing_frameworks_assume_developers/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o75baju",
          "author": "penguinzb1",
          "text": "the translation problem is real, but there's a second issue underneath it: even with good domain expert input, the test set usually only covers the cases they can articulate. the failures that matter are the ones nobody anticipated.\n\nwhat's worked for us: give domain experts access to simulated versions of their actual workflows and let them just run the agent. they don't need to write scenarios, they surface the gaps themselves as they go. 'it never should have done that' is better input than anything you'd get from a spec written in advance.",
          "score": 3,
          "created_utc": "2026-02-24 15:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7592ap",
          "author": "QuoteBackground6525",
          "text": "Yes! We had the same issue with our customer service AI. Our support team knew exactly what kinds of tricky customer requests would break the system, but translating that knowledge into test code was always a bottleneck. Now our support lead connects their runbooks and FAQ docs, describes problematic scenarios in plain language, and we get comprehensive test coverage including adversarial cases. The key was finding a platform that treats testing as a cross-functional activity rather than just a developer task. Much more effective than the old approach of engineers guessing what good behavior looks like.",
          "score": 2,
          "created_utc": "2026-02-24 14:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o759d2l",
              "author": "Outrageous_Hat_9852",
              "text": "Uh, interesting! Any tools you have been using for this that were helpful?",
              "score": 1,
              "created_utc": "2026-02-24 14:59:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7blztw",
          "author": "Illustrious_Echo3222",
          "text": "This is such a real bottleneck. A lot of agent testing frameworks feel like classic unit testing tools with an LLM wrapper, which assumes the engineer both defines and encodes ‚Äúcorrectness.‚Äù But for most agent use cases, correctness is domain shaped, not purely technical.\n\nWhat I‚Äôve seen work better is separating test authoring from test execution.\n\nInstead of asking domain experts to write code, give them structured ways to define:\n\n* Example scenarios in plain language\n* ‚ÄúGood vs bad‚Äù response pairs\n* Acceptance rubrics with weighted criteria\n\nThen have engineers translate those into executable evals or, better yet, build a thin layer that auto-generates test cases from structured forms. Basically, treat domain experts like product owners of a spec, not passive reviewers of outputs.\n\nAnother useful pattern is gold conversation capture. Let SMEs flag real transcripts as ‚Äúideal,‚Äù ‚Äúborderline,‚Äù or ‚Äúfail,‚Äù and continuously sample from production logs for evaluation sets. That keeps nuance intact because it‚Äôs grounded in real behavior, not hypothetical test cases.\n\nI also think pair-review style workflows help. Domain expert defines the intent and failure boundaries. Engineer encodes it. Then both review eval drift over time. It becomes collaborative rather than translational.\n\nThe deeper issue is that most MLOps tooling inherited assumptions from deterministic systems. Agents are probabilistic and contextual. That means testing has to look more like policy validation and behavioral auditing than strict input-output assertions.\n\nCurious if you‚Äôre exploring tooling here or just noticing the gap. It feels like there‚Äôs space for much better human-in-the-loop eval design.",
          "score": 2,
          "created_utc": "2026-02-25 13:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bnowe",
              "author": "Outrageous_Hat_9852",
              "text": "Thanks, this helps! I am exploring tools right now, via lists like this: [https://github.com/kelvins/awesome-mlops](https://github.com/kelvins/awesome-mlops)\n\nOne that I came across that puts an emphasis on collaboration and SMEs in particular is this: [https://github.com/rhesis-ai/rhesis](https://github.com/rhesis-ai/rhesis)",
              "score": 1,
              "created_utc": "2026-02-25 13:36:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76w5a3",
          "author": "Downtown-Height5899",
          "text": "Use BDD framework",
          "score": 1,
          "created_utc": "2026-02-24 19:25:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r923zw",
      "title": "Deploy ML Models Securely on K8s: KitOps + KServe Integration Guide",
      "subreddit": "mlops",
      "url": "https://youtu.be/0gXe_q458K4",
      "author": "iamjessew",
      "created_utc": "2026-02-19 15:26:42",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r923zw/deploy_ml_models_securely_on_k8s_kitops_kserve/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rbo5pi",
      "title": "We‚Äôre seeing 8‚Äì10x difference between execution time and billed time on bursty LLM workloads. Is this normal?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rbo5pi/were_seeing_810x_difference_between_execution/",
      "author": "pmv143",
      "created_utc": "2026-02-22 15:07:58",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "We profiled a 25B-equivalent workload recently.\n\n\\~8 minutes actual inference time\n\n\\~100+ minutes billed time under a typical serverless setup\n\nMost of the delta was:\n\n‚Ä¢ Model reloads\n\n‚Ä¢ Idle retention between requests\n\n‚Ä¢ Scaling behavior\n\nFor teams running multi-model or long-tail deployments, \n\nAre you just absorbing this overhead?\n\nOr have you found a way to align billing closer to actual execution time?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rbo5pi/were_seeing_810x_difference_between_execution/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6yoi6p",
          "author": "nebulaidigital",
          "text": "Yes, that 8‚Äì10x gap can be ‚Äúnormal‚Äù in serverless-ish setups, but it‚Äôs usually a sign you‚Äôre paying for cold starts, model load, and retention policies that don‚Äôt match your traffic shape. A few levers that often help: keep-warm pools for the long tail, pin a smaller set of models per endpoint (or use an in-process router), move weights to local NVMe and aggressively cache artifacts, and separate preprocessing/postprocessing from GPU-bound inference so the GPU container stays hot. If you can, measure: cold start time, model load time, queueing, and actual GPU utilization. What‚Äôs your request interarrival distribution and max tolerated p95 latency?",
          "score": 1,
          "created_utc": "2026-02-23 15:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrblp",
              "author": "pmv143",
              "text": "That makes sense. Especially about  retention not matching traffic shape. In our case the traffic is really bursty with long idle gaps, so the keep-warm strategy feels expensive quickly.\n\nHave you seen setups that avoid warm pools entirely without eating 40‚Äì60s reload times?",
              "score": 1,
              "created_utc": "2026-02-23 15:29:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o712d83",
          "author": "Outrageous_Hat_9852",
          "text": "That billing/execution gap usually points to queuing delays, connection pooling issues, or the provider's internal batching - especially with bursty traffic patterns. Are you measuring wall-clock time from request start to response end, or just the actual inference time? Proper tracing (OpenTelemetry works well for this) can help you break down where those extra milliseconds are hiding - we've seen teams discover everything from DNS resolution delays to token counting overhead that wasn't obvious in basic logging.",
          "score": 1,
          "created_utc": "2026-02-23 22:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o712xw3",
              "author": "pmv143",
              "text": "Exactly. Most ppl measure model execution time but ignore end to end end wall clock. Queuing, cold starts, connection pooling, and provider batching , all these can easily dwarf the actual forward pass. \n\nThis is also why separating ‚Äòcompute time‚Äô from ‚Äòbilled time‚Äô becomes critical in bursty workloads.",
              "score": 1,
              "created_utc": "2026-02-23 22:02:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra6rff",
      "title": "OpenStack vs other entire stacks",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ra6rff/openstack_vs_other_entire_stacks/",
      "author": "No-Fig-8614",
      "created_utc": "2026-02-20 20:37:21",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.79,
      "text": "I've been looking around for the entire end to end stack for inference providing on hardware. There is OpenStack which gives a good end to end solution. I can't remember but there are others out there that have the entire end to end inference stack solution. Can anyone help me remember other stacks that are similar and opensource (even if they have the closed source add-ons for additional features). ",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1ra6rff/openstack_vs_other_entire_stacks/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6hz043",
          "author": "gscjj",
          "text": "Lllm-d or Kserve?",
          "score": 1,
          "created_utc": "2026-02-20 21:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ng18i",
          "author": "repilicus",
          "text": "Kserve is nice",
          "score": 1,
          "created_utc": "2026-02-21 19:32:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raw3l1",
      "title": "I built a small library to version and compare LLM prompts (because Git wasn‚Äôt enough)",
      "subreddit": "mlops",
      "url": "/r/LLMDevs/comments/1ravxjq/i_built_a_small_library_to_version_and_compare/",
      "author": "ankursrivas",
      "created_utc": "2026-02-21 16:54:39",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1raw3l1/i_built_a_small_library_to_version_and_compare/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o6nwekq",
          "author": "Internal-Tackle-1322",
          "text": "Interesting problem. In document pipelines I‚Äôve seen prompt drift caused not only by wording changes but also by upstream dependency shifts (model version updates, temperature defaults, tokenizer changes).\n\nHave you considered versioning execution context separately from prompt text? That‚Äôs often where reproducibility breaks down.",
          "score": 2,
          "created_utc": "2026-02-21 20:59:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oqf7f",
              "author": "ankursrivas",
              "text": "That‚Äôs a great point ‚Äî and I completely agree.\n\nRight now the library versions prompt text explicitly, and logs execution metadata per run (model name, latency, tokens, etc.).\n\nBut you‚Äôre absolutely right that reproducibility often breaks due to execution context drift:\n\n‚Ä¢ Model version changes\n‚Ä¢ Temperature defaults\n‚Ä¢ Tokenizer differences\n‚Ä¢ Max token limits\n‚Ä¢ System-level prompts\n\nAt the moment, those can be logged via metadata in log(), but they aren‚Äôt versioned as a first-class ‚Äúexecution context object.‚Äù\n\nSeparating prompt versioning from execution context versioning is something I‚Äôve been thinking about, especially for more reproducible evaluation workflows.\n\nAppreciate you raising that ‚Äî it‚Äôs a very real issue in production pipelines.",
              "score": 2,
              "created_utc": "2026-02-21 23:46:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s6h4h",
          "author": "SpiritedChoice3706",
          "text": "Neat, I'm going to for sure flag this one. About a year ago I was experimenting with MLFlow's abilities. They might have gotten better, but basically it was solving a similar problem but within the existing MLFlow framework. Ie, you had to have an instance, and the experiment tracking format they used could get tricky with anything off HF. Basically you're tied not only to their tools, but their storage and formatting.\n\nI like how lightweight this is - it lets the user decide how they want to track and store this data, but also can be used as a one-off in notebooks. Looking forward to trying this out.",
          "score": 1,
          "created_utc": "2026-02-22 15:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s8l4o",
              "author": "ankursrivas",
              "text": "Appreciate that ‚Äî thank you!",
              "score": 1,
              "created_utc": "2026-02-22 15:23:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9fcau",
      "title": "Need Data for MLFlow Agent",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r9fcau/need_data_for_mlflow_agent/",
      "author": "lauptimus",
      "created_utc": "2026-02-19 23:39:59",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,   \nI'm working on a project involving making an agent that can interact with MLFlow logs and provide analysis and insights into experiment runs. So far, I've been using a bit of dummy data, but it would be great if anyone would help me understand where to get some real data from.  \nI don't have compute to run a lot of DL experiments. If anyone has any logs lying around, or knows where I can find some, I'd be grateful if they can share.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r9fcau/need_data_for_mlflow_agent/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6c495h",
          "author": "data-intel-dev",
          "text": "For this case, maybe, you should use a Pycaret to create experiments easier. What do you think ?",
          "score": 1,
          "created_utc": "2026-02-20 00:12:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}