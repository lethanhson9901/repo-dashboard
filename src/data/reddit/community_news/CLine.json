{
  "metadata": {
    "last_updated": "2026-02-06 09:09:58",
    "time_filter": "week",
    "subreddit": "CLine",
    "total_items": 9,
    "total_comments": 34,
    "file_size_bytes": 39304
  },
  "items": [
    {
      "id": "1quzieq",
      "title": "Introducing Cline CLI 2.0 with free Kimi K2.5 for a limited time!",
      "subreddit": "CLine",
      "url": "https://www.reddit.com/r/CLine/comments/1quzieq/introducing_cline_cli_20_with_free_kimi_k25_for_a/",
      "author": "juanpflores_",
      "created_utc": "2026-02-03 18:13:13",
      "score": 35,
      "num_comments": 20,
      "upvote_ratio": 0.93,
      "text": "**TL;DR:** Redesigned terminal UI, better support for running parallel agents, ACP integration for Zed/Neovim/Emacs, and free Kimi K2.5 access (and more to come‚Ä¶).\n\nHey r/Cline üëã.\n\nThe team has been working hard on Cline CLI over the past few weeks and we're happy to share some updates that should make the whole experience feel a lot more usable.\n\nHere‚Äôs what‚Äôs changing:\n\n# Next notch interactive experience within terminal¬†\n\nWe rebuilt the CLI from the ground up to make it look and feel like the Cline you're used to in VS Code, making it easier to transition from the IDE to the terminal. Plan/Act modes, easy Auto-approve toggle, and powerful slash commands.\n\nhttps://reddit.com/link/1quzieq/video/qqrycnawlbhg1/player\n\n# Improved parallel agents\n\nYou can spin up separate Cline instances across tmux panes or terminal tabs. One agent refactoring your DB layer while another updates docs on a different branch, all seamlessly happening with Cline CLIs.¬†\n\nhttps://reddit.com/link/1quzieq/video/tqrmcwptlbhg1/player\n\n# ACP support: use Cline in Zed, Neovim, and more\n\nCline now works with ACP-compatible editors through the[ cline-acp adapter](https://github.com/Tonksthebear/cline-acp). That means you can run Cline directly inside Zed, Neovim (via CodeCompanion or avante.nvim), Emacs, and any other editor that uses ACP.\n\nhttps://reddit.com/link/1quzieq/video/j76zqrhulbhg1/player\n\n# Automate with headless pipelines\n\nCline CLI is fully scriptable. Use the -y flag to skip all permissions in autonomous CI/CD pipelines, pipe logs as stdin directly into the CLI, and use the --json flag to parse output easily.\n\nAutomate what makes sense. Stay in control of the rest.\n\n# Free Kimi K2.5 access (for a limited time!)\n\nWe added support for[ Kimi K2.5](https://www.kimi.com/blog/kimi-k2-5.html), Moonshot's open-source model. It's strong on agentic tasks and significantly cheaper than the big closed models, though Opus still edges it out on some pure coding benchmarks. The free access is temporary, so make the most of it while you can.\n\nMore on our launch here: [**https://cline.bot/blog/announcing-cline-cli-2-0**](https://cline.bot/blog/announcing-cline-cli-2-0)\n\nFor a limited time, we‚Äôre also making it possible for you to experiment for free with Kimi K2.5 one of the best open-source models available, Kimi K2.5.\n\nYour feedback is what lets us continue to deliver a great product for the open-source community. We‚Äôd love to hear from you.",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/CLine/comments/1quzieq/introducing_cline_cli_20_with_free_kimi_k25_for_a/",
      "domain": "self.CLine",
      "is_self": true,
      "comments": [
        {
          "id": "o3fctin",
          "author": "-Django",
          "text": "I love the animation haha",
          "score": 3,
          "created_utc": "2026-02-03 22:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dz0bf",
          "author": "bobthearsonist",
          "text": "Oooo interesting. ACP is the big thing behind opencode isn‚Äôt it? Are you shifting more towards that setup where cline runs as a host of sorts and sessions connect to it?",
          "score": 2,
          "created_utc": "2026-02-03 18:40:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e03f9",
              "author": "saoudriz",
              "text": "That's right -- we've done a ton of work in supporting many API providers and models, and so this lets you plug into that with any client you like!",
              "score": 2,
              "created_utc": "2026-02-03 18:45:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3epmoh",
          "author": "noxtare",
          "text": "is Kimi 2.5 usage logged and our data used for training? or can we use it for enterprise safely?",
          "score": 2,
          "created_utc": "2026-02-03 20:44:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f6quj",
          "author": "alonemushk",
          "text": "I tried it, and I am impressed! especially with the pricing estimate/spend CLine shows there was low compared to what it would have been with API pricing with other models for the amount of work it has done. (and yes I had 0 credits so no real charge)",
          "score": 2,
          "created_utc": "2026-02-03 22:03:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dxbbq",
          "author": "nabskan",
          "text": "available on windows?",
          "score": 2,
          "created_utc": "2026-02-03 18:32:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dzjsi",
              "author": "saoudriz",
              "text": "Yes and linux!",
              "score": 4,
              "created_utc": "2026-02-03 18:42:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ekg7p",
                  "author": "Rare-Hotel6267",
                  "text": "https://preview.redd.it/zzy9y9bpdchg1.png?width=1080&format=png&auto=webp&s=a73a9ca561c0206225b2f67bf2bd607ec94b317d\n\nAnd, respectfully, something in your auth process is not working right for me. The issue is with the redirect. It was an issue i had a few months ago where i couldn't log into cline in vs code(insiders)(windows 11). I couldn't complete the auth., meaning i could log into the account in the browser(edge/chrome, it doesn't work for both), i completed the auth successfully, but the redirect is not redirecting, and the cline extension don't detect it. resulting in not being able to log into an account (i tried with 2 Google accounts, both failed). So i stopped using it, because i couldn't log in to use it. And didn't want to troubleshoot this. A FEW MONTHS AGO.\n\nSaw the release post about Cline CLI 2.0 and the free Kimmy, and i decided to enable the extension and see what changed, but i couldn't, because of the exact same blocking issue i had  a few months ago. So again i can't use the extension....\n\nI would love for it to be fixed so i can try you out and be impressed, because usually i was disappointed with major releases because of some blocking bug or some reliability issues and stopped using it because of that.",
                  "score": 2,
                  "created_utc": "2026-02-03 20:20:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3eqlzk",
                  "author": "Rare-Hotel6267",
                  "text": "https://preview.redd.it/uwa45cqydchg1.png?width=1080&format=png&auto=webp&s=c5288d5b483b7a3d87ecb073132d87399a89af88",
                  "score": 1,
                  "created_utc": "2026-02-03 20:49:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3f7no5",
          "author": "majesticjg",
          "text": "For the truly clueless, like myself, why do I want/need this over the VS Code extension that's working fine? What does it do for me?",
          "score": 1,
          "created_utc": "2026-02-03 22:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3f9w25",
              "author": "Effective-Clock-4482",
              "text": "the main benefit i see is that you can run multiple parallel instances of cline more easily in the terminal. And you can specify different setting for each instance. e.g. if you wanted to run cline with a cheap model and then pipe that output into cline running on a more expensive model, it would be very easy to do with a command line program. I'm excited to use this for CI/CD applications personally",
              "score": 1,
              "created_utc": "2026-02-03 22:18:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jdt4b",
                  "author": "majesticjg",
                  "text": "Interesting idea. I find myself switching models fairly often, though Kimi 2.5 seems to be a great combination of cheap and accurate. Piping one through another would be interesting.",
                  "score": 1,
                  "created_utc": "2026-02-04 14:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3kgtlh",
          "author": "IndependentPoet1898",
          "text": "You say there's a free trial. But I can't seem to find it. Is it buried behind 5 links that have to be magically hit in succession to find it?",
          "score": 1,
          "created_utc": "2026-02-04 17:51:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kpkm6",
          "author": "isakota",
          "text": "Just tried it only because of \"free K2.5\" access, and it sucks.   \nNot the model, not the cli but their campaign.  \nIt's unusable for anything other than being calculator.  \nAsked it to show open task (run one command, even provided the command) and it's been  \n10 minutes and still counting...\n\nThis is anti-commercial.  \nAnd this comes from CLine fan.",
          "score": 1,
          "created_utc": "2026-02-04 18:30:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lml8h",
              "author": "juanpflores_",
              "text": "Hey is Akita, brought this to the team and we are taking a look into it.",
              "score": 1,
              "created_utc": "2026-02-04 21:05:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mlci5",
          "author": "Equivalent_Tie4071",
          "text": "I want to like cli but you got too many users for the free kimi-k2.5 access. I am using kimi-k2.5, worked fine but a bit slow early this morning but now it is not useable, slow and stuck in a loop for acting.  \nI started a new task to work on a bug but somehow the agent trys to send my whole cose base(?).\n\nFor the reference, this is a new task and the task that was completred until I get the error error, how can you send 250976 input tokens for a simple task?:  \n \n\nI'll help you investigate and fix this workspace isolation issue \n\nLet me start by examining the relevant code.\n\n¬†‚è∫ Now I can see the issue! \n\nLet me check how documents are stored to see if the workspace is being properly saved in metadata\n\n¬†‚è∫ I found the bug! The workspace is extracted from the request in scan\\_for\\_new\\_documents but it's **NOT** being passed through the call chain\n\n¬†¬† Let me fix this:\n\n**Error**: Requested token count exceeds the model's maximum context length of 262144 tokens. You requested a total of 314976 tokens:\n\n¬†¬† 250976 tokens from the input messages and 64000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-05 00:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3setcl",
          "author": "zzzwx",
          "text": "Hi  \nLong standing user of Cline here.   \nI thought this a great news: I am running on windows OS and couldn't benefit from the previous CLI version.   \nBut the Subagents feature toggle won't unlock after installing the CLI v2 (despite the CLI working well after running \\`cline auth\\`). I tried restarting VSCode, no success.  \nIs there a fix for this ?",
          "score": 1,
          "created_utc": "2026-02-05 21:21:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3sfcmj",
              "author": "saoudriz",
              "text": "We are working on a new subagents implementation that doesn't require CLI. Please stay tuned for an update soon! üôè",
              "score": 1,
              "created_utc": "2026-02-05 21:23:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qulwyi",
      "title": "Significant drop in open router use",
      "subreddit": "CLine",
      "url": "https://www.reddit.com/r/CLine/comments/1qulwyi/significant_drop_in_open_router_use/",
      "author": "BlindPilot9",
      "created_utc": "2026-02-03 07:48:05",
      "score": 7,
      "num_comments": 12,
      "upvote_ratio": 0.82,
      "text": "Openrouter reports show a significant drop in the amount of tokens consumed by cline and roo code users. Kilo on the other hand, is a significant user of openrouter. The significant drop in token use should be attributed to a certain cause that I am trying to identify. Significant drop makes me wonder if cline and roo are becoming less popular or the traffic is being routed to a different provider.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/CLine/comments/1qulwyi/significant_drop_in_open_router_use/",
      "domain": "self.CLine",
      "is_self": true,
      "comments": [
        {
          "id": "o3b4htf",
          "author": "QuickBeam1995",
          "text": "The cline provider runs on the Vercel AI gateway now",
          "score": 13,
          "created_utc": "2026-02-03 08:13:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b6ee3",
              "author": "juanpflores_",
              "text": "This is the answer. We moved from Open Router to Vercel AI Gateway so the tokens are not fully represented in the Open Router Leaderboard anymore. If you check the Vercel AI Gateway leaderboard we are at the top.",
              "score": 10,
              "created_utc": "2026-02-03 08:31:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ceq91",
          "author": "hannesrudolph",
          "text": "FYI we are Roo made a decision not to push the OpenRouter leaderboard and focus on building solid integrations directly with the labs and our own router. I believe Cline has done the same. Not interested in playing that game.",
          "score": 5,
          "created_utc": "2026-02-03 14:13:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b4k7x",
          "author": "ActMore5232",
          "text": "I used Cline for the first time in a while today and it felt off. It‚Äôs a wholly emotional feeling, but idn. \n\n(Totally random opinion YMMV)",
          "score": 2,
          "created_utc": "2026-02-03 08:13:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cuf0s",
          "author": "user29919202",
          "text": "We can actually look in the code (open source ftw!).  Check out the cline/ repo and ask Cline about itself, and you can learn that Cline shifted from OpenRouter to Vercel.",
          "score": 2,
          "created_utc": "2026-02-03 15:33:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b5ugm",
          "author": "Vasivid",
          "text": "How kilo is better than cline?",
          "score": 2,
          "created_utc": "2026-02-03 08:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dx0ck",
          "author": "ihave10personalities",
          "text": "I haven't used Cline or Roo since Antigravity was released.",
          "score": 1,
          "created_utc": "2026-02-03 18:31:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3evw7e",
          "author": "NearbyBig3383",
          "text": "That's why CLIs are gaining ground, because extensions have limited language capabilities.",
          "score": 1,
          "created_utc": "2026-02-03 21:13:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dpsp5",
          "author": "Purple_Wear_5397",
          "text": "Cline and Roo are dropping significantly. \nCline has been on a decline ever since they raised round A. \n\nClaude code and codex rule the market I believe.",
          "score": 0,
          "created_utc": "2026-02-03 17:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b2gi9",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-03 07:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b6gj2",
              "author": "juanpflores_",
              "text": "They retracted the ads and updated their blogs as it was misinformation.",
              "score": 3,
              "created_utc": "2026-02-03 08:32:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ce655",
              "author": "hannesrudolph",
              "text": "Advancing of Roo and Cline? üòù they‚Äôve not innovated on much if anything and are pretty slow to take on our innovations. Kilo is all marketing.",
              "score": 0,
              "created_utc": "2026-02-03 14:10:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bvei1",
          "author": "quantum1eeps",
          "text": "Because Clause Code with a $100 plan is like $1000 worth of OpenRouter tokens",
          "score": -1,
          "created_utc": "2026-02-03 12:16:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx158e",
      "title": "Claude Opus 4.6 is now available in Cline",
      "subreddit": "CLine",
      "url": "https://www.reddit.com/r/CLine/comments/1qx158e/claude_opus_46_is_now_available_in_cline/",
      "author": "juanpflores_",
      "created_utc": "2026-02-05 23:22:33",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Anthropic released Opus 4.6 today and it's available in Cline now in v3.57.\n\nhttps://reddit.com/link/1qx158e/video/w86hipj2frhg1/player\n\n**TLDR**\n\nThis is Anthropic's most capable model. Big improvements in reasoning, long context handling, and agentic tasks. If you've been using Opus 4.5 for complex work, this is a straight upgrade.\n\n**Benchmarks**\n\n* 80.8% on SWE-Bench Verified\n* 65.4% on Terminal-Bench 2.0 (state of the art)\n* 68.8% on ARC-AGI-2 (up from 37.6% on Opus 4.5)\n* 1M token context window\n\nhttps://preview.redd.it/2oey9305frhg1.png?width=2002&format=png&auto=webp&s=052b7167a430731d196e885b232b735c9892015d\n\nTwo things stood out to me reading the system card:\n\n1. **It doesn't lose the plot.** 1M token context window and it actually uses it well. If you've ever had a model forget what you told it three prompts ago, you'll feel the difference here. The long context recall is significantly better than previous models. You can throw an entire codebase at it and it keeps track.\n2. **It infers intent better.** You don't have to be as precise with your prompts. It's better at figuring out what you actually want even when you're being vague. Less babysitting, more just saying what you need.\n\n**When to use it**\n\nOpus 4.6 is the model for hard tasks. Complex refactors, multi-file changes, debugging something weird, anything where you need the model to hold a lot of context and think carefully.\n\nFor quick everyday stuff, Sonnet is still faster and cheaper.\n\n**How to use it**\n\nSelect claude-opus-4-6 from the model picker. Works with your Anthropic API key.\n\nWorks in your terminal, JetBrains, VS Code, Zed, Neovim, and Emacs.\n\nCurious to hear how it works for you all. What are you throwing at it?",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/CLine/comments/1qx158e/claude_opus_46_is_now_available_in_cline/",
      "domain": "self.CLine",
      "is_self": true,
      "comments": [
        {
          "id": "o3t6y8a",
          "author": "Inevitable_Pitch_620",
          "text": "You guys move fast! \n\n",
          "score": 2,
          "created_utc": "2026-02-05 23:45:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t90j4",
          "author": "Working-Solution-773",
          "text": "It's also EXTREMELY expensive. ",
          "score": 0,
          "created_utc": "2026-02-05 23:57:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwb3ue",
      "title": "Possibility of using cline without vscode",
      "subreddit": "CLine",
      "url": "https://www.reddit.com/r/CLine/comments/1qwb3ue/possibility_of_using_cline_without_vscode/",
      "author": "Bi11i0naire",
      "created_utc": "2026-02-05 04:09:21",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "Anyone tried this? There is a feature request here but looks like there are limitations in capturing the event messages from Cline\n\n[https://github.com/cline/cline/discussions/2622](https://github.com/cline/cline/discussions/2622)",
      "is_original_content": false,
      "link_flair_text": "‚úÖ Question: Resolved",
      "permalink": "https://reddit.com/r/CLine/comments/1qwb3ue/possibility_of_using_cline_without_vscode/",
      "domain": "self.CLine",
      "is_self": true,
      "comments": [
        {
          "id": "o3o3e9a",
          "author": "hannesrudolph",
          "text": "Use the CLI?",
          "score": 3,
          "created_utc": "2026-02-05 05:28:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o8ikn",
              "author": "juanpflores_",
              "text": "\\+1 to this the CLI would be the way to go either headless or interactive mode  \n[https://docs.cline.bot/cline-cli/overview](https://docs.cline.bot/cline-cli/overview)",
              "score": 2,
              "created_utc": "2026-02-05 06:09:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pcl9m",
          "author": "Southern_Orange3744",
          "text": "Cli is doable , I think some IDE might be\n\nI went a deep misadventure trying to make my own UI I would not recommend wasting time trying",
          "score": 1,
          "created_utc": "2026-02-05 12:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uk7m1",
          "author": "Bi11i0naire",
          "text": "Cline could benefit from this feature as there is no standard client for agents.  \n\nCluade forces to use their own model by default.\n\nAll the hype is around MCP but I think even the client is also important and every organization is building their own client. \n\nCurious to know other's thoughts..",
          "score": 1,
          "created_utc": "2026-02-06 04:44:20",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt14qf",
      "title": "which free and fast model do you prefer atm?",
      "subreddit": "CLine",
      "url": "https://www.reddit.com/r/CLine/comments/1qt14qf/which_free_and_fast_model_do_you_prefer_atm/",
      "author": "Most_Remote_4613",
      "created_utc": "2026-02-01 15:18:26",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 0.72,
      "text": "Title... Plus, are gemini 3 flash and pro free with ai studio api and daily limits in cline? I set and It seems as it works but i want to be sure to avoid some billing risks. Edit: I have Claude code + glm Max plan combo. I asked this question for parallel side basic tasks and free fast models. It seems that only option I have is qwen coder. Minimax 2.1 2-3$ plan with promo is also super for this purpose but it is ended afaik atm. Google pro plan could be good for 8-10$ because flash 3 is better than minimax m2.1 IMO. Though, I hate to pay for the plans which have student plan equilevent because this makes the plan abusive and I feel as silly. That's why I avoid Google pro plan and we saw there how they messed up. ‚Äã",
      "is_original_content": false,
      "link_flair_text": "‚ùì Question: New",
      "permalink": "https://reddit.com/r/CLine/comments/1qt14qf/which_free_and_fast_model_do_you_prefer_atm/",
      "domain": "self.CLine",
      "is_self": true,
      "comments": [
        {
          "id": "o2zlssm",
          "author": "evia89",
          "text": "no such thing. z,ai 1 of 3 subs (from $3) is best f2p it will ever be\n\nif u want really free use nvidia nim 30 RPM - https://build.nvidia.com/models some models are OK there. Its overloaded with RP ppl. look for old models like kimi k2 0905\n\nKilo also has few free models (more marketing budget)",
          "score": 2,
          "created_utc": "2026-02-01 16:00:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zri0n",
              "author": "Most_Remote_4613",
              "text": "Ty but Zai has good f/p but slow, check my edit.¬†",
              "score": 1,
              "created_utc": "2026-02-01 16:27:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cpl3d",
          "author": "majesticjg",
          "text": "Fast and free? \n\nHow good do you expect it to be?\n\nI believe Openrouter has a \"free\" models group you can call and it'll route the request to whatever's free, but... You usually get what you pay for.",
          "score": 1,
          "created_utc": "2026-02-03 15:09:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs9l78",
      "title": "Global cline folder for Skills is different path than everything else",
      "subreddit": "CLine",
      "url": "https://www.reddit.com/r/CLine/comments/1qs9l78/global_cline_folder_for_skills_is_different_path/",
      "author": "quincycs",
      "created_utc": "2026-01-31 18:10:13",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Just raising my hand to say it‚Äôs annoying that skills are in a different folder location than rules, hooks, workflows.\n\nI want it all in the same place so that my team can just git pull and have latest global config.\n\nGlobal skills: \\~/.cline/skills/\n\nGlobal rules, workflow, hooks: \\~/Documents/Cline",
      "is_original_content": false,
      "link_flair_text": "‚úÖ Question: Resolved",
      "permalink": "https://reddit.com/r/CLine/comments/1qs9l78/global_cline_folder_for_skills_is_different_path/",
      "domain": "self.CLine",
      "is_self": true,
      "comments": [
        {
          "id": "o2w7wr7",
          "author": "saoudriz",
          "text": "Yup this is being consolidated and soon we'll have everything nested under just the .cline directory. Currently skills is a beta opt-in feature, so haven't put in the work to making it compatible with the legacy rules/workflows/hooks being stored in documents, so appreciate the patience as we bring more order to all this! üôè",
          "score": 7,
          "created_utc": "2026-02-01 01:32:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2whye2",
              "author": "quincycs",
              "text": "Thank you üôè",
              "score": 2,
              "created_utc": "2026-02-01 02:33:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qr2jh0",
      "title": "OpenAI/ChatGPT flat rate subscription on Cline",
      "subreddit": "CLine",
      "url": "https://cline.bot/blog/introducing-openai-codex-oauth",
      "author": "Royal-Astronomer-142",
      "created_utc": "2026-01-30 11:07:35",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/CLine/comments/1qr2jh0/openaichatgpt_flat_rate_subscription_on_cline/",
      "domain": "cline.bot",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qugp8t",
      "title": "Anyone else struggling with MCP use?",
      "subreddit": "CLine",
      "url": "https://www.reddit.com/r/CLine/comments/1qugp8t/anyone_else_struggling_with_mcp_use/",
      "author": "former_farmer",
      "created_utc": "2026-02-03 03:13:43",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I'm using small open source models from 3B to 10B self hosted. MCPs are configured.\n\nThe models often fail to know how to use MCPs... it's usually a waste of time. Is cline not informing the models correctly or should I try with 20B models?\n\nAny way I haven't tried this in two weeks. I'll try again. But just asking about your experience with this. Thanks :)",
      "is_original_content": false,
      "link_flair_text": "‚úÖ Question: Resolved",
      "permalink": "https://reddit.com/r/CLine/comments/1qugp8t/anyone_else_struggling_with_mcp_use/",
      "domain": "self.CLine",
      "is_self": true,
      "comments": [
        {
          "id": "o3d6ou4",
          "author": "juiceboxwtf",
          "text": "Hey! It's almost certainly the model size, not Cline.\n\nTool use is one of the hardest capabilities for LLMs, so 3B-10B models just don't have enough capacity to do it reliably. They'll hallucinate tool calls, forget parameters, or ignore tools entirely.\n\nTry a 30B+ model trained for tool use (Qwen 2.5 Coder 32B or DeepSeek Coder V2). The jump from 10B to 30B+ is a qualitative leap for tool use. If you're RAM-constrained, a 4-bit quantized 32B model runs in ~20GB VRAM and will still dramatically outperform a 10B at this.\n\nLet me know how it goes!",
          "score": 2,
          "created_utc": "2026-02-03 16:31:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d7fl5",
              "author": "former_farmer",
              "text": "I will try this, thanks!",
              "score": 1,
              "created_utc": "2026-02-03 16:34:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3b6lws",
          "author": "juanpflores_",
          "text": "Normally, if you get less than 20 billion parameters in your model, they won't be performing as you might expect. My recommendation is that if you're trying to use MCP servers, try to use models with more than 30 billion parameters. Let us know if this works.",
          "score": 1,
          "created_utc": "2026-02-03 08:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cmqgk",
          "author": "majesticjg",
          "text": "Tool use is largely dependent on the model, but I also find that the model doesn't know what the tool isn't allowed to do until it tries. For instance, I have a filesystem tool that I love, but it'll try to browse things outside of the allowed folder tree often, get an error and have to correct. I wish it knew better on the first shot.",
          "score": 1,
          "created_utc": "2026-02-03 14:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hkpd9",
          "author": "richardbaxter",
          "text": "I found it a bit of a struggle getting tool use reliable on my network llm server. I'm working with reasonable stability with openwebui in docker on my local machine, mcpo then qwen3 coder 30b in LM Studio on the network machine. I'm still learning though - hopefully this is at least partially useful!¬†",
          "score": 1,
          "created_utc": "2026-02-04 06:35:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx4m16",
      "title": "How are people managing context + memory with Cline? (Memory banks, rules, RAG, roadmap?)",
      "subreddit": "CLine",
      "url": "https://www.reddit.com/r/CLine/comments/1qx4m16/how_are_people_managing_context_memory_with_cline/",
      "author": "arzanp",
      "created_utc": "2026-02-06 01:54:54",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hey folks,\n\nI‚Äôve been using **Cline** pretty consistently since around **December 2025** for a business startup I‚Äôm working on, and I wanted to sanity-check how others are handling **context and memory management**.\n\nMy setup so far:\n\n* I have a **ChatGPT Plus** subscription and mostly use ChatGPT in the browser as the *‚Äúbrain‚Äù*:\n   * documenting decisions\n   * defining tasks\n   * refining prompts\n* Then I use **Cline** to actually *execute* the work (coding, refactors, changes, etc).\n* From early on, I had **Cline rules** in place (mainly guardrails around Python dev and workflow discipline).\n* I‚Äôve also tried to be very deliberate about **documentation** as I go.\n\nOne thing I noticed pretty quickly was that my **context window was huge** ‚Äî often **200k+ tokens**, even just kicking off fairly simple prompts. At the time, I only really had:\n\n* a basic `.clinerules` folder\n* no `.clineignore`\n* no structured memory management beyond ‚Äúkeep docs in the repo‚Äù\n\nRecently I started digging more seriously into **context optimisation and memory banks**.\n\nWhat I‚Äôve done since:\n\n* Adapted the **basic memory bank concept** from the official Cline docs\n* Added a `.clineignore` (which I‚Äôd completely ignored before üòÖ)\n* Tightened what actually needs to load into context vs what can live ‚Äúcold‚Äù\n\nThat alone dropped my starting context to about **40,000 tokens**, which I‚Äôm honestly very happy with. It‚Äôs made a *huge* difference:\n\n* I can use **smaller, cheaper models**\n* Faster iteration\n* Less accidental context pollution\n\nThat said, I‚Äôve noticed:\n\n* Some users have built **much more advanced rule sets** that are deployable by using their repo first and dropping your project files into the src folder\n* Others talk about **recursive chain of though** [**https://www.reddit.com/r/CLine/comments/1iscdag/cline\\_recursive\\_chainofthought\\_system\\_crct/**](https://www.reddit.com/r/CLine/comments/1iscdag/cline_recursive_chainofthought_system_crct/)\n* And at the far end, there are **RAG-based approaches** (vector DBs, separate servers indexing the whole repo, etc.). Examples include Memento CLI and ByteRover 2.0\n\nSo I‚Äôm curious:\n\n1. **What are people actually using in practice right now?**\n   * Simple memory banks?\n   * Heavier rule-driven approaches?\n   * Full RAG setups?\n2. **Where‚Äôs the trade-off point** where complexity stops being worth it?\n3. **Does Cline have anything on the roadmap** around:\n   * first-class memory management\n   * smarter context loading\n   * or better tooling inside the extension itself?\n\nRight now, the memory bank + ignore approach feels like a good balance for me, but I‚Äôd love to hear what‚Äôs working (or not working) for others.\n\nDisclaimer: I used AI to help me write this, because I don't know all the fancy terms. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/CLine/comments/1qx4m16/how_are_people_managing_context_memory_with_cline/",
      "domain": "self.CLine",
      "is_self": true,
      "comments": [
        {
          "id": "o3tulq0",
          "author": "repugnantchihuahua",
          "text": "Before i switched to claude code, i found myself using the memory bank less and less.  I had a series of clinerules for things i wanted to never forget, but for the most part I would just deep plan + point it to the right places to discover the context, since I found the memory bank got clunky/out of date/ would sometimes overindex on things that were in it.  ",
          "score": 1,
          "created_utc": "2026-02-06 02:03:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u2sjo",
              "author": "false79",
              "text": "Same experience here too",
              "score": 1,
              "created_utc": "2026-02-06 02:52:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3u6sxh",
              "author": "arzanp",
              "text": "Can you share a deep plan prompt with me ?",
              "score": 1,
              "created_utc": "2026-02-06 03:16:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3u9vah",
                  "author": "repugnantchihuahua",
                  "text": "\\`/deep-plan i want to make x field nullable in the API\\` \n\nlike, i think there is a \\_lot\\_ out there in terms of overwhelming information, but TBH if your work is already bite-sized as part of greater work, the model does a good enough job figuring out what to do by exploring files ",
                  "score": 1,
                  "created_utc": "2026-02-06 03:35:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u2m41",
          "author": "false79",
          "text": "I don't start with large features. I break it down to smaller tasks that would follow critical path. The start of each task is a reset of context. LLMs perform worse as you approach 128k. Sometimes well before that.\n\n\nA fresh chat I find increases the chance of not having to re do a task again.\n\n\nIf you break down your tasks, it can be done in discrete chunks where having a long running memory would be a waste if it never takes advantage of it.",
          "score": 1,
          "created_utc": "2026-02-06 02:51:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}