{
  "metadata": {
    "last_updated": "2026-01-20 08:59:57",
    "time_filter": "week",
    "subreddit": "ChatGPTCoding",
    "total_items": 13,
    "total_comments": 128,
    "file_size_bytes": 149317
  },
  "items": [
    {
      "id": "1qgg33n",
      "title": "The value of $200 a month AI users",
      "subreddit": "ChatGPTCoding",
      "url": "https://i.redd.it/vjufy3zbh5eg1.png",
      "author": "thehashimwarren",
      "created_utc": "2026-01-18 18:23:06",
      "score": 261,
      "num_comments": 210,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qgg33n/the_value_of_200_a_month_ai_users/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0c0asl",
          "author": "spiffco7",
          "text": "We all remember 5$ uber and free doordash",
          "score": 184,
          "created_utc": "2026-01-18 18:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ctg3l",
              "author": "SnowLower",
              "text": "noooo pls not like that",
              "score": 34,
              "created_utc": "2026-01-18 20:48:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0dn6an",
                  "author": "Maumau93",
                  "text": "yes, exactly like that. youll be paying $2000 and still be fed adverts or influenced responses from advertisers",
                  "score": 39,
                  "created_utc": "2026-01-18 23:19:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0gq8u7",
                  "author": "LegitimateCopy7",
                  "text": "what else would it be? sustainability 101.",
                  "score": 2,
                  "created_utc": "2026-01-19 12:18:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0hosw4",
                  "author": "guywithknife",
                  "text": "There is no reality where it’s not like that.\n\nEven if the cost to them is only $10, there is no way they won’t raise the price anyway once they feel people are locked in enough.",
                  "score": 0,
                  "created_utc": "2026-01-19 15:35:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0fbr67",
              "author": "TheMacMan",
              "text": "Some of us remember how PayPal would pay you $20 to signup back in 1999.",
              "score": 10,
              "created_utc": "2026-01-19 05:02:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ksom1",
              "author": "brainrotbro",
              "text": "Yup. New tech is always subsidized by investor money.",
              "score": 1,
              "created_utc": "2026-01-20 00:25:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ck2co",
              "author": "ConstantExisting424",
              "text": "I remember when a hershey cost a nickel!",
              "score": -4,
              "created_utc": "2026-01-18 20:03:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0f751n",
                  "author": "OtherwiseAlbatross14",
                  "text": "That's inflation. \n\nThis post and that comment are about startups creating markets by using venture capital money to subsidize the cost of the services they provide into users grow accustomed to using them and then start the enshittification process once they hit critical mass.",
                  "score": 10,
                  "created_utc": "2026-01-19 04:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0c3kkm",
              "author": "Exp5000",
              "text": "I remember a pound of skirt steak costing around 12 bucks now it's about 20",
              "score": -17,
              "created_utc": "2026-01-18 18:44:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ekhp3",
                  "author": "ImmediateKick2369",
                  "text": "Where? $29.99 by me.",
                  "score": 3,
                  "created_utc": "2026-01-19 02:19:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0f6e8x",
                  "author": "Jolva",
                  "text": "That sounds delicious. Corn or flour?",
                  "score": 1,
                  "created_utc": "2026-01-19 04:25:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0c1a09",
          "author": "neuronexmachina",
          "text": "I'd be very surprised if the marginal cost of an average $200/mo user is anywhere near $2000/mo, especially for a provider like Google that produces energy-efficient TPUs.",
          "score": 53,
          "created_utc": "2026-01-18 18:34:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cesms",
              "author": "ExpressionComplex121",
              "text": "It's one of those things that for us, we rent and pay X amount and we pay the same no matter if we max out the gfx or don't use it at all.\n\nI'm leaning towards we are overpaying by abundance ($100-$250 a month) and its not what the costs to operate for one user. We're paying off collectively for training and free users (who already pay in a different way technically as most behavior and data is used for improving)\n\nI'm pretty sure unless you constantly max out the resources 24x7x4 you don't even cost $50 and most users don't.",
              "score": 10,
              "created_utc": "2026-01-18 19:37:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0gsakq",
                  "author": "Natural_Squirrel_666",
                  "text": "I'm building a complex agent and using raw API, of course. The app has to take into account a lot of things which go into context and the agent has to be able to keep the convo consistent => even with like 3-10 messages per day it's often around 30 bucks per month. And that's very minimal usage. Max tokens I had in a message was 90,000. I do use compaction and caching. Still. I mean, for my use case it's a good deal since I get what I want. But for coding larger context is required and definitely more than 3-10 messages per days... So...",
                  "score": 2,
                  "created_utc": "2026-01-19 12:34:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0f6zhx",
                  "author": "Slow-Occasion1331",
                  "text": "> I'm pretty sure unless you constantly max out the resources 24x7x4 you don't even cost $50 and most users don't.\n\nI can’t talk too much about it but if you’re using large models, ie what you’d get on a $200 plan, and hitting token limits on a semi regular basis, you’d be costing both oai and cc well, well, fucking substantially more than $2000 a month. \n\nInference costs are a bitch",
                  "score": 6,
                  "created_utc": "2026-01-19 04:29:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0exjwx",
                  "author": "ZenCyberDad",
                  "text": "Yep I cancelled the $200 ChatGPT pro plan after many months of using it to complete a video project for the government using Sora 1. Without 24/7 usage it just really didn’t make sense to pay that much when I can just use the same models over API with larger context windows. That’s the secret, the $200 plan doesn’t give you the same sized context windows",
                  "score": 2,
                  "created_utc": "2026-01-19 03:29:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0g0yl6",
                  "author": "spottiesvirus",
                  "text": ">I'm pretty sure unless you constantly max out the resources 24x7x4 you don't even cost $50 and most users don't.\n\nIf API prices are somewhat accurate (and I believe they may be underpriced as well) 50$/month are like... A couple messages per day with Claude Opus\n\nIt's the opposite, atm VC's money is paying for training, R&D and subsidized advertising. I fear this will be a Uber/Airbnb situation",
                  "score": -1,
                  "created_utc": "2026-01-19 08:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0fc2vw",
              "author": "TheMacMan",
              "text": "You have to consider that they need to offset the costs of millions of freeloaders to even break even.",
              "score": 4,
              "created_utc": "2026-01-19 05:04:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0cakrc",
              "author": "jovialfaction",
              "text": "Yes there's crazy margin on API cost, which they need to offset the training costs, but by itself it doesn't \"cost\" the provider thousands of dollars to provide the tokens of those coding plans",
              "score": 3,
              "created_utc": "2026-01-18 19:17:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ly43q",
              "author": "Ok_Road_8710",
              "text": "I'm considering that people just blast off shit, not understanding LTV and potential upsells.",
              "score": 1,
              "created_utc": "2026-01-20 04:15:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0mhssu",
              "author": "WeMetOnTheMountain",
              "text": "I always wonder this myself I use lots of sub agents and dialectical loops which are extremely token heavy.  If I look at what API cost is I would definitely spend at least $5,000 a month.  But here's the thing If they are not at capacity then it probably doesn't cost much more to have me hammering down on processors than them just running without me hammering on them.\n\n\nThen there are weeks that I'm doing other stuff and I barely touch my subscriptions at all.  It's the typical internet service where people who aren't using it are subsidizing people who are.",
              "score": 1,
              "created_utc": "2026-01-20 06:35:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0msqrt",
              "author": "neoqueto",
              "text": "70B-class, text-only models can run on a 5090 if you're lucky, at glacial speeds (tps, ttft). That's a GPT-4 tier model. Capable, sure. But because it's slower you gotta imagine it being hammered more often, though still not 24/7.\n\nI am mentioning a 5090 because it costs roughly a year's worth of $200/mo payments and is capable of running models that are worth something.\n\nSo it's probably not like \"renting out a few 5090s exclusively for a single user\". Even at the very worst. Because a 24/7 usage is not typical. And they have access to economies of scale, various means of load balancing, even better, more optimal hardware. However running the model and running it just for you is not the only cost. Even innovation has to be accounted for. \n\nI'd say $2000 of value sounds like the absolute upper limit still within reasonable figures. But the spread is massive, we don't have enough information.\n\nI am NOT an OAI apologist. Just trying to estimate the numbers with my peabrain.",
              "score": 1,
              "created_utc": "2026-01-20 08:11:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0c9n7u",
              "author": "thehashimwarren",
              "text": "We don't know internal numbers, but from what we're told inference compute is wildly expensive",
              "score": -5,
              "created_utc": "2026-01-18 19:12:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0cff5r",
                  "author": "West-Negotiation-716",
                  "text": "You clearly have never used a local LLM, you should try it",
                  "score": 9,
                  "created_utc": "2026-01-18 19:40:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0csr7w",
                  "author": "neuronexmachina",
                  "text": "I'd be curious about where you've been hearing that and when. My understanding is that inference compute costs per token have gone down a few orders of magnitude in the past couple years.",
                  "score": 2,
                  "created_utc": "2026-01-18 20:45:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0c0396",
          "author": "ChainOfThot",
          "text": "The thing is most people aren't using 200 dollars worth. I'm sure tons of companies are paying for these tools and their devs don't even use them a ton",
          "score": 67,
          "created_utc": "2026-01-18 18:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cxxo4",
              "author": "lupin-the-third",
              "text": "I think people also don't realize there are open source models that are catching up with the big guys. If these catch up to claude and codex in utility and intelligence they sort of force a price point. After that it's a battle of tooling and integration which open source and unfortunately google/Microsoft will have an advantage in.",
              "score": 11,
              "created_utc": "2026-01-18 21:14:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0eermg",
                  "author": "Different_Doubt2754",
                  "text": "I don't see how open source models can force a price point. When you pay for AI, you aren't really paying for the model. You are paying for the service it provides. Sure, you can download an open source model and run it if you want, but you won't be getting the capabilities that GitHub Copilot or Claude Code or whatever Google comes up with provides you.\n\nOpen source models really have no effect on the price of proprietary models, unless of course they are cheaper to run. But that applies to competitor proprietary models too, not just open source.",
                  "score": 2,
                  "created_utc": "2026-01-19 01:48:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0etfzm",
                  "author": "WAHNFRIEDEN",
                  "text": "There’s nothing close to 5.2 Pro or 5.2 Codex",
                  "score": 0,
                  "created_utc": "2026-01-19 03:06:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0gku7c",
                  "author": "MacrosInHisSleep",
                  "text": "Which ones?",
                  "score": 0,
                  "created_utc": "2026-01-19 11:35:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0c7exq",
              "author": "johnfkngzoidberg",
              "text": "Folks don’t seem to realize AI is in the “get you hooked” phase.  They’re all operating at a massive loss to establish the tech in your workflows, get you interested, and normalize AI as a tool. After people adopt it more the price will go up dramatically. \n\nCrack and meth dealers have used this technique for decades.  Netflix did it, phone carriers do it, cable TV did it.  \n\nIf AI providers manage to corner the market on hardware (which they’re doing right now), AI will be like oxygen in Total Recall.  They want insanely priced RAM and GPUs, because they can afford it and you can’t. They’ll just pass the cost on to the consumers.",
              "score": 38,
              "created_utc": "2026-01-18 19:02:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0c92dm",
                  "author": "ChainOfThot",
                  "text": "This isn't true, most leading labs would be profitable if they weren't investing in next gen models. Each new Nvidia chip gets massively more efficient at tokens/sec as well, price won't go up. All we've seen is they use the more tokens to provide more access to better intelligence. First thinking mode, now agentic mode, and so on. Blackwell to Rubin is going to be another massive leap as well and we'll see it play out this year.",
                  "score": 22,
                  "created_utc": "2026-01-18 19:10:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0cd4ps",
                  "author": "evia89",
                  "text": "> Folks don’t seem to realize AI is in the “get you hooked” phase\n\nThere will be cheap providers like z.ai for ~20$/month or n@n0gpt ($8/60k requests). They are not top tier but good enough to do most tasks",
                  "score": 5,
                  "created_utc": "2026-01-18 19:29:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0dxz0d",
                  "author": "dogesator",
                  "text": "“Operating at a massive loss” \nExcept they’re not though, the latest data suggests both OpenAI and Anthropic actually have positive operating margins, not negative.\nBoth companies are overall in the red financially due to capex spent on building out datacenters for the next gen and next next gen, but they’re current inference operations are already making more revenue than what it costs to produce the tokens and more than what it cost to train the model that is producing those tokens.",
                  "score": 3,
                  "created_utc": "2026-01-19 00:16:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0cpxyc",
                  "author": "AppealSame4367",
                  "text": "Difference is: There are global competitors from the get go. They are instantly launching in a market where others try to undercut them. They cannot stop with the 200$ per month subscriptions.\n\nMe, user of openai from the first hour, claude max user, with credits on windsurf, copilot, openrouter, I just try to get used to coding with Mistral CLI and API, because I am sick of American companies catering to a fascist regime and it's institutions. They threaten everybody and now they threaten Europe, so fuck them.\n\nSince many people feel this way, they won't sell big on the international stage in the near future. Because why would I choose AI from some American assholes when I can have slightly less capable AI from Europe / China + runners in Europe or other countries?",
                  "score": 4,
                  "created_utc": "2026-01-18 20:31:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0dwnqa",
                  "author": "Western_Objective209",
                  "text": "You can get a lot of usage of cheaper stuff like GLM for very little money. The cheaper stuff will continue to get better",
                  "score": 2,
                  "created_utc": "2026-01-19 00:09:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0cf4u3",
                  "author": "West-Negotiation-716",
                  "text": "You seem to forget that we will all be able to train gpt5 on our cell phones in 10 years",
                  "score": 1,
                  "created_utc": "2026-01-18 19:39:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0c4se5",
              "author": "opbmedia",
              "text": "I use about 30-40% of the tokens. But I can't step down to the next plan. But for $200, it's basically free compare to what it replaces (a couple of junior devs).",
              "score": 4,
              "created_utc": "2026-01-18 18:50:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0d8dr8",
                  "author": "FableFinale",
                  "text": "This is the big selling point. It's not whether it's objectively cheap, but even if it cost $4000/mo that's still way cheaper than even a single junior dev.",
                  "score": 2,
                  "created_utc": "2026-01-18 22:08:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0dtbri",
              "author": "TheDuhhh",
              "text": "Yeah I feel this is their business model. Initially, the first few users will max use it and generating a loss, but those users will advertise to others who will then subscribe but not use it enough so they in some sense  subsidize the power users.",
              "score": 1,
              "created_utc": "2026-01-18 23:51:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0dtpkm",
              "author": "one-wandering-mind",
              "text": "Yeah. I'd say they are operating at a loss but not to the degree that people think based on people posting and reading Reddit about this. \n\n\nSimilar to how gyms make money. Most people that have memberships don't go or don't go very often. If they did, the membership would cost 3x as much. ",
              "score": 1,
              "created_utc": "2026-01-18 23:53:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0dwiqu",
              "author": "Western_Objective209",
              "text": "At my work we use it with bedrock, and I'm not a $2000/month user, more like a $800/month. It's a lot of money, but we get so much done it's justified. Most of the people use $0/month, and have a GH and MS Copilot sub that they get near zero usage from. Kind of balances out",
              "score": 1,
              "created_utc": "2026-01-19 00:08:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0kjuwi",
              "author": "_crs",
              "text": "I don’t usually agree with Mr. Theo Browne but he had a good point about the usage of AI within the various subscription bands. People with $20 plans tend to use less than $20 worth. People spending $200 on a plan tend to use up to $200 or far more. There’s a big gap between the “average human” wanting to dabble in AI and builders/developer that will squeeze every drop out of their plans and more.",
              "score": 1,
              "created_utc": "2026-01-19 23:38:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0c406w",
              "author": "BERLAUR",
              "text": "I'm sure that there's some cases out there where this holds true but if I look how much tokens we're burning we must be costing them money. \n\n\nThere's a hug push to \"AI-ize\" all manual tasks now since if the models keep improving, eventually they'll do better than highly skilled humans anyway.",
              "score": 1,
              "created_utc": "2026-01-18 18:46:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ceiuj",
                  "author": "TheMightyTywin",
                  "text": "Yeah I’m on the $200 codex plan and I just used it to rewrite *all* of our docs in an enterprise application. Almost 500 high quality docs.\n\nI did hit the weekly limit doing this but I gotta imagine I used way more than $200 in tokens",
                  "score": 1,
                  "created_utc": "2026-01-18 19:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0c0vgi",
          "author": "max1c",
          "text": "I'm not paying $2000 for RAM and $2000 for using AI. Pick one.",
          "score": 33,
          "created_utc": "2026-01-18 18:32:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0e5cb7",
              "author": "ElementNumber6",
              "text": "> Pick one.\n\nThey already have.  They picked \"you will no longer own capable hardware\".  This is the first step toward that.\n\nNow please pay up.",
              "score": 6,
              "created_utc": "2026-01-19 00:55:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0c9s99",
              "author": "Aranthos-Faroth",
              "text": "Good point actually, at some point models will become good enough for most people’s needs to be run locally - so to stop that maybe they’re fucking over the ram and gpu markets so that can’t happen.",
              "score": -1,
              "created_utc": "2026-01-18 19:13:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0cq6u9",
                  "author": "Mean_Employment_7679",
                  "text": "Not yet. I bought a 5090 partly thinking I might be able to cancel subscriptions. No. Sad.",
                  "score": 4,
                  "created_utc": "2026-01-18 20:33:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0c2o5z",
          "author": "no-name-here",
          "text": "1. Big providers like OpenAI have already said that inference is profitable for them. It’s the training of new models that is not profitable.\n2. Others have already pointed out that a ton of people don’t max out their possible usage per month, making them particularly profitable.",
          "score": 23,
          "created_utc": "2026-01-18 18:40:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0c6fc6",
              "author": "Keep-Darwin-Going",
              "text": "Mostly the corporate that do not max out, individual typically do because they will upgrade and downgrade accordingly to their needs while company just but a fixed plan and give to everyone. Which is also why Claude is more profitable than openai because they are way more corporate focus.",
              "score": 4,
              "created_utc": "2026-01-18 18:57:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0gbbi3",
              "author": "huzaa",
              "text": ">Big providers like OpenAI have already said that inference is profitable for them. It’s the training of new models that is not profitable.\n\nSo? It's like if a car company said: \"Manufacturing the cars are profitable, the only thing which pull us into red is the R&D and new models\"\n\nIf the market wants the new models and the competition is high they still have to produce the new models.",
              "score": 1,
              "created_utc": "2026-01-19 10:09:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mrgbp",
                  "author": "no-name-here",
                  "text": "> like if a car company\n\nCar companies are probably the opposite of the example you want to make - their R&D costs are only ~5%, the vast majority of costs are the costs to deliver each car.\n\nA better example would be software, such as Adobe, etc., where they have some increased costs per piece of software licensed (customer support, marketing, sales), but most of the costs are the development of new product versions.\n\nSo the better analogy would be **Adobe** selling a **$200/mo** license to you, but provided greater than $200/mo of software if valued at the per-unit price while still only charging you $200/mo.\n\nAnd the OP tweet is *100%* about increasing inference, and 0% about needing or causing any new R&D or new models.",
                  "score": 1,
                  "created_utc": "2026-01-20 07:59:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0mx4ml",
                  "author": "DrProtic",
                  "text": "If they hit a wall with R&D they will scale it back and fall back to sustainable business model. \n\nAnd the wall is at the same place for everyone, if the tech is what limits them.",
                  "score": 1,
                  "created_utc": "2026-01-20 08:52:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0c4tbk",
          "author": "lam3001",
          "text": "It ends like Napster and Uber?\nEg eventually the free/cheap stuff disappears and you end up with a lower quality service or none at all or more expensive and it gets worse before it gets better … and then slowly gets worse again",
          "score": 5,
          "created_utc": "2026-01-18 18:50:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0gc9gf",
              "author": "gxsr4life",
              "text": "Napster and Uber were/are not critical to enterprise and corportations.",
              "score": 1,
              "created_utc": "2026-01-19 10:18:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0hec1p",
                  "author": "DeliciousArcher8704",
                  "text": "Neither are LLMs",
                  "score": 1,
                  "created_utc": "2026-01-19 14:44:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0e25cg",
          "author": "Crinkez",
          "text": "It won't be hugely relevant in a few years. Hardware is getting exponentially faster, and we continue to get software improvements. Today's 70B models trade blows with models 10x the size from 18 months ago. The memory shortage may last a while but production will increase. We'll eventually get to the point where enthusiasts can run near top end models on local hardware.\n\n\nIt will be a few years, but unless the world goes mad in unrelated topics, AI power and availability will improve, and costs will fall.",
          "score": 4,
          "created_utc": "2026-01-19 00:38:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0e7q9t",
              "author": "thehashimwarren",
              "text": "I'm hoping this is what will happen",
              "score": 3,
              "created_utc": "2026-01-19 01:08:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0ggxad",
              "author": "EronEraCam",
              "text": "Hardware is no longer getting exponentially faster, same for storage. No one really wants to admit it, but with our current fab processes we crashed into diminishing returns a few years ago. Hopefully there is another hardware breakthrough soon, but Moore's Law hasn't looked good for half a decade now and it really does suck.\n\n\nI miss the years when hardware deflation outstripped inflation by a mile.",
              "score": 0,
              "created_utc": "2026-01-19 11:00:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0inzbl",
                  "author": "Crinkez",
                  "text": "Do you people do no research at all in your free time? https://www.youtube.com/watch?v=mvbsTCTXLJQ",
                  "score": 1,
                  "created_utc": "2026-01-19 18:13:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0c1xao",
          "author": "CC_NHS",
          "text": "my expectation is that it will end with close to free. the behaviour of eating a certain cost to retain user base, is aiming towards a win state where one will have 'the user base' they can monetise more heavily afterwards once the competition is pushed aside. aka Uber etc.\n\nI do not think this tech is the same as the kinds of things that method worked for in the past. the only way for that to happen is to capture the user base at the hardware/ operating system level for lock in, which is probably what they are all aiming for. But until that happens (or if) the 'war' will just continue with better, cheaper, more accessible for us :)\n\nwhy I say end with close to free. is because once the monopoly is obtained by a few companies, then revenue will likely be the typical user-as-product type deal all big tech go for, simply because there will still be more than one company doing this, and open source is following the heels of the giants the whole way there.",
          "score": 6,
          "created_utc": "2026-01-18 18:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ccuda",
          "author": "HeftyCry97",
          "text": "Wild to assume the value of the $200 Claude code plan is $2000\n\nJust because that’s the API price, doesn’t mean it’s worth it \n\nIf anything - it means $2000 of API is really worth $200, if that. \n\nOpen source models are getting better. At some point reality needs to set in that the costs of inference is greatly exaggerated.",
          "score": 6,
          "created_utc": "2026-01-18 19:28:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0f697g",
              "author": "kevin074",
              "text": "Exactly, how do you even start the calculations for that? Not every CEO has a PhD in math and even then it’s just speculated monetary worth, which we all know is meaningless to investors",
              "score": 0,
              "created_utc": "2026-01-19 04:25:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0chuhi",
          "author": "logicalish",
          "text": "I mean, says the guy that is assuming people will pay >0$ for a wrapper around said LLM coding plans? So far, their potentially monetizable features are not super attractive, and regardless I fail to understand how much of the 200/2000$ he expects they will be able to capture once the market stabilizes.",
          "score": 2,
          "created_utc": "2026-01-18 19:52:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dfi3h",
          "author": "9oshua",
          "text": "The estimate is that OSS models are 7 months behind frontier models. The answer is pay for your own inference machine, DL the best version it can handle and do as much inference as you want.",
          "score": 2,
          "created_utc": "2026-01-18 22:41:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ixow0",
          "author": "El_Danger_Badger",
          "text": "Do the max tiers get access to better models? I hear they get faster responses on deep reasoning, but who knows. I imagine $200 must be well worth the extra expense up from the $20 tiers. Free tiers are useless. You can't possibly do long term work capped at a few messages per day. $20 is free for this. best money I've ever spent. ",
          "score": 2,
          "created_utc": "2026-01-19 18:56:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kg398",
              "author": "mariebks",
              "text": "They do get access to Pro models for OpenAI $200/mo plan, but no Pro Claude model on MAX",
              "score": 2,
              "created_utc": "2026-01-19 23:17:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0kzppf",
                  "author": "El_Danger_Badger",
                  "text": "Wow! I can't imagine even imagine what that next tier model must be doing. Decisions... Which future tech to choose, I suppose. Thanks! Very good to know. ",
                  "score": 1,
                  "created_utc": "2026-01-20 01:03:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jqhww",
          "author": "jonplackett",
          "text": "1. Everything gets cheaper. \n2. They don’t plan on making their money selling us commoners a monthly sub, they plan on selling a replacement for us to our billionaire owners.",
          "score": 2,
          "created_utc": "2026-01-19 21:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0c7z8q",
          "author": "ajwin",
          "text": "I think the cost of inference will come down in orders of magnitude each year. Even NVidias deal with Groq will likely lead to massive reductions in token inference pricing  else why do it?",
          "score": 4,
          "created_utc": "2026-01-18 19:04:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cbh1q",
              "author": "who_am_i_to_say_so",
              "text": "It seems like everyone forgets Moore's law. These models already produce production-worthy code (not great, but a start) and at this level, the cost of operation will continue to drop, not increase.",
              "score": 4,
              "created_utc": "2026-01-18 19:21:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0clqod",
                  "author": "shif",
                  "text": "Isn't Moore's law dead? last I heard we got to the point where quantum mechanics are becoming an issue due to the size of transistors",
                  "score": 5,
                  "created_utc": "2026-01-18 20:11:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0cftpn",
                  "author": "West-Negotiation-716",
                  "text": "Exactly, how are people forgetting that we now have a million dollar super computer in our pocket.\n\nWe will be able to train our own gpt5 on our laptop in 10 years, and on our cell phone in 15",
                  "score": 3,
                  "created_utc": "2026-01-18 19:42:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0crt4p",
          "author": "TCaller",
          "text": "Cost per unit of intelligence will only go down. That’s ultimately the only thing that matters.",
          "score": 2,
          "created_utc": "2026-01-18 20:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dbizs",
          "author": "formatme",
          "text": "[Z.AI](http://Z.AI) already won in my eyes with their pricing, and their open source model is in the top 10",
          "score": 2,
          "created_utc": "2026-01-18 22:22:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dkv9l",
          "author": "According-Tip-457",
          "text": "Sucks for them. I'm MILKING their models to the MAX, all while chucking with my Monster AI build. Local models are catching up quick. Only a matter of time. ;) By time cost goes up to $500/m I'll be chucking running Minimax 7.4 on my machine free of charge.",
          "score": 2,
          "created_utc": "2026-01-18 23:07:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0c4b8m",
          "author": "opbmedia",
          "text": "I am on the $200/month codex plan. It is okay, does most things okay and are quite bone headed at other times. It is however 100% more preferable than paying $6-8000/month for a warm body. so It's a win. It makes me work more (since the response is 10-100x faster) and faster. It's a good thing. I'd probably pay $2000 a month, not that I want to because there will be others undercutting the price.",
          "score": 1,
          "created_utc": "2026-01-18 18:48:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0c6gxp",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-18 18:57:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0c6gzj",
              "author": "AutoModerator",
              "text": "Your comment appears to contain promotional or referral content, which is not allowed here.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-18 18:57:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0cicsx",
          "author": "real_serviceloom",
          "text": "Local LLMs are getting better at a rate that this isn't a big concern for me",
          "score": 1,
          "created_utc": "2026-01-18 19:54:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0clmxn",
          "author": "holyknight00",
          "text": "That's the providers fault not ours. They should be optimizing costs to make the 200$ worth it.",
          "score": 1,
          "created_utc": "2026-01-18 20:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0cqb6u",
          "author": "Tupptupp_XD",
          "text": "Cost of intelligence keeps going down. Next year, we will have models equally as capable as codex 5.2 xhigh or Opus 4.5 for 10x cheaper",
          "score": 1,
          "created_utc": "2026-01-18 20:33:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0cxm68",
          "author": "027a",
          "text": "Tbh, I think the pool of people willing to pay $20-$40/mo and use less-than $20-$40 in usage is much larger than the group who will pay $200 and use $2000; and somewhere in that margin + some intelligent model routing to help control costs + costs go down + small models get more intelligent, there's still plenty of profit. These model companies aren't unprofitable because of inference, they're unprofitable because of footprint expansion & training.",
          "score": 1,
          "created_utc": "2026-01-18 21:12:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dfh8x",
          "author": "echo-whoami",
          "text": "There’s also a group of people who is expecting not to get RCEd through their coding tool.",
          "score": 1,
          "created_utc": "2026-01-18 22:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dfw2a",
          "author": "nethingelse",
          "text": "I mean, the thing is that not EVERYONE on a $200, $20, $9, etc. plan is utilizing all of the limits of that plan per month. Especially in OpenAI/ChatGPT-land where the userbase is more universal than just devs/tech-y people. The idea of a subscription in this context is you don't want everyone ever to use the plan up, so that you have profitable users that can subsidize people who do maximize.\n\nAt the end of the day, no one but OpenAI has access to their numbers since they're not publicy traded, but I'd imagine (with knowledge of open source/local models) inference is closer to profitability than training new models is, and new models is where the cost sink is.",
          "score": 1,
          "created_utc": "2026-01-18 22:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0doriu",
          "author": "garloid64",
          "text": "it ends at the same end user cost but now profitably because hardware got ten times better, again",
          "score": 1,
          "created_utc": "2026-01-18 23:27:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0e3q9a",
          "author": "damanamathos",
          "text": "I maxed out my $200 OpenAI account and have 2 $200 Claude accounts because 1 maxes out each week.\n\nI'm tempted to bite the bullet and just pay thousands per month for the API to better integrate it across my systems...",
          "score": 1,
          "created_utc": "2026-01-19 00:46:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g3qfs",
          "author": "Square_Weather_8137",
          "text": "There already papers on establishing lower token use for the same context. **Retrieval-Augmented Generation (RAG)** applied to conversation history is one i read recently. i could assume that price and reduced power usage will converge",
          "score": 1,
          "created_utc": "2026-01-19 08:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g6sgq",
          "author": "RiriaaeleL",
          "text": "Thanks god alphabet runs on ads, bard being a free product is insane, they could ask a lot of money for it",
          "score": 1,
          "created_utc": "2026-01-19 09:26:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g721q",
          "author": "pip25hu",
          "text": "For me $200 a month is already ridiculous. If even that can't cover the company's costs, then they have a dire future ahead of them.",
          "score": 1,
          "created_utc": "2026-01-19 09:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gbkt3",
          "author": "huzaa",
          "text": "If they have ask for $2000 per dev just to be profitable, what amount would give them an actual good ROI? $3000, $4000? At that point companies would be better of just outsourcing. No wonder they want government money...",
          "score": 1,
          "created_utc": "2026-01-19 10:11:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0glhu7",
          "author": "RealMadalin",
          "text": "Burning vc money ;)",
          "score": 1,
          "created_utc": "2026-01-19 11:40:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0grkr1",
          "author": "Crashbox3000",
          "text": "I was going through some angst about this myself recently and did some basic research on the cost of these subscriptions to the providers and I was pleasantly surprised that this narrative about massive subsidies is just not accurate. These companies are making a profit on these plans and they are using them as a sales funnel for other services either now or in the future. There is not evidence that I could find to indicate that prices of subscriptions will do anything but go down or stay flat and include more. \n\nSeems like a lot of hype to get people super happy to pay $200 and feel lucky.",
          "score": 1,
          "created_utc": "2026-01-19 12:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gwd3a",
          "author": "jointheredditarmy",
          "text": "There’s a couple of different articles that actually did the math and came to very different conclusions. Either LLMs are making some money or losing a little bit of money. No one is losing their shirts.\n\nYou can actually try this yourself. If you have an AWS rep (or equivalent on GCP or Microsoft cloud or whatever), ask for dedicated instance prices. It’ll come with a price tag and a “estimated inputs / output tokens per hour”metric for each model. This number should be the raw capabilities number, since you’re “buying” the entire instance. The first thing you’ll see is that the numbers are jarring. For example, the “public” per token pricing for Opus is $5 per million input tokens and $25 per million output tokens. 5x ratio. The actual capabilities is more like 100 input tokens for every 1 output token. This means the hosted providers are making a shitload off input tokens, which are essentially free.\n\nSo im not convinced they’re losing their shirts on inference alone. It’s the massive salary bloat killing them right now.",
          "score": 1,
          "created_utc": "2026-01-19 13:01:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0h60qm",
              "author": "thehashimwarren",
              "text": "Please link me if you still have those articles. Would like to read",
              "score": 1,
              "created_utc": "2026-01-19 13:59:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0im4u2",
          "author": "Giant_leaps",
          "text": "lol i might actually use copilot if things get to expensive or maybe i'll try to run a local version if gpus become cheaper",
          "score": 1,
          "created_utc": "2026-01-19 18:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0in7aw",
          "author": "Responsible-Buyer215",
          "text": "People think they’re getting value when they’re actually feeding it all their ideas while AI quietly harvests the best ideas and innovations to present to the people it’s actually operating for. \n\nWe leave in an age where everyone happily uploads their personal design diaries to AI for help but don’t realise they’re just giving away their most valuable ideas away for free",
          "score": 1,
          "created_utc": "2026-01-19 18:10:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0inspy",
          "author": "all_over_the_map",
          "text": "Isn't the price and the \"value\" what the market will bear? Maybe what he thinks is worth $2,000 is what the rest of us think is worth $200?",
          "score": 1,
          "created_utc": "2026-01-19 18:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iw89s",
          "author": "HarambeTenSei",
          "text": "I only feel satisfied with my cursor does 10m+ tokens per prompt ",
          "score": 1,
          "created_utc": "2026-01-19 18:49:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iwvtg",
          "author": "Crafty_Ball_8285",
          "text": "I don’t understand any of this. wtf?",
          "score": 1,
          "created_utc": "2026-01-19 18:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jd2tg",
          "author": "SomeWonOnReddit",
          "text": "They don’t need to win $200 users. The real professionals get AI for free through work. They don’t need to pay anything.",
          "score": 1,
          "created_utc": "2026-01-19 20:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jtw0b",
              "author": "thehashimwarren",
              "text": "Would you agree that the $200 users are the champions who convince a company to buy the team plans?",
              "score": 1,
              "created_utc": "2026-01-19 21:25:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jmgye",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-19 20:50:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jmh0t",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-19 20:50:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jtvyi",
          "author": "Aperturebanana",
          "text": "There’s a point when the model will be good enough for majority of things compared to the skill necessary to ship a coding project one shot.\n\nInference gets cheaper over-time due to advancements in model development.",
          "score": 1,
          "created_utc": "2026-01-19 21:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lfbnv",
          "author": "Low-Efficiency-9756",
          "text": "This is free silly imo. We’re going to either\n\nA. Finalize fusion with AI\nB. Finalize fusion with AI\n\nCompute cost will continue to go down\n\nA. Increase power supply\nB. Decrease power requirements for inference over time\nC. Increase capability of OSS models that make SOTA models less mainstream. \n\nD. Who fucking knows we can’t predict the future.",
          "score": 1,
          "created_utc": "2026-01-20 02:29:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mbkf0",
          "author": "Current-Buy7363",
          "text": "It ends the same way every other startup ends. Everything is free until the VC money ends and the investors want there money back.\n\nThis is the oldest game that every startup up runs, you burn cash in return for customer acquisition then you bump up the price when the funding isn’t enough\n\nThis path is obviously not sustainable. Companies like chatpgt can survive off this business model because most customers will use less inference than they pay, they use power users to suck in normies on the low tiers. Then later they can close the taps for power users and they’ll still have the normies who are happy to buy $5-10-20/month, but without the inference hungry devs and power users who will have the choice of API cost of gtfo",
          "score": 1,
          "created_utc": "2026-01-20 05:45:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mjwnv",
          "author": "toadi",
          "text": "After a full year of using LLMs in professional software development. I can say they are awesome as tools in the process. Also if you use them right there is not much difference between and opensource model vs the closed source models. \n\nThe opensource models are much cheaper to use.",
          "score": 1,
          "created_utc": "2026-01-20 06:53:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mljua",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-20 07:06:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mljvw",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-20 07:06:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0c5hhg",
          "author": "hejj",
          "text": "I'm ok with unsustainable business models not being sustainable.  If we have to face a future where large scale production of AI slop media content, easily automated misinformation and scamming, and mass IP theft aren't financially viable, then I'm ok with that and I look forward to being able to afford computer hardware again so that I can run coding models locally.  And if it all turns into a pricing race to the bottom for vended AI inference, that's ok too.",
          "score": 1,
          "created_utc": "2026-01-18 18:53:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0c239c",
          "author": "DauntingPrawn",
          "text": "They will always need us. We're really the only ones putting the models through the paces, informing them (through our internet complaints) when their changes degrade model performance. We are the canary in the coalmine for their inference stack and optimization techniques that often fail. We monitor their systems in a way they cannot. They can't do business without us, and more and more we can't do business without them.",
          "score": -1,
          "created_utc": "2026-01-18 18:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0c5bbg",
          "author": "Illustrious-Film4018",
          "text": "Yeah and people complain about the limits and having to pay $200. These people are idiots, they have no idea how much VC money they're burning and how much it would've cost before AI to hire a human dev. \n\nThis is proof that they don't deserve anything. AI gives unworthy idiots capabilities they should never have in the first place. And anyone who thinks it's a good idea to democratize literally EVERYTHING, so nothing is sacred anymore, is also an idiot. This is going to destroy the economy, it's not sustainable at all. You'll see where this leads... Nothing is free in life, you all are going to pay for it, one way or another (worse) way.",
          "score": -4,
          "created_utc": "2026-01-18 18:52:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0c63xm",
          "author": "Final_Alps",
          "text": "I am enjoying the venture subsidised AI lifestyle. \n\nI always eagerly participate in the newest venture funded fad and get great value for very little money. It’s about the only downward wealth transfer we have left in much of the West.",
          "score": 0,
          "created_utc": "2026-01-18 18:56:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cia1a",
              "author": "logicalish",
              "text": "Have you noticed how once the honeymoon phase you’re talking about ends, the wealth inequality has only gotten significantly worse? We’re trading a very short period of personal benefit for the few, for future pain for all.",
              "score": 2,
              "created_utc": "2026-01-18 19:54:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0cjcnq",
                  "author": "Final_Alps",
                  "text": "Is there anything you think I, personally, or we, collectively, can do about it?",
                  "score": 1,
                  "created_utc": "2026-01-18 19:59:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0d6ti7",
          "author": "nanokeyo",
          "text": "Anthropic is making money… stop crying by them.",
          "score": 0,
          "created_utc": "2026-01-18 22:00:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qeq6yd",
      "title": "Codex is about to get fast",
      "subreddit": "ChatGPTCoding",
      "url": "https://i.redd.it/faicwqlvmrdg1.png",
      "author": "thehashimwarren",
      "created_utc": "2026-01-16 19:49:14",
      "score": 222,
      "num_comments": 94,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qeq6yd/codex_is_about_to_get_fast/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzzk4r5",
          "author": "TheMacMan",
          "text": "Press release for those curious. It's a partnership allowing OpenAI to utilize Cerebras wafers. No specific dates, just rolling out in 2026.\n\n[https://www.cerebras.ai/blog/openai-partners-with-cerebras-to-bring-high-speed-inference-to-the-mainstream](https://www.cerebras.ai/blog/openai-partners-with-cerebras-to-bring-high-speed-inference-to-the-mainstream)",
          "score": 33,
          "created_utc": "2026-01-16 20:32:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02zfdk",
              "author": "amarao_san",
              "text": "So, even more chip production capacity is eaten away.\n\nThey took GPUs. I wasn't a gamer, so I didn't protest.\n\nThey took RAM. I wasn't much of a ram hoarder, so I didn't protest.\n\nThey took SSD. I wasn't much of space hoarder, so I didn't protest.\n\nThen they come for chips. Computation including. But there was none near me to protest, because of ai girlfriends and slop...",
              "score": 15,
              "created_utc": "2026-01-17 10:08:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04z2lj",
                  "author": "eli_pizza",
                  "text": "You were planning to do something else with entirely custom chips built for inference?",
                  "score": 9,
                  "created_utc": "2026-01-17 17:26:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0i9i4d",
                  "author": "_jgusta_",
                  "text": "(i got the joke, don't worry)",
                  "score": 1,
                  "created_utc": "2026-01-19 17:08:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o00bssx",
          "author": "UsefulReplacement",
          "text": "It might also become randomly stupid and unreliable, just like the Anthropic models. When you run the inference across different hardware stacks, you have a variety of differences and subtle but performance-impacting bugs show up. It’s a challenging problem keeping the model the same across hardware.",
          "score": 54,
          "created_utc": "2026-01-16 22:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08bltx",
              "author": "JustThall",
              "text": "My team was running into all sorts of bugs when run a mix and match training and inference stacks with llama/mistral models. I can only imagine the hell they gonna run into with MoE and different hardware support of mixed precision types.",
              "score": 5,
              "created_utc": "2026-01-18 03:39:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o015x5b",
              "author": "YourKemosabe",
              "text": "Was looking for this comment. God I hope they don’t ruin Codex too.",
              "score": 3,
              "created_utc": "2026-01-17 01:39:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o02scmk",
              "author": "Tolopono",
              "text": "Its the same weights and same math though. I dont see how it would change anything ",
              "score": 3,
              "created_utc": "2026-01-17 09:01:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o033um9",
                  "author": "UsefulReplacement",
                  "text": "clearly you have no clue then",
                  "score": -7,
                  "created_utc": "2026-01-17 10:49:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzzwpra",
          "author": "aghowl",
          "text": "What is Cerebras?",
          "score": 13,
          "created_utc": "2026-01-16 21:32:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzzgrt",
              "author": "innocentVince",
              "text": "Inference provider with custom hardware.",
              "score": 16,
              "created_utc": "2026-01-16 21:45:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o002kha",
                  "author": "io-x",
                  "text": "Are they public?",
                  "score": 3,
                  "created_utc": "2026-01-16 21:59:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o04h6c1",
                  "author": "eli_pizza",
                  "text": "Custom hardware built for inference speed. Currently the fastest throughput for open source models, by a lot.",
                  "score": 2,
                  "created_utc": "2026-01-17 16:02:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o00l1qv",
                  "author": "pjotrusss",
                  "text": "what does it mean? more GPUs?",
                  "score": 2,
                  "created_utc": "2026-01-16 23:35:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0gmz1q",
              "author": "chawza",
              "text": "They provide x times inference speed with x times amount of price.",
              "score": 1,
              "created_utc": "2026-01-19 11:52:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ij90b",
                  "author": "aghowl",
                  "text": "makes sense. thanks.",
                  "score": 1,
                  "created_utc": "2026-01-19 17:52:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o00ctl6",
          "author": "Square-Ambassador-92",
          "text": "Nobody asked for fast … we need very intelligent",
          "score": 26,
          "created_utc": "2026-01-16 22:50:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00e7v3",
              "author": "Outrageous-Thing-900",
              "text": "Codex is extremely slow, and a lot of people complain about it",
              "score": 40,
              "created_utc": "2026-01-16 22:58:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o018l51",
                  "author": "not_the_cicada",
                  "text": "It also continuously forgets how to walk the code base and uses really odd choices that bog it down and make it even slower. ",
                  "score": 8,
                  "created_utc": "2026-01-17 01:56:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o02gayo",
                  "author": "SpyMouseInTheHouse",
                  "text": "Those who complain are welcome to move to Claude code.",
                  "score": 1,
                  "created_utc": "2026-01-17 07:10:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00h0jc",
              "author": "mimic751",
              "text": "Be a developer",
              "score": 9,
              "created_utc": "2026-01-16 23:13:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o00mjls",
                  "author": "Ok_Possible_2260",
                  "text": "Find out your code is shit in 10 seconds is better than 40 minutes. ",
                  "score": 5,
                  "created_utc": "2026-01-16 23:44:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o02aina",
              "author": "realfunnyeric",
              "text": "It’s brilliant. But slow. This is the right move.",
              "score": 5,
              "created_utc": "2026-01-17 06:19:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00pm2p",
              "author": "Shoddy-Marsupial301",
              "text": "I ask for fast..",
              "score": 2,
              "created_utc": "2026-01-17 00:01:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o04jzen",
              "author": "eli_pizza",
              "text": "Couldn’t disagree more. Very fast inference means I can work with a coding agent in real time, instead of kicking off a request and doing something else while it works and switching back.   I think a lot of the multi agent orchestration stuff going on now is really a hack because inference is so slow. \n\nAnd if something looks off in the diff I’m more likely to guide it to do better if it makes the update instantly. \n\nMy GLM 4.6 subscription on Cerebras is great for front end work. I can just say “make the text colors darker” “no not that dark” and see the changes instantly.",
              "score": 1,
              "created_utc": "2026-01-17 16:15:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0jpvk6",
              "author": "Pitch_Moist",
              "text": "I am asking for fast.",
              "score": 1,
              "created_utc": "2026-01-19 21:06:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00kew9",
          "author": "whawkins4",
          "text": "Yeah, but is it GOOD?",
          "score": 4,
          "created_utc": "2026-01-16 23:32:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o038gke",
          "author": "jonas_c",
          "text": "Faster codex with existing models or a fast model that no one wants?",
          "score": 3,
          "created_utc": "2026-01-17 11:32:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00qikd",
          "author": "dalhaze",
          "text": "Yeah also quantized to ass",
          "score": 6,
          "created_utc": "2026-01-17 00:06:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzvxcr",
          "author": "AppealSame4367",
          "text": "Yes, that would really be something!",
          "score": 2,
          "created_utc": "2026-01-16 21:28:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o043g2q",
          "author": "Sufficient-Year4640",
          "text": "What does he mean by fast exactly? I've been using Codex for a while and it seems pretty fast. Like is it actually slower than Claude or something?",
          "score": 2,
          "created_utc": "2026-01-17 14:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04oxrm",
              "author": "thehashimwarren",
              "text": "People report that Claude Opus 4.5 is faster",
              "score": 2,
              "created_utc": "2026-01-17 16:39:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ajimt",
          "author": "Adventurous-Bet-3928",
          "text": "Damn. I was in a call with Cerebras and was asking them why the big AI companies weren't using them just a few weeks ago.",
          "score": 2,
          "created_utc": "2026-01-18 14:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bv9w2",
              "author": "thehashimwarren",
              "text": "That's funny!",
              "score": 1,
              "created_utc": "2026-01-18 18:07:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kf9vv",
          "author": "drhenriquesoares",
          "text": "Fast marketing is key.",
          "score": 2,
          "created_utc": "2026-01-19 23:13:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00cm1k",
          "author": "OccassionalBaker",
          "text": "It needs to be right before I can get excited about it being fast - being wrong faster isn’t that useful.",
          "score": 3,
          "created_utc": "2026-01-16 22:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00edo5",
              "author": "touhoufan1999",
              "text": "Codex with gpt-5.2-xhigh is as accurate as you can get at the moment. Extremely low hallucination rates even on super hard tasks. It's just very slow right now. Cerebras says they're around 20x faster than NVIDIA at inference.",
              "score": 5,
              "created_utc": "2026-01-16 22:59:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o02n508",
                  "author": "OccassionalBaker",
                  "text": "I’ve been writing code for 20 years and have to disagree that the hallucinations are very low, I’m constantly fixing its errors.",
                  "score": 0,
                  "created_utc": "2026-01-17 08:12:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzzuzxd",
          "author": "MXBT9W9QX96",
          "text": "Wow huge news",
          "score": 2,
          "created_utc": "2026-01-16 21:24:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00u3um",
          "author": "Opinion-Former",
          "text": "Fast is good, compliant and following instructions is better.",
          "score": 1,
          "created_utc": "2026-01-17 00:27:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08z4dm",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-18 06:23:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08z4en",
                  "author": "AutoModerator",
                  "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
                  "score": 1,
                  "created_utc": "2026-01-18 06:23:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01r47b",
          "author": "roinkjc",
          "text": "It’s the best for complicated setups, I hope they keep it that way",
          "score": 1,
          "created_utc": "2026-01-17 03:55:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02wozm",
          "author": "GnistAI",
          "text": "Fast, as in tokens per second? The limiting factor right now is not tokens per second, it is bugs per hour.",
          "score": 1,
          "created_utc": "2026-01-17 09:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02x203",
          "author": "tango650",
          "text": "How is \"low latency\" different from \"fast\" in the context of inference. Anyone ?",
          "score": 1,
          "created_utc": "2026-01-17 09:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04verw",
              "author": "ExcitingAssistance",
              "text": "Same as ping vs download speed",
              "score": 2,
              "created_utc": "2026-01-17 17:08:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04znn9",
                  "author": "tango650",
                  "text": "Thanks for your input. It is quite unusable but thanks anyway.",
                  "score": 1,
                  "created_utc": "2026-01-17 17:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o07th7p",
              "author": "hellomistershifty",
              "text": "Time to first token vs tokens/second",
              "score": 2,
              "created_utc": "2026-01-18 01:58:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o09cv9a",
                  "author": "tango650",
                  "text": "Thanks. Do you know how hardware of the processor influences this ? And what order of difference are we talking about ?",
                  "score": 1,
                  "created_utc": "2026-01-18 08:24:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03thxs",
          "author": "phylter99",
          "text": "We'll be able to burn through our credits faster than ever.",
          "score": 1,
          "created_utc": "2026-01-17 14:01:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05bmq0",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-17 18:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05bmt3",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-17 18:24:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ktken",
          "author": "Tushar_BitYantriki",
          "text": "Nice, it's about time a decent model gets fast.\n\nhaiku is too silly, Composer 1 is decent.\n\nI hate having to waste opus or sonnet, or GPT 2 or 1 on the grunt work of writing code, after the design and examples are ready in the plan.\n\nGPT-mini is decent, though.",
          "score": 1,
          "created_utc": "2026-01-20 00:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mc2ns",
          "author": "CrypticZombies",
          "text": "At the low price of $549.99 per day",
          "score": 1,
          "created_utc": "2026-01-20 05:49:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01abm1",
          "author": "bhannik-itiswatitis",
          "text": "oh nice, fast hallucinations",
          "score": 0,
          "created_utc": "2026-01-17 02:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02hh6t",
              "author": "popiazaza",
              "text": "This is GPT 5, not Gemini.",
              "score": 4,
              "created_utc": "2026-01-17 07:20:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00lphz",
          "author": "Zealousideal-Idea-72",
          "text": "Who uses OpenAI anymore though?  Anthropic (coding) and Gemini (general purpose) have surpassed them.",
          "score": -7,
          "created_utc": "2026-01-16 23:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00o96y",
              "author": "Kooky_Tourist_3945",
              "text": "900 million active monthly users.\nAre you dumb.",
              "score": 7,
              "created_utc": "2026-01-16 23:53:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00ptux",
              "author": "NotSGMan",
              "text": "You wont believe how good codex 5.2 xhigh is",
              "score": 8,
              "created_utc": "2026-01-17 00:02:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01s9yr",
                  "author": "Freed4ever",
                  "text": "Or just high...",
                  "score": 1,
                  "created_utc": "2026-01-17 04:03:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o01vhe4",
                  "author": "ThisGuyCrohns",
                  "text": "Not even close to opus",
                  "score": 0,
                  "created_utc": "2026-01-17 04:25:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0173yc",
              "author": "rambouhh",
              "text": "I dont know codex seems to be very very popular right now. The consensus seems to be shifting to that codex is better for longer complex tasks but slower, and CC is better for the simple stuff because it is so much faster",
              "score": 4,
              "created_utc": "2026-01-17 01:47:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01vfbr",
                  "author": "ThisGuyCrohns",
                  "text": "Not really. Claude is where it’s at. Codex was good 3 months ago. Claude overtook that and there isn’t a reason to go back",
                  "score": 1,
                  "created_utc": "2026-01-17 04:25:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00slgd",
              "author": "iritimD",
              "text": "Anyone who is serious about coding uses either a mix of cc and 5.2 codex or just codex",
              "score": 6,
              "created_utc": "2026-01-17 00:18:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o012ci2",
                  "author": "robogame_dev",
                  "text": "TIL I’m not serious about coding :’(",
                  "score": 2,
                  "created_utc": "2026-01-17 01:17:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o02a6mv",
                  "author": "TenshiS",
                  "text": "Opus 4.5 undefeated",
                  "score": 1,
                  "created_utc": "2026-01-17 06:17:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qcr3zw",
      "title": "Ralph Loop inspired me to build this - AI decides what Claude Code does next orchestrating claude code until task is done",
      "subreddit": "ChatGPTCoding",
      "url": "https://i.redd.it/idx5kfij8cdg1.png",
      "author": "RegionCareful7282",
      "created_utc": "2026-01-14 16:02:37",
      "score": 34,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qcr3zw/ralph_loop_inspired_me_to_build_this_ai_decides/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzn1hwb",
          "author": "BaCaDaEa",
          "text": "This looks really cool, man. We've got a collaboration coming up for the community (be on the look out for that guys!) but once that's over, I'd be happy to pin you for a while!",
          "score": 1,
          "created_utc": "2026-01-15 00:10:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmpuax",
          "author": "ihateredditors111111",
          "text": "lol not trying to be a hater that’s cool and all but it’s just funny thinking what clusterfucks it might end up building",
          "score": 3,
          "created_utc": "2026-01-14 23:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqh4rj",
              "author": "DrummerHead",
              "text": "It all hinges in the verification step. If that doesn't work, then it's a loop of poop.",
              "score": 3,
              "created_utc": "2026-01-15 14:42:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzmxqfq",
              "author": "RickyDontLoseThat",
              "text": "I think this is how we end up with SkyNet.",
              "score": 1,
              "created_utc": "2026-01-14 23:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznkzi7",
                  "author": "JohnnyLovesData",
                  "text": "![gif](giphy|xT5LMzIK1AdZJ4cYW4)",
                  "score": 1,
                  "created_utc": "2026-01-15 02:00:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzsd121",
          "author": "WolfeheartGames",
          "text": "Reinventing behavior trees one tiny step at a time. I am working on a full RAG with an integrated behavior tree for the actual agent harness to handle this intelligently.\n\nhttps://github.com/NoSaaS-me/Vlt-Bridge/tree/020-bt-oracle-agent",
          "score": 2,
          "created_utc": "2026-01-15 19:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxlcae",
          "author": "sridoodla",
          "text": "Can this use the claude subscription for claude code?",
          "score": 2,
          "created_utc": "2026-01-16 15:15:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02god3",
          "author": "Gustafssonz",
          "text": "How much those these running tasks cost in the end?",
          "score": 1,
          "created_utc": "2026-01-17 07:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0dwv1q",
              "author": "Adept_Possibility_66",
              "text": "yes",
              "score": 2,
              "created_utc": "2026-01-19 00:10:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvikb6",
          "author": "quantier",
          "text": "Could this work with Kilo Code or the other local AI extensions",
          "score": 1,
          "created_utc": "2026-01-16 06:14:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc0fhl",
      "title": "Agent observability is way different from regular app monitoring - maintainer's pov",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qc0fhl/agent_observability_is_way_different_from_regular/",
      "author": "dinkinflika0",
      "created_utc": "2026-01-13 19:17:50",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 0.73,
      "text": "Work at [Maxim](https://getmax.im/Max1m) on the observability side. Been thinking about how traditional APM tools just don't work for agent workflows.\n\nAgents aren't single API calls. They're multi-turn conversations with tool invocations, retrieval steps, reasoning chains, external API calls. When something breaks, you need the entire execution path, not just error logs.\n\nWe built distributed tracing at multiple levels - sessions for full conversations, traces for individual exchanges, spans for specific steps like LLM calls or tool usage. Helps a lot when debugging.\n\nThe other piece that's been useful is running automated evals continuously on production logs. Track quality metrics (relevance, faithfulness, hallucination rates) alongside the usual stuff like latency and cost. Set thresholds, get alerts in Slack when things go sideways.\n\nAlso built custom dashboards since production agents need domain-specific insights. Teams track success rates for workflows, compare model versions, identify where things break.\n\nHardest part has been capturing context across async operations and handling high-volume traffic without killing performance. Making traces actually useful for debugging instead of just noise takes work.\n\nWanted to know how others are handling observability for multi-step agents in production? DMs are always welcome for discussion!",
      "is_original_content": false,
      "link_flair_text": "Resources And Tips",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qc0fhl/agent_observability_is_way_different_from_regular/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qh2udm",
      "title": "whats the codex limits like for the pro plan of chat gpt?",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qh2udm/whats_the_codex_limits_like_for_the_pro_plan_of/",
      "author": "alosopa123456",
      "created_utc": "2026-01-19 12:27:29",
      "score": 12,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "I'm considering moving off of cursor, I barely use it for anything except doing mini bug fixes/feature requests.\n\nI would like to use AI in other editors, I'm a c# programmer mainly so cursor isnt doing much for me rn. I never hit cursors limits, so hows Codexes limits lookin?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qh2udm/whats_the_codex_limits_like_for_the_pro_plan_of/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "o0h1z3d",
          "author": "Previous-Display-593",
          "text": "I dont have any hard numbers but it feel pretty generous to me. I only hit limits when I was doing 8 hours a day coding over the holiday.",
          "score": 4,
          "created_utc": "2026-01-19 13:36:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0latjs",
              "author": "Hellerox",
              "text": "What model are you using?",
              "score": 1,
              "created_utc": "2026-01-20 02:04:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0lbu8k",
                  "author": "Previous-Display-593",
                  "text": "5.2 extra high for everything lol.",
                  "score": 2,
                  "created_utc": "2026-01-20 02:09:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0lwjzs",
              "author": "ECrispy",
              "text": "the $200 tier basically says 'unlimited' so not surprised, but what about the plus plan? how many hrs/day would you say?",
              "score": 1,
              "created_utc": "2026-01-20 04:05:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mrmvn",
                  "author": "touhoufan1999",
                  "text": "It says unlimited (with fair use) for the website, not the Codex CLI. As long as you're not hammering the website I doubt they'd lock you out.",
                  "score": 1,
                  "created_utc": "2026-01-20 08:00:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ipqsv",
          "author": "McNemarra",
          "text": "Generous for $20 but I can easily hit weekly limit",
          "score": 3,
          "created_utc": "2026-01-19 18:21:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gs96d",
          "author": "1427538609",
          "text": "I'm surprised you don't consider Co-pilot. I don't do a lot of C#, but in the few times I did it, the Visual Studio built in co-pilot worked well for me (w/ Github Copilot subscription).",
          "score": 2,
          "created_utc": "2026-01-19 12:33:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0h0epj",
              "author": "alosopa123456",
              "text": "The main reason was i just wanted GPT pro subscription, i enjoy just asking it questions and burn through the free 5.2 limit.",
              "score": 2,
              "created_utc": "2026-01-19 13:27:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0kq723",
              "author": "real_serviceloom",
              "text": "they limit context sizes massively and do other shenanigans",
              "score": 2,
              "created_utc": "2026-01-20 00:12:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0gv2ur",
          "author": "jonydevidson",
          "text": "It basically feels unlimited to me. Every evening, if there are some unresolved bugs that I haven't tackled, I'll leave it running to write logging and then iterate on the code, run tests and do so in the loop until the bugs are fixed, and that'll sometimes take an hour, sometimes 5. \n\nThe lowest I've ever gotten was 25% of weekly quota left.\n\nI use the extension in VSCode, I use the CLI around the computer for a bunch of stuff that's not coding related. It basically changed how I interact with the computer.",
          "score": 1,
          "created_utc": "2026-01-19 12:53:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0h09vf",
              "author": "alosopa123456",
              "text": "awesome thank you!",
              "score": 1,
              "created_utc": "2026-01-19 13:26:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0hukge",
              "author": "justaRndy",
              "text": "Refactoring 50k lines of code in 15 files or so, couple hours work. Plus plan only, extra high setting. Actually reset today, barely went below 50%  last week. Also can stay in the same chat window forever, context auto cleans every couple tasks. Managed to get a bunch of new features added to my app like stackable effect filters, precision calculation down to 1e30 digits precision, an UI slider for number of cpu cores in use for calculations... I'm just wondering what kind of gigatasks people are running that they are complaining about rate limits even on the 200$ plan.",
              "score": 1,
              "created_utc": "2026-01-19 16:01:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0im8lu",
                  "author": "jonydevidson",
                  "text": "They have 3-4 people using it at the same time, that would be my guess.",
                  "score": 1,
                  "created_utc": "2026-01-19 18:05:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0isiy7",
                  "author": "KnifeFed",
                  "text": "> can stay in the same chat window forever, context auto cleans every couple tasks\n\nIt auto *compacts*, meaning you'll still have remnants of completely unrelated things in your context if you never create a new session.",
                  "score": 1,
                  "created_utc": "2026-01-19 18:33:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jskht",
          "author": "Ok-Version-8996",
          "text": "I have been having issues with gpt giving code, then finding reason to switch it to some other code, then say it’s wrong again.\n\nSo I finally fact checked it with Claude free version, that was awesome.  I basically have to AI’s working together and it solved all my problems.\n\nClaude is super limiting tho, so I went and bought they dang sub so I pay both lowest tier subs, but I have a pretty badass dynamic duo of AI superpower for $30/ month",
          "score": 1,
          "created_utc": "2026-01-19 21:19:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mrvxa",
          "author": "touhoufan1999",
          "text": "For my first week of Pro, I easily ran out of the weekly usage limits but it was mostly because I was hammering everything with xhigh and having it refactor garbage from Opus. It's the second or third week now I think and I only change from gpt-5.2-xhigh if I need something done quickly (e.g. summarizing changes, documentation etc). Not coming near a limit. Using it for 2-3 hours a day for most days however. Sometimes with 2 CLIs but usually one.",
          "score": 1,
          "created_utc": "2026-01-20 08:03:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbpr58",
      "title": "Is there a realistic application for vibecoding in healthcare?",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qbpr58/is_there_a_realistic_application_for_vibecoding/",
      "author": "liltoxicThunder820",
      "created_utc": "2026-01-13 12:13:50",
      "score": 10,
      "num_comments": 16,
      "upvote_ratio": 0.73,
      "text": "Asking this as someone who's kind of in the healthtech field. Like I keep seeing vibecoding used for fast prototypes and internal tools, but I am curious where people draw the line in a regulated environment.\n\nAre there realistic use cases where speed actually helps without creating compliance or maintenance nightmares? Would love to hear examples of where this has worked in practice, especially for non core clinical workflows.\n\nThere are plenty of tools that help streamline it but I'm curious if there's a longterm opportunity just to fast track prototypes and all that (Examples like Replit, Specode, Lovable, etc)",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qbpr58/is_there_a_realistic_application_for_vibecoding/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzfy1ed",
          "author": "TheCountEdmond",
          "text": "I build healthcare systems and we 100% use AI coding tools. It's going to vary company by company but everyone has access to copilot and is encouraged to use it.\n\nWe also religiously review and test with very high standards so we haven't had issues. For HIPAA there are cloud providers that are compliant so unsure what the other poster is talking about.",
          "score": 14,
          "created_utc": "2026-01-13 23:19:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhi0fy",
              "author": "ThePlotTwisterr----",
              "text": "openai offers a custom price for [BAA](https://help.openai.com/en/articles/8660679-how-can-i-get-a-business-associate-agreement-baa-with-openai), mainly targeted at those who require HIPAA compliance with GPT.",
              "score": 3,
              "created_utc": "2026-01-14 04:41:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0mx74y",
              "author": "theitfox",
              "text": "Similar here. I work in the banking sector, which is also heavily regulated. We do use lots of AI coding, but everyone is responsible for the code they submitted.",
              "score": 1,
              "created_utc": "2026-01-20 08:52:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzhl6e6",
          "author": "L1amm",
          "text": "Prototypes and internal tools? Sure why not!\n\nVibecoding an entire EMR system? Fuck no.",
          "score": 3,
          "created_utc": "2026-01-14 05:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzk3cjb",
              "author": "Much-Journalist3128",
              "text": "They use AI for insurance claim reviews and... denials lololol",
              "score": 2,
              "created_utc": "2026-01-14 15:57:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm6zy7",
                  "author": "admiral_nivak",
                  "text": "Very different to an EMR or Claim processing system.",
                  "score": 2,
                  "created_utc": "2026-01-14 21:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzhlzkx",
          "author": "M44PolishMosin",
          "text": "Have written pre-approved requirements driven by a risk management process and test to your requirements. Doesn't matter who or what actually writes the code.",
          "score": 3,
          "created_utc": "2026-01-14 05:09:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh7lq6",
          "author": "eli_pizza",
          "text": "Prototypes presumably don't contain any actual protected health information so you can make them however you want",
          "score": 1,
          "created_utc": "2026-01-14 03:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhqjj0",
          "author": "huzbum",
          "text": "Possibly, but you would need to understand the regulatory requirements and be able to verify that they are being met.  It doesn't matter how the code is created, but you have to understand what it is actually doing and whether or not it meets the requirements.",
          "score": 1,
          "created_utc": "2026-01-14 05:43:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzivou7",
          "author": "cornelln",
          "text": "Where is there not an opportunity for vibe coding other the where you want no software or anything build by software?",
          "score": 1,
          "created_utc": "2026-01-14 11:52:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpzdcy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-15 13:06:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpzdf1",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-15 13:06:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o08yucz",
          "author": "botapoi",
          "text": "i'm building something related to the field rn and have found blink with chatgpt super useful for getting a basic version up fast. it handles the auth and backend stuff so you can really nail the core logic and ngl, it's way quicker than setting up firebase from scratch. dont forget to prompt the agent to search and implement security features in your project",
          "score": 1,
          "created_utc": "2026-01-18 06:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lnxg2",
          "author": "Tiny_Habit5745",
          "text": "We’ve actually used Specode for healthcare prototypes and internal tools, not production clinical logic. Where it really works is admin workflows, intake, reporting, and ops stuff once the data model and permissions are locked.",
          "score": 1,
          "created_utc": "2026-01-20 03:16:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzeqtc5",
          "author": "kidajske",
          "text": "Absolutely fucking not.",
          "score": 1,
          "created_utc": "2026-01-13 19:52:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze0nvg",
          "author": "thisdude415",
          "text": "The key regulatory hurdle in the US is HIPAA/HITECH which means you can’t use any cloud software including AI providers without specific agreements in place. I suspect that is true of all the providers you mentioned (and also cloudflare and vercel) \n\nIf you’re a small to medium practice (like a few psychologists or something), there’s a lot of space for vibe coded local apps to do things like intake forms or tracking or stuff like that. \n\nBut in general, the risks for fucking up when you’re dealing with other people’s health data is very high.",
          "score": 0,
          "created_utc": "2026-01-13 17:55:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcyx2d",
      "title": "Agent reliability testing is harder than we thought it would be",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qcyx2d/agent_reliability_testing_is_harder_than_we/",
      "author": "dinkinflika0",
      "created_utc": "2026-01-14 20:46:46",
      "score": 10,
      "num_comments": 14,
      "upvote_ratio": 0.78,
      "text": "I work at [Maxim](https://getmax.im/Max1m) building testing tools for AI agents. One thing that surprised us early on - hallucinations are way more insidious than simple bugs.\n\nRegular software bugs are binary. Either the code works or it doesn't. But agents hallucinate with full confidence. They'll invent statistics, cite non-existent sources, contradict themselves across turns, and sound completely authoritative doing it.\n\nWe built multi-level detection because hallucinations show up differently depending on where you look. Sometimes it's a single span (like a bad retrieval step). Sometimes it's across an entire conversation where context drifts and the agent starts making stuff up.\n\nThe evaluation approach we landed on combines a few things - faithfulness checks (is the response grounded in retrieved docs?), consistency validation (does it contradict itself?), and context precision (are we even pulling relevant information?). Also PII detection since agents love to accidentally leak sensitive data.\n\nPre-production simulation has been critical. We run agents through hundreds of scenarios with different personas before they touch real users. Catches a lot of edge cases where the agent works fine for 3 turns then completely hallucinates by turn 5.\n\nIn production, we run automated evals continuously on a sample of traffic. Set thresholds, get alerts when hallucination rates spike. Way better than waiting for user complaints.\n\nHardest part has been making the evals actually useful and not just noisy. Anyone can flag everything as a potential hallucination, but then you're drowning in false positives.\n\nNot trying to advertise but just eager to know how others are handling this in different setups and what other tools/frameworks/platforms are folks using for hallucination detection for production agents :)",
      "is_original_content": false,
      "link_flair_text": "Resources And Tips",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qcyx2d/agent_reliability_testing_is_harder_than_we/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzobddm",
          "author": "pbalIII",
          "text": "32% citing quality as the top production blocker tracks with what I've seen. The hard part isn't catching obvious failures... it's proving regression after a model swap when the output looks fine but behaves differently.\n\nWhat's helped me:\n- Gold set of ~50 nasty cases, tagged by failure mode\n- Re-run on every prompt change, not just deploys\n- LLM-as-judge for tone and formatting, deterministic checks for tool calls\n\nThe 89% observability vs 52% evals gap tells you where most teams are stuck. They can see what happened, but can't say if it was right.",
          "score": 3,
          "created_utc": "2026-01-15 04:43:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo40mc",
          "author": "deadweightboss",
          "text": "This is why companies like Anthropic (and my own, two years before) inject different system prompts depending on the context. \n\n\nLong context eval ks super hard because long context datasets aren’t really there. It’s why Google struggles so much at post training.",
          "score": 2,
          "created_utc": "2026-01-15 03:54:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxp025",
          "author": "real_serviceloom",
          "text": "@mods spam!",
          "score": 2,
          "created_utc": "2026-01-16 15:32:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo10j4",
          "author": "realzequel",
          "text": "Regular software bugs are binary. Either the code works or it doesn't\n\n  \nhuh? How many bugs have you encountered? I’ve seen all kinds of bugs that only happen occasionally or race conditions. Binary? Hah, you must be new to software development.",
          "score": 3,
          "created_utc": "2026-01-15 03:35:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzreuio",
              "author": "Herect",
              "text": "Intermittent bugs are the worse. If you can find the exact conditions that trigger it, the battle is half won already.",
              "score": 2,
              "created_utc": "2026-01-15 17:17:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmxeaw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-14 23:48:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmxecu",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-14 23:48:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmxoiw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-14 23:50:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmxon6",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-14 23:50:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrm5m0",
          "author": "no_witty_username",
          "text": "In agents these things are prevalent only if the harness is not set up well + bad system prompt. System prompt should be detailed yet concise, describing the role of the agent, its capabilities, its limitations, meta-cognitive information about its own framework, what the tools do and what data to trust and what data to take with a grain of salt. also metadata should be in place via harness system that helps in making said decisions. specking of harnesses, it should be designed from bottom up with a goal to allow easy and good vitrification via the agent. and a lot of system messages that guide the agent in many respects. all of these things are a bare minimum to get an agent working well, let alone other things like proper context management via smart auto compaction, rag, etc...",
          "score": 1,
          "created_utc": "2026-01-15 17:50:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzm89uk",
          "author": "Illustrious-Film4018",
          "text": "Really, according to people on r/accelerate, SOTA models don't hallucinate anymore and if it hallucinates, it's because you're using the wrong model or doing something wrong.",
          "score": 1,
          "created_utc": "2026-01-14 21:43:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmefyb",
              "author": "creaturefeature16",
              "text": "That's because those people are \"AI incels\" and not worth paying any attention to. They hate their own humanity and would prefer to leave it behind than take responsibility and do something good with their lives. ",
              "score": 5,
              "created_utc": "2026-01-14 22:11:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzmkc7u",
                  "author": "Illustrious-Film4018",
                  "text": "I agree, and they've never actually used AI for anything important.",
                  "score": 2,
                  "created_utc": "2026-01-14 22:40:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzmec7t",
              "author": "mossiv",
              "text": "I’ve been having hallucinations in very simple prompt windows. GPT especially. Within one sentence it told me a lie, I called it out and it gaslit me. It hallucinated, got it wrong and denied all accountability. If orgs are claiming hallucinations happen less it’s because they are steering them to be more authoritative. Which is worse.",
              "score": 1,
              "created_utc": "2026-01-14 22:11:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdtz6e",
      "title": "Need people to get excited part 2",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qdtz6e/need_people_to_get_excited_part_2/",
      "author": "External_Ad1549",
      "created_utc": "2026-01-15 19:57:04",
      "score": 7,
      "num_comments": 19,
      "upvote_ratio": 0.62,
      "text": "Three months ago I posted here saying I had found GLM-4.5 and coding suddenly felt like binge watching a Netflix series. Not because it was smarter, but because the flow never broke and affordable. I tried explaining that feeling to people around me and it mostly went over their heads.Then I shared it here  \n[https://www.reddit.com/r/ChatGPTCoding/comments/1nov9ab/need\\_people\\_to\\_get\\_excited/](https://www.reddit.com/r/ChatGPTCoding/comments/1nov9ab/need_people_to_get_excited/)\n\nSince then I’ve tried Cline, Claude Code, OpenCode. All of them are good tools and genuinely useful, but that original feeling didn’t really come back. It felt like improvement, not a shift.\n\nYesterday I tried Cerebras running GLM-4.7 and it was awesome. Around 1000 t/s output. Not just fast output the entire thinking phase completes almost instantly. In OpenCode, the model reasoned and responded in under a second, and my brain didn't even get the chance to lose focus.\n\nThat’s when it clicked for me: latency was the invisible friction all along. We’ve been trained to tolerate it, so we stopped noticing it. When it disappears, the experience changes completely. It feels less like waiting for an assistant and more like staying inside your own train of thought.\n\nI just wanted to share it with you guys because this good news only you can understand\n\nnote: We can't use Cerebras like a daily driver yet, their coding plans exclusive and brutal rate limits, they are able to achieve this bathroom tile size cpus, very interesting stuff I hope they succeed and do well\n\ntldr; discovered cerebras",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qdtz6e/need_people_to_get_excited_part_2/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzylqsm",
          "author": "PutPurple844",
          "text": "I got excited with the speed, too, not so much with the output. But it's insane once it is stable, we will kind of have zero downtime between iterations.",
          "score": 3,
          "created_utc": "2026-01-16 17:56:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyn8yq",
              "author": "External_Ad1549",
              "text": "yes exactly waiting for that",
              "score": 1,
              "created_utc": "2026-01-16 18:03:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzx178o",
          "author": "neurosurge",
          "text": "I tried GLM-4.7 (free and paid) for a couple of days, but it seems like it just hallucinates constantly and puts out low quality plans and code. \n\nI always have a different model/agent check another model’s work, and I always find a few things that need tweaked or updated. Every prompt with GLM-4.7 seemed to output garbage. I don’t know if it’s my setup with OpenCode or just poor model performance. \n\nI hope they get it fixed in a future release. The pricing and token allocation are amazing, especially compared to Anthropic’s offerings, but the reasoning seems to need a lot of work still.",
          "score": 2,
          "created_utc": "2026-01-16 13:34:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx2hx1",
              "author": "External_Ad1549",
              "text": "this is entirely based on my experience, the llm  any llm will have some capacity for it. some can create only functions, some only can create files, some can create modules, very costly ones can vibe code entire project. If you ask beyond the capacity it will start hallucinate.\n\nalso sometimes for cost cutting they reduce the capacity of llm, happens to chatgpt as well. GLM 4.7 is the work horse for me. Validation and stabilizing I will give it gemini 3 flash.\n\nany llm model which is not unlimited tokens gives kind of anxiety for me and also I work in mainly backend python systems so that might be different. \n\nI can say u can trust 4.7 for python backend",
              "score": 1,
              "created_utc": "2026-01-16 13:41:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuazf9",
          "author": "keepthepace",
          "text": "There is a reason why NVidia bought Groq. They have competition coming!",
          "score": 1,
          "created_utc": "2026-01-16 01:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv8zq1",
              "author": "External_Ad1549",
              "text": "and cerebras teaming up with gpt all good things",
              "score": 1,
              "created_utc": "2026-01-16 05:04:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvrpyc",
          "author": "real_serviceloom",
          "text": "Ok this is interesting because for me glm 4.7 has been absolutely slow as molasses. I tried the open code free glm 4.7 provider and also the cerebras version and both were incredibly slow.",
          "score": 1,
          "created_utc": "2026-01-16 07:30:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwvxbr",
              "author": "External_Ad1549",
              "text": "I am on max plan still sometimes feels like slow, but cerebras is on different league",
              "score": 1,
              "created_utc": "2026-01-16 13:03:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzxoosu",
                  "author": "real_serviceloom",
                  "text": "I dunno for me even on cerebras a /review on opencode just hangs forever compared to minimax m2.1. None of them come close to gpt 5.2 or opus 4.5 though. That's why I feel like z.ai lies a bit with their benchmarks showing them so close to opus. I know they have to make money but their marketing should be more honest. ",
                  "score": 1,
                  "created_utc": "2026-01-16 15:30:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwtfjm",
          "author": "popiazaza",
          "text": "Try Windsurf SWE-1.5. Should be based on GLM-4.6 and it run on Cerebras if you use Fast model.\n\nNormal model is also decently fast, and it's free to use for limited time.",
          "score": 1,
          "created_utc": "2026-01-16 12:48:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwvrap",
              "author": "External_Ad1549",
              "text": "really?? this is the info I require I wish it is glm 4.7 but something yeah thanks",
              "score": 1,
              "created_utc": "2026-01-16 13:02:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o056wlo",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-17 18:02:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o056woc",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-17 18:02:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05badc",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-17 18:22:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05bagk",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-17 18:22:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0enij6",
          "author": "SilencedObserver",
          "text": "You don’t know why the word “need” means.",
          "score": 1,
          "created_utc": "2026-01-19 02:36:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdrqw7",
      "title": "From your experience: practical limits to code generation for a dynamic web page? (here is mine)",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qdrqw7/from_your_experience_practical_limits_to_code/",
      "author": "toolznbytes",
      "created_utc": "2026-01-15 18:35:51",
      "score": 5,
      "num_comments": 15,
      "upvote_ratio": 0.86,
      "text": "(*using ChatGPT Business*)\n\nI'm asking ChatGPT for a self-contained HTML page, with embedded CSS and javascript, with a detailed specification I describe and refine.\n\nI successfully obtained a working page but it starts to derail here and there more and more often after a while, as the conversation goes on.\n\nI'm at iteration 13 or so, with a handful of preparation questions before.\n\nThe resulting html page has:\n\n* 4k CSS\n* 13k script\n* 3k data (as script const, not counted in the 13k)\n* 19k total with html\n* all the display, data parsing, list and 2 buttons are working well.\n\nI'm happy but has I said, at the step before it started to skip all the 3k data, using a placeholder instead. And before the data to process was damaged (edited).\n\nSo for me, it's near the practical limit I think. I'm afraid I'm run in more and more random regressions as I push further.\n\nMy questions:\n\n1. How far can you go before the need to split the tasks and stitch them together by hand?\n2. Is there any way to make it handle this kind of task in a more robust way?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qdrqw7/from_your_experience_practical_limits_to_code/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzsv6wv",
          "author": "Trotskyist",
          "text": "Don’t use ChatGPT for this task. Look into codex-cli or Claude code. I prefer Claude Code, but I believe codex is included with business plans.\n\nFor context: I’ve created full stack web applications with the above tools well into the hundreds of thousands of lines of code (not that LoC is a metric that tells you anything about quality, but it is an indication of complexity.)\n\nI mean on some level it’s impressive you’ve made it this far in only the chat interface, but you’re making things way harder on yourself than they need to be.",
          "score": 4,
          "created_utc": "2026-01-15 21:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx9hki",
          "author": "jonydevidson",
          "text": "Jesus fuck, why are you generating code in the web editor?\n\n1. Download VSCode\n2. Download Git (this is your game-save mechanic and will save your life)\n3. In VSCode, download Codex extension from OpenAI. Log in with your ChatGPT account. \n4. Create new folder on your disk. This will be your project folder. In VSCode, File > Open folder... and open it. This is now your WORKSPACE\n5. Tell the agent to initialize the git repository. This will now start tracking file changes. You STAGE changes that you wish to save, and then you make COMMITs. A commit is a fully human readable save-point in your project. You can navigate back to it and restore your project to that point or any other point, see the changes made in that commit, pick files to be reset back to the state at that commit and so much more. Download the Git Graph extension in VSCode but I recommend a Git UI like Fork.dev \n6. NOW you tell the agent what you want it to do. Every time you complete a feature and it's working, you commit the changes in Git. If at any point you or the agent fuck up, just discard the changes in Git and start over from your last save point - no fear of losing full progress or going in too deep in the wrong direction.",
          "score": 2,
          "created_utc": "2026-01-16 14:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz66fv",
              "author": "toolznbytes",
              "text": "Thank you for all this! 👍👍👍\n\nI realize I did something in a strange way... It worked surprisingly well, up to a point.\n\nOk, my next tool will be done with that workflow 😤\n\nBut now I'm out of the free trial, so I'll have to pay somewhere , codex or Claude or something. I guess they don't have pay per use.",
              "score": 1,
              "created_utc": "2026-01-16 19:27:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz82o7",
                  "author": "jonydevidson",
                  "text": "Codex limits will get you much further than Claude Code. Opus will eat the usage quotas, while with Codex you can actually use GPT5.2 Codex on Medium and get decent usage, while having a much smarter model than Sonnet 4.5.\n\nGive each one a try and see what you like, in the end it's just $20 and you'll be getting a lot more value out of it. Even if you completely fail and ship nothing, you'll have learned valuable skills in working with frontier tech that's going to be the future of work everywhere.",
                  "score": 1,
                  "created_utc": "2026-01-16 19:36:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzu9bys",
          "author": "evia89",
          "text": "Meh. Use good structure then web pack to static",
          "score": 1,
          "created_utc": "2026-01-16 01:34:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv5bol",
              "author": "toolznbytes",
              "text": "Sorry, can you say that again, please?",
              "score": 1,
              "created_utc": "2026-01-16 04:40:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzw4a0p",
                  "author": "evia89",
                  "text": "https://www.perplexity.ai/search/how-to-use-webpack-to-get-stat-iQLeJ_MVSymCr5a3Uhv7CQ#0",
                  "score": 1,
                  "created_utc": "2026-01-16 09:24:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvntx0",
          "author": "BattermanZ",
          "text": "As another commenter said, use Codex in VS Code or codex-cli. You will one shot your task. Best of all, it's included in your chatgpt subscription.",
          "score": 1,
          "created_utc": "2026-01-16 06:57:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwhigc",
          "author": "Tema_Art_7777",
          "text": "I use codex with gpt-5.2 - it is very good.",
          "score": 1,
          "created_utc": "2026-01-16 11:22:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx5c8x",
          "author": "NinjaLanternShark",
          "text": "I bailed out of coding via chat long before this kind of complexity. Just too frustrating. Personally I like VS Code with Copilot, but Codex, Claude or any similar coding-specific, agentic interface is the only way to handle tasks more than a few screens-worth of code.",
          "score": 1,
          "created_utc": "2026-01-16 13:56:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qelrpl",
      "title": "For loves sake no more AI frameworks. Lets move to AI infrastructure",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qelrpl/for_loves_sake_no_more_ai_frameworks_lets_move_to/",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-16 17:08:57",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.65,
      "text": "Every three minutes, there is a new agent framework that hits the market.\n\nPeople need tools to build with, I get that. But these abstractions differ oh so slightly, viciously change, and stuff everything in the application layer (some as black box, some as white) so now I wait for a patch because i've gone down a code path that doesn't give me the freedom to make modifications. Worse, these frameworks don't work well with each other so I must cobble and integrate different capabilities (guardrails, unified access with enterprise-grade secrets management for LLMs, etc).\n\nI want agentic infrastructure - clear separation of concerns - a jam/mern or LAMP stack like equivalent. I want certain things handled early in the request path (guardrails, tracing instrumentation, orchestration), I want to be able to design my agent instructions in the programming language of my choice (business logic), I want smart and safe retries to LLM calls using a robust access layer, and I want to pull from data stores via tools/functions that I define.\n\nI want simple libraries, I don't want frameworks. And I want to deliver agents to production in ways which is framework-agnostic and protocol-native.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qelrpl/for_loves_sake_no_more_ai_frameworks_lets_move_to/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzyctyx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-16 17:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzycu25",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-16 17:17:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzyiqmn",
          "author": "dumbledork99",
          "text": "What's your view on haystack. I have always found it allows you to choose how deep down the rabbit hole you want to go.",
          "score": 1,
          "created_utc": "2026-01-16 17:43:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzymocc",
              "author": "AdditionalWeb107",
              "text": "There are some obvious ones\n\n1/ Durable checkpointing and replay (compute infra). Maybe [Temporal](https://github.com/temporalio/temporal)?  \n2/ Data plane for routing, orchestration, observability, and moderation. Maybe [Plano](https://github.com/katanemo/plano)?  \n3/ Memory Infra for context compression and expansion? Maybe [Mem0](https://github.com/mem0ai/mem0)?  \n4/ Hybrid store (plus vector)? Maybe PostgreSQL?",
              "score": 1,
              "created_utc": "2026-01-16 18:01:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz7lvs",
                  "author": "dumbledork99",
                  "text": "I meant what are your views on [Haystack](http://haystack.deepaet.ai) ?\nHave you tried it?",
                  "score": 1,
                  "created_utc": "2026-01-16 19:34:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qgqrvi",
      "title": "Plano 0.4.3 ⭐️ Filter Chains via MCP and OpenRouter Integration",
      "subreddit": "ChatGPTCoding",
      "url": "https://i.redd.it/n1mwwe88n7eg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-19 01:40:12",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qgqrvi/plano_043_filter_chains_via_mcp_and_openrouter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0ede99",
          "author": "AdditionalWeb107",
          "text": "Link to repo: [https://github.com/katanemo/plano](https://github.com/katanemo/plano)",
          "score": 1,
          "created_utc": "2026-01-19 01:40:35",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf8d9o",
      "title": "Best autocomplete/next edit suggestion extension for VS Code?",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qf8d9o/best_autocompletenext_edit_suggestion_extension/",
      "author": "MiddleCodd",
      "created_utc": "2026-01-17 08:55:12",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I have used Cursor and Windsurf in the past, and both offered really powerful autocomplete and next-edit suggestions (like Windsurf Supercomplete/Tab and Cursor Tab). Their ability to predict not only new code but also nice tab completions based on recent context really sped up my workflow.\n\nNowadays, my employer requires us to use VS Code with GitHub Copilot. While Copilot's chat/agent mode has quite improved over the past months, its tab suggestions (referred to as “inline suggestions” or “next edit suggestions”) don’t quite match the level of quality I experienced with Cursor or Windsurf. The completions feel less intuitive and less context-aware.\n\nI’m wondering if there are any extensions specifically designed for autocomplete or tab suggestions. I’m not just looking for an extension that help with autocompleting new code, but also those that can provide smart tab completions on existing code based on stuff like recent changes, linter errors, or previously accepted edits like Cursor/Windsurf. \n\nMy goal would be to continue using GitHub Copilot for the chat/agent mode, but to replace its tab completions with another extension focused specifically on smarter inline suggestions.\n\nI don't mind paying a monthly subscription.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qf8d9o/best_autocompletenext_edit_suggestion_extension/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "o031hhc",
          "author": "NoEngineering3321",
          "text": "I am in the same position as you are. For me, windsurf was overall best, while copilot struggles with basics sometimes. I'm currently using Antigravity, but can't make any judgement yet.   \nJust to warn you.    \nConsult your idea with your employer. Having an enterprise subscription means the company won't use data to feed their tools with. While, your personal subscription is used.    \nMost companies are working with one tool and all others are forbidden",
          "score": 2,
          "created_utc": "2026-01-17 10:28:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hfnxq",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-19 14:51:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hfo0x",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-19 14:51:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qh9aca",
      "title": "Quick Question: What do you need most from your AI Coding Tools?",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qh9aca/quick_question_what_do_you_need_most_from_your_ai/",
      "author": "Jbbrack03",
      "created_utc": "2026-01-19 16:43:46",
      "score": 3,
      "num_comments": 9,
      "upvote_ratio": 0.64,
      "text": "Hey folks!\n\nI've been deep in the Claude Code / AI coding agent space for a while, and I'm doing market research to determine whether a tool I'm building could actually solve real problems.\n\nMany projects fail because the dev never asks the community about what they want, and about what problems they actually face. So I'm making no assumptions! Below is a link to a Google Forms questionnaire that has a few quick questions. Completely anonymous (no email required). This will help to shape the direction of what I'm building. Thank you for partnering in this process!\n\n[https://forms.gle/LAXwhxPfqbVzGT3j6](https://forms.gle/LAXwhxPfqbVzGT3j6)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qh9aca/quick_question_what_do_you_need_most_from_your_ai/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "o0i8flr",
          "author": "funbike",
          "text": "You aren't \"curious\".  You are doing market research. Be honest about your intentions.",
          "score": 7,
          "created_utc": "2026-01-19 17:03:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0i92hv",
              "author": "Jbbrack03",
              "text": "Yes, that's fair and I didn't mean to imply otherwise. Statistically 42% of software products fail because the developer assumed that they knew what their audience wanted, and just bulldozed ahead without actually asking. And that's all I'm doing. Asking so that what I build helps the community and isn't just more spam.",
              "score": -4,
              "created_utc": "2026-01-19 17:06:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ilrzd",
                  "author": "Cast_Iron_Skillet",
                  "text": "Why not come out and say it? People see through lies and half truths.",
                  "score": 4,
                  "created_utc": "2026-01-19 18:03:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0k9ab5",
          "author": "Frequent-Complaint-6",
          "text": "Money!",
          "score": 2,
          "created_utc": "2026-01-19 22:42:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0idxj3",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-19 17:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0idxl2",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-19 17:28:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l7mpa",
          "author": "Mice_With_Rice",
          "text": "I will consider answering if it is built on a public git repo under a GLP license with multiple contributors that are not affiliated with eachother.",
          "score": 1,
          "created_utc": "2026-01-20 01:47:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}