{
  "metadata": {
    "last_updated": "2026-01-18 16:49:53",
    "time_filter": "week",
    "subreddit": "ChatGPTCoding",
    "total_items": 12,
    "total_comments": 101,
    "file_size_bytes": 104427
  },
  "items": [
    {
      "id": "1qeq6yd",
      "title": "Codex is about to get fast",
      "subreddit": "ChatGPTCoding",
      "url": "https://i.redd.it/faicwqlvmrdg1.png",
      "author": "thehashimwarren",
      "created_utc": "2026-01-16 19:49:14",
      "score": 187,
      "num_comments": 83,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qeq6yd/codex_is_about_to_get_fast/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzzk4r5",
          "author": "TheMacMan",
          "text": "Press release for those curious. It's a partnership allowing OpenAI to utilize Cerebras wafers. No specific dates, just rolling out in 2026.\n\n[https://www.cerebras.ai/blog/openai-partners-with-cerebras-to-bring-high-speed-inference-to-the-mainstream](https://www.cerebras.ai/blog/openai-partners-with-cerebras-to-bring-high-speed-inference-to-the-mainstream)",
          "score": 29,
          "created_utc": "2026-01-16 20:32:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02zfdk",
              "author": "amarao_san",
              "text": "So, even more chip production capacity is eaten away.\n\nThey took GPUs. I wasn't a gamer, so I didn't protest.\n\nThey took RAM. I wasn't much of a ram hoarder, so I didn't protest.\n\nThey took SSD. I wasn't much of space hoarder, so I didn't protest.\n\nThen they come for chips. Computation including. But there was none near me to protest, because of ai girlfriends and slop...",
              "score": 11,
              "created_utc": "2026-01-17 10:08:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04z2lj",
                  "author": "eli_pizza",
                  "text": "You were planning to do something else with entirely custom chips built for inference?",
                  "score": 8,
                  "created_utc": "2026-01-17 17:26:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o00bssx",
          "author": "UsefulReplacement",
          "text": "It might also become randomly stupid and unreliable, just like the Anthropic models. When you run the inference across different hardware stacks, you have a variety of differences and subtle but performance-impacting bugs show up. It’s a challenging problem keeping the model the same across hardware.",
          "score": 48,
          "created_utc": "2026-01-16 22:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08bltx",
              "author": "JustThall",
              "text": "My team was running into all sorts of bugs when run a mix and match training and inference stacks with llama/mistral models. I can only imagine the hell they gonna run into with MoE and different hardware support of mixed precision types.",
              "score": 4,
              "created_utc": "2026-01-18 03:39:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o015x5b",
              "author": "YourKemosabe",
              "text": "Was looking for this comment. God I hope they don’t ruin Codex too.",
              "score": 3,
              "created_utc": "2026-01-17 01:39:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o02scmk",
              "author": "Tolopono",
              "text": "Its the same weights and same math though. I dont see how it would change anything ",
              "score": 2,
              "created_utc": "2026-01-17 09:01:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o033um9",
                  "author": "UsefulReplacement",
                  "text": "clearly you have no clue then",
                  "score": -6,
                  "created_utc": "2026-01-17 10:49:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzzwpra",
          "author": "aghowl",
          "text": "What is Cerebras?",
          "score": 14,
          "created_utc": "2026-01-16 21:32:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzzgrt",
              "author": "innocentVince",
              "text": "Inference provider with custom hardware.",
              "score": 15,
              "created_utc": "2026-01-16 21:45:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o002kha",
                  "author": "io-x",
                  "text": "Are they public?",
                  "score": 4,
                  "created_utc": "2026-01-16 21:59:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o04h6c1",
                  "author": "eli_pizza",
                  "text": "Custom hardware built for inference speed. Currently the fastest throughput for open source models, by a lot.",
                  "score": 2,
                  "created_utc": "2026-01-17 16:02:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o00l1qv",
                  "author": "pjotrusss",
                  "text": "what does it mean? more GPUs?",
                  "score": 3,
                  "created_utc": "2026-01-16 23:35:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o00ctl6",
          "author": "Square-Ambassador-92",
          "text": "Nobody asked for fast … we need very intelligent",
          "score": 26,
          "created_utc": "2026-01-16 22:50:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00e7v3",
              "author": "Outrageous-Thing-900",
              "text": "Codex is extremely slow, and a lot of people complain about it",
              "score": 36,
              "created_utc": "2026-01-16 22:58:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o018l51",
                  "author": "not_the_cicada",
                  "text": "It also continuously forgets how to walk the code base and uses really odd choices that bog it down and make it even slower. ",
                  "score": 7,
                  "created_utc": "2026-01-17 01:56:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o02gayo",
                  "author": "SpyMouseInTheHouse",
                  "text": "Those who complain are welcome to move to Claude code.",
                  "score": 1,
                  "created_utc": "2026-01-17 07:10:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00h0jc",
              "author": "mimic751",
              "text": "Be a developer",
              "score": 10,
              "created_utc": "2026-01-16 23:13:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o00mjls",
                  "author": "Ok_Possible_2260",
                  "text": "Find out your code is shit in 10 seconds is better than 40 minutes. ",
                  "score": 6,
                  "created_utc": "2026-01-16 23:44:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o02aina",
              "author": "realfunnyeric",
              "text": "It’s brilliant. But slow. This is the right move.",
              "score": 5,
              "created_utc": "2026-01-17 06:19:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00pm2p",
              "author": "Shoddy-Marsupial301",
              "text": "I ask for fast..",
              "score": 2,
              "created_utc": "2026-01-17 00:01:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o04jzen",
              "author": "eli_pizza",
              "text": "Couldn’t disagree more. Very fast inference means I can work with a coding agent in real time, instead of kicking off a request and doing something else while it works and switching back.   I think a lot of the multi agent orchestration stuff going on now is really a hack because inference is so slow. \n\nAnd if something looks off in the diff I’m more likely to guide it to do better if it makes the update instantly. \n\nMy GLM 4.6 subscription on Cerebras is great for front end work. I can just say “make the text colors darker” “no not that dark” and see the changes instantly.",
              "score": 1,
              "created_utc": "2026-01-17 16:15:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00kew9",
          "author": "whawkins4",
          "text": "Yeah, but is it GOOD?",
          "score": 4,
          "created_utc": "2026-01-16 23:32:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o038gke",
          "author": "jonas_c",
          "text": "Faster codex with existing models or a fast model that no one wants?",
          "score": 3,
          "created_utc": "2026-01-17 11:32:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00qikd",
          "author": "dalhaze",
          "text": "Yeah also quantized to ass",
          "score": 6,
          "created_utc": "2026-01-17 00:06:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzvxcr",
          "author": "AppealSame4367",
          "text": "Yes, that would really be something!",
          "score": 2,
          "created_utc": "2026-01-16 21:28:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o043g2q",
          "author": "Sufficient-Year4640",
          "text": "What does he mean by fast exactly? I've been using Codex for a while and it seems pretty fast. Like is it actually slower than Claude or something?",
          "score": 2,
          "created_utc": "2026-01-17 14:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04oxrm",
              "author": "thehashimwarren",
              "text": "People report that Claude Opus 4.5 is faster",
              "score": 2,
              "created_utc": "2026-01-17 16:39:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00cm1k",
          "author": "OccassionalBaker",
          "text": "It needs to be right before I can get excited about it being fast - being wrong faster isn’t that useful.",
          "score": 2,
          "created_utc": "2026-01-16 22:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00edo5",
              "author": "touhoufan1999",
              "text": "Codex with gpt-5.2-xhigh is as accurate as you can get at the moment. Extremely low hallucination rates even on super hard tasks. It's just very slow right now. Cerebras says they're around 20x faster than NVIDIA at inference.",
              "score": 3,
              "created_utc": "2026-01-16 22:59:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o02n508",
                  "author": "OccassionalBaker",
                  "text": "I’ve been writing code for 20 years and have to disagree that the hallucinations are very low, I’m constantly fixing its errors.",
                  "score": 0,
                  "created_utc": "2026-01-17 08:12:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzzuzxd",
          "author": "MXBT9W9QX96",
          "text": "Wow huge news",
          "score": 1,
          "created_utc": "2026-01-16 21:24:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00u3um",
          "author": "Opinion-Former",
          "text": "Fast is good, compliant and following instructions is better.",
          "score": 1,
          "created_utc": "2026-01-17 00:27:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08z4dm",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-18 06:23:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08z4en",
                  "author": "AutoModerator",
                  "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
                  "score": 1,
                  "created_utc": "2026-01-18 06:23:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01r47b",
          "author": "roinkjc",
          "text": "It’s the best for complicated setups, I hope they keep it that way",
          "score": 1,
          "created_utc": "2026-01-17 03:55:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02wozm",
          "author": "GnistAI",
          "text": "Fast, as in tokens per second? The limiting factor right now is not tokens per second, it is bugs per hour.",
          "score": 1,
          "created_utc": "2026-01-17 09:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02x203",
          "author": "tango650",
          "text": "How is \"low latency\" different from \"fast\" in the context of inference. Anyone ?",
          "score": 1,
          "created_utc": "2026-01-17 09:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04verw",
              "author": "ExcitingAssistance",
              "text": "Same as ping vs download speed",
              "score": 2,
              "created_utc": "2026-01-17 17:08:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04znn9",
                  "author": "tango650",
                  "text": "Thanks for your input. It is quite unusable but thanks anyway.",
                  "score": 1,
                  "created_utc": "2026-01-17 17:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o07th7p",
              "author": "hellomistershifty",
              "text": "Time to first token vs tokens/second",
              "score": 2,
              "created_utc": "2026-01-18 01:58:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o09cv9a",
                  "author": "tango650",
                  "text": "Thanks. Do you know how hardware of the processor influences this ? And what order of difference are we talking about ?",
                  "score": 1,
                  "created_utc": "2026-01-18 08:24:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03thxs",
          "author": "phylter99",
          "text": "We'll be able to burn through our credits faster than ever.",
          "score": 1,
          "created_utc": "2026-01-17 14:01:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05bmq0",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-17 18:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05bmt3",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-17 18:24:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ajimt",
          "author": "Adventurous-Bet-3928",
          "text": "Damn. I was in a call with Cerebras and was asking them why the big AI companies weren't using them just a few weeks ago.",
          "score": 1,
          "created_utc": "2026-01-18 14:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01abm1",
          "author": "bhannik-itiswatitis",
          "text": "oh nice, fast hallucinations",
          "score": 0,
          "created_utc": "2026-01-17 02:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02hh6t",
              "author": "popiazaza",
              "text": "This is GPT 5, not Gemini.",
              "score": 4,
              "created_utc": "2026-01-17 07:20:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00lphz",
          "author": "Zealousideal-Idea-72",
          "text": "Who uses OpenAI anymore though?  Anthropic (coding) and Gemini (general purpose) have surpassed them.",
          "score": -5,
          "created_utc": "2026-01-16 23:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00o96y",
              "author": "Kooky_Tourist_3945",
              "text": "900 million active monthly users.\nAre you dumb.",
              "score": 6,
              "created_utc": "2026-01-16 23:53:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00ptux",
              "author": "NotSGMan",
              "text": "You wont believe how good codex 5.2 xhigh is",
              "score": 6,
              "created_utc": "2026-01-17 00:02:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01s9yr",
                  "author": "Freed4ever",
                  "text": "Or just high...",
                  "score": 1,
                  "created_utc": "2026-01-17 04:03:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o01vhe4",
                  "author": "ThisGuyCrohns",
                  "text": "Not even close to opus",
                  "score": 1,
                  "created_utc": "2026-01-17 04:25:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0173yc",
              "author": "rambouhh",
              "text": "I dont know codex seems to be very very popular right now. The consensus seems to be shifting to that codex is better for longer complex tasks but slower, and CC is better for the simple stuff because it is so much faster",
              "score": 3,
              "created_utc": "2026-01-17 01:47:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01vfbr",
                  "author": "ThisGuyCrohns",
                  "text": "Not really. Claude is where it’s at. Codex was good 3 months ago. Claude overtook that and there isn’t a reason to go back",
                  "score": 1,
                  "created_utc": "2026-01-17 04:25:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00slgd",
              "author": "iritimD",
              "text": "Anyone who is serious about coding uses either a mix of cc and 5.2 codex or just codex",
              "score": 5,
              "created_utc": "2026-01-17 00:18:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o012ci2",
                  "author": "robogame_dev",
                  "text": "TIL I’m not serious about coding :’(",
                  "score": 2,
                  "created_utc": "2026-01-17 01:17:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o02a6mv",
                  "author": "TenshiS",
                  "text": "Opus 4.5 undefeated",
                  "score": 1,
                  "created_utc": "2026-01-17 06:17:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qcr3zw",
      "title": "Ralph Loop inspired me to build this - AI decides what Claude Code does next orchestrating claude code until task is done",
      "subreddit": "ChatGPTCoding",
      "url": "https://i.redd.it/idx5kfij8cdg1.png",
      "author": "RegionCareful7282",
      "created_utc": "2026-01-14 16:02:37",
      "score": 33,
      "num_comments": 9,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qcr3zw/ralph_loop_inspired_me_to_build_this_ai_decides/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzn1hwb",
          "author": "BaCaDaEa",
          "text": "This looks really cool, man. We've got a collaboration coming up for the community (be on the look out for that guys!) but once that's over, I'd be happy to pin you for a while!",
          "score": 1,
          "created_utc": "2026-01-15 00:10:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmpuax",
          "author": "ihateredditors111111",
          "text": "lol not trying to be a hater that’s cool and all but it’s just funny thinking what clusterfucks it might end up building",
          "score": 3,
          "created_utc": "2026-01-14 23:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqh4rj",
              "author": "DrummerHead",
              "text": "It all hinges in the verification step. If that doesn't work, then it's a loop of poop.",
              "score": 3,
              "created_utc": "2026-01-15 14:42:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzmxqfq",
              "author": "RickyDontLoseThat",
              "text": "I think this is how we end up with SkyNet.",
              "score": 1,
              "created_utc": "2026-01-14 23:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznkzi7",
                  "author": "JohnnyLovesData",
                  "text": "![gif](giphy|xT5LMzIK1AdZJ4cYW4)",
                  "score": 1,
                  "created_utc": "2026-01-15 02:00:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzsd121",
          "author": "WolfeheartGames",
          "text": "Reinventing behavior trees one tiny step at a time. I am working on a full RAG with an integrated behavior tree for the actual agent harness to handle this intelligently.\n\nhttps://github.com/NoSaaS-me/Vlt-Bridge/tree/020-bt-oracle-agent",
          "score": 2,
          "created_utc": "2026-01-15 19:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxlcae",
          "author": "sridoodla",
          "text": "Can this use the claude subscription for claude code?",
          "score": 2,
          "created_utc": "2026-01-16 15:15:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02god3",
          "author": "Gustafssonz",
          "text": "How much those these running tasks cost in the end?",
          "score": 1,
          "created_utc": "2026-01-17 07:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvikb6",
          "author": "quantier",
          "text": "Could this work with Kilo Code or the other local AI extensions",
          "score": 1,
          "created_utc": "2026-01-16 06:14:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc0fhl",
      "title": "Agent observability is way different from regular app monitoring - maintainer's pov",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qc0fhl/agent_observability_is_way_different_from_regular/",
      "author": "dinkinflika0",
      "created_utc": "2026-01-13 19:17:50",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 0.73,
      "text": "Work at [Maxim](https://getmax.im/Max1m) on the observability side. Been thinking about how traditional APM tools just don't work for agent workflows.\n\nAgents aren't single API calls. They're multi-turn conversations with tool invocations, retrieval steps, reasoning chains, external API calls. When something breaks, you need the entire execution path, not just error logs.\n\nWe built distributed tracing at multiple levels - sessions for full conversations, traces for individual exchanges, spans for specific steps like LLM calls or tool usage. Helps a lot when debugging.\n\nThe other piece that's been useful is running automated evals continuously on production logs. Track quality metrics (relevance, faithfulness, hallucination rates) alongside the usual stuff like latency and cost. Set thresholds, get alerts in Slack when things go sideways.\n\nAlso built custom dashboards since production agents need domain-specific insights. Teams track success rates for workflows, compare model versions, identify where things break.\n\nHardest part has been capturing context across async operations and handling high-volume traffic without killing performance. Making traces actually useful for debugging instead of just noise takes work.\n\nWanted to know how others are handling observability for multi-step agents in production? DMs are always welcome for discussion!",
      "is_original_content": false,
      "link_flair_text": "Resources And Tips",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qc0fhl/agent_observability_is_way_different_from_regular/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qaomxf",
      "title": "Workflows for sharing information between ChatGPT and Codex (or other agents)?",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qaomxf/workflows_for_sharing_information_between_chatgpt/",
      "author": "99ducks",
      "created_utc": "2026-01-12 07:30:10",
      "score": 14,
      "num_comments": 24,
      "upvote_ratio": 1.0,
      "text": "I often do a lot of brainstorming in chatgpt and then generate a HANDOFF.md to copy and paste for codex to review.\n\n\n\nI've tried using the \"Work with apps\" feature to connect with vs code, but that doesn't work well. There's a lot of back and forth to ensure you have the correct vscode tab open, it often writes to the wrong file, and you have to manually enable it every time.\n\n\n\nDoes anybody have a better solution they like?\n\nedit: @mods, the requirement to add a user flair breaks posting on old reddit with no error message.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qaomxf/workflows_for_sharing_information_between_chatgpt/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzdqjlh",
          "author": "BaCaDaEa",
          "text": "My apologies for that! \n\nThe user flair requirement is one of the few things that have worked to throttle spam. It's still not perfect, but it easily reduces more than half the spam that gets through.\n\nI'll see what I can do to fix the issue",
          "score": 1,
          "created_utc": "2026-01-13 17:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4y0ge",
          "author": "popiazaza",
          "text": "Not sure why you have to do that, but this may help: https://github.com/robertpiosik/CodeWebChat",
          "score": 3,
          "created_utc": "2026-01-12 09:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz505ga",
              "author": "99ducks",
              "text": "Thanks I'll look into that.\n\nI'll often start brainstorming projects/features on my phone in the chatgpt app while away from the computer and then want to move to a coding agent later once I'm ready to build.",
              "score": 1,
              "created_utc": "2026-01-12 10:14:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz512r2",
                  "author": "popiazaza",
                  "text": "I see. Maybe store your story somewhere like Linear? It is well supported on ChatGPT and you could use MCP to read your plan.",
                  "score": 1,
                  "created_utc": "2026-01-12 10:22:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz5fyrd",
          "author": "Competitive_Travel16",
          "text": "Tell chat to write an INSTRUCTIONS.md file in a code block and then prompt your agent to do what it says?\n\nThe GitHub connector app used by codex web is not available in chat for some reason. (I think this is a bug, but it might technically be an uncompleted feature.)",
          "score": 1,
          "created_utc": "2026-01-12 12:26:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5h76p",
              "author": "99ducks",
              "text": "That's what I'm doing now. My challenge is, how do I automatically get information from INSTRUCTIONS.md to codex. Copy & paste is a slow process.",
              "score": 1,
              "created_utc": "2026-01-12 12:34:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz5jqwi",
                  "author": "Competitive_Travel16",
                  "text": "I don't understand, which part of copy & paste is slow? Pasting into codex-cli directly? Can you open a shell and `cat > INSTRUCTIONS.md` [paste] <ctrl-D> instead?",
                  "score": 1,
                  "created_utc": "2026-01-12 12:52:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz5ifei",
          "author": "boz_lemme",
          "text": "My flow is pretty much the same only I have an [instructions.md](http://instructions.md) for universal guidelines and a [requirements.md](http://requirements.md) for the thing I want to build. I've been looking into [Plai](http://useplai.com) and that seems promising. But still I notice that when I go fully manual, I tend to overlook important considerations. So I stick to my workflow.",
          "score": 1,
          "created_utc": "2026-01-12 12:43:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5omwq",
          "author": "samuel79s",
          "text": "codex has a mcp-server mode, although it's stdio only. That means it needs to be coupled with a software that acts as a bridge (mcpo, mcp-proxy, etc...)\n\nJust did a quick test using mcpo configuring a Custom GPT and it worked (it opened a session and summarized the content of a project). \n\nThe mcp \"developer mode\" should work fine, too (better, since supports thinking models).",
          "score": 1,
          "created_utc": "2026-01-12 13:22:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz6454u",
              "author": "99ducks",
              "text": "It's my understanding to use a MCP server with ChatGPT Desktop you have to expose the server to the internet via something like ngrok. And then you need an OAuth 2.0 flow in front of it.\n\nIs that how you have it set up?",
              "score": 1,
              "created_utc": "2026-01-12 14:47:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz6a19i",
                  "author": "samuel79s",
                  "text": "Not exactly because it was a quick check and I find easier to set up agpt action with mcpo\n\nBut yes, that was the point.\n\nCheck these (the last one it was the one I set up but it doesn't allow to use thinking models so it's not a good fit for this use case).\n\n\nhttps://www.reddit.com/r/mcp/comments/1obebf3/proxying_local_mcps_for_chatgpt_secure_way/\n\nhttps://old.reddit.com/r/mcp/comments/1nfqmyg/local_mcps_in_chatgpt_yolo_mode/\n\nhttps://harmlesshacks.blogspot.com/2025/05/using-mcp-servers-from-chatgpt.html?m=1\n\nThe timeouts can be a problem, though.",
                  "score": 1,
                  "created_utc": "2026-01-12 15:17:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz5p53v",
          "author": "ggone20",
          "text": "Work with apps works basically every time. I don’t get the issue?",
          "score": 1,
          "created_utc": "2026-01-12 13:25:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz63jpg",
              "author": "99ducks",
              "text": "What apps do you work with?\n\nIt constantly fails for me when working with VS Code. Let's say I have a markdown file (README.md) and python file (main.py) open and ask it to update the markdown file. Most of the time it will write the markdown to both README.md and main.py.",
              "score": 1,
              "created_utc": "2026-01-12 14:44:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz6d84m",
                  "author": "ggone20",
                  "text": "I use many of the apps but Cursor for this specific thing we’re talking about. I’m not sure what you mean about having multiple things open - you can only have one file open at a time; whatever file is ‘focused’ is the one that Chat can edit. \n\nAs you mentioned I often brainstorm with Chat, then I have it write a few specs and readme using the work with apps. Then I have codex cli/ide and a Claude code do the actual work. \n\nI don’t use Cursor AI features so I’m assuming just VS Code would probably be the same. Sorry I don’t really get how it can try to write multiple files so not much to comment on there. You can’t change the focus file while things are writing from the ChatGPT app - that sometimes messes things up kind of like you mentioned but I learned quick to just not change the focus. \n\nBest of luck finding a solution that works!",
                  "score": 1,
                  "created_utc": "2026-01-12 15:33:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz68c83",
          "author": "Tryin2Dev",
          "text": "RepoPrompt has this built in.",
          "score": 1,
          "created_utc": "2026-01-12 15:09:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzae8of",
          "author": "sply450v2",
          "text": "I make a skill in Codex What it does is it generates a detailed prompt of the task I want to give ChatGPT, whether it be brainstorming or coming up with a plan for a feature. It then creates a zip file of all the relevant repo files that are applicable to that task, and then I just upload the zip into ChatGPT on the Pro model. \n\nThe Pro model is basically the best model that exists so my plans are always crispy.",
          "score": 1,
          "created_utc": "2026-01-13 03:21:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbq97e",
          "author": "Ok_Signature9963",
          "text": "I can relate. One workaround that helped me is exposing a local markdown/docs endpoint and letting agents pull from it directly; lightweight tunneling tools like [Pinggy.io](http://Pinggy.io) make that pretty painless without juggling editor state or copy-paste loop",
          "score": 1,
          "created_utc": "2026-01-13 09:32:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzehar3",
          "author": "WolfeheartGames",
          "text": "This has direct integration to chatgpt web you can set up. I'm working on a different fork adding a lot of features, but if you want a base to build from heres an earlier version demo: https://huggingface.co/spaces/MCP-1st-Birthday/Vault.MCP\n\nhttps://github.com/bigwolfeman/Document-MCP",
          "score": 1,
          "created_utc": "2026-01-13 19:09:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbpr58",
      "title": "Is there a realistic application for vibecoding in healthcare?",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qbpr58/is_there_a_realistic_application_for_vibecoding/",
      "author": "liltoxicThunder820",
      "created_utc": "2026-01-13 12:13:50",
      "score": 11,
      "num_comments": 14,
      "upvote_ratio": 0.76,
      "text": "Asking this as someone who's kind of in the healthtech field. Like I keep seeing vibecoding used for fast prototypes and internal tools, but I am curious where people draw the line in a regulated environment.\n\nAre there realistic use cases where speed actually helps without creating compliance or maintenance nightmares? Would love to hear examples of where this has worked in practice, especially for non core clinical workflows.\n\nThere are plenty of tools that help streamline it but I'm curious if there's a longterm opportunity just to fast track prototypes and all that (Examples like Replit, Specode, Lovable, etc)",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qbpr58/is_there_a_realistic_application_for_vibecoding/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzfy1ed",
          "author": "TheCountEdmond",
          "text": "I build healthcare systems and we 100% use AI coding tools. It's going to vary company by company but everyone has access to copilot and is encouraged to use it.\n\nWe also religiously review and test with very high standards so we haven't had issues. For HIPAA there are cloud providers that are compliant so unsure what the other poster is talking about.",
          "score": 13,
          "created_utc": "2026-01-13 23:19:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhi0fy",
              "author": "ThePlotTwisterr----",
              "text": "openai offers a custom price for [BAA](https://help.openai.com/en/articles/8660679-how-can-i-get-a-business-associate-agreement-baa-with-openai), mainly targeted at those who require HIPAA compliance with GPT.",
              "score": 3,
              "created_utc": "2026-01-14 04:41:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzhl6e6",
          "author": "L1amm",
          "text": "Prototypes and internal tools? Sure why not!\n\nVibecoding an entire EMR system? Fuck no.",
          "score": 3,
          "created_utc": "2026-01-14 05:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzk3cjb",
              "author": "Much-Journalist3128",
              "text": "They use AI for insurance claim reviews and... denials lololol",
              "score": 2,
              "created_utc": "2026-01-14 15:57:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm6zy7",
                  "author": "admiral_nivak",
                  "text": "Very different to an EMR or Claim processing system.",
                  "score": 2,
                  "created_utc": "2026-01-14 21:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzhlzkx",
          "author": "M44PolishMosin",
          "text": "Have written pre-approved requirements driven by a risk management process and test to your requirements. Doesn't matter who or what actually writes the code.",
          "score": 3,
          "created_utc": "2026-01-14 05:09:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh7lq6",
          "author": "eli_pizza",
          "text": "Prototypes presumably don't contain any actual protected health information so you can make them however you want",
          "score": 1,
          "created_utc": "2026-01-14 03:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhqjj0",
          "author": "huzbum",
          "text": "Possibly, but you would need to understand the regulatory requirements and be able to verify that they are being met.  It doesn't matter how the code is created, but you have to understand what it is actually doing and whether or not it meets the requirements.",
          "score": 1,
          "created_utc": "2026-01-14 05:43:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzivou7",
          "author": "cornelln",
          "text": "Where is there not an opportunity for vibe coding other the where you want no software or anything build by software?",
          "score": 1,
          "created_utc": "2026-01-14 11:52:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpzdcy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-15 13:06:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpzdf1",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-15 13:06:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o08yucz",
          "author": "botapoi",
          "text": "i'm building something related to the field rn and have found blink with chatgpt super useful for getting a basic version up fast. it handles the auth and backend stuff so you can really nail the core logic and ngl, it's way quicker than setting up firebase from scratch. dont forget to prompt the agent to search and implement security features in your project",
          "score": 1,
          "created_utc": "2026-01-18 06:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzeqtc5",
          "author": "kidajske",
          "text": "Absolutely fucking not.",
          "score": 1,
          "created_utc": "2026-01-13 19:52:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze0nvg",
          "author": "thisdude415",
          "text": "The key regulatory hurdle in the US is HIPAA/HITECH which means you can’t use any cloud software including AI providers without specific agreements in place. I suspect that is true of all the providers you mentioned (and also cloudflare and vercel) \n\nIf you’re a small to medium practice (like a few psychologists or something), there’s a lot of space for vibe coded local apps to do things like intake forms or tracking or stuff like that. \n\nBut in general, the risks for fucking up when you’re dealing with other people’s health data is very high.",
          "score": 0,
          "created_utc": "2026-01-13 17:55:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcyx2d",
      "title": "Agent reliability testing is harder than we thought it would be",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qcyx2d/agent_reliability_testing_is_harder_than_we/",
      "author": "dinkinflika0",
      "created_utc": "2026-01-14 20:46:46",
      "score": 10,
      "num_comments": 14,
      "upvote_ratio": 0.78,
      "text": "I work at [Maxim](https://getmax.im/Max1m) building testing tools for AI agents. One thing that surprised us early on - hallucinations are way more insidious than simple bugs.\n\nRegular software bugs are binary. Either the code works or it doesn't. But agents hallucinate with full confidence. They'll invent statistics, cite non-existent sources, contradict themselves across turns, and sound completely authoritative doing it.\n\nWe built multi-level detection because hallucinations show up differently depending on where you look. Sometimes it's a single span (like a bad retrieval step). Sometimes it's across an entire conversation where context drifts and the agent starts making stuff up.\n\nThe evaluation approach we landed on combines a few things - faithfulness checks (is the response grounded in retrieved docs?), consistency validation (does it contradict itself?), and context precision (are we even pulling relevant information?). Also PII detection since agents love to accidentally leak sensitive data.\n\nPre-production simulation has been critical. We run agents through hundreds of scenarios with different personas before they touch real users. Catches a lot of edge cases where the agent works fine for 3 turns then completely hallucinates by turn 5.\n\nIn production, we run automated evals continuously on a sample of traffic. Set thresholds, get alerts when hallucination rates spike. Way better than waiting for user complaints.\n\nHardest part has been making the evals actually useful and not just noisy. Anyone can flag everything as a potential hallucination, but then you're drowning in false positives.\n\nNot trying to advertise but just eager to know how others are handling this in different setups and what other tools/frameworks/platforms are folks using for hallucination detection for production agents :)",
      "is_original_content": false,
      "link_flair_text": "Resources And Tips",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qcyx2d/agent_reliability_testing_is_harder_than_we/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzobddm",
          "author": "pbalIII",
          "text": "32% citing quality as the top production blocker tracks with what I've seen. The hard part isn't catching obvious failures... it's proving regression after a model swap when the output looks fine but behaves differently.\n\nWhat's helped me:\n- Gold set of ~50 nasty cases, tagged by failure mode\n- Re-run on every prompt change, not just deploys\n- LLM-as-judge for tone and formatting, deterministic checks for tool calls\n\nThe 89% observability vs 52% evals gap tells you where most teams are stuck. They can see what happened, but can't say if it was right.",
          "score": 3,
          "created_utc": "2026-01-15 04:43:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo40mc",
          "author": "deadweightboss",
          "text": "This is why companies like Anthropic (and my own, two years before) inject different system prompts depending on the context. \n\n\nLong context eval ks super hard because long context datasets aren’t really there. It’s why Google struggles so much at post training.",
          "score": 2,
          "created_utc": "2026-01-15 03:54:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo10j4",
          "author": "realzequel",
          "text": "Regular software bugs are binary. Either the code works or it doesn't\n\n  \nhuh? How many bugs have you encountered? I’ve seen all kinds of bugs that only happen occasionally or race conditions. Binary? Hah, you must be new to software development.",
          "score": 3,
          "created_utc": "2026-01-15 03:35:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzreuio",
              "author": "Herect",
              "text": "Intermittent bugs are the worse. If you can find the exact conditions that trigger it, the battle is half won already.",
              "score": 2,
              "created_utc": "2026-01-15 17:17:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmxeaw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-14 23:48:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmxecu",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-14 23:48:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmxoiw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-14 23:50:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmxon6",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-14 23:50:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrm5m0",
          "author": "no_witty_username",
          "text": "In agents these things are prevalent only if the harness is not set up well + bad system prompt. System prompt should be detailed yet concise, describing the role of the agent, its capabilities, its limitations, meta-cognitive information about its own framework, what the tools do and what data to trust and what data to take with a grain of salt. also metadata should be in place via harness system that helps in making said decisions. specking of harnesses, it should be designed from bottom up with a goal to allow easy and good vitrification via the agent. and a lot of system messages that guide the agent in many respects. all of these things are a bare minimum to get an agent working well, let alone other things like proper context management via smart auto compaction, rag, etc...",
          "score": 1,
          "created_utc": "2026-01-15 17:50:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxp025",
          "author": "real_serviceloom",
          "text": "@mods spam!",
          "score": 1,
          "created_utc": "2026-01-16 15:32:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzm89uk",
          "author": "Illustrious-Film4018",
          "text": "Really, according to people on r/accelerate, SOTA models don't hallucinate anymore and if it hallucinates, it's because you're using the wrong model or doing something wrong.",
          "score": 0,
          "created_utc": "2026-01-14 21:43:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmefyb",
              "author": "creaturefeature16",
              "text": "That's because those people are \"AI incels\" and not worth paying any attention to. They hate their own humanity and would prefer to leave it behind than take responsibility and do something good with their lives. ",
              "score": 3,
              "created_utc": "2026-01-14 22:11:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzmkc7u",
                  "author": "Illustrious-Film4018",
                  "text": "I agree, and they've never actually used AI for anything important.",
                  "score": 1,
                  "created_utc": "2026-01-14 22:40:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzmec7t",
              "author": "mossiv",
              "text": "I’ve been having hallucinations in very simple prompt windows. GPT especially. Within one sentence it told me a lie, I called it out and it gaslit me. It hallucinated, got it wrong and denied all accountability. If orgs are claiming hallucinations happen less it’s because they are steering them to be more authoritative. Which is worse.",
              "score": 0,
              "created_utc": "2026-01-14 22:11:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qbl5em",
      "title": "which ai dev tools are actually worth using? my experience",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qbl5em/which_ai_dev_tools_are_actually_worth_using_my/",
      "author": "Tough_Reward3739",
      "created_utc": "2026-01-13 07:31:57",
      "score": 7,
      "num_comments": 22,
      "upvote_ratio": 0.74,
      "text": "\n\ni’ve been trying a bunch of ai dev tools over the past six months, mostly to see what actually holds up in real projects. cursor, cosine, claude, roocode, coderabbit, a few langchain-style setups, and some others that sounded promising at first. a couple stuck. most didn’t.\n\nthe biggest takeaway for me wasn’t about any single tool, but how you use them. ai works best when you’re very specific, when you already have a rough plan, and when you don’t just dump an entire repo and hope for magic. smaller chunks, clearer intent, and always reviewing the output yourself made a huge difference.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qbl5em/which_ai_dev_tools_are_actually_worth_using_my/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzbhc48",
          "author": "NoEngineering3321",
          "text": "I never had the chance to try Cursor. Windsurf is pretty good in repository orientation.  GitHub copilot sometimes stucks at basics e.g. closing brackets. Starting with Antigravity",
          "score": 3,
          "created_utc": "2026-01-13 08:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc2v1z",
          "author": "Fine_Factor_456",
          "text": "no trash talk. better you go and use zed editor. it's fuckin awesome.",
          "score": 3,
          "created_utc": "2026-01-13 11:27:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc020l",
          "author": "alokin_09",
          "text": ">ai works best when you’re very specific, when you already have a rough plan \n\nI mean, yeah, that applies to pretty much any AI tool/model you use, doesn't matter the task or use case. It's a no-brainer.\n\nLike I use kilo code most of the time and mix different models depending on what I'm doing, but the same principle you're describing applies there too",
          "score": 2,
          "created_utc": "2026-01-13 11:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzck2ea",
          "author": "meckstss",
          "text": "It changes so fast it is hard to pin a question like this down.   The models get updated what feels like weekly and I tend to jump around.   I'm starting to break away and build my own agents with Autogen that work alongside of what I'm trying to do.    The LLMs are just too large, and get out of date, or they will bring in context that has nothing to do with the goal.    I have found that training my own model with code I've written and documented does very well.   I have certain rules for when I create classes, how I reuse functions, polymorphism, SOLID principles, code formatting, TDD, story writing, all of it.   I have a collection of 167 agents that I can pass a high level feature to and it will ask probing questions, write psuedo test cases, convert them to requirements, architect the libraries, classes, methods and properties.  It models database for state management and performance, it ensures logging and observability, it's a whole firm.   I think this is where this industry will end up.",
          "score": 2,
          "created_utc": "2026-01-13 13:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdp493",
          "author": "bbvvmmkj",
          "text": "oepncode + antigravity accounts linked (do as many) then use opus 4.5 + performs good with oh my opencode too",
          "score": 2,
          "created_utc": "2026-01-13 16:50:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc2ivg",
          "author": "Medical-Farmer-2019",
          "text": "I have tried many tools and gradually discovered that different tools have their own advantages. For example, Codex is very strong when writing complex back-end logic code (but it also takes more time). Antigravity is better in terms of front-end aesthetics (when you don't provide any design drafts). Claude code is strong in all aspects. If you can afford the price, I highly recommend using it.",
          "score": 1,
          "created_utc": "2026-01-13 11:24:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc8l3e",
          "author": "evilbarron2",
          "text": "I can write code, but I wouldn’t call myself a dev. For me, writing code isn’t an end (even if I enjoy it occasionally) - it’s a means to an end. I wouldn’t call myself very much like a tool that I can just “dump an entire repo on” and get magic, or even just decent, working output. I’m just not that interested in the craft of coding, any more than I am in carpentry or house painting - it’s just a task I’m a greater goal for me.\n\nThere is no existing tool I’ve found that can accomplish this. Everything I’ve tried requires a ton of focus and is covered with potential land mines waiting to go off.\n\nCoding LLMs are helpful for people who want to be coders. They are worse than useless for people who don’t.",
          "score": 1,
          "created_utc": "2026-01-13 12:12:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzckx0s",
          "author": "Adventurous-Fruit344",
          "text": "I have a different angle for you: financial. \n\nFor $20:\n\nIf you use API tokens you will go broke in about 2-3 days if you're doing any serious coding with long context. Exception to this is Grok - it's substantially cheaper, but I haven't used it yet and I hear the quality is subpar. I imagine it is not that bad if you're guiding it with specifics and know how to review it. It's what I'll be trying next.\n\nIf you pay for Claude Pro, you can use VSCode / Zed Editor plugins that allow you to use Claude Code and Codex respectively, but you will go broke (weekly limit) with Claude in about 2 days if not quicker.\n\nCodex in VSCode (via the subscription) is a winner for me because, unless you use Very Hard thinking, it will power through the month (barely, but substantial) - while doing really solid work, albeit a touch slow.\n\nMy use case: I'm guiding/desigining/reviewing and Codex is writing all the code for me; I don't use it as a \"create this entire feature for me and don't stop until all the tests you wrote pass\"",
          "score": 1,
          "created_utc": "2026-01-13 13:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcstyr",
          "author": "Tryin2Dev",
          "text": "RepoPrompt.",
          "score": 1,
          "created_utc": "2026-01-13 14:15:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdqnop",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-13 17:07:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdqnqz",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-13 17:07:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzg05q2",
          "author": "drunnells",
          "text": "I've been happy with a $20 OpenAI subscription and codex.",
          "score": 1,
          "created_utc": "2026-01-13 23:30:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nziyywk",
          "author": "SidLais351",
          "text": "The tools that stick for me are the ones that reduce back and forth. Autocomplete alone gets old fast. I look for things that help with reviews, tests, and understanding changes. I’ve ended up keeping Qodo in my workflow because it ties code generation, review, and repo context together, which saves more time than point tools.",
          "score": 1,
          "created_utc": "2026-01-14 12:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpnleh",
          "author": "Ecstatic-Junket2196",
          "text": "cursor + traycer (under $50) and i've used them for months",
          "score": 1,
          "created_utc": "2026-01-15 11:43:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqunhk",
          "author": "damaki",
          "text": "Now that Jetbrains tools have Claude Code integration, using Claude Code is a breeze. No need for the fiddly command line tools. I have never been a VSCode guy, so it's really nice.",
          "score": 1,
          "created_utc": "2026-01-15 15:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzra1rx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-15 16:56:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzra1vq",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-15 16:56:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o075l45",
          "author": "Lucky_Clock4188",
          "text": "raw chatgpt window, copy paste code into chat. 💯",
          "score": 1,
          "created_utc": "2026-01-17 23:53:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08z1by",
          "author": "botapoi",
          "text": "i've been chipping away at a simple crm for small businesses lately and ngl, setting up auth and backend from scratch is always a drag. i've been using blink with Claude to speed things up, honestly the built in auth saved me a ton of time compared to wrestling with Firebase again.",
          "score": 1,
          "created_utc": "2026-01-18 06:22:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbrwj3",
          "author": "CC_NHS",
          "text": "I found tools direct from provider such as Claude code, codex, Gemini cli, qwen cli etc, have a much better capability than wrappers such as Cursor. even with the same model it often seemed a vast difference, so I just stick to the source now. don't trust wrappers in between",
          "score": 1,
          "created_utc": "2026-01-13 09:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbtcye",
          "author": "Competitive_Act4656",
          "text": "I’ve definitely noticed that breaking down tasks and being specific with prompts makes a huge difference too. When I started using tools like myNeutron and Sider, it really helped me keep track of all my notes and context across different AI sessions. I was constantly losing track of ideas and outputs before, but now it feels much more manageable. I ended up sticking with myNeutron because the free option was more than enough for my needs. It’s been a game-changer for maintaining project continuity.",
          "score": 0,
          "created_utc": "2026-01-13 10:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdtvcr",
          "author": "Tiny-Telephone4180",
          "text": "Yeah, that lines up with my experience too. Tool-wise, the setup that’s held up best for me is Claude Code with [GLM 4.7](https://z.ai/subscribe?ic=J1YSHA0WA2). GLM is way stronger than people expect (close to Sonnet for real coding) and cheap enough that you can iterate a lot without worrying about caps. Using it in smaller, intentional chunks through the CLI fits exactly what you’re describing. \n\nhttps://preview.redd.it/91zgnxi8i5dg1.png?width=247&format=png&auto=webp&s=266feb8579e9b7504e4d850005e5776f002f19a6",
          "score": 0,
          "created_utc": "2026-01-13 17:24:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdtz6e",
      "title": "Need people to get excited part 2",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qdtz6e/need_people_to_get_excited_part_2/",
      "author": "External_Ad1549",
      "created_utc": "2026-01-15 19:57:04",
      "score": 7,
      "num_comments": 18,
      "upvote_ratio": 0.63,
      "text": "Three months ago I posted here saying I had found GLM-4.5 and coding suddenly felt like binge watching a Netflix series. Not because it was smarter, but because the flow never broke and affordable. I tried explaining that feeling to people around me and it mostly went over their heads.Then I shared it here  \n[https://www.reddit.com/r/ChatGPTCoding/comments/1nov9ab/need\\_people\\_to\\_get\\_excited/](https://www.reddit.com/r/ChatGPTCoding/comments/1nov9ab/need_people_to_get_excited/)\n\nSince then I’ve tried Cline, Claude Code, OpenCode. All of them are good tools and genuinely useful, but that original feeling didn’t really come back. It felt like improvement, not a shift.\n\nYesterday I tried Cerebras running GLM-4.7 and it was awesome. Around 1000 t/s output. Not just fast output the entire thinking phase completes almost instantly. In OpenCode, the model reasoned and responded in under a second, and my brain didn't even get the chance to lose focus.\n\nThat’s when it clicked for me: latency was the invisible friction all along. We’ve been trained to tolerate it, so we stopped noticing it. When it disappears, the experience changes completely. It feels less like waiting for an assistant and more like staying inside your own train of thought.\n\nI just wanted to share it with you guys because this good news only you can understand\n\nnote: We can't use Cerebras like a daily driver yet, their coding plans exclusive and brutal rate limits, they are able to achieve this bathroom tile size cpus, very interesting stuff I hope they succeed and do well\n\ntldr; discovered cerebras",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qdtz6e/need_people_to_get_excited_part_2/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzylqsm",
          "author": "PutPurple844",
          "text": "I got excited with the speed, too, not so much with the output. But it's insane once it is stable, we will kind of have zero downtime between iterations.",
          "score": 3,
          "created_utc": "2026-01-16 17:56:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyn8yq",
              "author": "External_Ad1549",
              "text": "yes exactly waiting for that",
              "score": 1,
              "created_utc": "2026-01-16 18:03:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzx178o",
          "author": "neurosurge",
          "text": "I tried GLM-4.7 (free and paid) for a couple of days, but it seems like it just hallucinates constantly and puts out low quality plans and code. \n\nI always have a different model/agent check another model’s work, and I always find a few things that need tweaked or updated. Every prompt with GLM-4.7 seemed to output garbage. I don’t know if it’s my setup with OpenCode or just poor model performance. \n\nI hope they get it fixed in a future release. The pricing and token allocation are amazing, especially compared to Anthropic’s offerings, but the reasoning seems to need a lot of work still.",
          "score": 2,
          "created_utc": "2026-01-16 13:34:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx2hx1",
              "author": "External_Ad1549",
              "text": "this is entirely based on my experience, the llm  any llm will have some capacity for it. some can create only functions, some only can create files, some can create modules, very costly ones can vibe code entire project. If you ask beyond the capacity it will start hallucinate.\n\nalso sometimes for cost cutting they reduce the capacity of llm, happens to chatgpt as well. GLM 4.7 is the work horse for me. Validation and stabilizing I will give it gemini 3 flash.\n\nany llm model which is not unlimited tokens gives kind of anxiety for me and also I work in mainly backend python systems so that might be different. \n\nI can say u can trust 4.7 for python backend",
              "score": 1,
              "created_utc": "2026-01-16 13:41:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuazf9",
          "author": "keepthepace",
          "text": "There is a reason why NVidia bought Groq. They have competition coming!",
          "score": 1,
          "created_utc": "2026-01-16 01:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv8zq1",
              "author": "External_Ad1549",
              "text": "and cerebras teaming up with gpt all good things",
              "score": 1,
              "created_utc": "2026-01-16 05:04:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvrpyc",
          "author": "real_serviceloom",
          "text": "Ok this is interesting because for me glm 4.7 has been absolutely slow as molasses. I tried the open code free glm 4.7 provider and also the cerebras version and both were incredibly slow.",
          "score": 1,
          "created_utc": "2026-01-16 07:30:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwvxbr",
              "author": "External_Ad1549",
              "text": "I am on max plan still sometimes feels like slow, but cerebras is on different league",
              "score": 1,
              "created_utc": "2026-01-16 13:03:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzxoosu",
                  "author": "real_serviceloom",
                  "text": "I dunno for me even on cerebras a /review on opencode just hangs forever compared to minimax m2.1. None of them come close to gpt 5.2 or opus 4.5 though. That's why I feel like z.ai lies a bit with their benchmarks showing them so close to opus. I know they have to make money but their marketing should be more honest. ",
                  "score": 1,
                  "created_utc": "2026-01-16 15:30:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwtfjm",
          "author": "popiazaza",
          "text": "Try Windsurf SWE-1.5. Should be based on GLM-4.6 and it run on Cerebras if you use Fast model.\n\nNormal model is also decently fast, and it's free to use for limited time.",
          "score": 1,
          "created_utc": "2026-01-16 12:48:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwvrap",
              "author": "External_Ad1549",
              "text": "really?? this is the info I require I wish it is glm 4.7 but something yeah thanks",
              "score": 1,
              "created_utc": "2026-01-16 13:02:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o056wlo",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-17 18:02:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o056woc",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-17 18:02:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05badc",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-17 18:22:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05bagk",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-17 18:22:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdrqw7",
      "title": "From your experience: practical limits to code generation for a dynamic web page? (here is mine)",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qdrqw7/from_your_experience_practical_limits_to_code/",
      "author": "toolznbytes",
      "created_utc": "2026-01-15 18:35:51",
      "score": 6,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "(*using ChatGPT Business*)\n\nI'm asking ChatGPT for a self-contained HTML page, with embedded CSS and javascript, with a detailed specification I describe and refine.\n\nI successfully obtained a working page but it starts to derail here and there more and more often after a while, as the conversation goes on.\n\nI'm at iteration 13 or so, with a handful of preparation questions before.\n\nThe resulting html page has:\n\n* 4k CSS\n* 13k script\n* 3k data (as script const, not counted in the 13k)\n* 19k total with html\n* all the display, data parsing, list and 2 buttons are working well.\n\nI'm happy but has I said, at the step before it started to skip all the 3k data, using a placeholder instead. And before the data to process was damaged (edited).\n\nSo for me, it's near the practical limit I think. I'm afraid I'm run in more and more random regressions as I push further.\n\nMy questions:\n\n1. How far can you go before the need to split the tasks and stitch them together by hand?\n2. Is there any way to make it handle this kind of task in a more robust way?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qdrqw7/from_your_experience_practical_limits_to_code/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzsv6wv",
          "author": "Trotskyist",
          "text": "Don’t use ChatGPT for this task. Look into codex-cli or Claude code. I prefer Claude Code, but I believe codex is included with business plans.\n\nFor context: I’ve created full stack web applications with the above tools well into the hundreds of thousands of lines of code (not that LoC is a metric that tells you anything about quality, but it is an indication of complexity.)\n\nI mean on some level it’s impressive you’ve made it this far in only the chat interface, but you’re making things way harder on yourself than they need to be.",
          "score": 4,
          "created_utc": "2026-01-15 21:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx9hki",
          "author": "jonydevidson",
          "text": "Jesus fuck, why are you generating code in the web editor?\n\n1. Download VSCode\n2. Download Git (this is your game-save mechanic and will save your life)\n3. In VSCode, download Codex extension from OpenAI. Log in with your ChatGPT account. \n4. Create new folder on your disk. This will be your project folder. In VSCode, File > Open folder... and open it. This is now your WORKSPACE\n5. Tell the agent to initialize the git repository. This will now start tracking file changes. You STAGE changes that you wish to save, and then you make COMMITs. A commit is a fully human readable save-point in your project. You can navigate back to it and restore your project to that point or any other point, see the changes made in that commit, pick files to be reset back to the state at that commit and so much more. Download the Git Graph extension in VSCode but I recommend a Git UI like Fork.dev \n6. NOW you tell the agent what you want it to do. Every time you complete a feature and it's working, you commit the changes in Git. If at any point you or the agent fuck up, just discard the changes in Git and start over from your last save point - no fear of losing full progress or going in too deep in the wrong direction.",
          "score": 2,
          "created_utc": "2026-01-16 14:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz66fv",
              "author": "toolznbytes",
              "text": "Thank you for all this! 👍👍👍\n\nI realize I did something in a strange way... It worked surprisingly well, up to a point.\n\nOk, my next tool will be done with that workflow 😤\n\nBut now I'm out of the free trial, so I'll have to pay somewhere , codex or Claude or something. I guess they don't have pay per use.",
              "score": 1,
              "created_utc": "2026-01-16 19:27:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz82o7",
                  "author": "jonydevidson",
                  "text": "Codex limits will get you much further than Claude Code. Opus will eat the usage quotas, while with Codex you can actually use GPT5.2 Codex on Medium and get decent usage, while having a much smarter model than Sonnet 4.5.\n\nGive each one a try and see what you like, in the end it's just $20 and you'll be getting a lot more value out of it. Even if you completely fail and ship nothing, you'll have learned valuable skills in working with frontier tech that's going to be the future of work everywhere.",
                  "score": 1,
                  "created_utc": "2026-01-16 19:36:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzu9bys",
          "author": "evia89",
          "text": "Meh. Use good structure then web pack to static",
          "score": 1,
          "created_utc": "2026-01-16 01:34:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv5bol",
              "author": "toolznbytes",
              "text": "Sorry, can you say that again, please?",
              "score": 1,
              "created_utc": "2026-01-16 04:40:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzw4a0p",
                  "author": "evia89",
                  "text": "https://www.perplexity.ai/search/how-to-use-webpack-to-get-stat-iQLeJ_MVSymCr5a3Uhv7CQ#0",
                  "score": 1,
                  "created_utc": "2026-01-16 09:24:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvntx0",
          "author": "BattermanZ",
          "text": "As another commenter said, use Codex in VS Code or codex-cli. You will one shot your task. Best of all, it's included in your chatgpt subscription.",
          "score": 1,
          "created_utc": "2026-01-16 06:57:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwhigc",
          "author": "Tema_Art_7777",
          "text": "I use codex with gpt-5.2 - it is very good.",
          "score": 1,
          "created_utc": "2026-01-16 11:22:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx5c8x",
          "author": "NinjaLanternShark",
          "text": "I bailed out of coding via chat long before this kind of complexity. Just too frustrating. Personally I like VS Code with Copilot, but Codex, Claude or any similar coding-specific, agentic interface is the only way to handle tasks more than a few screens-worth of code.",
          "score": 1,
          "created_utc": "2026-01-16 13:56:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qavy6k",
      "title": "The hidden memory problem in coding agents",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qavy6k/the_hidden_memory_problem_in_coding_agents/",
      "author": "Arindam_200",
      "created_utc": "2026-01-12 14:13:24",
      "score": 4,
      "num_comments": 14,
      "upvote_ratio": 0.59,
      "text": "When coding agents start breaking down in real repos, the issue usually isn’t the model.\n\nIt’s memory.\n\nMost coding agents today either:\n\n* dump large chunks of code into context (vector RAG), or\n* keep long conversation histories verbatim\n\nBoth approaches scale poorly.\n\nFor code, remembering *more* is often worse than remembering *less*. Agents pull in tests, deprecated files, migrations, or old implementations that look “similar” but are architecturally irrelevant. Reasoning quality drops fast once the context window fills with noise.\n\nWhat’s worked better in practice is treating memory as a **structured, intentional state**, not a log.\n\nFor coding agents, a few patterns matter a lot:\n\n* Compressed memory: store decisions and constraints, not raw discussions.\n* Intent-driven retrieval: instead of “similar files,” ask “where is this implemented?” or “what breaks if I change this?” This is where agentic search and context trees outperform vector RAG.\n* Strategic forgetting: tests, backups, and deprecated code shouldn’t compete with live implementations in context.\n* Temporal awareness: recent refactorings matter more than code from six months ago, unless explicitly referenced.\n* Consolidation over time: repeated fixes, refactor rules, and style decisions should collapse into durable memory instead of reappearing as fresh problems.\n\nIn other words, good coding agents don’t treat a repo like text. They treat it like a system with structure, boundaries, and history.\n\nOnce you do that, token usage drops, reasoning improves, and agents stop hallucinating imports from files that shouldn’t even be in scope.\n\nOne interesting approach I’ve seen recently, while using Claude code with [ByteRover](https://www.byterover.dev/) ( I use the free tier), is storing this kind of curated context as versioned “memory bullets” that agents can pull selectively instead of re-deriving everything each time.\n\nThe takeaway for me:\n\nbetter coding agents won’t come from bigger context windows, they’ll come from better memory discipline.\n\nWould love your opinions around this!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qavy6k/the_hidden_memory_problem_in_coding_agents/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nz6tgsp",
          "author": "InfraScaler",
          "text": "It gets hard to engage online when the other side is just copy pasting from ChatGPT, even if the topic is interesting and something you're actively working on. The Internet is really dead.",
          "score": 7,
          "created_utc": "2026-01-12 16:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9cpor",
              "author": "WheresMyEtherElon",
              "text": "This sub at least is on its last breathe, crumbling from the ineptitude of vibe coders and the very obvious attempts at guerilla marketing like this thread. Were are all the cool kids going now to discuss serious llm-based development without being flooded by clumsy attempts at viral marketing?",
              "score": 2,
              "created_utc": "2026-01-12 23:59:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzb3h9r",
                  "author": "Unique-Drawer-7845",
                  "text": "You don't want to follow the cool kids for this: you want the geeks and nerds. Of course it's kinda cool to be a geek/nerd now. But anyway. If the places you want to get to were easy to find, they'd be infected with bots like this place.  You'll have to put in some effort!",
                  "score": 2,
                  "created_utc": "2026-01-13 06:04:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz9om8s",
              "author": "Trotskyist",
              "text": "It's exhausting. The worst is people who are unwilling to deviate from their priors and just keep churning out LLM responses that fit their perspective",
              "score": 2,
              "created_utc": "2026-01-13 01:03:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz9zbqs",
              "author": "tr14l",
              "text": "This is an ad bot bro. There's no one there",
              "score": 2,
              "created_utc": "2026-01-13 02:01:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6518i",
          "author": "TripleFreeErr",
          "text": "https://docs.cline.bot/prompting/cline-memory-bank\n\ncan’t agree more. There are some ways to trick current agents into managing their memory",
          "score": 2,
          "created_utc": "2026-01-12 14:52:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6dcrb",
          "author": "psychometrixo",
          "text": "Consider Steve Yegges beads.  It looks like project management software but it is not project management software \n\nIt's a way to organize your markdown files (your memories) so you can tell what to work on AND what work spawned this work\n\nYou can have multiple associations.  So you can have memory beads, design beads, etc. in addition to task-based beads.\n\nIt isn't the final solution but something like this is a step in the right direction",
          "score": 2,
          "created_utc": "2026-01-12 15:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza4dos",
          "author": "com-plec-city",
          "text": "my 2 cents: i've noticed that my human memory can create connections between parts of the code much better than LLMs. Thought my brain can't remember all the words like AI can, somehow I can almost see those wires when i'm changing some code manually.\n\nI kinda know what will I be messing up when manually changing a function. I get flashes of things that happened in the past that help me edit the code. Even some friend's joke from 10 years ago helps, as I write the code and realize how poor of a solution i'm making and decide to go the other way. \n\nI can also see \"the big picture\" too, what the software really means, even if I can only recall tiny bits of the code.",
          "score": 2,
          "created_utc": "2026-01-13 02:28:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6u6r7",
          "author": "Angelsomething",
          "text": "I started instructing mine to use json as a memory for each project. seems to work fine for now. then again, my projects are small.",
          "score": 1,
          "created_utc": "2026-01-12 16:51:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz70s1o",
          "author": "FancyAd4519",
          "text": "trying to solve this with https://context-engine.ai .. we are providing solid agentic ROI now utilizing graph rag features, local llm decoders (as well as via apis if cant run local) personal vector store with semantic search.. Actual benchmarks to prove its weight not just a toy or another context vibe coded shop… Also free open core(Albeit we moved to BSL strictly to prevent people from using it as a  saas, free for individuals and companies self hosted) highly recommend if your in this pickle.",
          "score": 1,
          "created_utc": "2026-01-12 17:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7125i",
              "author": "FancyAd4519",
              "text": "We are dumping precise chunks not the entire thing; everything has been optimized for compression being able to run code navigation and context for the agent in UNDER 1-2k tokens (10 tool calls of our search) vs hey lets dump an entire 14k line function. this is what seperates us",
              "score": 1,
              "created_utc": "2026-01-12 17:22:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz8p3nk",
          "author": "aiworld",
          "text": "Try Claude Code Infinite. It will change your life. [https://github.com/crizCraig/claude-code-infinite](https://github.com/crizCraig/claude-code-infinite) \\- We structure message histories as a tree and semantically chunk to avoid adding overly large code blocks to context. In addition, we return a bread crumb of summaries for returned chunks to provide the larger picture around when / where the retrieved memory occurred (e.g. this error occurred after doing the refactor of X, during step Y.)",
          "score": 1,
          "created_utc": "2026-01-12 21:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6whih",
          "author": "WheresMyEtherElon",
          "text": "You don't drop agents in an entire repo on their own. You tell them what to look for. And your repo should also be well organized so that a human can easily find his way, which makes it easy for an llm as well. You shouldn't need to keep the entire repo in memory to understand what a function, a method, a class or a feature is doing.\n\nThe problem isn't memory, it's gigantic and messy codebases. And it's worse if you're vibe-coded that repo to begin with.",
          "score": 1,
          "created_utc": "2026-01-12 17:01:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qelrpl",
      "title": "For loves sake no more AI frameworks. Lets move to AI infrastructure",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qelrpl/for_loves_sake_no_more_ai_frameworks_lets_move_to/",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-16 17:08:57",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.63,
      "text": "Every three minutes, there is a new agent framework that hits the market.\n\nPeople need tools to build with, I get that. But these abstractions differ oh so slightly, viciously change, and stuff everything in the application layer (some as black box, some as white) so now I wait for a patch because i've gone down a code path that doesn't give me the freedom to make modifications. Worse, these frameworks don't work well with each other so I must cobble and integrate different capabilities (guardrails, unified access with enterprise-grade secrets management for LLMs, etc).\n\nI want agentic infrastructure - clear separation of concerns - a jam/mern or LAMP stack like equivalent. I want certain things handled early in the request path (guardrails, tracing instrumentation, orchestration), I want to be able to design my agent instructions in the programming language of my choice (business logic), I want smart and safe retries to LLM calls using a robust access layer, and I want to pull from data stores via tools/functions that I define.\n\nI want simple libraries, I don't want frameworks. And I want to deliver agents to production in ways which is framework-agnostic and protocol-native.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qelrpl/for_loves_sake_no_more_ai_frameworks_lets_move_to/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nzyctyx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-16 17:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzycu25",
              "author": "AutoModerator",
              "text": "Sorry, your submission has been removed due to inadequate account karma.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPTCoding) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-01-16 17:17:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzyiqmn",
          "author": "dumbledork99",
          "text": "What's your view on haystack. I have always found it allows you to choose how deep down the rabbit hole you want to go.",
          "score": 1,
          "created_utc": "2026-01-16 17:43:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzymocc",
              "author": "AdditionalWeb107",
              "text": "There are some obvious ones\n\n1/ Durable checkpointing and replay (compute infra). Maybe [Temporal](https://github.com/temporalio/temporal)?  \n2/ Data plane for routing, orchestration, observability, and moderation. Maybe [Plano](https://github.com/katanemo/plano)?  \n3/ Memory Infra for context compression and expansion? Maybe [Mem0](https://github.com/mem0ai/mem0)?  \n4/ Hybrid store (plus vector)? Maybe PostgreSQL?",
              "score": 1,
              "created_utc": "2026-01-16 18:01:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz7lvs",
                  "author": "dumbledork99",
                  "text": "I meant what are your views on [Haystack](http://haystack.deepaet.ai) ?\nHave you tried it?",
                  "score": 1,
                  "created_utc": "2026-01-16 19:34:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qb369h",
      "title": "Best tools, flows, agents for app migration.",
      "subreddit": "ChatGPTCoding",
      "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1qb369h/best_tools_flows_agents_for_app_migration/",
      "author": "odrakcir",
      "created_utc": "2026-01-12 18:39:20",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "ok so, I'm currently giving support to a nextjs + mui app and now my client wants to migrade to tailwind. I'm taking this oportunity to go one step further and migrate to some other tools, for example, zod, for validations, improve typings and testing. From your own experience, what would be the best way to achieve such migration? this app is mostly large tables and forms. I'm looking for recomendations, vscode vs a fork, claude vs openai vs gemini; In general, any service that would help me.\n\n  \nthanks in advance.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/ChatGPTCoding/comments/1qb369h/best_tools_flows_agents_for_app_migration/",
      "domain": "self.ChatGPTCoding",
      "is_self": true,
      "comments": [
        {
          "id": "nz7qtni",
          "author": "kidajske",
          "text": "Bmad method is probably a good bet. It creates epics and then you do a flow of create story > implement code > code review > mark progress in sprint file with the agent. It's good for deterministic refactoring like this because it keeps track of stuff well enough that you're able to clear the context of claude code between each of the steps in the flow I described so context bloat issues are minimized. I use it in claude code with sonnet, presumably opus would be even better but it consumes a lot of tokens so I'm not willing to spend the money for that.",
          "score": 1,
          "created_utc": "2026-01-12 19:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbqpug",
          "author": "Square-Yak-6725",
          "text": "My go to is VSCode with the Github Copilot Chat extension. Super good value. For a big refactor like that, start in plan mode and have it make migration documentation that you review and plan with it very carefully and meticulously. Flip to agent mode and go through the plan step by step testing along the way.",
          "score": 1,
          "created_utc": "2026-01-13 09:36:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl5e8v",
          "author": "Funny-Anything-791",
          "text": "You may want to give [ChunkHound](https://chunkhound.github.io) a spin it's an open source local first code intelligence engine that quickly extracts deep insights like a true core developer who've been there since the beginning. It'll also reduce your monthly token usage as a side effect of increasing your agent's accuracy",
          "score": 1,
          "created_utc": "2026-01-14 18:47:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09ewsb",
          "author": "botapoi",
          "text": "migrating between css frameworks can be a pain ngl. i've been using [blink.new](http://blink.new) with chatgpt for a side project, a bookmark manager with ai organization to speed up development. it handles auth and backend stuff so i focus on the UI mostly. for migrations like this, i think it's a good alternative to wrestling with manual setups",
          "score": 1,
          "created_utc": "2026-01-18 08:43:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}