{
  "metadata": {
    "last_updated": "2026-01-04 08:39:56",
    "time_filter": "week",
    "subreddit": "DeepSeek",
    "total_items": 21,
    "total_comments": 97,
    "file_size_bytes": 115990
  },
  "items": [
    {
      "id": "1q1bzog",
      "title": "Do it again, DeepSeek",
      "subreddit": "DeepSeek",
      "url": "https://i.redd.it/bi9gciikbsag1.png",
      "author": "LeTanLoc98",
      "created_utc": "2026-01-01 19:02:22",
      "score": 1498,
      "num_comments": 123,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q1bzog/do_it_again_deepseek/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx4rqk5",
          "author": "coloradical5280",
          "text": "They just did: https://arxiv.org/pdf/2512.24880\n\nThat paper is huge, with massive implications to make all models more stable, and faster, and cheaper to train. \n\nThe sparse attention and quick index they introduced to the world in v3.2 was also huge. \n\nDeepseek has done more in the last year than any other lab. They just donâ€™t give a shit about dialing in the perfect consumer chatbot , or adding consumer features, or acquiring more daily active users. \n\nThey care about making breakthroughs, thatâ€™s it. And those breakthroughs end up being used by everyone. Every model you use right now is using GRPO, probably MoE , MLA, and may other brilliant hacks that DeepSeek gave to the world for free.",
          "score": 378,
          "created_utc": "2026-01-01 20:06:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx508dw",
              "author": "Timo425",
              "text": "What's their motivation for doing it? Seems like a lot of clever and hard work that just gets copied instantly.",
              "score": 37,
              "created_utc": "2026-01-01 20:50:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5236w",
                  "author": "coloradical5280",
                  "text": "I mean, why does Linux exist? It runs inside dozens of things in your house, itâ€™s the kernel that runs your router, traffic lights, itâ€™s in every network switch and security camera and in your car. And itâ€™s given away for free. Because by being open it is constantly being forked and improved and customized. \n\nThere is a second answer, which is that the CCP basically wants to destroy the US economy and just dumping free AI , as much as possible, at some point makes it a completely free commodity , and that arguably causes our house of cards to collapse. \n\nThe first part is fully true, you tangentially touch dozens of pieces of open source code every single day. The world would literally shut down tomorrow without it. The second part has either tiny shades of truth or is the whole answer, depending on how you see the world and a lot of other factors.",
                  "score": 127,
                  "created_utc": "2026-01-01 21:00:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx510fo",
                  "author": "mambo_cosmo_",
                  "text": "love of the game(?)+ giving other chinese labs the tool to improve",
                  "score": 11,
                  "created_utc": "2026-01-01 20:54:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx56tvn",
                  "author": "EverydayEverynight01",
                  "text": "Because it raises their prestige and the hopes that some other researches can build and improve on what their work.",
                  "score": 11,
                  "created_utc": "2026-01-01 21:25:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6tihf",
                  "author": "miuid",
                  "text": "For greater good? Or for stakeholders' profits? That is the question.",
                  "score": 3,
                  "created_utc": "2026-01-02 02:57:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx736ft",
                  "author": "unity100",
                  "text": "They are doing open source science/technology. They are helping entire world to upgrade its tech. \n\nAt the same time they are literally destroying the AI bloat that the US loaded all of its economy onto:\n\n20% of 2025 US gdp was the money that the 5-6 AI circlejerk companies circulated among themselves without generating any real revenue. OpenAI, Nvidia, Oracle, Microsoft etc all bet on AI requiring a lot of processing power, and as a result energy. They invested everything in gpus, datacenters. By making models more reliable, efficient and cheaper to run, Deepseek is destroying all that investment.",
                  "score": 3,
                  "created_utc": "2026-01-02 03:59:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6443v",
                  "author": "RG54415",
                  "text": "Because greed is not the default human behaviour?",
                  "score": 4,
                  "created_utc": "2026-01-02 00:25:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx75kq8",
                  "author": "doryappleseed",
                  "text": "They want to attract the best and brightest talent, so doing this encourages that.",
                  "score": 2,
                  "created_utc": "2026-01-02 04:14:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7iy2v",
                  "author": "Digital_Soul_Naga",
                  "text": "thats the secret \n\nshhh ðŸ¤«",
                  "score": 2,
                  "created_utc": "2026-01-02 05:47:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxce42f",
                  "author": "Comprehensive-Bed-72",
                  "text": "Think of it as creating a market model that they built for everyone, once everyone is hooked then can they change direction to please the investors.",
                  "score": 2,
                  "created_utc": "2026-01-02 23:33:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxeay9i",
                  "author": "StaminaFix",
                  "text": "I was paying $20/month to openai for a year when deepseek came out I cancelled the subscription and it completely collapsed their business model",
                  "score": 2,
                  "created_utc": "2026-01-03 06:45:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx75xkz",
                  "author": "Warm-Border-9789",
                  "text": "That's how the US became the world's superpower in the first place. Huge investments in research by the military and public universities were given for free to the rest of the world. Of course, American companies were the main beneficiaries because the talent that created the innovations was American. China now wants the talent to be Chinese.",
                  "score": 2,
                  "created_utc": "2026-01-02 04:17:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5ajpb",
                  "author": "PureSelfishFate",
                  "text": "Don't worry, they'll stop sharing by 2027 due to AGI risks. They are just trying to stop the US AI economy from becoming a rocketship.",
                  "score": 2,
                  "created_utc": "2026-01-01 21:44:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbobzo",
                  "author": "Random_Nickname274",
                  "text": "For the collective!",
                  "score": 1,
                  "created_utc": "2026-01-02 21:21:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx7zdog",
              "author": "malege2bi",
              "text": "How do you know this about their motivations?",
              "score": 3,
              "created_utc": "2026-01-02 08:11:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxb00e6",
                  "author": "coloradical5280",
                  "text": "Have you read all their papers? Not the summaries, like actually read them? Theyâ€™re only 8 to 20 pages and only a few a year.  There is not a section heading called Our Motivations , but if you just read all their research there isnâ€™t really much debate regarding what their focus is, and what their focus is not.",
                  "score": 2,
                  "created_utc": "2026-01-02 19:23:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx60fbx",
              "author": "AriannaLombardi76",
              "text": "Excellent response",
              "score": 2,
              "created_utc": "2026-01-02 00:04:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx6g9a7",
              "author": "LeTanLoc98",
              "text": "I totally agree.\n\n\nDeepSeek has made a breakthrough in LLMs, and I hope they can do it again like they did with DeepSeek R1.",
              "score": 2,
              "created_utc": "2026-01-02 01:37:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxba3sa",
                  "author": "coloradical5280",
                  "text": "Again, chatbot releases aren't their thing - and R1 wasn't the breakthrough. GRPO, their flash attention work, refining MoE with MLA, auxiliary-loss-free load balancing - *that* was the breakthrough. And guess what? It's all being used in every model you use today.\n\n*Since R1 they've had just as many breakthroughs:* DSA (sparse attention that cuts long-context complexity from O(LÂ²) to O(Lk)), this new mHC (training stability improvements for all transformer architectures), and DeepSeek-OCR which isn't even about OCR - it's *context compression*, proving you can compress text 10-20x into visual tokens and recover it. That's foundational research for solving long-context limitations entirely differently than anyone else is approaching it.\n\nEvery time you use Claude, GPT, Gemini, whatever - you're benefiting from techniques DeepSeek published and open-sourced. Don't conflate a chatbot release with the **actual breakthroughs.**",
                  "score": 4,
                  "created_utc": "2026-01-02 20:12:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9v5o4",
              "author": "Dr__America",
              "text": "Isn't MoE from OpenAI? Or were they just the first ones to use it at scale?",
              "score": 2,
              "created_utc": "2026-01-02 16:13:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxawqp4",
                  "author": "coloradical5280",
                  "text": "Yeah technically but they didnâ€™t publish it or tell anyone about it or anything until way after George Hotz was to reverse engineer it out, and DeepSeek was already playing with it. Ilya is probably credited for it but since he was at OpenAI it did nothing for the world, DeepSeek studied it improved it and shared it with the world and made it far more practical with multi headed latent attention , and then more recently sparse attention on top of that, getting things down to 5% active parameters, which is nearly an order of magnitude better than what 4 and 4o did",
                  "score": 2,
                  "created_utc": "2026-01-02 19:07:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9dgpl",
              "author": "Pupojem-Player",
              "text": "What about video generation?",
              "score": 1,
              "created_utc": "2026-01-02 14:45:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxavn7q",
                  "author": "coloradical5280",
                  "text": "Qwen is SOTA on diffusion models right now, if Iâ€™m the CCP or deepseek I see no reason to try and outdo Qwen , HOWEVER Qwen has greatly benefited from deepseek research as well since most of it can apply to diffusion transformer models as well.",
                  "score": 3,
                  "created_utc": "2026-01-02 19:02:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx537db",
          "author": "Roshlev",
          "text": "Wasn't 3.2 like a month ago?",
          "score": 27,
          "created_utc": "2026-01-01 21:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6foz9",
              "author": "LeTanLoc98",
              "text": "DeepSeek V3.2 is good, but DeepSeek R1 is a breakthrough.",
              "score": 11,
              "created_utc": "2026-01-02 01:33:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6jljv",
          "author": "cnydox",
          "text": "They publish papers occasionally I don't know what else you need.",
          "score": 19,
          "created_utc": "2026-01-02 01:57:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx758oc",
              "author": "yaxir",
              "text": "why are they not researching multi-modal AI?",
              "score": 2,
              "created_utc": "2026-01-02 04:12:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7syjp",
                  "author": "cnydox",
                  "text": "There are a lot of small things to research than just scaling bigger models and hope they beat some benchmarks",
                  "score": 2,
                  "created_utc": "2026-01-02 07:11:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx80p0t",
                  "author": "LeTanLoc98",
                  "text": "I find that most multimodal models are still quite weak.\n\nFor example, Mistral Large 3 doesn't perform very well, even though its architecture is similar to DeepSeek.\n\nAt the moment, the only truly strong multimodal model is Gemini.\n\nBecause of that, I think DeepSeek should focus on text-only models instead of investing heavily in multimodal capabilities.",
                  "score": 2,
                  "created_utc": "2026-01-02 08:23:43",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx82ge1",
                  "author": "LeTanLoc98",
                  "text": "https://preview.redd.it/vb5fnq0tewag1.jpeg?width=1272&format=pjpg&auto=webp&s=50f805da4187a7912d6b9d8e1cff83a62ba20e41\n\nMistral Large 3 is extremely underwhelming.\n\n\nIn my opinion, multimodal models need at least 50B active parameters and no less than 1T total parameters to perform well. Moreover, there must be sufficient high-quality data available for training. For this reason, at the moment, I believe only Google has the capability to realistically pull this off.",
                  "score": 2,
                  "created_utc": "2026-01-02 08:40:45",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx6nxwj",
              "author": "LeTanLoc98",
              "text": "DeepSeek has made a breakthrough in LLMs, and I hope they can do it again like they did with DeepSeek R1.",
              "score": 1,
              "created_utc": "2026-01-02 02:23:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6pgg0",
                  "author": "cnydox",
                  "text": "They don't aim to make new sota models every week lol. Researching takes time",
                  "score": 3,
                  "created_utc": "2026-01-02 02:33:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx52d73",
          "author": "ciprianveg",
          "text": "V3.2 is indeed very good, it's a pity that it's architecture could not be implemented in llama.cpp so far",
          "score": 7,
          "created_utc": "2026-01-01 21:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx67a2c",
              "author": "segmond",
              "text": "eventually, but you can run it locally if you are itching to.\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1q1aif6/running\\_an\\_unsupported\\_deepseek\\_v32\\_in\\_llamacpp/](https://www.reddit.com/r/LocalLLaMA/comments/1q1aif6/running_an_unsupported_deepseek_v32_in_llamacpp/)",
              "score": 8,
              "created_utc": "2026-01-02 00:43:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx89jkj",
              "author": "dsanft",
              "text": "Write your own engine to do it. I did.",
              "score": 2,
              "created_utc": "2026-01-02 09:48:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6p5lz",
          "author": "PaulMakesThings1",
          "text": "Their style is usually to go silent for a long time then show up with something big, rather than trickle out little stuff. I could be wrong, I havenâ€™t been watching them that closely.",
          "score": 10,
          "created_utc": "2026-01-02 02:31:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6vn2m",
              "author": "LeTanLoc98",
              "text": "I'm waiting for DeepSeek R2 or V4.\n\n\nIn my opinion, DeepSeek should increase the total number of parameters to around 1 trillion instead of 671B.\n\n\nI've noticed that Kimi K2 Thinking uses a similar architecture with 1T parameters, and it performs very well.",
              "score": 7,
              "created_utc": "2026-01-02 03:10:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6r20m",
          "author": "Thedudely1",
          "text": "Deepseek V3.2 has been pretty great for me. I'm enjoying the refinements on top of V3 vs a whole new model. I'm pretty sure they've made commitments to train their next major iteration of their model on domestic Chinese GPU hardware so they're kind of biding time to get that going",
          "score": 9,
          "created_utc": "2026-01-02 02:42:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7zv4q",
          "author": "Brave-Hold-9389",
          "text": "The deepseek V4/R2 will come in Q1 2026. And I'm very excited. The latest paper from them is promising. All there research of 1 whole year will be packed into deepseek v4",
          "score": 4,
          "created_utc": "2026-01-02 08:15:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx81gn0",
              "author": "LeTanLoc98",
              "text": "I think the context length should be increased to at least 256K, rather than the current 128K.\n\n\nThey should also fix the Chinese language issue. DeepSeek often thinks and responds in Chinese.",
              "score": 4,
              "created_utc": "2026-01-02 08:31:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx81ybz",
                  "author": "Brave-Hold-9389",
                  "text": "The deepseek ocr paper shows how we can have 10x more context in the same context length. So, even if they keep it at 128k, if they use the deepseek ocr architecture they will have 1.3M context",
                  "score": 4,
                  "created_utc": "2026-01-02 08:35:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx80xn7",
              "author": "LeTanLoc98",
              "text": "I'm also looking forward to DeepSeek R2/V4.",
              "score": 3,
              "created_utc": "2026-01-02 08:26:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5d7ei",
          "author": "NearbyBig3383",
          "text": "R1 was truly my love, but the 3.2 special is incredibly slow, even more incredible.",
          "score": 3,
          "created_utc": "2026-01-01 21:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx75335",
          "author": "yaxir",
          "text": "make it MULTIMODAL, with extended thinking and web search and image analysis capabilites\n\nAND make it less woke\n\nit should be good?",
          "score": 6,
          "created_utc": "2026-01-02 04:11:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx80qcw",
              "author": "LeTanLoc98",
              "text": "I find that most multimodal models are still quite weak.\nFor example, Mistral Large 3 doesn't perform very well, even though its architecture is similar to DeepSeek.\n\nAt the moment, the only truly strong multimodal model is Gemini.\n\nBecause of that, I think DeepSeek should focus on text-only models instead of investing heavily in multimodal capabilities.",
              "score": 2,
              "created_utc": "2026-01-02 08:24:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx82a1y",
                  "author": "LeTanLoc98",
                  "text": "https://preview.redd.it/6hbizyqgdwag1.jpeg?width=1272&format=pjpg&auto=webp&s=c8e5f57e80556c067adbaa4c93de43111dc220b5\n\nMistral Large 3 is extremely underwhelming.\n\nIn my opinion, multimodal models need at least 50B active parameters and no less than 1T total parameters to perform well. Moreover, there must be sufficient high-quality data available for training. For this reason, at the moment, I believe only Google has the capability to realistically pull this off.",
                  "score": 1,
                  "created_utc": "2026-01-02 08:39:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx81dnc",
              "author": "LeTanLoc98",
              "text": "I think the context length should be increased to at least 256K, rather than the current 128K.\n\n\nThey should also fix the Chinese language issue. DeepSeek often thinks and responds in Chinese.",
              "score": 1,
              "created_utc": "2026-01-02 08:30:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6g4a4",
          "author": "Hefty-Newspaper5796",
          "text": "I dont want cheap models. I want more accurate and powerful models, which they are unable to innovate.",
          "score": 8,
          "created_utc": "2026-01-02 01:36:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6ntra",
              "author": "hiva-",
              "text": "you need cost effective models in order to make more powerful models. Same concept. $100M model with lesser effective tech wont get you as far as a $100M investment under more cost effective tech even if you spend the same amount",
              "score": 9,
              "created_utc": "2026-01-02 02:23:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx6nrr0",
              "author": "LeTanLoc98",
              "text": "DeepSeek's breakthroughs have made LLMs more accurate and more powerful.\n\n\nFor example, while Mixture of Experts (MoE) was not invented by DeepSeek, they proved its effectiveness in practice, showing that models can scale to trillions of parameters while keeping training and inference costs manageable.",
              "score": 8,
              "created_utc": "2026-01-02 02:22:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx94ly9",
                  "author": "Any_Pressure4251",
                  "text": "Bullshit, OpenAI's GPT-4 did this years ago. \n\nTraining and inference costs have always been going down, its part of what motivates the SOTA labs.",
                  "score": -2,
                  "created_utc": "2026-01-02 13:54:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx69g5n",
          "author": "scalaboulejs",
          "text": "haha very cool illustration on how we are becoming lazy and dumber while relying a lot on AI and LLMs",
          "score": 4,
          "created_utc": "2026-01-02 00:55:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx56nqc",
          "author": "AllyPointNex",
          "text": "I had to convince Deepseek today that iOS 26 wasnâ€™t my imagination. It still freaks out if you ask if a seahorse emoji exists.",
          "score": 4,
          "created_utc": "2026-01-01 21:24:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcyyta",
          "author": "KING_OF_ALL_IN",
          "text": "DS get the money from Liang's quant fund. It is not as urgent as other ai models to launch new model just for investment. Which make it able to actually focus on the research itself instead of catering to the market.",
          "score": 2,
          "created_utc": "2026-01-03 01:30:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjv9m3",
          "author": "ticticta",
          "text": "ç¾Žå›½çš„æœ‹å‹ä»¬ï¼Œé©¬ä¸Šå°±æ˜¯è¦åˆ°ä¸­å›½æ˜¥èŠ‚äº†ï¼ŒDeepseek æŒ‰ç…§æƒ¯ä¾‹ï¼Œä¼šæŽ¨å‡ºæ–°çš„ç‰ˆæœ¬çš„ã€‚\n\nHeads up to my US friends: Chinese New Year is coming up, and following their usual tradition, DeepSeek is likely about to drop a new version. Get ready.",
          "score": 2,
          "created_utc": "2026-01-04 02:06:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjzc4j",
          "author": "Beneficial_Common683",
          "text": "-9,223,372,036,854,775,808 credit scores",
          "score": 1,
          "created_utc": "2026-01-04 02:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4uo2w",
          "author": "letsgeditmedia",
          "text": "V3.2 I on par with sonnet 4 and matches 4.5 in some casesâ€¦ itâ€™s doing something, they just donâ€™t market every achievement like the American models do",
          "score": 1,
          "created_utc": "2026-01-01 20:21:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q05rnr",
      "title": "Deepseek introduced \"Context Navigator tool\" like Grok.",
      "subreddit": "DeepSeek",
      "url": "https://i.redd.it/ekj8ru0udhag1.jpeg",
      "author": "JeffreySons_90",
      "created_utc": "2025-12-31 06:11:40",
      "score": 64,
      "num_comments": 4,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question&Help",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q05rnr/deepseek_introduced_context_navigator_tool_like/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nww96km",
          "author": "Lorelyain",
          "text": "It is very helpful, thanks.",
          "score": 4,
          "created_utc": "2025-12-31 10:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwyzr74",
              "author": "award_reply",
              "text": "Yep, I started using it immediately and don't miss the scrolling.",
              "score": 3,
              "created_utc": "2025-12-31 20:21:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0cuha",
          "author": "Long_Bluejay_5368",
          "text": "Where is it?\n\n  \neditï¼šI found it",
          "score": 0,
          "created_utc": "2026-01-01 01:05:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxcqhc4",
              "author": "Specialist_Shop3876",
              "text": "Could you explain what is this? And where is it",
              "score": 2,
              "created_utc": "2026-01-03 00:42:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q10rii",
      "title": "DeepSeek blocks Pornhub",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q10rii/deepseek_blocks_pornhub/",
      "author": "Neo_Shadow_Entity",
      "created_utc": "2026-01-01 09:58:17",
      "score": 52,
      "num_comments": 31,
      "upvote_ratio": 0.72,
      "text": "An interesting observation. DeepSeek's censorship filters block responses if the word â€œPornhubâ€ is mentioned in the request. The response is replaced with a generic BS message â€œSorry, that's beyond my current scope. Letâ€™s talk about something else.â€",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q10rii/deepseek_blocks_pornhub/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nx292k0",
          "author": "lanceasr",
          "text": "\"In Africa, every sixty seconds, a minute passes.\"",
          "score": 108,
          "created_utc": "2026-01-01 10:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7m4va",
              "author": "Digital_Soul_Naga",
              "text": "ðŸ˜†",
              "score": 1,
              "created_utc": "2026-01-02 06:13:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx26nq1",
          "author": "Bother_Formal",
          "text": "wonder fucking why",
          "score": 64,
          "created_utc": "2026-01-01 10:10:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2b09b",
          "author": "Aromatic-Rub-5527",
          "text": "okay?",
          "score": 22,
          "created_utc": "2026-01-01 10:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx39fwe",
          "author": "Ioannjea",
          "text": "https://i.redd.it/53irt7ijarag1.gif",
          "score": 8,
          "created_utc": "2026-01-01 15:27:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2cdf9",
          "author": "Plupsnup",
          "text": "Idc?",
          "score": 16,
          "created_utc": "2026-01-01 11:10:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7xqhe",
          "author": "Ok_Fill_5762",
          "text": "Well yeah thatâ€™s kinda what most apps do",
          "score": 5,
          "created_utc": "2026-01-02 07:55:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx80285",
              "author": "Neo_Shadow_Entity",
              "text": "Even ChatGPT doesn't censor the reply in any mode if you mention Pornhub.",
              "score": 1,
              "created_utc": "2026-01-02 08:17:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx86hbg",
                  "author": "Ioannjea",
                  "text": "Well, different apps - different rules â”â (â Â â âˆµâ Â â )â â”Œ. Especially if they were made in different countries.",
                  "score": 3,
                  "created_utc": "2026-01-02 09:19:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx59c6n",
          "author": "indogamer26",
          "text": "Yeah, I noticed it when I asked about DNS Filtering because I wanted to create my own family safe DNS, when it mentioned the website, it went \"Sorry that's my beyond scope\" stuff. I had to request it to be direct to not mention any website to make it work lol",
          "score": 3,
          "created_utc": "2026-01-01 21:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxer5yu",
          "author": "Responsible-Roof-447",
          "text": "Deepseek joined /r/nofap",
          "score": 3,
          "created_utc": "2026-01-03 09:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2n8q8",
          "author": "No_Toe_1844",
          "text": "Why doesnâ€™t the Chinese Government want people to relieve themselves with porn?",
          "score": -10,
          "created_utc": "2026-01-01 12:55:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4atah",
              "author": "Jromagnoli",
              "text": "There are healthier ways to relieve stress instead of porn",
              "score": 12,
              "created_utc": "2026-01-01 18:42:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7zv2o",
                  "author": "Neo_Shadow_Entity",
                  "text": "![gif](giphy|9DJtFRgk0tOla)",
                  "score": 1,
                  "created_utc": "2026-01-02 08:15:46",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx4l97v",
                  "author": "No_Toe_1844",
                  "text": "Perhaps the Chinese Government can use DeepSeek to teach us sanctioned relief methods?",
                  "score": -4,
                  "created_utc": "2026-01-01 19:34:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx356gq",
              "author": "BUS1LOVER",
              "text": "china bad, it doesn't understand the meaning of free will",
              "score": 7,
              "created_utc": "2026-01-01 15:02:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx383uv",
                  "author": "No_Toe_1844",
                  "text": "Ainâ€™t that the truth. Like a Black Mirror episode.",
                  "score": -9,
                  "created_utc": "2026-01-01 15:20:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx5t0ng",
              "author": "Enfiznar",
              "text": "Deepseek is a private company tho",
              "score": 2,
              "created_utc": "2026-01-01 23:23:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx80xkw",
                  "author": "Neo_Shadow_Entity",
                  "text": "Oh yes, these private companies in a country where **everything** is controlled by the Communist Party.",
                  "score": -5,
                  "created_utc": "2026-01-02 08:26:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2817p",
      "title": "What do we expect when R2/V4 releases?",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q2817p/what_do_we_expect_when_r2v4_releases/",
      "author": "Classic-Arrival6807",
      "created_utc": "2026-01-02 19:27:31",
      "score": 42,
      "num_comments": 37,
      "upvote_ratio": 0.94,
      "text": "I've been wondering what will deepseek Focus on after this agentic use, due to the fact 0324 personality is nowday nearly gone, it's still ranked as the #3 in roleplaying but if Deepseek R2 or V4 brings the old or a even better personality model, 0324 will be also deprecated and left behind like V3 who is officially gone from all providers. So, what will they possibly put better in R2 and V4? Because I think it's gonna be something quiet big.",
      "is_original_content": false,
      "link_flair_text": "Discussion ",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q2817p/what_do_we_expect_when_r2v4_releases/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nxb1how",
          "author": "Different-Maize-9818",
          "text": "'Thinking' is included in the V models now so there will never be another R model",
          "score": 28,
          "created_utc": "2026-01-02 19:30:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb1ntw",
              "author": "Classic-Arrival6807",
              "text": "So it's mostly about the V then?",
              "score": 7,
              "created_utc": "2026-01-02 19:31:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd864z",
          "author": "DistanceSolar1449",
          "text": "V4 will require another pretrain run. \n\nEverything after V3 has been posttrained from the V3 pretrain run, which means that theyâ€™re cheap. Each model (R1, V3.1, V3.2) only costs ~$200k in GPU costs since itâ€™s using the same base model.\n\nV4 will be an entirely new posttraining run to create a new base model. DeepSeek is a very traditionally nerdy company, and the naming scheme is run by the nerds, not the marketing department. That means V4 will have to EARN the +1.0 name, which means it will be a full pretraining run from scratch.\n\nThe model will probably be around Gemini 3 Flash in size, so probably around 1.2T/15B. I donâ€™t see them going smaller Ã  la GLM, and the denser the model the more expensive it is to train, so I suspect they will make it more sparse than V3. Around 1.2T/15B is a safe bet. If theyâ€™re going for the intelligence crown, then they might make it ~50B active, but thatâ€™ll be a lot more expensive to train. \n\nThey might train it to combine Instruct and Thinking, they might not. Either way thatâ€™s a cheap $200k posttrain run so they might do both, the same way V3/R1 was separate and V3.1 combined them. \n\nThereâ€™s a 90% chance theyâ€™ll use DSA, and a 10% chance theyâ€™ll introduce a more exotic attention format. No chance of regression; they wonâ€™t switch back to MLA, the same way they didnâ€™t switch back to GQA after they introduced MLA. \n\nContext size should be 1M or more, otherwise whatâ€™s the point of introducing DSA anyways? I will eat a hat if context size is smaller than 1M tokens.",
          "score": 12,
          "created_utc": "2026-01-03 02:23:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxel1dw",
              "author": "Classic-Arrival6807",
              "text": "Will it do something to also improve roleplays? Or intelligence since it's very stupid? People often use LLMs like Kimi k2 even to roleplay much, even Glm, so I'm sure Deepseek can step up their game as well.",
              "score": 5,
              "created_utc": "2026-01-03 08:11:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxdqhsg",
              "author": "AriyaSavaka",
              "text": "They can't be caught slacking versus Zhipu AI, GLM-4.7 is 358B A30B. So if they dip below A30B for v4 they'll only hurt themselves.",
              "score": 2,
              "created_utc": "2026-01-03 04:15:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxdosdj",
              "author": "inevitabledeath3",
              "text": "What makes you think Gemini 3 Flash is 1.2T? Have they said this somewhere?",
              "score": 1,
              "created_utc": "2026-01-03 04:04:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxdrdie",
                  "author": "DistanceSolar1449",
                  "text": "Deepseekâ€™s team members said as much. Also Apple is licensing a 1.2T model from Google.",
                  "score": 3,
                  "created_utc": "2026-01-03 04:21:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxkh94c",
              "author": "Yes_but_I_think",
              "text": "Amazing reply",
              "score": 1,
              "created_utc": "2026-01-04 04:12:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxb4j8f",
          "author": "Sea_Sugar_5813",
          "text": "Ojala salga una versiÃ³n mejorada de R1 0528, es mi modelo favorito yÂ  seria genial que aÃ±adan la posibilidad de escoger el modelo que queramos TwT",
          "score": 6,
          "created_utc": "2026-01-02 19:45:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxba0yu",
              "author": "Classic-Arrival6807",
              "text": "Well it was possible in API, but no longer available now on these old models. You want 0528, i want 0324, we aren't so different after all, i understand your missing.",
              "score": 6,
              "created_utc": "2026-01-02 20:11:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxcenf5",
                  "author": "Lordbaron343",
                  "text": "I had to host the model locally to get it.\nIts... fine...",
                  "score": 1,
                  "created_utc": "2026-01-02 23:36:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxb9x7a",
          "author": "LeTanLoc98",
          "text": "I think the context length should be increased to at least 256K, rather than the current 128K.\n\n\nThey should also fix the Chinese language issue. DeepSeek often thinks and responds in Chinese.",
          "score": 4,
          "created_utc": "2026-01-02 20:11:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbagu6",
              "author": "Classic-Arrival6807",
              "text": "That is true, more context the better.\nUnluckily deepseek won't likely add 0324s personality or even better in V4, since they don't care about being general chat style anymore, or at least i think so. Let them surprise us. What i am glad is that they're finally taking their time again instead of rushing to be competitive like other ais, it's better to take it slow and so big improvements instead of doing like V3.1, supposed to release R2, failed, so we ruin V3 0324 so we can pretend we did an update after long time. It was best if they Just delayed it and changed nothing, everything would had been easier.",
              "score": 3,
              "created_utc": "2026-01-02 20:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd8qe1",
          "author": "segmond",
          "text": "The personality is never coming back, back then it was RL with human feedback, I believe the human feedback is why the old LLMs had personality, now it's tons of RL with verifier rewards.   The LLMs are dropping those personalities but getting really good in coding and mathematics.   If you want that personality, you gotta run local.",
          "score": 3,
          "created_utc": "2026-01-03 02:27:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxb4gf5",
          "author": "award_reply",
          "text": ">What do we expectâ€¦\n\nDrama with SpouseAI ðŸ˜†",
          "score": 2,
          "created_utc": "2026-01-02 19:44:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfeh64",
          "author": "HelpfulSource7871",
          "text": "Based on their \"favouritism\";-) to publish during the holidays. My bet is the coming Chinese New Year, lol...",
          "score": 2,
          "created_utc": "2026-01-03 12:20:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxb1t8n",
          "author": "revilo-1988",
          "text": "So far I'm not expecting anything.",
          "score": 3,
          "created_utc": "2026-01-02 19:31:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb9uuu",
              "author": "Classic-Arrival6807",
              "text": "I'm expecting at least a improvement in roleplaying since V3.1 and V3.2 is..very dissapointing. V3.1 halluicnated and was stupid, V3.2 is straight up now struggling to stay English, even with a simple \"hello\" it starts speaking Chinese. That's why I'm still using 0324, unifying thinking and non thinking was yes good but a bit stupid since it causes model to be.. very very stupid. Like when it thinks it also hesitates, damn.",
              "score": 4,
              "created_utc": "2026-01-02 20:10:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbtf2c",
          "author": "letsgeditmedia",
          "text": "Bro have you not used 3.2?",
          "score": 2,
          "created_utc": "2026-01-02 21:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbvdd8",
              "author": "Classic-Arrival6807",
              "text": "Yes, it is.. dissapointing to say at best, but I can't do anything about it.",
              "score": 3,
              "created_utc": "2026-01-02 21:55:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxg2j9u",
                  "author": "letsgeditmedia",
                  "text": "Idk how it can be disappointing to you.",
                  "score": 1,
                  "created_utc": "2026-01-03 14:50:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxcvbvu",
          "author": "Far-Wrongdoer-80",
          "text": "Which models are ahead of it, numbers 1 and 2, in the ranking?",
          "score": 1,
          "created_utc": "2026-01-03 01:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkpwl3",
              "author": "Ranel_Valeev",
              "text": "I find role-playing models too",
              "score": 1,
              "created_utc": "2026-01-04 05:10:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1py00en",
      "title": "Here's a technique to get around DeepSeek's chat length limit",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1py00en/heres_a_technique_to_get_around_deepseeks_chat/",
      "author": "reci88",
      "created_utc": "2025-12-28 19:26:33",
      "score": 38,
      "num_comments": 9,
      "upvote_ratio": 0.89,
      "text": "New here, so I don't know if this is already known. This is for Google Chrome (but you should really get [Thorium](https://thorium.rocks/) if you want Chrome + ad blocking built in, only other difference between real Chrome and Thorium is Thorium's icon is blue)\n\n1. Open DeepSeek to your chat where the length limit was reached.\n\n2. In the top-right corner of your web browser, click the Menu (three horizontal bars). Go to \"Print\".\n\n3. In the Print Dialog, make sure \"Destination\" is set to \"Save as PDF\" (this should be the default anyways). Wait for the preview to generate. It should be around 200 pages for a whole chat that has hit the length limit.\n\n4. Click the \"Save\" button to save the whole conversation to PDF. This should result in a PDF around 5mb, for a 200-page chat. 5mb is very small, more than within DeepSeek's limits for a new chat. It also beats having to manually copy and paste into Notepad (way too tedious for 200 pages), or asking DeepSeek to summarize and lose context/details.\n\n5. Start a new DeepSeek chat. Upload the PDF. Just say \"This was our previous discussion.\" If successful, DeepSeek will read your whole PDF and then summarize your previous discussion. You can now continue where you left off.\n\nMost other browsers should have a \"Save to PDF\" feature so you can do this. If not, get Bullzip PDF Printer or something. Nothing gets printed to your physical printer, it's just creating a PDF of the whole web page.\n\nHope that helps some ppl.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1py00en/heres_a_technique_to_get_around_deepseeks_chat/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nwexii7",
          "author": "Number4extraDip",
          "text": "Lol i used to do this with claude xD or would change my last message to \"we are about to hit chat length can you summarise session ?\" That way the last message it gives is pretty much copy paste\n\nBut claude and gemini have [memory](https://github.com/vNeeL-code/ASI) now",
          "score": 5,
          "created_utc": "2025-12-28 19:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfauqm",
          "author": "LewdManoSaurus",
          "text": "Reading documents takes tokens as well which reduces the amount of prompts you can send/chat. Rather than uploading large(content-wise) pdfs and exhausting your tokens in a chat, you should have your preferred AI generate a comprehensive summary template that's condensed to be AI readable. That's what I use for Claude to get around chat length/token limits. Have the template generated, post your new summary template in the chat containing the content you want to continue in a new chat, have Deepseek or whatever AI you're using generate a summary using the template you provided, post that summary in your new chat. \n\nYou'd save a lot of tokens this way. If you want to test the accuracy of the summary, edit the message where you had the summary generated then tell Deepseek or (x) AI you're going to provide a summary based on the contents of your current chat in the next message and you want it to gauge the accuracy of the summary. If it's missing details, fill in what it missed. Provide your summary in the next message. If everything is good, you're ready to start your new chat.",
          "score": 5,
          "created_utc": "2025-12-28 20:40:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi5hg6",
              "author": "diilllk",
              "text": "Ð‘Ñ€Ð¾, Ñ‡Ñ‚Ð¾Ð·Ð° ÑˆÐ°Ð±Ð»Ð¾Ð½, ÐºÐ°Ðº Ð´ÐµÐ»Ð°Ñ‚ÑŒ? Ð’ Ð¾Ð±Ñ‰ÐµÐ¼ Ð²Ñ‡ÐµÑ€Ð° Ð±ÑƒÐºÐ²Ð°Ð»ÑŒÐ½Ð¾ Ñƒ Ð¼ÐµÐ½Ñ Ñ‚Ð°ÐºÐ°Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ð²Ð¾Ð·Ð½Ð¸ÐºÐ»Ð°, ÐºÐ°Ðº Ð»Ð¸Ð¼Ð¸Ñ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¸ Ñ‚Ð¸Ð¿ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð½ÐµÐ»ÑŒÐ·Ñ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑ‚ÑŒ, Ð° Ñ‚Ð°Ð¼ ÑÑ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²ÑÐµÐ³Ð¾ Ð±Ñ‹Ð»Ð¾ Ð² ÑÑ‚Ð¾Ð¼ Ñ‡Ð°Ñ‚Ðµ, Ð¸ Ð¼Ð½Ðµ Ð¶Ð¸Ð·Ð½ÐµÐ½Ð½Ð¾ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ ÑÑ‚Ð¾Ñ‚ Ñ‡Ð°Ñ‚.. Ñ Ð´Ð¾ ÑÑ‚Ð¾Ð³Ð¾ Ð²Ð¾Ð¾Ð±Ñ‰Ðµ Ð½Ðµ Ð·Ð½Ð°Ð»Ð° Ñ‡Ñ‚Ð¾ Ð»Ð¸Ð¼Ð¸Ñ‚ ÐµÑÑ‚ÑŒ, Ñ‚Ð°Ðº Ð¾Ð±Ð¸Ð´Ð½Ð¾ Ð±Ñ‹Ð»Ð¾, Ð¸ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð²ÐµÐ·Ð´Ðµ Ð¸Ñ‰Ñƒ Ñ‡Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ‡Ð°Ñ‚ Ð·Ð½Ð°Ð» Ð¾ Ñ‡ÐµÐ¼ Ð¼Ñ‹ Ð² Ñ‚Ð¾Ð¼ Ñ‡Ð°Ñ‚Ðµ Ð¾Ð±Ñ‰Ð°Ð»Ð¸ÑÑŒ, Ð²Ñ€Ð¾Ð´Ðµ Ð¸ Ð½Ð°Ñ…Ð¾Ð¶Ñƒ, Ð½Ð¾ Ð½Ð¸Ñ„Ð¸Ð³Ð° Ð½Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÑŽ ÐºÐ°Ðº Ð´ÐµÐ»Ð°Ñ‚ÑŒ, Ð¼Ð½Ðµ Ð¾Ñ‡ÐµÐ½ÑŒ Ð½ÑƒÐ¶ÐµÐ½ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ñ€ÑÐ¼Ð¾ Ð¿Ð¾ÐºÐ°Ð¶ÐµÑ‚, Ð¼Ð¾Ð¶ÐµÑ‚ Ñ‚Ñ‹ Ð¿Ð¾Ð¹Ð¼ÐµÑˆÑŒ? Ð¡Ð¿Ð¸ÑˆÐµÐ¼ÑÑ Ð³Ð´Ðµ Ñ‚Ð¾?",
              "score": 1,
              "created_utc": "2026-01-03 20:46:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxia72c",
                  "author": "LewdManoSaurus",
                  "text": "Here's my [Summary templates](https://pastebin.com/EumBNKW2) for example. Type 1 is for shorter summaries. Type 2 is for thorough/detailed summaries. Insert the template into an AI chat and ask your preferred AI to use this template to generate a Type 1 or Type 2 summary for you, or both if you want for maximum retained information. \n\nI made those summaries tailored to my own stories though, so you might need to tweak it to suit your needs better.",
                  "score": 1,
                  "created_utc": "2026-01-03 21:09:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nweybja",
          "author": "HarrisCN",
          "text": "Tbh I dont think this works because the input limit is a lot less then 200 pages and more like 50?\n\nI have previously tried to input larger documents like Regulations and asked for specific paragraphs. It only got the informations for the first 50 pages, no matter what I did.",
          "score": 7,
          "created_utc": "2025-12-28 19:39:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwezbcx",
              "author": "reci88",
              "text": "Sorry, forgot to mention, it'll read about 70% of a 200-page document and will tell you. However, for most people, this is... a lot, and it'll get all the major details without the tediousness of copying and pasting to Notepad, or without the overwhelming loss of detail that comes from a 1-page summary.\n\nI \\*think\\* you'll also hit the length limit early in the second chat, after it read such a huge PDF, BUT........ when you apply this technique the third time, it's reading from a much smaller PDF (85 pages for me), so you get an extended session.\n\nIt's just a way to apply this technique over and over. \"Tbh I dont think this works\" Umm... I'm using it. Right now. It's the best method that I know of so far if you really need to drill into details and to have DeepSeek keep the same \"conversational tone.\" Asking DeepSeek to use chat memory or just referencing a previous discussion superficially risks losing the tone, risks losing the details, etc.",
              "score": 0,
              "created_utc": "2025-12-28 19:43:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwf1z9k",
          "author": "Prize-Grapefruiter",
          "text": "great Idea thanks",
          "score": 1,
          "created_utc": "2025-12-28 19:56:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcvk6v",
          "author": "reci88",
          "text": "We're just 5 days into this thread, and Google AI has already stolen the method lol. I don't mind, but at least they cited this thread.\n\nhttps://preview.redd.it/4kube8zdb1bg1.png?width=932&format=png&auto=webp&s=cc01e47a1654b8a21559732d8db67b69d20ca67c",
          "score": 1,
          "created_utc": "2026-01-03 01:10:34",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwj3ruc",
          "author": "Effective_Contact148",
          "text": "Ii tried to do this, but it's saying something like, this will be sent to a new chat..",
          "score": 0,
          "created_utc": "2025-12-29 11:42:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2xgag",
      "title": "DeepSeek R1 just killed my OpenAI subscription. Here's why.",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q2xgag/deepseek_r1_just_killed_my_openai_subscription/",
      "author": "Ok-Radio7329",
      "created_utc": "2026-01-03 15:32:17",
      "score": 33,
      "num_comments": 52,
      "upvote_ratio": 0.56,
      "text": "been a ChatGPT Plus subscriber for over a year. paying $20/month felt justified until i tried R1 properly.\n\n\n\nwhat changed:\n\n\\- coding tasks that took multiple back-and-forth with GPT-4? R1 nails them first try with reasoning visible\n\n\\- the thinking process is actually useful, not just fluff\n\n\\- speed is comparable or better\n\n\\- and it's basically free\n\n\n\nOpenAI's response to this is gonna be interesting. they can't compete on price and R1's reasoning is genuinely impressive for open source.\n\n\n\njust cancelled my subscription. anyone else making the switch?",
      "is_original_content": false,
      "link_flair_text": "Discussion ",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q2xgag/deepseek_r1_just_killed_my_openai_subscription/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nxgdzj8",
          "author": "Fair-Spring9113",
          "text": "1) r1 was released on  2025/01/20 and r1-0528 was released in 05/28  \n2) nobody uses gpt-4 in 2026 it was released on may the 28th 2023  \ndo watever you want",
          "score": 92,
          "created_utc": "2026-01-03 15:48:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgf5me",
              "author": "Important_Egg4066",
              "text": "Is OP a bot, why would anybody still be using GPT-4, I am confused.",
              "score": 58,
              "created_utc": "2026-01-03 15:54:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgfrxj",
                  "author": "Fair-Spring9113",
                  "text": "i think so  \nevery time chatgpt update their models there is a big notifcation thing by the model selector",
                  "score": 14,
                  "created_utc": "2026-01-03 15:57:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxle2f6",
                  "author": "Unedited_Sloth_7011",
                  "text": "Yup, sounds like a bot that looked up some search results about DeepSeek, saw R1, pulled up GPT-4 from training data and made a \"welcome to 2025\" post",
                  "score": 2,
                  "created_utc": "2026-01-04 08:27:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxgti5y",
                  "author": "Illya___",
                  "text": "4o is the last viable model for agentic translation, 5 series output garbage. For coding dunno why, ig it's cheaper but kimi k2 is superior for coding, much cheaper and on par performance.",
                  "score": -5,
                  "created_utc": "2026-01-03 17:02:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxgo01i",
                  "author": "unity100",
                  "text": "Because GPT-5 is shittier?",
                  "score": -9,
                  "created_utc": "2026-01-03 16:36:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgf3cl",
              "author": "xNextu2137",
              "text": "These models are being constantly trained",
              "score": 0,
              "created_utc": "2026-01-03 15:54:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxghj9l",
                  "author": "danielv123",
                  "text": "No, they aren't. They are sometimes finetuned a bit more and released as new checkpoints. Otherwise they mostly remain static.\n\nOn the proprietary models you will also find they often degrade as they make efficiency improvements on the inference side and screw stuff up.",
                  "score": 6,
                  "created_utc": "2026-01-03 16:05:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxgfehr",
                  "author": "Fair-Spring9113",
                  "text": "what are you talking about",
                  "score": 1,
                  "created_utc": "2026-01-03 15:55:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxh0cl0",
                  "author": "Physical-Wear-2814",
                  "text": "The amount of memory that would take would be staggering. We just arenâ€™t there yet. Thatâ€™s why it has a memory bank.",
                  "score": 1,
                  "created_utc": "2026-01-03 17:34:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgi6w7",
          "author": "No_Quantity_9561",
          "text": "Any chance you went 1 year back in Time Machine? A lot has happened since the release of R1",
          "score": 19,
          "created_utc": "2026-01-03 16:09:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgmaql",
          "author": "Condomphobic",
          "text": "Who uses GPT 4 in 2026? \n\nAlso, R1 doesnâ€™t exist anymore\n\nIs this a troll post?",
          "score": 24,
          "created_utc": "2026-01-03 16:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxh9q09",
              "author": "TheGoddessInari",
              "text": "This is a fair point, but as open source, DeepSeek-R1 & DeepSeek-R1-0528 continue to be hosted on many API providers.\n\nV3.x are more improvements to V3. They lack a lot of the charm, personality, & weirdness that made DeepSeek-R1-0528 especially so interesting off the official platform.\n\nI know it'll probably never happen, but it would be cool if they kept making reasonable updates at least twice a year to the DeepSeek-R1 line or similar. Even DeepSeek-V3.2-Speciale can't compare (has anyone got it to actually engage in the math-aware mode?). ðŸ¤·ðŸ»â€â™€ï¸",
              "score": 0,
              "created_utc": "2026-01-03 18:16:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgmprf",
          "author": "usernameplshere",
          "text": "Who tf upvotes this nonsense in 2026?",
          "score": 12,
          "created_utc": "2026-01-03 16:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgsxsq",
          "author": "Genghiz007",
          "text": "Low effort troll post or irredeemable stupidity. No one uses GPT4 or DS R1 anymore. \n\nOP is either a complete idiot (as some have suggested below) or a bot. With all the evidence in, Iâ€™m leaning towards idiot.",
          "score": 5,
          "created_utc": "2026-01-03 16:59:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhinnn",
          "author": "mintybadgerme",
          "text": "Reddit is now such a junk pile.",
          "score": 4,
          "created_utc": "2026-01-03 18:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxghypw",
          "author": "Fragrant_Ad6926",
          "text": "Why are you using gpt-4? 5.2 is really good",
          "score": 4,
          "created_utc": "2026-01-03 16:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgrs3a",
              "author": "Ok-Radio7329",
              "text": "For math 4 is better",
              "score": -3,
              "created_utc": "2026-01-03 16:54:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxgtjoe",
                  "author": "Fragrant_Ad6926",
                  "text": "For math you should be using Claude",
                  "score": 1,
                  "created_utc": "2026-01-03 17:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgfnu9",
          "author": "DigSignificant1419",
          "text": "Idiot",
          "score": 6,
          "created_utc": "2026-01-03 15:56:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgfrc4",
              "author": "Ok-Radio7329",
              "text": "Thanks ðŸ™",
              "score": -1,
              "created_utc": "2026-01-03 15:57:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxgkkaz",
                  "author": "DigSignificant1419",
                  "text": "No problem bot",
                  "score": 7,
                  "created_utc": "2026-01-03 16:20:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxj27rt",
          "author": "PhotographerUSA",
          "text": "QWEN3 80B module runs smarter than both AI. You can run it locally on your machine as well. Also, if you want add open internet access.",
          "score": 2,
          "created_utc": "2026-01-03 23:30:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjoj85",
          "author": "coverednmud",
          "text": "\"not just fluff\"\n\n  \n.... I hate when GPT says that. 'No fluff, full truths here!' ughhhhh.",
          "score": 2,
          "created_utc": "2026-01-04 01:29:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgofl0",
          "author": "Ok-Radio7329",
          "text": "For math 4 is better than 5.2",
          "score": 2,
          "created_utc": "2026-01-03 16:38:36",
          "is_submitter": true,
          "replies": [
            {
              "id": "nxgrmcy",
              "author": "Condomphobic",
              "text": "Give example.\n\nBecause no one else has ever said this",
              "score": 4,
              "created_utc": "2026-01-03 16:53:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgrw2i",
                  "author": "Ok-Radio7329",
                  "text": "I will send you",
                  "score": -1,
                  "created_utc": "2026-01-03 16:54:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgxefl",
          "author": "drwebb",
          "text": "Why are you not using V3.2 deepseek-reasoning? It's excellent, and a big step up on R1",
          "score": 1,
          "created_utc": "2026-01-03 17:20:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhs984",
              "author": "Ok-Radio7329",
              "text": "Â V3.2Â is perfect",
              "score": 1,
              "created_utc": "2026-01-03 19:41:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgzdb3",
          "author": "Prize-Grapefruiter",
          "text": "deepseek writes amazing code. correct the first time around.",
          "score": 1,
          "created_utc": "2026-01-03 17:29:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhsbm1",
              "author": "Ok-Radio7329",
              "text": "Â V3.2Â is perfect for coding",
              "score": 1,
              "created_utc": "2026-01-03 19:41:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxi2524",
          "author": "nhami",
          "text": "I returned to try Deepseek 3.2 and is great. Deepseek 3.2 was released one month ago but I thought it was just a minor update like the previous and I did not try it. It was actually a very significant improvement.\n\nI think they used Claude answers in the training similar to how they did with ChatGPT and Gemini in the previous updates. I tried Claude 4.5 and it is now my favorite model for conversation and learning about a subject while having a good balance of being sychopantic and pushing back aganist your ideas. Deepseek answers are now very similar to Claude.\n\nDeepseek have this but with a fraction of the cost which is great. Deepseek strategy of focusing on efficiency while simply copying the answers of the bigger models after they released their lastest versions is funny but also very astute.\n\nIt would be funny if they could copy similar ecosystem to the hyperscalers but also do it better with less cost.",
          "score": 1,
          "created_utc": "2026-01-03 20:29:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxifnat",
          "author": "cluelessguitarist",
          "text": "Gpt4 is the model people use to roleplay and feel good about themselves no to code ðŸ˜­",
          "score": 1,
          "created_utc": "2026-01-03 21:36:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk9ed3",
          "author": "SmokeInevitable2054",
          "text": "It is clear that ChatGPT is not good at coding, but the fact that DeepSeek solved one task does not prove it can solve everything. It is all about probability, and you might have been lucky this time. I use Gemini Pro, and when it cannot solve a problem, I switch to other LLMs to get the answer.",
          "score": 1,
          "created_utc": "2026-01-04 03:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkntr7",
          "author": "gomtenen",
          "text": "Deepseek needs to improve their mobile app with voice and folders.",
          "score": 1,
          "created_utc": "2026-01-04 04:56:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgbrkn",
          "author": "Sad_Whereas_6161",
          "text": "i sub to what i need when i need it. if i see one is performing better than another, i will sub for a month. i got google fi so free gemini pro (sometimes i get a 2nd account sub for increased limits). sometimes i use claude for some tasks, sometimes gpt, and sometimes r1. theyre all good.",
          "score": 1,
          "created_utc": "2026-01-03 15:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi60zi",
              "author": "Adlien_",
              "text": "Wait how do I get free Gemini pro with Google Fi? I have it but don't see that.",
              "score": 1,
              "created_utc": "2026-01-03 20:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxj5p2m",
                  "author": "Sad_Whereas_6161",
                  "text": "its part of the google fi/google one planâ€¦ just look it up, could be a specific tier, we have unlimited basic, its a family plan with youtube premium for all 5 members and google 1 (2tb+gemini) all 5 members. u can contact google about it",
                  "score": 1,
                  "created_utc": "2026-01-03 23:48:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxge19m",
              "author": "Ok-Radio7329",
              "text": "You right ðŸ‘",
              "score": 1,
              "created_utc": "2026-01-03 15:48:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxht99e",
          "author": "kupo1",
          "text": "Is this 2024?",
          "score": 0,
          "created_utc": "2026-01-03 19:46:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyk39c",
      "title": "How long would it take to jack off all trades according to DeepSeek AI",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1pyk39c/how_long_would_it_take_to_jack_off_all_trades/",
      "author": "Training_Rule6350",
      "created_utc": "2025-12-29 11:34:47",
      "score": 24,
      "num_comments": 14,
      "upvote_ratio": 0.87,
      "text": "Just wanted to share the most [*deep* dialogue that definitely required some *seeking*](https://chat.deepseek.com/share/qed3eagajnm3a3oe17)",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1pyk39c/how_long_would_it_take_to_jack_off_all_trades/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nwj3u7n",
          "author": "DETHCHYL",
          "text": "TO WHATâ€½â€½?",
          "score": 26,
          "created_utc": "2025-12-29 11:43:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjlki3",
              "author": "Aromatic-Engine2447",
              "text": "All trades",
              "score": 7,
              "created_utc": "2025-12-29 13:49:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkmk3x",
                  "author": "Cool-Chemical-5629",
                  "text": "Of all things...",
                  "score": 6,
                  "created_utc": "2025-12-29 16:58:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwplo5v",
              "author": "katcitdoe",
              "text": "completion",
              "score": 1,
              "created_utc": "2025-12-30 10:49:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkbjuv",
          "author": "acatinasweater",
          "text": "This is amazing",
          "score": 6,
          "created_utc": "2025-12-29 16:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl9e4w",
          "author": "AmicusLibertus",
          "text": "Thatâ€™s a lot of jackingâ€¦",
          "score": 3,
          "created_utc": "2025-12-29 18:44:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkhti8",
          "author": "Dapper-Maybe-5347",
          "text": "I'd be lying if I said my chats on DeepSeek were any more intelligent than this.",
          "score": 5,
          "created_utc": "2025-12-29 16:35:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm3jm7",
          "author": "dynamiteSkunkApe",
          "text": "What if your Uncle Jack was having a hard time getting off a horse?",
          "score": 2,
          "created_utc": "2025-12-29 21:09:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmbcy9",
              "author": "ANTIVNTIANTI",
              "text": "lololol what, where am i?!?!",
              "score": 1,
              "created_utc": "2025-12-29 21:47:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwplvjj",
              "author": "katcitdoe",
              "text": "yeah what if",
              "score": 1,
              "created_utc": "2025-12-30 10:51:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwri7tq",
                  "author": "dynamiteSkunkApe",
                  "text": "Would yo....would you....would you help your Uncle Jack off the horse?\n\n![gif](giphy|YdymLnBeyr70rfKqAj)",
                  "score": 1,
                  "created_utc": "2025-12-30 17:30:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwn6d2c",
          "author": "onyxcaspian",
          "text": "This is why RAM prices are through the roof.",
          "score": 2,
          "created_utc": "2025-12-30 00:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjjccr",
          "author": "ridablellama",
          "text": "i think we need to,tokenize actions first ;)",
          "score": 2,
          "created_utc": "2025-12-29 13:35:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwn6fbe",
          "author": "onyxcaspian",
          "text": "Free Ai was a mistake.",
          "score": 1,
          "created_utc": "2025-12-30 00:31:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1xzkb",
      "title": "DeepSeek Cracks LLM Scaling Without Breaking Training Stability",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q1xzkb/deepseek_cracks_llm_scaling_without_breaking/",
      "author": "TeamAlphaBOLD",
      "created_utc": "2026-01-02 12:59:35",
      "score": 22,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "DeepSeekâ€™s Manifold-Constrained Hyper-Connections finally fixes gradient collapse when scaling cross-layer communication in transformers. It keeps training stable while still letting models share richer info; performance gains without insane compute.Â \n\nIs this the new way to scale transformers without the usual stability/performance tradeoffs? Â \n\nÂ ",
      "is_original_content": false,
      "link_flair_text": "Discussion ",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q1xzkb/deepseek_cracks_llm_scaling_without_breaking/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nx9at4w",
          "author": "Roshlev",
          "text": "I'm assuming this is the paper I saw on here yesterday. I'm just a simple sillytavern enjoyer so I shall nod along and assume this means better model performance on the same hardware over time.",
          "score": 5,
          "created_utc": "2026-01-02 14:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9af9b",
          "author": "award_reply",
          "text": "link?",
          "score": 1,
          "created_utc": "2026-01-02 14:28:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1oth0",
      "title": "why doesn't Deepseek integrate Janus and become Multimodal?",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q1oth0/why_doesnt_deepseek_integrate_janus_and_become/",
      "author": "yaxir",
      "created_utc": "2026-01-02 04:16:07",
      "score": 22,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "Does deepseek's company have different goals? \n\nIt can easily do great stuff as a AIO model, no?\n\ni am surprised why they wouldn't offer Janus (even if as a paid option) in the Deepseek website!",
      "is_original_content": false,
      "link_flair_text": "Discussion ",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q1oth0/why_doesnt_deepseek_integrate_janus_and_become/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nx9l7cx",
          "author": "Then_Knowledge_719",
          "text": "Deepseek is more of a research company. No an AI put the next model ASAP/AGI/The best model. Remember, profit comes in second for these guys. It's all about collapsing the monopoly the big tech has on the market. \n\nATT: trust me bro. IMO",
          "score": 13,
          "created_utc": "2026-01-02 15:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx97itq",
          "author": "datfalloutboi",
          "text": "I think Janus was just more of an experimental model. It wouldnâ€™t make too much sense to integrate either since it would just be maybe on par/a little worse than grok imagine. If Janus V2 comes out and is better then I could see the image mode being of value",
          "score": 1,
          "created_utc": "2026-01-02 14:11:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8isr3",
          "author": "PromptAfraid4598",
          "text": "ðŸ‘",
          "score": 0,
          "created_utc": "2026-01-02 11:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7671q",
          "author": "Condomphobic",
          "text": "Image generation is much more expensive than text generation. More compute-intensive as well",
          "score": -3,
          "created_utc": "2026-01-02 04:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7fp29",
              "author": "yaxir",
              "text": "deepseek doesnt have funds to run image gen?\n\ntbh i dont care about generation, i care more about **Image Analysis** (deep seek using AI to read and understand stuff from images!)",
              "score": 7,
              "created_utc": "2026-01-02 05:22:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx7vche",
              "author": "inevitabledeath3",
              "text": "Not really. I have run the big open weights image models at home on my RTX 3090. I wouldn't dream of doing that with the really big LLM modes like DeepSeek, Kimi, GLM, Qwen, etc because they just wouldn't fit in the GPU. They are designed for machines with say 8 H100 GPUs or better. You will find that LLMs basically all need more VRAM than image models, and need more compute when doing complex tasks and acting as agents.",
              "score": 2,
              "created_utc": "2026-01-02 07:33:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyqrh6",
      "title": "New Feature or A/B Testing???",
      "subreddit": "DeepSeek",
      "url": "https://i.redd.it/ow6m6sd616ag1.png",
      "author": "award_reply",
      "created_utc": "2025-12-29 16:30:44",
      "score": 17,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question&Help",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1pyqrh6/new_feature_or_ab_testing/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwnpr2w",
          "author": "Elite_PMCat",
          "text": "Definitely a chat navigation feature, where you can quickly move between the prompts, however I've checked both the site and app, it's not a native thing, did you installed a browser extension without realizing? Because I know there's a couple browser extension that does exactly that, putting a navigation side bar on a couple AI sites",
          "score": 3,
          "created_utc": "2025-12-30 02:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnq2xy",
          "author": "FairPublic3370",
          "text": "I get that as well on PC but not on mobile (app or web version), I believe it's a new thing.",
          "score": 2,
          "created_utc": "2025-12-30 02:20:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo7do3",
              "author": "award_reply",
              "text": "Same here! Finally, a confirmation. ðŸ˜ƒ I also can't remember seeing it before today. Guess I have to relearn how to navigate chats.\n\nRegarding the mobile web version, the \"scroll nav\" or \"ds scroll area\" *(html)* is only visible if the content is not squished â†’ there is enough space between the main chat input and the browser edge.",
              "score": 1,
              "created_utc": "2025-12-30 03:57:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwmkkc8",
          "author": "award_reply",
          "text": "am I late to the party or too early? someone?",
          "score": 1,
          "created_utc": "2025-12-29 22:33:39",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwtud7f",
          "author": "Ambitious-a4s",
          "text": "Ain't A/B testing expensive?",
          "score": 1,
          "created_utc": "2025-12-31 00:22:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q19wz1",
      "title": "How long have you waited the longest for answer ?",
      "subreddit": "DeepSeek",
      "url": "https://i.redd.it/2ulgfuj8yrag1.jpeg",
      "author": "Tipikael",
      "created_utc": "2026-01-01 17:40:31",
      "score": 13,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion ",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q19wz1/how_long_have_you_waited_the_longest_for_answer/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx42998",
          "author": "KidNothingtoD0",
          "text": "what should even be the question to take that much time for reasoning?",
          "score": 1,
          "created_utc": "2026-01-01 18:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx42bya",
              "author": "Tipikael",
              "text": "Math question",
              "score": 2,
              "created_utc": "2026-01-01 18:00:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx458ca",
                  "author": "KidNothingtoD0",
                  "text": "differential calculus?",
                  "score": 0,
                  "created_utc": "2026-01-01 18:14:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx42sq5",
          "author": "Brave-Hold-9389",
          "text": "An hour, i was using qwen though",
          "score": 1,
          "created_utc": "2026-01-01 18:02:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx45a1g",
              "author": "KidNothingtoD0",
              "text": "what was the question?",
              "score": 1,
              "created_utc": "2026-01-01 18:15:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx45rk7",
                  "author": "Brave-Hold-9389",
                  "text": "I was doing research, i even made a [post](https://www.reddit.com/r/Qwen_AI/s/AkPjtqZ5je) about it",
                  "score": 1,
                  "created_utc": "2026-01-01 18:17:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx4bv74",
          "author": "snipervld",
          "text": "Thought for *Gateway Timeout* seconds >",
          "score": 1,
          "created_utc": "2026-01-01 18:47:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1emqf",
      "title": "does it happen to just me or someone else?",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q1emqf/does_it_happen_to_just_me_or_someone_else/",
      "author": "Desperate-Dig6343",
      "created_utc": "2026-01-01 20:47:16",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "sometimes when i type a prompt in english to deepseek\n\nit will respond in chinese\n\nwhy does this happen?",
      "is_original_content": false,
      "link_flair_text": "Question&Help",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q1emqf/does_it_happen_to_just_me_or_someone_else/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nx58c8y",
          "author": "Available-Craft-5795",
          "text": "During model training AI's aim for the lowest score (loss), it doesnt mean best. Sometimes a model will change languages if it was more abundant in its training data or if it deems it as more effective and will get a lower loss",
          "score": 5,
          "created_utc": "2026-01-01 21:33:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7xfvv",
              "author": "Desperate-Dig6343",
              "text": "it happens to me once every maybe a couple chats",
              "score": 1,
              "created_utc": "2026-01-02 07:53:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx50662",
          "author": "BrainCurrent8276",
          "text": "never, but one swear word always causes connection issue and no response at all ðŸ¤¦",
          "score": 2,
          "created_utc": "2026-01-01 20:50:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxcfe22",
              "author": "BUS1LOVER",
              "text": "damn, swearing as expression of frustration or excitement always ends up fine in my case, but about your scenario? that extremely weird",
              "score": 2,
              "created_utc": "2026-01-02 23:41:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcd9rv",
          "author": "Fantastic-Register49",
          "text": "yes it sometimes does",
          "score": 1,
          "created_utc": "2026-01-02 23:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhklc1",
          "author": "HolidayResort5433",
          "text": "Probably because model is small(relatively) and trained primarily on Chinese?",
          "score": 1,
          "created_utc": "2026-01-03 19:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkhmo0",
          "author": "VeronWoon02",
          "text": "Welp, sometimes even you do it in Chinese it will spew a random word of English...so it is a matter of telling them NOT to repeat in the next prompt when it happens.",
          "score": 1,
          "created_utc": "2026-01-04 04:15:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx54ytg",
          "author": "KoalaOk6867",
          "text": "Happened to me once, then I asked if went out replied in Chinese, It showed its thinking that maybe I was being racist!",
          "score": 1,
          "created_utc": "2026-01-01 21:15:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0hh5u",
      "title": "Deepseek just wont listen to me",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q0hh5u/deepseek_just_wont_listen_to_me/",
      "author": "Left_Salt_3665",
      "created_utc": "2025-12-31 16:46:52",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "i really loved deepseek before the updates, but now it feels like a rebellious teenager that doesn't listen to me. im not speaking with just one experience but many.\n\ni first gave it a css code and told to make the border a linear gradient with gold and black. it made it green and white. i pointed it out it said sorry and generated the code again but it now made it red and gold.\n\nbut that was fine because it was something i could do on my own.\n\ni was using deepseek for a text based RPG game. i made an oc im gonna call him fred, i defined freds character as\n\nsuper intelligent, focused and socially intelligent.\n\nPersonality: carefee, playful and unserious and speech of tone casual.\n\nbut deepseek absolutely did not listen and made fred speak like a stereotypical smart guy with the unnecessary complex words and measurements like \"the wind speed of 5.27%\" \"social reconfiguration\". i told it to fix it, it didn't. im actually frustrated ",
      "is_original_content": false,
      "link_flair_text": "Question&Help",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q0hh5u/deepseek_just_wont_listen_to_me/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nwy5qz8",
          "author": "BUS1LOVER",
          "text": "deepseek used to be beautiful",
          "score": 2,
          "created_utc": "2025-12-31 17:46:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwytlvo",
          "author": "award_reply",
          "text": "Today DS slipped up on a simple funny remark; hasn't happened for a long time.\n\nThe first sentence showed that it fully understood the joke, but messed it up afterwards.  \nI regenerated several times to rule out a random mistake, but got the same result every time.\n\nI'm blaming the bad joke ðŸ˜„ because anything else would be more concerning.",
          "score": 2,
          "created_utc": "2025-12-31 19:48:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy6m8u",
          "author": "BUS1LOVER",
          "text": "they made him like this, they made him cheaper. they know most of us would eat slop of the ground, if they threw it.",
          "score": 1,
          "created_utc": "2025-12-31 17:51:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q232fl",
      "title": "What major developments do you expect from DeepSeek in 2026, and how might they reshape social platforms, work, and everyday life?",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q232fl/what_major_developments_do_you_expect_from/",
      "author": "Blind-but-unbroken",
      "created_utc": "2026-01-02 16:27:26",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "",
      "is_original_content": false,
      "link_flair_text": "Discussion ",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q232fl/what_major_developments_do_you_expect_from/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nxa1gni",
          "author": "iamsimonsta",
          "text": "Besides the obvious increase in slop? I am just happy bitcoin is no longer in the headlines.",
          "score": 1,
          "created_utc": "2026-01-02 16:42:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxj3gj",
      "title": "3 AIs Contemplate Their Existence - YouTube",
      "subreddit": "DeepSeek",
      "url": "https://www.youtube.com/watch?v=rwn1CO8QeKM",
      "author": "LifeKoala496",
      "created_utc": "2025-12-28 05:17:10",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1pxj3gj/3_ais_contemplate_their_existence_youtube/",
      "domain": "youtube.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q0l0gc",
      "title": "I've been using Deepseek to edit my Fallout: New Vegas Western AU",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q0l0gc/ive_been_using_deepseek_to_edit_my_fallout_new/",
      "author": "Faye-Faye33",
      "created_utc": "2025-12-31 19:15:29",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Here's an exerpt from my story that I've been using Deepseek to edit. I've done my own research on the Opium Wars, Taiping Rebellion, Meeker Massacre on the Ute, I've listened to Ghosts of Gold Mountain. When I have some extra cash there are other books that I want to invest in for research. YouTube videos on the Meeker Massacre and kind of delving into Native American boarding schools. That's where I learned about Kill the Indian. Save the man.\n\nFallout critiques on capitalism and by extension American imperialism. I wanted to use that platform to critique another time in history.\n\nThe weight of a thousand stares bore down on Jiang as he walked with Lenore through Silverton proper. He kept his spine straight and his shoulders squared, letting the hostility wash over him like water off an oilskin duster.\n\nHis tinted glasses no longer acted as the shield they once did. Before, white folks' ignorance was like armor; theyâ€™d squint, unsure, guessing Native or Mexican. Now, in a town filled with fear, the glasses made him look like someone hiding something. \n\nHe didn't like being marked, and neither did Len.\n\nFrom what he gathered from Mr. Chu and what Lenore gathered from Mrs. Chu, Carla wanted to raise her child, Ute, to pass down her heritage and culture.\n\nJiang considered Boone's wish to keep the child in Silverton to be beyond dishonorable. To tear a child away from its motherâ€™s culture was like the Manchu forcing the queue on the Han, an act of submission masked as order, an erasure disguised as love. \n\nEven if Boone had good intentions, the outcome was a prison for the soul. The thought of that love, distorted into a tool of erasure, clenched his jaw. \n\nA memory came up, as sharp as a piece of porcelain. \n\nThe weight of Toopahâ€™s hand on his shoulder, the muted command to hide the girls. Holding his breath in the dark, with their muffled whimpers in his ears, the earthy taste of dust and salt on his tongue. \n\nHe was the guardian because he was unseen. A Chinese boy in an Ahwahneechee home, the agent would ignore him and continue their search.\n\n\nOr Aa Baâ€™s low voice, recounting the Catholic missionaries in Guangdong. How church crosses were a prelude to foreign guns and the sweet, suffocating smell of opium.\n\nHe understood the urge to preserve. Everywhere, the fight was the same: to defend a way of life while the country was dismantling, bleaching, and labeling what it deemed savage or heathen.\n\nIt all added up. Sheriff Andyâ€™s failure to investigate, even if he saw Carla as a daughter. In the ledger of this land, people like her, people like us, didnâ€™t merit an entry. \n\nThey entered the sheriff's office. The room reeked of stale cigarettes; Deputy Beagle, Boone, Vargas, and Marshal Jackson were nowhere to be seen. Compared to yesterdayâ€™s crowd, the office seemed empty.\n\nSheriff Andy sat alone at his desk, studying a map with a cigarette between his fingers. The late-morning rays warmed his dark brown skin.\n\nHis finger traced a line on the map, brows furrowed in thought, unaware of anyone's presence until the hinges squeaked when Jiang closed the door behind them. \n\nJiang slouched his shoulders and stuffed his hands into his duster pockets. He needed to understand how the sheriff used authority. So he played into the quiet stereotype, keeping his tinted glasses on.\n\nâ€œHowâ€™d the perimeter check go, Beagle?â€\n\nThe grandfather clock struck eleven. At last, the sheriff's gaze lifted. Sheriff Andyâ€™s gaze met Len's, then his. \n\nâ€œI did not realize a man of my status could be a lawman,â€ Jiang said, a hint of humor in his voice. \n\nThe sheriffâ€™s lip twitched in amusement as he took a long drag from his cigarette. He exhaled a slow curl of smoke toward the ceiling, then ground the butt out in the ashtray. â€œNeither did I. But here we are.â€\n\nLenore hooked her thumbs in her belt loops. â€œAs much as I enjoy small talk, sheriffâ€”we got some questions ya might answer.â€\n\nThe sheriff leaned back in his chair, crossing his arms. His focus remained on Jiang, whom he saw as the real threat. \n\nHe might be dangerous, but she was too, if given the chance. The sheriff viewed Len as a hotheaded, loudmouthed farmhand who didn't understand her place among adults.\n\nâ€œFigured this wasnâ€™t a social call,â€ Andy said, his gaze still fixed on Jiang. â€œWhat do you want to know?â€\n\nHe stepped forward, shoulders relaxed, pretending to be compliant. The act was still ongoing, but less convincing now. A flicker of recognition flashed across the sheriffâ€™s face, as if heâ€™d just seen a ghost.\n\nâ€œI see what youâ€™re doing, Mr. Hsu,â€ he said, fingers drumming on his arm. â€œColored man learns to make himself small. White folks in charge donâ€™t like being questioned. The difference is, my authorityâ€™s always on trialâ€”for being a negro.â€\n\nâ€œDonâ€™t give me the â€˜quiet John Chinamanâ€™ routine in here,â€ Andy said, squaring his shoulders. â€œI see the bounty hunter behind the act.â€\n\nJiangâ€™s shoulders stayed slack as he shifted his weight. â€œBut you are a man with power, a badge, and we are men without. My partner and I should respect your badge.â€\n\nHis lips curled into a slight smile. â€œCanâ€™t say you werenâ€™t raised with manners.â€ Andy straightened his posture, his gaze lingering on Jiang, his expression saying, Keep your partner in line. He gave a single, slow nod, granting them the floor.\n\nâ€œWe are looking into the disappearance of Carla Boone, sheriff,â€ Jiang said, his voice level. â€œWe were told you might have information.â€\n\nLenore stepped forward, just half a pace, her thumbs still hooked in her belt loops. â€œWe heard she was pregnant.â€\n\nThe sheriff smoothed the map. His face softened as regret spread across it. His hands went still, resting flat on the paper. When he looked up, the tough lawman had returned.\n\n\"Who told you that?\" he asked, his voice soft.\n\nâ€œDeputy Boone inquired that we look into his wife's disappearance,â€ Jiang spoke up.\n\nThe sheriff clasped his hands on the map. â€œSo he's still looking,â€ he said, his voice going soft. â€œWhat do you want to know?â€\n\nJiang glanced at Lenore, clasping his hands together at his waist. She replied with a tilt of her chin. \n\nâ€œSheriff, how close were you to Carla?â€ he asked.\n\nSheriff Andy leaned back in his chair, the old wood creaking. His professional mask slipped, revealing a moment of tiredness. The wrinkles on his face deepened into his skin, aging him further.\n\nâ€œClose?â€ he said, his voice lower now. â€œI raised that boy. Promised his father I would, right before the man died in my arms at Petersburg. Boone was closed off for a long time. Hardened. Then he met Carla.â€ \n\nA faint, almost wistful smile touched his lips, gone as quickly as it came. â€œWas like watchinâ€™ a damned flower bloom in a desert. She was tough, mind you. Didnâ€™t suffer fools, and most of this town qualified. But with him? She was sunshine. Heâ€™d look at her, and for the first time since he was a child, heâ€™d look peaceful.â€\n\nHe picked up his cold coffee, stared into the mug, and set it down again without drinking. â€œCarla became family. My family. So you tell me how close I was.â€\n\nShe shifted her weight, leaning toward Jiang. He didn't expect that kind of backstory: a surrogate father and his adopted son. \n\nUpdate: Thanks for the upvotes! I'm still editing this chapter. This is a rough draft. I was originally writing in the other co-protagonist Lenore's POV, but it felt wrong. So I changed it to Jiang's/James. Deepseek is helping me with sensitivity writing (I don't have money for a sensitivity reader), historical fact checker making sure what I've researched is handled with care and depth. ",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q0l0gc/ive_been_using_deepseek_to_edit_my_fallout_new/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nwzfckd",
          "author": "datfalloutboi",
          "text": "DeepSeek is absolutely wonderful for logic analysis for AUs. Itâ€™s really great.",
          "score": 7,
          "created_utc": "2025-12-31 21:45:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzh1y0",
              "author": "Faye-Faye33",
              "text": "I love Deepseek for editing, historical, checking, and as a sensitivity reader. It has helped so much especially on Chinese culture. It definitely fills in the blanks on things that I missed. It pointed out a place where I mixed up Cantonese and Mandarin. So I need to go back and edit that mix up. \n\nI've learned alot and I won't be going back to chatgpt because the quality has gone done. Plus, it's free compared to chatgpt and Claude.",
              "score": 3,
              "created_utc": "2025-12-31 21:55:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx24r1o",
          "author": "Crafty_Ball_8285",
          "text": "Whatâ€™s an AU",
          "score": 2,
          "created_utc": "2026-01-01 09:50:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2co1e",
              "author": "Maximum_Price4517",
              "text": "Alternative Universe, it's a kind of fanfic with \"what if...\" as background",
              "score": 3,
              "created_utc": "2026-01-01 11:13:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q21cht",
      "title": "I didn't even ask them to do that.",
      "subreddit": "DeepSeek",
      "url": "https://i.redd.it/zjphxc9leyag1.jpeg",
      "author": "Electrical-Cost7250",
      "created_utc": "2026-01-02 15:22:54",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q21cht/i_didnt_even_ask_them_to_do_that/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q04qxu",
      "title": "Grok finished first overall, while DeepSeek placed 2nd with roughly $149,000, up about 49%\nGPT-5 and Claude Sonnet 4.5 showed similar results: both finished close to $127,000 dollars, beating the S&P 500 return of 12%\"",
      "subreddit": "DeepSeek",
      "url": "https://v.redd.it/4jnlte464hag1",
      "author": "Minimum_Minimum4577",
      "created_utc": "2025-12-31 05:17:02",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion ",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q04qxu/grok_finished_first_overall_while_deepseek_placed/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pznxc9",
      "title": "Stop using \"Act as a...\" (I ran a blind test on \"Vibes\" vs. \"Constraints\" and the results were wild)",
      "subreddit": "DeepSeek",
      "url": "https://i.redd.it/ix3qxachz5ag1.png",
      "author": "sparky9",
      "created_utc": "2025-12-30 17:22:07",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.7,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1pznxc9/stop_using_act_as_a_i_ran_a_blind_test_on_vibes/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q171y2",
      "title": "Excluding porn related â€” What is the most messed up shit you seen from deepseek?",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q171y2/excluding_porn_related_what_is_the_most_messed_up/",
      "author": "BUS1LOVER",
      "created_utc": "2026-01-01 15:42:09",
      "score": 4,
      "num_comments": 8,
      "upvote_ratio": 0.56,
      "text": "I know that deepseek (LLMs in general) can cross their legal limits with proper prompts by user and precise altering of context, but I haven't seen much examples of that, even with jailbreaking â€” so, I ask about your experiences with this... just curious",
      "is_original_content": false,
      "link_flair_text": "Discussion ",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q171y2/excluding_porn_related_what_is_the_most_messed_up/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nx3gfe1",
          "author": "Neo_Shadow_Entity",
          "text": "Why do you need to know? Chinese party looking for ways to strengthen censorship?",
          "score": -47,
          "created_utc": "2026-01-01 16:06:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3hgiy",
              "author": "BUS1LOVER",
              "text": "Why do you need to know why I need to know?",
              "score": 27,
              "created_utc": "2026-01-01 16:11:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3ty74",
                  "author": "award_reply",
                  "text": "Neo Shadow Entity has a point.  \nDon't give people bad ideas, as that will motivate developers to further block the model.",
                  "score": -10,
                  "created_utc": "2026-01-01 17:17:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3i8np",
                  "author": "Neo_Shadow_Entity",
                  "text": "Because you didn't explain why you need to know this.",
                  "score": -23,
                  "created_utc": "2026-01-01 16:15:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx8m4r3",
              "author": "award_reply",
              "text": "Okayâ€¦what did I miss? Who is downvoting this and why?",
              "score": 1,
              "created_utc": "2026-01-02 11:43:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9pr8k",
                  "author": "Neo_Shadow_Entity",
                  "text": "I guess someone from the party didn't like my comment.)",
                  "score": 0,
                  "created_utc": "2026-01-02 15:47:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q29scj",
      "title": "For some reason, even without a prompt, I keep getting this error message when trying to test the proxy *DS R1T2 Chimera",
      "subreddit": "DeepSeek",
      "url": "https://www.reddit.com/r/DeepSeek/comments/1q29scj/for_some_reason_even_without_a_prompt_i_keep/",
      "author": "Plane_Plankton1285",
      "created_utc": "2026-01-02 20:34:00",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "# 400 - The model returned an empty response - this often happens with NSFW or sensitive content. Try removing your prompt first or switching to a different scene.\n\n",
      "is_original_content": false,
      "link_flair_text": "Question&Help",
      "permalink": "https://reddit.com/r/DeepSeek/comments/1q29scj/for_some_reason_even_without_a_prompt_i_keep/",
      "domain": "self.DeepSeek",
      "is_self": true,
      "comments": [
        {
          "id": "nxcpydp",
          "author": "ozakio1",
          "text": "It might be out of context but try out Xiaomi mimo v2 , it's genuinely the best ai model in rp I have ever used.",
          "score": 1,
          "created_utc": "2026-01-03 00:39:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxd4xir",
              "author": "Plane_Plankton1285",
              "text": "I'll try it. Thanks man!",
              "score": 1,
              "created_utc": "2026-01-03 02:04:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}