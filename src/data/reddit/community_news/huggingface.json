{
  "metadata": {
    "last_updated": "2026-01-17 16:48:01",
    "time_filter": "week",
    "subreddit": "huggingface",
    "total_items": 3,
    "total_comments": 0,
    "file_size_bytes": 4823
  },
  "items": [
    {
      "id": "1qebso3",
      "title": "Different Facial Expressions from One Face Using FLUX.2 [klein] 9B",
      "subreddit": "huggingface",
      "url": "https://i.redd.it/de2u02bckodg1.png",
      "author": "Substantial-Fee-3910",
      "created_utc": "2026-01-16 09:45:32",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1qebso3/different_facial_expressions_from_one_face_using/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qd8dza",
      "title": "Built a quiet safety-first app from lived experience — looking for honest feedback (not promotion)",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1qd8dza/built_a_quiet_safetyfirst_app_from_lived/",
      "author": "blazedinfinity",
      "created_utc": "2026-01-15 03:19:04",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I’m sharing this carefully and with respect.\n\nI built a small Android app called MINHA based on my own lived experience with long cycles of sobriety, relapse, and medical consequences. \nThis is not a motivation app, not a tracker, not therapy, and not a replacement for professional help.\n\nMINHA does one thing only:\nIt slows a person down during risky moments using calm language, restraint, and friction. No streaks, no dopamine, no encouragement to “push through.”\n\nBefore releasing it publicly, I’m looking for 3–5 people who are in recovery, supporting someone in recovery, or working in mental health — to\nsanity-check: the language (does anything feel unsafe or wrong?)\nthe flow during moments of distress\nwhat should not exist in such an app\n\nI am not asking anyone to download or promote it publicly.\n\nPrivate feedback — including “don’t release this” — is genuinely welcome.\n\nIf this resonates, please comment or DM.\n\nIf not, that’s completely fine too.\nThank you for reading.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1qd8dza/built_a_quiet_safetyfirst_app_from_lived/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qc49jl",
      "title": "Curious ablation: GPT-like LM trained with *frozen* 16‑dim *binary* token-ID embeddings (n_embed=16) It still learns end-to-end and generates coherent text.",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1qc49jl/curious_ablation_gptlike_lm_trained_with_frozen/",
      "author": "AVBochkov",
      "created_utc": "2026-01-13 21:40:05",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "Curious, fully reproducible result: I trained a GPT-like decoder-only Transformer whose entire input embedding table is **frozen** and replaced with a **16‑dimensional binary token-ID code** (values are strictly 0/1) — **this is not 16-bit quantization**.\n\nEven without trainable or semantically-initialized token embeddings, the model still trains end-to-end and can generate non-trivial text.\n\n**Key details**\n\n* `vocab_size = 65536`, `n_embed = 16` (since `2^16 = 65536`, the code uniquely identifies each token)\n* deterministic expansion `16 → d_model=1024` via `repeat_interleave` (`scale = 64`)\n* the full frozen embedding table is published (`embeddings.txt`) for auditability\n\nRepro note + verification script:  \n  \n[https://huggingface.co/blog/Bochkov/emergent-semantics-beyond-token-embeddings](https://huggingface.co/blog/Bochkov/emergent-semantics-beyond-token-embeddings)\n\nModel repo:  \n  \n[https://huggingface.co/Bochkov/emergent-semantics-model-16-bit-269m](https://huggingface.co/Bochkov/emergent-semantics-model-16-bit-269m)\n\nThe broader question is **where semantic structure emerges** in decoder-only Transformers when the input embedding layer is not trained and does not explicitly encode semantics.\n\n\n\nhttps://preview.redd.it/nhmy5ekqr6dg1.png?width=1590&format=png&auto=webp&s=042a3443e89ec31b0c7b0d16e178c26ba1386dfb\n\nLicense: Apache-2.0",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1qc49jl/curious_ablation_gptlike_lm_trained_with_frozen/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": []
    }
  ]
}