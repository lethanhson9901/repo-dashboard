{
  "metadata": {
    "last_updated": "2026-01-14 02:43:46",
    "time_filter": "week",
    "subreddit": "huggingface",
    "total_items": 8,
    "total_comments": 5,
    "file_size_bytes": 16833
  },
  "items": [
    {
      "id": "1q4ub51",
      "title": "Small LLMs for SQL Generation",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1q4ub51/small_llms_for_sql_generation/",
      "author": "RamiKrispin",
      "created_utc": "2026-01-05 18:49:43",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "Any recommendations for open-weighted small LLMs to support a SQL AI agent? Is there any category that tracks the performance of models in SQL generation tasks? Thx!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1q4ub51/small_llms_for_sql_generation/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": [
        {
          "id": "nxweot8",
          "author": "frank_brsrk",
          "text": "Ehy mate are u trying to build an sql agent? I can help u with that. Write me if u are interested",
          "score": 1,
          "created_utc": "2026-01-05 22:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx5r6p",
              "author": "RamiKrispin",
              "text": "Thanks, I already have a SQL agent. I am looking to use lightweight models optimized for SQL.",
              "score": 1,
              "created_utc": "2026-01-06 00:42:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz0wid",
      "title": "CLI tool to use transformer and diffuser models",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1pz0wid/cli_tool_to_use_transformer_and_diffuser_models/",
      "author": "zashboy",
      "created_utc": "2025-12-29 22:54:01",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "At some point over the summer, I wanted to try out some image and video models from HF locally, but I didn't want to open up my IDE and hardcode my prompts each time. I've been looking for tools that would give me an Ollama CLI-like experience, but I couldn't find anything like that, so I started building something for myself. It works with the models I'm interested in and more. \n\nSince then, I haven't checked if there are any similar or better tools because this one meets my needs, but maybe there's something new out there already. I'm just sharing it in case it's useful to anyone else for quickly running image-to-image, text-to-image, text-to-video, text-to-speech and speech-to-text models locally. Definitely, if you have AMD GPUs like I do.\n\n[https://github.com/zb-ss/hftool](https://github.com/zb-ss/hftool)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1pz0wid/cli_tool_to_use_transformer_and_diffuser_models/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q0fjqc",
      "title": "I had gemini make a picture of HuggingFace. By breaking down the possible meanings of the term hugging face and then had it make a picture.",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1q0fjqc/i_had_gemini_make_a_picture_of_huggingface_by/",
      "author": "Seninut",
      "created_utc": "2025-12-31 15:25:36",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "https://preview.redd.it/cui90w415kag1.png?width=1024&format=png&auto=webp&s=20af3698faae8806ddb8329857f7827e988ba10b\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1q0fjqc/i_had_gemini_make_a_picture_of_huggingface_by/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q328mi",
      "title": "LLM to help with character cards",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1q328mi/llm_to_help_with_character_cards/",
      "author": "slrg1968",
      "created_utc": "2026-01-03 18:35:24",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "HI!\n\nIs there an LLM out there that is specifically trained (or fine tuned or whatever) to help the user create viable character cards... like i would tell it... \"*my character is a 6 foot tall 20 year old college sophomore. he likes science, and hates math and english, he wears a hoodie and jeans, has brown hair, blue eyes. he gets along well with science geeks because he is one, he tries to get along with jocks but sometimes they pick on him.*\" etc etc etc\n\nonce that was added the program or model or whatever would ask any pertinent questions about the character, and then spit out a properly formatted character card for use in silly tavern or other RP engines. Things like figuring out his personality type and including that in the card would be a great benefit\n\nThanks\n\nTIM",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1q328mi/llm_to_help_with_character_cards/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": [
        {
          "id": "nxigodd",
          "author": "dizzyelk",
          "text": "There's [CardProjector](https://huggingface.co/SlerpE/CardProjector-27B-v4). It does alright, especially paired with a card that's specific to creating other character cards.\n\nAnd, while not specifically for creating cards, I found that [Maginum-Cydoms](https://huggingface.co/Casual-Autopsy/Maginum-Cydoms-24B) did alright with making cards.",
          "score": 1,
          "created_utc": "2026-01-03 21:41:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzdi5b",
          "author": "Tony_009_",
          "text": "Give llm a preset prompt that it’s a character card maker",
          "score": 1,
          "created_utc": "2026-01-06 10:00:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7999x",
      "title": "I made 64 swarm agents compete to write gpu kernels",
      "subreddit": "huggingface",
      "url": "https://i.redd.it/ldpnwbwt04cg1.png",
      "author": "kwa32",
      "created_utc": "2026-01-08 11:20:37",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1q7999x/i_made_64_swarm_agents_compete_to_write_gpu/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pyv0vf",
      "title": "Reachy Mini IDE Prototype",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1pyv0vf/reachy_mini_ide_prototype/",
      "author": "Creative-Scene-6743",
      "created_utc": "2025-12-29 19:06:57",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I received my **Reachy Mini**, and instead of sticking with the usual “SSH-terminal juggling” workflow, I wanted to see if I could configure something that feels closer to modern day IDE workflow using VS Code as a base.\n\nThe goal for this IDE:  \n \\- Remote development directly on Reachy Mini  \n \\- Run programs inside Reachy Mini’s App Python environement   \n \\- Full Python debugging support  \n \\- Primitive, but realtime performance monitoring \n\nI ended up combining VS Code with [Remote SSH](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh), [SSH monitor](https://marketplace.visualstudio.com/items?itemName=femtowork.ssh-remote-monitor) and installation of [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) in Remote Extension Host to enable debugging. Full step-by-step guide availlable [here](https://tobrun.github.io/blog/reachy-mini-ide/) \n\n[Remote Python Debugging](https://preview.redd.it/c6m4olwgy6ag1.png?width=2384&format=png&auto=webp&s=f02c52af4d71dd3127a758c44cbf590925596c7b)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1pyv0vf/reachy_mini_ide_prototype/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": [
        {
          "id": "nxdbdt9",
          "author": "clem59480",
          "text": "very cool",
          "score": 1,
          "created_utc": "2026-01-03 02:42:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1e4y9",
      "title": "Generate OpenAI Embeddings Locally with embedding-adapters library ( 70× faster embedding generation! )",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1q1e4y9/generate_openai_embeddings_locally_with/",
      "author": "Interesting-Town-433",
      "created_utc": "2026-01-01 20:27:09",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[EmbeddingAdapters ](https://medium.com/@ace0278/generate-openai-style-embeddings-locally-with-minilm-adapter-f43ec9c3b7da)is a Python library for translating between embedding model vector spaces.\n\nIt provides plug-and-play adapters that map embeddings produced by one model into the vector space of another — locally or via provider APIs — enabling cross-model retrieval, routing, interoperability, and migration **without re-embedding an existing corpus**.\n\nIf a vector index is already built using one embedding model, embedding-adapters allows it to be queried using another, without rebuilding the index.\n\n**GitHub:**  \n[https://github.com/PotentiallyARobot/EmbeddingAdapters/](https://github.com/PotentiallyARobot/EmbeddingAdapters/)\n\n**PyPI:**  \n[https://pypi.org/project/embedding-adapters/](https://pypi.org/project/embedding-adapters/)\n\n# Example\n\nGenerate an OpenAI embedding locally from minilm+adapter:\n\n    pip install embedding-adapters\n    \n    embedding-adapters embed \\\n      --source sentence-transformers/all-MiniLM-L6-v2 \\\n      --target openai/text-embedding-3-small \\\n      --flavor large \\\n      --text \"where are restaurants with a hamburger near me\"\n\nThe command returns:\n\n* an embedding in the target (OpenAI) space\n* a confidence / quality score estimating adapter reliability\n\n# Model Input\n\nAt inference time, the adapter’s **only input is an embedding vector** from a source model.  \nNo text, tokens, prompts, or provider embeddings are used.\n\nA pure **vector → vector** mapping is sufficient to recover most of the retrieval behavior of larger proprietary embedding models for in-domain queries.\n\n# Benchmark results\n\n**Dataset:** SQuAD (8,000 Q/A pairs)\n\n**Latency (answer embeddings):**\n\n* MiniLM embed: **1.08 s**\n* Adapter transform: **0.97 s**\n* OpenAI API embed: **40.29 s**\n\n≈ **70× faster** for local MiniLM + adapter vs OpenAI API calls.\n\n**Retrieval quality (Recall@10):**\n\n* MiniLM → MiniLM: **10.32%**\n* Adapter → Adapter: **15.59%**\n* Adapter → OpenAI: **16.93%**\n* OpenAI → OpenAI: **18.26%**\n\nBootstrap difference (OpenAI − Adapter → OpenAI): **\\~1.34%**\n\nFor in-domain queries, the MiniLM → OpenAI adapter recovers \\~**93%** of OpenAI retrieval performance and substantially outperforms MiniLM-only baselines.\n\n# How it works (high level)\n\nEach adapter is trained on a **restricted domain**, allowing it to specialize in interpreting the semantic signals of smaller models and projecting them into higher-dimensional provider spaces while preserving retrieval-relevant structure.\n\nA quality score is provided to determine whether an input is well-covered by the adapter’s training distribution.\n\n# Practical uses in Python applications\n\n* Query an existing vector index built with one embedding model using another\n* Operate mixed vector indexes and route queries to the most effective embedding space\n* Reduce cost and latency by embedding locally for in-domain queries\n* Evaluate embedding providers before committing to a full re-embed\n* Gradually migrate between embedding models\n* Handle provider outages or rate limits gracefully\n* Run RAG pipelines in air-gapped or restricted environments\n* Maintain a stable “canonical” embedding space while changing edge models\n\n# Supported adapters\n\n* MiniLM ↔ OpenAI\n* OpenAI ↔ Gemini\n* E5 ↔ MiniLM\n* E5 ↔ OpenAI\n* E5 ↔ Gemini\n* MiniLM ↔ Gemini\n\nThe project is under active development, with ongoing work on additional adapter pairs, domain specialization, evaluation tooling, and training efficiency.\n\nPlease Like/Upvote if you found this interesting",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1q1e4y9/generate_openai_embeddings_locally_with/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q6kdj2",
      "title": "Need advice: open-source surgical LLM fine-tune (90k Q&A) — multi-turn stability, RL (DPO), and RAG",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1q6kdj2/need_advice_opensource_surgical_llm_finetune_90k/",
      "author": "Patient_Ad1095",
      "created_utc": "2026-01-07 16:43:11",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I’m planning to fine-tune OSS-120B (or Qwen3-30B-A3B-Thinking-2507) on a mixed corpus: \\~10k human-written Q&A pairs plus \\~80k carefully curated synthetic Q&A pairs that we spent a few months generating and validating. The goal is to publish an open-weight model on Hugging Face and submit the work to an upcoming surgical conference in my country. The model is intended to help junior surgeons with clinical reasoning/support and board-style exam prep.\n\nI’m very comfortable with RAG + inference/deployment, but this is my first time running a fine-tuning effort at this scale. I’m also working with a tight compute budget, so I’m trying to be deliberate and avoid expensive trial-and-error. I’d really appreciate input from anyone who’s done this in practice:\n\n1. Multi-turn behavior: If I fine-tune on this dataset, will it noticeably degrade multi-turn / follow-up handling? Should I explicitly add another 5–10k dialog-style, multi-turn examples (with coreference + follow-ups), or will the base model generally preserve conversational robustness without increased hallucination?\n2. SFT vs RL: The dataset is \\~25% MCQs and \\~75% open-ended answers; MCQs include rationales/explanations. Would you recommend RL after SFT here? If yes, what approach makes the most sense (e.g., DPO/IPO/KTO/ORPO vs PPO-style RLHF), and what data format + rough scale would you target for the preference/reward step?\n3. Two inference modes: I want two user-facing modes: clinical support and exam preparation. Would you bake the mode-specific system prompts into SFT/RL (i.e., train with explicit instruction headers), and if so, would you attach them to every example or only a subset to avoid over-conditioning?\n4. RAG / tool use at inference: If I’m going to pair the model with RAG and/or a web-search tool at inference time, should that change how I structure fine-tuning or RL? For example: training with retrieved context, citations, tool-call patterns, refusal policies, or “answer only from context” constraints.\n5. Model choice: Between OSS-20B and Qwen3-30B-A3B, which would you pick for this use case? I slightly prefer OSS-20B for general non-coding performance, but I’m unsure whether its chat/harmony formatting or any architecture/format constraints create extra friction or difficulties during SFT/RL.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1q6kdj2/need_advice_opensource_surgical_llm_finetune_90k/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": [
        {
          "id": "nys0k7k",
          "author": "bunnydathug22",
          "text": "Oo this is interesting.\n\nLet us house the \"entire\" backend for you  and help you build it. :)",
          "score": 1,
          "created_utc": "2026-01-10 12:46:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}