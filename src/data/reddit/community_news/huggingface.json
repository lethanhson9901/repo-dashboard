{
  "metadata": {
    "last_updated": "2026-02-22 02:59:49",
    "time_filter": "week",
    "subreddit": "huggingface",
    "total_items": 4,
    "total_comments": 5,
    "file_size_bytes": 11870
  },
  "items": [
    {
      "id": "1r94hor",
      "title": "Pruned GPT-OSS-20B to 9B, Saved MoE, fine-tuned on 100K examples. Sharing what actually worked and what didn't.",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1r94hor/pruned_gptoss20b_to_9b_saved_moe_finetuned_on/",
      "author": "Disastrous_Bid5976",
      "created_utc": "2026-02-19 16:54:54",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.77,
      "text": "https://preview.redd.it/bvw6jsjkehkg1.jpg?width=1280&format=pjpg&auto=webp&s=1dcb9169b4c0cc0bab58454d234ace29c211769b\n\nI have 16GB RAM. GPT-OSS-20B won't even load in 4-bit quantization on my machine. So I spent weeks trying to make a version that actually runs on normal hardware. This is GPT-OSS-Nano. Built for people like me who don't have a server rack under their desk. \n\n**The pruning**\n\nStarted from the 20B intermediate checkpoint and did structured pruning down to 9B. Gradient-based importance scoring for heads and FFN layers. After the cut the model was honestly kind of dumb - reasoning performance tanked pretty hard. Expected, but still rough to see.\n\n**Fine-tuning**\n\n100K chain-of-thought gpt oss 120B examples (math, logic, code). QLoRA on an H200 with Unsloth — about 2x faster than vanilla training. 2 epochs, nothing fancy.\n\nThe SFT made a bigger difference than I expected post-pruning. The model went from producing vaguely structured outputs to actually laying out steps properly.\n\nWeights are up on HF if anyone wants to poke at it:  \n [huggingface.co/squ11z1/gpt-oss-nano](http://huggingface.co/squ11z1/gpt-oss-nano)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1r94hor/pruned_gptoss20b_to_9b_saved_moe_finetuned_on/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": [
        {
          "id": "o6a6sq3",
          "author": "PhotographerUSA",
          "text": "So, limited with what you're using lol",
          "score": 1,
          "created_utc": "2026-02-19 18:19:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a7pzn",
              "author": "Disastrous_Bid5976",
              "text": "XDD, H200 was rented compute for SFT",
              "score": 1,
              "created_utc": "2026-02-19 18:24:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7zeg9",
      "title": "GLONET just dropped on",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1r7zeg9/glonet_just_dropped_on/",
      "author": "Ill_Ranger_8547",
      "created_utc": "2026-02-18 10:24:43",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "Just came across this and thought it was worth sharing — GLONET is now available on HuggingFace.\n\nIt’s a deep learning model for global ocean forecasting that runs in less than 3 seconds. Trained on 30 years of ocean reanalysis data, it can predict oceanographic variables (temperature, salinity, currents…) at a global scale.\n\nNot gonna lie, this might just make traditional physics-based ocean models obsolete\n\nhttps://huggingface.co/mercator-ocean/GLONET",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1r7zeg9/glonet_just_dropped_on/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r8544v",
      "title": "We built a golf forecasting model that outperforms GPT‑5; model and dataset are open-sourced on Hugging Face",
      "subreddit": "huggingface",
      "url": "/r/LocalLLaMA/comments/1r853l6/we_built_a_golf_forecasting_model_that/",
      "author": "LightningRodLabs",
      "created_utc": "2026-02-18 14:55:17",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1r8544v/we_built_a_golf_forecasting_model_that/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r92rhp",
      "title": "Serving 200 to 300 custom HF models on a single H100 node with bursty traffic. Here’s what broke first.",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1r92rhp/serving_200_to_300_custom_hf_models_on_a_single/",
      "author": "pmv143",
      "created_utc": "2026-02-19 15:51:12",
      "score": 3,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "We’ve been running a reference deployment focused purely on long tail custom models from HF. Not foundation models, not 24/7 traffic. Think small fine tuned models that get hit sporadically.\n\nRight now we’re serving around 200 to 300 distinct custom models on a single H100. Traffic is bursty. Most models sit idle most of the time.\n\nA few things we’ve learned:\n\n\t1.“Scale to zero” is not enough by itself.\n\nIf your restore path replays container pull, framework init, weight load, CUDA context, kernel warmup, you are just hiding the cold start, not solving it.\n\n\t2.Warm pools quietly turn into manual capacity planning.\n\nA lot of setups end up prewarming with dummy calls. At that point you are basically running your own warm GPU fleet.\n\n\t3.Multi model scheduling becomes the real problem.\n\nIt’s less about raw throughput and more about deterministic restore and eviction policy under memory pressure.\n\n\t4.Billing alignment matters more than peak latency.\n\nFor bursty workloads, users care more about not paying for idle VRAM than shaving 50 ms off steady state latency.\n\nNot sure how others here are handling long tail deployments.\n\nAre you prewarming? Keeping models resident? Relying on autoscale?\n\nWhat’s your restore time from zero to first token for a 10 to 20 GB model in practice?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1r92rhp/serving_200_to_300_custom_hf_models_on_a_single/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": [
        {
          "id": "o6beask",
          "author": "qubridInc",
          "text": "This is exactly where most multi-model setups hit limits. The problem stops being throughput and becomes **restore time + eviction policy + cost of idle VRAM**.\n\nIn practice, teams end up with a hybrid: small hot set kept resident, long-tail models loaded on demand with smarter caching and deterministic eviction. Cold start for 10–20GB models is still seconds, not ms.",
          "score": 2,
          "created_utc": "2026-02-19 21:49:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c4bbo",
          "author": "paulahjort",
          "text": "For bursty long-tail workloads the real answer isn't optimizing a single H100, it's about not committing to a single H100 in the first place.\n\nTreat each burst as a provisioning event rather than a scheduling event. Query spot availability across multiple providers in parallel at request time, provision the cheapest available GPU, run the model, terminate. For models that are 10-20GB the cold start from bare metal is often competitive with warm pool restore times once you factor in the VRAM you're not paying for at idle.\n\nThe tricky part obviously is making multi-cloud provisioning fast enough that it doesn't add latency... Try parallel provisioning tools like Terradev... ",
          "score": 2,
          "created_utc": "2026-02-20 00:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cruhh",
              "author": "pmv143",
              "text": "I agree that avoiding single-node commitment makes sense for longtail workloads.\n\nThe tradeoff I keep seeing is that provisioning becomes part of the critical path. If each burst triggers cloud selection, VM spin-up, image pull, weight hydration, etc., you’re shifting the reload cost rather than eliminating it.\n\nFor 10–20GB models, bare metal cold start can look competitive on paper. But once bursts get frequent, restore determinism and state reuse start to matter more than raw spot pricing.\n\nMay I ask how are you handling model weight locality across providers?",
              "score": 2,
              "created_utc": "2026-02-20 02:35:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6eyjf9",
                  "author": "paulahjort",
                  "text": "Pre-stage datasets strategically, compressed, in several common zones... 3 or 4 regions can take up most of cheap spot. \n\nMaintain a warm pool at your primary provider and only use cross-cloud arbitrage for overflow.",
                  "score": 1,
                  "created_utc": "2026-02-20 13:03:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lmbck",
          "author": "KvAk_AKPlaysYT",
          "text": "Curious, did you try this at any point?\n\nhttps://github.com/triton-inference-server/server",
          "score": 1,
          "created_utc": "2026-02-21 13:52:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m0jq7",
              "author": "pmv143",
              "text": "Yeah, we’ve experimented with Triton-style setups. It’s strong for steady state throughput and well structured batching. friction we see is  is in long-tail restore paths when models aren’t kept resident. The challenge becomes less about scheduling requests and more about deterministic state restore under memory pressure. I’m more interested in knowing how ppl are handling full context + CUDA state rebuild when models aren’t permanently warm.",
              "score": 1,
              "created_utc": "2026-02-21 15:14:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oxj8y",
          "author": "souna06",
          "text": "\"billing alignment matters more than peak latency\" — this is underrated. When you say that, are you tracking cost per model per hour, or more like idle VRAM as a % of total spend?\n\nCurious how you picked the H100 for this in the first place. Did you size it for the long tail or for peak concurrent models?",
          "score": 1,
          "created_utc": "2026-02-22 00:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p4ew9",
              "author": "pmv143",
              "text": "When I say billing alignment, I’m mostly looking at effective utilization over time. Specifically, how much VRAM is allocated but not actively serving tokens. In bursty, long tail workloads, idle residency can quietly dominate total spend.\n\nOn sizing, the H100 was chosen less for peak token throughput and more for memory headroom. With 200 to 300 distinct models, concurrency isn’t just about active requests. It’s about how many models compete for residency under memory pressure.",
              "score": 1,
              "created_utc": "2026-02-22 01:12:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}