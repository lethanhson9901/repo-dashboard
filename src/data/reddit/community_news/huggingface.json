{
  "metadata": {
    "last_updated": "2026-02-13 09:09:58",
    "time_filter": "week",
    "subreddit": "huggingface",
    "total_items": 2,
    "total_comments": 9,
    "file_size_bytes": 10156
  },
  "items": [
    {
      "id": "1r246g5",
      "title": "Zero-touch anomaly triage on the Epstein corpus(340k+ files sanitized release, not proof) HF Space link",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1r246g5/zerotouch_anomaly_triage_on_the_epstein/",
      "author": "Either_Pound1986",
      "created_utc": "2026-02-11 17:47:37",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.79,
      "text": "Alright I've tried now two times to post this to reddit and both times it was taken down. I tried datasets and I tried epstein subreddits. Third time is the charm I guess?\n\n\nI am not a data scientist. I’m sharing this so people can review the method and outputs directly.\n\nI downloaded the files from DOJ while the zip downloads were still available, and combined dataset 8 + dataset 11.\n\nBefore running the zero-touch pipeline in Colab, I did one preprocessing step:\n\n* converted PDFs to text\n\nWhat I did:\n\n* kept original file names so records can be mapped back to source files\n* kept extracted text as-is\n* did not OCR\n* did not manually clean fragmented text\n* did not add or rewrite content\n\nImportant context:\nA lot of text is fragmented because of how redactions were done in the source material. That noise is in the corpus and still present in my release.\n\nThis is also not just a simple database + keyword search.\nThe pipeline does lexical/statistical passes, embedding passes, clustering, and then anomaly/signal ranking to reduce search space.\n\nWhat “zero-touch” means here:\n\n* pipeline auto-detects input path\n* runs end-to-end without manual cherry-picking during scoring\n\nImportant:\nThis is triage, not proof.\nA high anomaly score means “look here first,” not “this is true” and not “this proves guilt.” NOTE there are false positives, again this is only to collapse possible search space. \n\nPublic release:\n\n* App: [https://huggingface.co/spaces/cjc0013/EpsteinWithAnomScore](https://huggingface.co/spaces/cjc0013/EpsteinWithAnomScore)\n* Dataset: [https://huggingface.co/datasets/cjc0013/EpsteinWithAnomScore/tree/main](https://huggingface.co/datasets/cjc0013/EpsteinWithAnomScore/tree/main)\n* Sanitized top-N file: public_method_sanitized_topN.jsonl\n\nWhat I sanitized in public top-N:\n\n* replaced direct identities with CASE IDs (CASE-000001 to CASE-000250)\n* kept rank, anomaly score, signal count, coarse signal tags, and short context windows\n* added raw_card_sha256 trace hash for integrity\n* removed raw internal hypothesis-card details\n\nZero-touch pipeline snapshot (run log):\n\n* auto-detected input: /content/TEXT\n* hashed files: 342,249\n* read/chunked files: 342,249\n* chunks built: 521,289\n* docs processed: 342,249 (ok=342,249 failed=0 skipped=0)\n* BM25 stats written\n* embeddings generated (BAAI/bge-large-en-v1.5)\n* PCA + KMeans auto-select with guardrails\n* cluster + fused outputs written automatically\n\nSanitized top-N snapshot:\n\n* rows: 250\n* rank range: 1 to 250\n* anomaly score range: 35.525 to 43.636\n* signal count range: 10 to 14\n* common tags:\n\n  * context_shift: 250/250\n  * embedding_outlier: 250/250\n  * entity_density: 250/250\n  * lexical_pressure: 248/250\n  * self_reference: 11/250\n\nHow to view anomaly results:\nGo to the Signal tab in the Hugging Face Space. It should be straightforward, but if anyone wants a quick walkthrough I can post one.\n\nWhy I’m posting this:\n\n* transparency\n* reproducibility\n* faster manual review by collapsing search space\n* clear separation between ranking signals and factual/legal conclusions",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1r246g5/zerotouch_anomaly_triage_on_the_epstein/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": [
        {
          "id": "o534138",
          "author": "ok-painter-1646",
          "text": "Can I have a tldr for what this is?",
          "score": 1,
          "created_utc": "2026-02-13 01:15:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o539cfb",
              "author": "Either_Pound1986",
              "text": "Basically: too many files for humans to read one by one, so I built a sorter. High score = maybe worth a closer look first, not “this is true.” Source text stayed as-is, with original filenames for traceability. \n\nI can provide a more in depth explanation if you want just let me know.",
              "score": 1,
              "created_utc": "2026-02-13 01:48:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxh6bo",
      "title": "Large Language Epstein Model",
      "subreddit": "huggingface",
      "url": "https://www.reddit.com/r/huggingface/comments/1qxh6bo/large_language_epstein_model/",
      "author": "ExternalAirlock",
      "created_utc": "2026-02-06 13:09:12",
      "score": 7,
      "num_comments": 15,
      "upvote_ratio": 0.77,
      "text": "Yall probably heard about a model trained only on old English texts, but has anyone trained a model purely on the Epstein files?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/huggingface/comments/1qxh6bo/large_language_epstein_model/",
      "domain": "self.huggingface",
      "is_self": true,
      "comments": [
        {
          "id": "o3wagju",
          "author": "itsforathing",
          "text": "Yeah, but every response is redacted",
          "score": 7,
          "created_utc": "2026-02-06 13:17:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wetrf",
              "author": "ExternalAirlock",
              "text": "Hey, alternatively, maybe there was a pattern between the length of the redacted segment and the overarching context. What if it could infer the meaning of redacted sections just like it inferred the meaning of regular words?",
              "score": 3,
              "created_utc": "2026-02-06 13:42:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o41y4vq",
                  "author": "Area51-Escapee",
                  "text": "Then you have a hallucinated Text. Then what?",
                  "score": 2,
                  "created_utc": "2026-02-07 09:14:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o42mptx",
                  "author": "Distinct-Target7503",
                  "text": "you are basically training for masked language modeling without a ground truth...",
                  "score": 1,
                  "created_utc": "2026-02-07 12:57:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o41snev",
          "author": "WesternKangaroo3406",
          "text": "this sounds actually like a fun idea hahaha",
          "score": 2,
          "created_utc": "2026-02-07 08:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40b5hm",
          "author": "Astralnugget",
          "text": "I’ll do it",
          "score": 1,
          "created_utc": "2026-02-07 01:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o451zes",
          "author": "GentlemanNasus",
          "text": "You would still need to train English first for it to decipher and understand Epstein files so it still won't be a \"purely\" Epstein model",
          "score": 1,
          "created_utc": "2026-02-07 20:35:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4eihca",
              "author": "ExternalAirlock",
              "text": "Well the files are in English, and there are a lot of them",
              "score": 1,
              "created_utc": "2026-02-09 08:35:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46lwu5",
          "author": "LastXmasIGaveYouHSV",
          "text": "You'll get a lot of short answers with bad ortography",
          "score": 1,
          "created_utc": "2026-02-08 02:01:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46lxhc",
          "author": "Coldshalamov",
          "text": "Yeah, Grok 4.20 that's why it's still not released",
          "score": 1,
          "created_utc": "2026-02-08 02:01:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dq7ub",
          "author": "ProfessionalShop9137",
          "text": "What would be the goal of it? Like some deep similarity search to find things in the files or have a model respond like an email from Epstein but not be RAG based?",
          "score": 1,
          "created_utc": "2026-02-09 04:35:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4eifdn",
              "author": "ExternalAirlock",
              "text": "Goal: for lulz\n\nAs I said, it was done with old English texts so it could be done with Epstein files",
              "score": 1,
              "created_utc": "2026-02-09 08:35:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4razwx",
          "author": "Happysedits",
          "text": "https://www.reddit.com/r/LocalLLaMA/s/WkjT1Tu7WU",
          "score": 1,
          "created_utc": "2026-02-11 06:43:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}