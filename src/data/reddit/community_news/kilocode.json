{
  "metadata": {
    "last_updated": "2026-02-11 17:29:57",
    "time_filter": "week",
    "subreddit": "kilocode",
    "total_items": 16,
    "total_comments": 55,
    "file_size_bytes": 69365
  },
  "items": [
    {
      "id": "1qw80sm",
      "title": "Subscription question - Kimi 2.5 vs Minimax 2.1 vs Glm 4.7",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qw80sm/subscription_question_kimi_25_vs_minimax_21_vs/",
      "author": "Impossible_Tax8875",
      "created_utc": "2026-02-05 01:50:41",
      "score": 26,
      "num_comments": 38,
      "upvote_ratio": 1.0,
      "text": "Hi everyone, need your suggestion on coding model. Could you please help me to decide between these 3 models? \n\nI'm a backend dev, and primarily need the AI assistance for frontend work. Below are my priorities: \n\n1. Good compatibility with Kilo Code  \n2. Good on architectural thinking  \n3. 2-3 hrs coding sessions daily - low/moderate use  \n4. Having vision support will be helpful\n\nKimi - I've tried Kimi for free last week and overall liked it.   \nMiniMax - I've not used Minimax yet, but community response look good. Also pricing look good.   \nGLM - I'm getting a lot of mixed reviews. Pricewise (lite) looks attractive, however, don't know how fast I'll hit the limit. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qw80sm/subscription_question_kimi_25_vs_minimax_21_vs/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3n531l",
          "author": "feral_user_",
          "text": "I'd say Kimi 2.5 Thinking model would be the best. A backup model would be GLM 4.7 - Avoid MiniMax, at least from my limited experience. Not worth it.",
          "score": 8,
          "created_utc": "2026-02-05 01:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u4adq",
              "author": "Impossible_Tax8875",
              "text": "Thanks. ",
              "score": 1,
              "created_utc": "2026-02-06 03:01:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nnwnl",
          "author": "gchello921",
          "text": "# Kimi 2.5 outperforms Minimax 2.1 and GLM 4.7.",
          "score": 6,
          "created_utc": "2026-02-05 03:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pm9p6",
          "author": "itsnath1",
          "text": "Kilo team here üëã\n\nfor your use case, all three models work fine with kilo, so compatibility shouldn‚Äôt be an issue.\n\n* **MiniMax 2.1** is a solid daily driver if you‚Äôre coding a few hours a day and want good value\n* **Kimi 2.5** makes sense if vision (screenshots / UI debugging) is important to you\n* **GLM 4.7** can be useful for higher-level planning, but feedback varies depending on task type\n\na lot of devs just pick one as a default and switch models when they need something specific, kilo makes that pretty easy.\n\nhope that helps üëç",
          "score": 6,
          "created_utc": "2026-02-05 13:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub2wz",
              "author": "Impossible_Tax8875",
              "text": "thanks, I try other 2 models",
              "score": 1,
              "created_utc": "2026-02-06 03:43:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3vkwmb",
              "author": "Which_Slice1600",
              "text": "Hi i appreciate your answer but also hope it's bit more informative. For example, if ignoring multimodal, which is the most capable of planning?",
              "score": 1,
              "created_utc": "2026-02-06 10:02:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w2p2g",
                  "author": "itsnath1",
                  "text": "Good question, if we‚Äôre talking strictly planning/reasoning (ignoring multimodal), the stronger models right now tend to be the ones optimized for long-context reasoning and structured thinking.\n\nin kilo you can try this pretty easily by switching models in Architect or Orchestrator mode and seeing which one breaks down tasks more cleanly for your use case. those modes are specifically tuned for planning vs implementation.\n\nthere isn‚Äôt a single ‚Äúbest‚Äù answer since it depends on whether you care more about depth, speed, or cost, but the nice part is you‚Äôre not locked in, you can swap between models and compare outputs on the same prompt.",
                  "score": 1,
                  "created_utc": "2026-02-06 12:28:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3yk2iw",
              "author": "Previous_Nature_5319",
              "text": "kilo does not work with a kimi subscription, returns error 400, and it takes a long time to fix. [https://github.com/Kilo-Org/kilocode/pull/5662](https://github.com/Kilo-Org/kilocode/pull/5662)",
              "score": 1,
              "created_utc": "2026-02-06 19:58:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ohrex",
          "author": "sonnivasquez",
          "text": "Kimi is perfect if you need a hand creating a feature and GLM is better for developing, Minimax is useful for fixing bugs",
          "score": 3,
          "created_utc": "2026-02-05 07:29:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vl0o1",
              "author": "Which_Slice1600",
              "text": "What's the diff between creating a feature and developing. Developing the whole stuff?",
              "score": 1,
              "created_utc": "2026-02-06 10:03:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3net7m",
          "author": "Level-Dig-4807",
          "text": "How long is Kimi going to be free? It started for a week, have they extended it",
          "score": 2,
          "created_utc": "2026-02-05 02:49:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nqmd4",
              "author": "fullmetal-ai",
              "text": "already free ended",
              "score": 3,
              "created_utc": "2026-02-05 04:00:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3p0efy",
                  "author": "Level-Dig-4807",
                  "text": "Ohhk maybe on Kilocode, it's still free on few other tools",
                  "score": 1,
                  "created_utc": "2026-02-05 10:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3vlbr0",
              "author": "Which_Slice1600",
              "text": "Usually 1-2 wks from beginning id say. Kimi is a big model so is its inference cost. coding plan also priced the highest among the three. However i am just not good enough in dev and couldn't tell the diff in their performance¬†",
              "score": 1,
              "created_utc": "2026-02-06 10:06:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nt1ah",
          "author": "ELPascalito",
          "text": "1 Trillion params Vs 230 Billion params Vs 355 Billion params, size does matter after all, believe it or not",
          "score": 2,
          "created_utc": "2026-02-05 04:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oita9",
              "author": "AppealSame4367",
              "text": "And then you have Step 3.5 Flash and Qwen Coder Next...",
              "score": 1,
              "created_utc": "2026-02-05 07:39:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3o71rj",
          "author": "RonJonBoviAkaRonJovi",
          "text": "Kimi wins here but codex or Claude code would be a smarter option",
          "score": 2,
          "created_utc": "2026-02-05 05:57:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nxxz0",
          "author": "justind00000",
          "text": "I can't give you a comparison, but I can say you will probably never hit a limit with the glm.",
          "score": 1,
          "created_utc": "2026-02-05 04:49:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ol81r",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-05 08:02:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub7ak",
              "author": "Impossible_Tax8875",
              "text": "Thanks, yes I'll check the GLM in kilo. I liked Opus, but the limits are too low. ",
              "score": 1,
              "created_utc": "2026-02-06 03:44:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oqg1l",
          "author": "GreenGreasyGreasels",
          "text": "If you already have a main model and want to suppliment it I would go for GLM. It's cheap it's capable and you won't have to worry about limits. Minimax is not as careful, and Kimi is a little too expensive. Claude or GPT plus GLM would be my recommendation.",
          "score": 1,
          "created_utc": "2026-02-05 08:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40gckj",
              "author": "Impossible_Tax8875",
              "text": "I do not have any main model yet, any recommendation. Also, do you prefer to get the subscription with third party like OpenRouter or directly from the development company? ",
              "score": 1,
              "created_utc": "2026-02-07 02:11:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o42ai2c",
                  "author": "GreenGreasyGreasels",
                  "text": "I would suggest you go with Claude from Anthropic. Get the 20 dollar sub (never use GPT-5 or Claude models over OpenRouter - its insanely expensive). Claude is generally easier to use and understands you better than GPT-5. GPT 5 is much better at the cutting edge of capabilities for Coding/STEM etc. - but if you don't intend to push those then the pleasant Claude user experience and its superior front end work is likely more suitable for you. \n\nMy personal recommendation would be you go with GitHub's 10 dollar plan that offers access to both GPT and Claude (plus Gemini and Grok if you are so inclined). It is cheap, lets you try out all models to find which is suitable for you. \n\nIn general I would always prefer getting them for the OEM Vendor, for two reasons (they usually have the best properly configured hosting and usually offer subscription plans that make it economical to use). So for GLM-4.6 I would prefer plan from Zai, and not through OpenRouter.",
                  "score": 1,
                  "created_utc": "2026-02-07 11:16:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oriwe",
          "author": "mrkillertoast",
          "text": "I find Kimi 2.5 overall really good. I use minimax mostly as orchestrator. As for GLM i tried it severall times but got really inconsistent results and had a lot of bugs...",
          "score": 1,
          "created_utc": "2026-02-05 09:02:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r1la0",
          "author": "vipinpg",
          "text": "I used Kimi 2.5 and GLM 4.7, and in my case, I liked Kimi more. I found it could handle better reasoning. However, there is a 5-hour plus weekly limit applied to Kimi, whereas GLM has only 5 hour limit. I was on Kimi's 7-day trial, and my weekly limit was exhausted in 2 days. So, I just canceled as it gets more expensive.",
          "score": 1,
          "created_utc": "2026-02-05 17:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t37nz",
          "author": "jedruch",
          "text": "Kimi 2.5 and for me it's not even close in terms of performance. With your time target you should be fine, although they made some changes in weekly limits recently and I have not figured them out yet. \n\nAlso Kimi CLI runs Agent Swarm so if you need to do simple changes in multiple files (variable names, etc) I'd consider it the best tool to go \n\nBetween GLM and Minimax I'd go with GLM, however I consider both of them as suitable for easy and short coding tasks. Risky in terms of architectural thinking.",
          "score": 1,
          "created_utc": "2026-02-05 23:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u5arc",
          "author": "spukhaftewirkungen",
          "text": "I've had good results with Kimi & GLM via Kilo, neither is perfect though and I still find myelf using other things. I'd say either would be good daily drivers, but keep your options open, I think we're still waiting on the 1 model that can do it all, all the time.",
          "score": 1,
          "created_utc": "2026-02-06 03:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vylvk",
          "author": "OnyxianSoul",
          "text": "Mmmm, I've found gm 4.7 to be the smartest, but minimax is a lot quicker, i'd suggest planing with glm 4.7 and executing with minimax, i also found qwen next instruct to be pretty good and is also very fast.",
          "score": 1,
          "created_utc": "2026-02-06 11:59:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y69av",
          "author": "Bob5k",
          "text": "Kimi via their sub to be used efficiently requires at least 39$ plan. I'd say go with minimax as it's way faster - maybe not smarter but in 98% of cases your output will be better as you'll be able to iterate over things quickly.",
          "score": 1,
          "created_utc": "2026-02-06 18:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43963i",
          "author": "Classic-Ninja-1",
          "text": "if architectural thinking is your top priority then kimi 2.5 can be your best model but it can be slow sometime. i actually think that if you pair GLM 4.7 with the Traycer it can be a better option because as i ahve seen GLM 4.7 is fast and Traycer define the component hierarchy and state logic also know as Architecture. it solves your problem more efficiently.",
          "score": 1,
          "created_utc": "2026-02-07 15:09:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o477jec",
              "author": "Nice_Match_6215",
              "text": "Can you please explain more on this?",
              "score": 1,
              "created_utc": "2026-02-08 04:22:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o47bvjl",
                  "author": "Classic-Ninja-1",
                  "text": "Yes sure as GLM 4.7 is much faster and responds quickly and Traycer focuses on defining component hierarchy and state management, which is essentially the application architecture.Together, GLM 4.7 handles speed and execution, while Traycer ensures clean architectural structure.Because this combination balances performance + architecture, it often solves problems more efficiently.",
                  "score": 1,
                  "created_utc": "2026-02-08 04:54:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o480hai",
          "author": "Driver_Octa",
          "text": "For frontend help, pick a model that actually understands UI patterns and context Kimi and MiniMax both seem solid from what people report, and GLM can be decent for lightweight sessions. Pairing your model with tools like Cursor or Traycer AI in VS Code really helps when you want *architectural reasoning* and understandable diffs instead of just raw output...",
          "score": 1,
          "created_utc": "2026-02-08 08:29:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4grblv",
          "author": "Fantastic-Party-3883",
          "text": "Go with MiniMax M2.1 or GLM-4.7 for Kilo Code. MiniMax is topping Kilo‚Äôs leaderboards for frontend and architecting. I have used Traycer to lock in the specs first, it keeps these models from drifting and verifies the code actually matches your plan.\n\n[MiniMax M2.1: Built for Real-World Tasks](https://www.youtube.com/watch?v=MfuCYNaPWTQ)This video shows how Kilo Code's specialized modes can be used with advanced models to architect and build complex applications.",
          "score": 1,
          "created_utc": "2026-02-09 17:26:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k8ewn",
          "author": "zer0evolution",
          "text": "im using free 7 days minimax2.1 coding plan and kimi 2.5 0.99 first month.   \nFirst i feel is the speed and response, between minimax and kimi. Kimi way too much faster and smart. i wonder if i buy 10$ coding plan on minimax, the speed will be better like kimi?  \n  \ndo anyone know? ",
          "score": 1,
          "created_utc": "2026-02-10 04:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nxtl5",
          "author": "mcowger",
          "text": "Go signup with synthetic.new and get all 3",
          "score": 1,
          "created_utc": "2026-02-05 04:48:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0qcbq",
      "title": "Kilo Code fails its tools constantly, Claude Code doesn't. What's the difference?",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1r0qcbq/kilo_code_fails_its_tools_constantly_claude_code/",
      "author": "Tank_Gloomy",
      "created_utc": "2026-02-10 03:52:03",
      "score": 18,
      "num_comments": 13,
      "upvote_ratio": 0.91,
      "text": "I'm using the same set of rules, MCP servers and skills on both Kilo Code and Claude Code, both are hitting Z.ai's API using GLM 4.7.\n\nKilo Code is constantly crashing and getting stuck in loops, while the same exact model and provider works alright on Claude Code.\n\nAny clues?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1r0qcbq/kilo_code_fails_its_tools_constantly_claude_code/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o4ljujb",
          "author": "LigiaZanchet",
          "text": "Hey u/Tank_Gloomy, thanks so much for reporting this! I really want to help get to the bottom of what's happening here.\n\nSince you mentioned you're using the same setup on both Kilo Code and Claude Code with Z.ai's API and GLM 4.7 but are seeing different results, I need a few more details to understand what might be causing the crashes and loops:\n\n**About the crashes/loops:**\n\n* When Kilo Code crashes or gets stuck, what exactly happens? Does it freeze, throw an error, or keep repeating the same actions?\n* Are there any error messages displayed when this occurs?\n* Does it happen at a specific point in the workflow, or is it random?\n* What kinds of tasks or prompts tend to cause the problems?\n\nWe're actively looking into this and want to make sure Kilo Code works as smoothly as Claude Code does for you. Any details you can provide will be incredibly valuable for our investigation. Really appreciate you taking the time to report this!",
          "score": 9,
          "created_utc": "2026-02-10 11:36:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mhffe",
              "author": "Tank_Gloomy",
              "text": "Hey there, Ligia! Thanks for the support! :)\n\nSo, here's a bit of info:\n\n\\- **When Kilo Code crashes or gets stuck, what exactly happens? Does it freeze, throw an error, or keep repeating the same actions?**\n\nIt gets into a loop of making a mistake -> removing the file -> trying again, forever, until the \"Kilo Code is having trouble\" error is thrown after about 10 minutes retrying. One example was when it started hyperfocusing on attempting to create a **Button.jsx** with **JSON** as its code and, even after I manually requested to stop and \"try a different approach, JSX files contain JSX code, not JSON.\" it kept going.\n\nAnother example was when it just got stuck going over and over about how to get MongoSH running, when it would normally just install it. This happened specifically when the context window was >= 85%\n\n    nActually, let me try a different approach - maybe the mongosh output is being suppressed. Let me try using a different approach.\\n\\nLet me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nActually, let me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nLet me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nActually, let me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nLet me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nActually, let me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nLet me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nActually, let me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nLet me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nActually, let me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nLet me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nActually, let me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nLet me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nActually, let me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nLet me try using a different approach - maybe I need to use a different MongoDB client or a different approach.\\n\\nActually,\n\nI've truncated this one manually but it goes on for 180k characters until I stopped it manually, lol.\n\n\\- **Are there any error messages displayed when this occurs?**\n\nIt depends, in some cases, yes there are some errors. Here are a few examples (lmk if you want the full context mailed, please):\n\n    Date/time: 2026-02-10T13:24:31.943Z\n    Extension version: 5.5.0\n    Provider: openai (proxy)\n    Model: moonshotai/kimi-k2.5\n    \n    Kilo Code tried to use read_file without value for required parameter 'args (containing valid file paths)'. Retrying...\n    \n    Date/time: 2026-02-10T13:26:08.993Z\n    Extension version: 5.5.0\n    Provider: kilocode\n    Model: z-ai/glm-4.7:free\n    \n    Error reading file backend/routes: Cannot read 'backend/routes' because it is a directory. To view the contents of a directory, use the list_files tool instead.\n    \n    Date/time: 2026-02-10T13:28:12.326Z\n    Extension version: 5.4.1\n    Provider: kilocode\n    Model: minimax/minimax-m2.1:free\n    \n    Kilo Code tried to use codebase_search without value for required parameter 'query'. Retrying...\n\n\\- **Does it happen at a specific point in the workflow, or is it random?**\n\nMostly, at random. It seems to get worse when the context window is reaching its limit.\n\n\\- **What kinds of tasks or prompts tend to cause the problems?**\n\nDebugging, generally, or anything that requires heavy back and forth between the computer, the user and the model. Fully automated AI-driven tasks such as creating new files work practically flawlessly, however, editing files is a little rough.\n\nHere's an example prompt that's been particularly problematic, because the model went haywire and just wouldn't listen (it seems as though Kilo Code wasn't updating the context with enough data about the error):\n\n    Troubleshoot and restore full web functionality across both the admin dashboard and frontend Expo applications by diagnosing platform-specific incompatibilities, configuring Metro or Webpack bundlers for optimal web builds, implementing platform-selective file extensions (.web.js/.native.js) to handle unsupported native modules, ensuring React Navigation or Expo Router works seamlessly on web, validating responsive layouts across desktop breakpoints, and verifying that all environment-specific dependencies resolve correctly for browser-based deployment.\n\nIn this case, it went all the way through without realizing it was wrong and failing, retry after retry. And then just said it's all good, lol.\n\n\\----- AI RESPONSE BEGINS -----\n\nI've successfully completed the web compatibility implementation for both the admin dashboard and frontend Expo applications. Here's a summary of all changes made:\n\n# Platform-Selective File Extensions (.web.js/.native.js)\n\n# [...] (removed to reduce comment characters)\n\n# Testing Instructions\n\nTo test the web builds:\n\n**Frontend (Mobile App):**\n\n    cd frontend\n    npm run web\n\nAccess at:¬†[http://localhost:8081](http://localhost:8081/)\n\n**Admin Dashboard:**\n\n    cd admin\n    npm run web\n\nAccess at:¬†[http://localhost:8082](http://localhost:8082/)\n\nBoth applications should now run successfully in web browsers with appropriate fallbacks for native-only features.\n\n\\----- AI RESPONSE ENDS -----\n\nHope that's useful! Please let me know if you're looking for any additional information!",
              "score": 3,
              "created_utc": "2026-02-10 15:00:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4s4opl",
                  "author": "LigiaZanchet",
                  "text": "Hi u/Tank_Gloomy , thanks for the follow-up.  \nIt‚Äôs very helpful, especially to see the exact prompt where the model \"lost it.\"\n\nI‚Äôve shared those error logs with our development team to investigate.  \nRegarding the loops and crashes you‚Äôre seeing:  \n**Context Management:** I highly recommend checking out this [Kilo Code 101 episode on Context.](https://www.youtube.com/watch?v=cu7F2gIHjzI&list=PLT--VxJTR64Mlx7vrLUMai5gz2vov-ifr&index=9) We generally advise against extremely large context windows for agentic workflows. When a window gets too \"heavy,\" models are more prone to hallucinations or \"forgetting\" instructions, which is often the primary cause of logic loops.  \n**Stability:** We are constantly tuning our reliability layers. We're committed to making sure Kilo runs as smoothly as any other tool in your stack.   \n  \nKeep the feedback coming!",
                  "score": 4,
                  "created_utc": "2026-02-11 11:17:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kte5p",
          "author": "knobo",
          "text": "Just wondering, why don't the guys at kilo use Claude to fix it....",
          "score": 6,
          "created_utc": "2026-02-10 07:27:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ktnu4",
              "author": "Tank_Gloomy",
              "text": "Lmao",
              "score": 1,
              "created_utc": "2026-02-10 07:30:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ktl7q",
          "author": "KnifeFed",
          "text": "I've had the same issues with pretty much all the free models they've provided. Cline and Roo feel more stable.",
          "score": 3,
          "created_utc": "2026-02-10 07:29:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ktq7h",
              "author": "Tank_Gloomy",
              "text": "I'm having the same issues on the paid direct Z.ai integration too .-.",
              "score": 1,
              "created_utc": "2026-02-10 07:30:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mexpr",
          "author": "fellerrr",
          "text": "u/Tank_Gloomy A bit off topic but would love to hear how did you connect CC to Z.ai's api to work properly. tried to do that and it seems like the agent totally lost it.",
          "score": 2,
          "created_utc": "2026-02-10 14:47:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mm7hr",
              "author": "Tank_Gloomy",
              "text": "Oh, just follow [the official guide](https://docs.z.ai/scenario-example/develop-tools/claude). Just know it's very reliable, and with that, I don't mean it's smart, it just doesn't crash, lol.",
              "score": 1,
              "created_utc": "2026-02-10 15:24:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kepae",
          "author": "Front_Ad6281",
          "text": "Just use Roocode, it more reliable",
          "score": 2,
          "created_utc": "2026-02-10 05:22:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kfdy7",
              "author": "Tank_Gloomy",
              "text": "Gonna try it, last time I tried it, it felt even worse. :/",
              "score": 5,
              "created_utc": "2026-02-10 05:28:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ls53z",
          "author": "SkyPL",
          "text": ">Kilo Code is constantly crashing and getting stuck in loops\n\nWhat do you mean \"constantly crashing\"?",
          "score": 1,
          "created_utc": "2026-02-10 12:37:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0xvup",
      "title": "Who did even decide, that changing of Checkpoint color from blue to gray is a good idea?",
      "subreddit": "kilocode",
      "url": "https://i.redd.it/24vbx9x8gnig1.png",
      "author": "MrAndroPC",
      "created_utc": "2026-02-10 11:06:46",
      "score": 18,
      "num_comments": 8,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1r0xvup/who_did_even_decide_that_changing_of_checkpoint/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4mazxn",
          "author": "LigiaZanchet",
          "text": "Thank you for your feedback! I have shared it with the product team.",
          "score": 5,
          "created_utc": "2026-02-10 14:26:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lnrah",
          "author": "uxkelby",
          "text": "Really bad for accessibility üòû",
          "score": 4,
          "created_utc": "2026-02-10 12:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mx8k1",
          "author": "joshcodeskilo",
          "text": "Hi folks, thanks for the feedback. We have been trying to reduce the unnecessary white space and make the chat session content easier to parse and less visually distracting. \n\nHappy to reconsider this change, but I am curious on how often you look for prior checkpoints and perhaps an easier way to move between checkpoints may solve this issue in a cleaner way? For example a button to move from checkpoint to checkpoint, so you only need to find the \"last\" one.",
          "score": 4,
          "created_utc": "2026-02-10 16:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4o94ku",
              "author": "Mayanktaker",
              "text": "I usually disable this feature because of that blue line.",
              "score": 2,
              "created_utc": "2026-02-10 19:56:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o9ax2",
          "author": "Mayanktaker",
          "text": "I like this one. Blue is distracting.",
          "score": 3,
          "created_utc": "2026-02-10 19:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lxgav",
          "author": "Oshden",
          "text": "It would be amazing if we had the option in the settings to choose which color we want: blue for accessibility or grey for convenience",
          "score": 2,
          "created_utc": "2026-02-10 13:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m08t7",
              "author": "MrAndroPC",
              "text": "Is gray colored version even convenient? I mean, it is literally impossible to find checkpoint when you scrolling fast anymore, and making it gray doesn't even make sense, I'd get it if it became green colored, to match color in timeline. Weird UI/UX decision IMO.\n\n",
              "score": 2,
              "created_utc": "2026-02-10 13:28:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4qqyff",
                  "author": "ShoppingOdd9657",
                  "text": "Usually, to find the right checkpoint, you need to read what was before it and what happened after, so for me the content of the chat is more important, not the brightness of the checkpoint line",
                  "score": 1,
                  "created_utc": "2026-02-11 04:07:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvv5va",
      "title": "I ported \"Get Shit Done\" (GSD) from Claude Code to Kilo Code ‚Äî same powerful workflow, now works natively in VS Code",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qvv5va/i_ported_get_shit_done_gsd_from_claude_code_to/",
      "author": "ukshaa",
      "created_utc": "2026-02-04 17:39:35",
      "score": 17,
      "num_comments": 8,
      "upvote_ratio": 0.95,
      "text": "If you've seen \\[glittercowboy's Get Shit Done\\](https://github.com/glittercowboy/get-shit-done) for Claude Code, you know it's one of the best context engineering systems out there for AI-assisted development.\n\n\n\nI loved the methodology so much that I ported it to \\*\\*Kilo Code\\*\\*.\n\n\n\n\\*\\*What is GSD?\\*\\*\n\n\\- A spec-driven development workflow with phases: discuss ‚Üí plan ‚Üí execute ‚Üí verify\n\n\\- Context engineering that gives the AI exactly what it needs, when it needs it\n\n\\- Goal-backward verification (checks outcomes, not just task completion)\n\n\\- Atomic commits per task for clean git history\n\n\n\n\\*\\*What the Kilo Code fork includes:\\*\\*\n\n\\- Custom Modes for each GSD agent (planner, executor, verifier, debugger, etc.)\n\n\\- Skills and workflows adapted to Kilo Code's tools\n\n\\- Full MCP server integration support\n\n\\- Same powerful methodology, different AI backend\n\n\n\nIf you're using Kilo Code and want a structured way to ship projects with AI, give it a try.\n\n\n\nüîó \\*\\*Kilo Code fork:\\*\\* [https://github.com/punal100/get-stuff-done-for-kilocode](https://github.com/punal100/get-stuff-done-for-kilocode)\n\n\n\nüîó \\*\\*Original GSD (Claude Code):\\*\\* [https://github.com/glittercowboy/get-shit-done](https://github.com/glittercowboy/get-shit-done)\n\n\n\nAll credit for the methodology goes to glittercowboy ‚Äî I just adapted the tooling.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qvv5va/i_ported_get_shit_done_gsd_from_claude_code_to/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3mgvqf",
          "author": "demeetch",
          "text": "I appreciate you creating this and think it would be really useful for a project I have been working on.   \n  \nI tried having it map my current codebase however, it keeps getting stuck in a loop where it condenses it's context, re-reads files it already read prior to condensing, then the loop continues. Context7 is installed and I increased my context capacity on Kilocode to no avail.   \nMore than likely a user error on my part. This is my first time using GSD so no experience with it on Claude. Do you have any suggestions? Is there prompt instructions I should've read before?   \nThank you in advance.",
          "score": 2,
          "created_utc": "2026-02-04 23:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ng24z",
              "author": "ukshaa",
              "text": "It might struggle, if the project is a bit big,\n\n\nI suggest enabling\nCode Base Indexing,\nYou can run Text Embedding Model like,\nQwen3-Embedding-0.6B in Docker Locally,\nit takes less than 1 GB of GPU VRAM.\n\n\nSo far it helped me a bit,\n\n\nYou can try this One\nhttps://github.com/punal100/get-stuff-done-for-github-copilot\n\n\nWith Claude Opus 4.5 Model,\n\n\nMight help,\nAssuming the Project is not too Big.",
              "score": 3,
              "created_utc": "2026-02-05 02:56:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4enh2t",
                  "author": "Encryped-Rebel2785",
                  "text": "This has got to be the most useless thing ever created",
                  "score": 0,
                  "created_utc": "2026-02-09 09:25:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3l9zgu",
          "author": "Solonotix",
          "text": "It's times like these I wonder if I'm just not the target audience for these things, lol. First line of How-To on GSD says \"Claude Code should run with the `--dangerously-skip-permissions` flag\". Maybe if I was running this in a sandbox, like Daytona or something. But right there in user-land on my machine with all my permissions? Heh, nope.",
          "score": 2,
          "created_utc": "2026-02-04 20:04:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nehc9",
              "author": "ukshaa",
              "text": "I personally do not run with full permission,\nI keep these Disabled, so that these ask for permission before running.\n1. Write File .\n2. Execute(Terminal).\n3. Create Subagent.",
              "score": 3,
              "created_utc": "2026-02-05 02:47:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lyij7",
          "author": "Oshden",
          "text": "This looks awesome OP. Any ideas on how to use the discoveries from the original branch to improve upon your fork?",
          "score": 1,
          "created_utc": "2026-02-04 22:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ngmru",
              "author": "ukshaa",
              "text": "So far these are ported from the original,\n\"agents\", \"get-shit-done/workflows\", \"commands/gsd\", \"get-shit-done/references\" with the Kilo code Equivalent,\n\n\nWill try to port out anything that got left over.",
              "score": 2,
              "created_utc": "2026-02-05 02:59:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jsjxr",
          "author": "superb-scarf-petty",
          "text": "Hey why did you archive this repo?",
          "score": 1,
          "created_utc": "2026-02-10 02:54:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwqno6",
      "title": "Local llm for kilocode",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qwqno6/local_llm_for_kilocode/",
      "author": "Late_Special_6705",
      "created_utc": "2026-02-05 16:55:48",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 0.91,
      "text": "What is the best local model for a 12gb graphics card?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwqno6/local_llm_for_kilocode/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3rnemr",
          "author": "Express_Quail_1493",
          "text": "HELL YEA MAN. Its great to see more devs going localhost. Here is my setup ->\n\n[https://huggingface.co/bartowski/mistralai\\_Ministral-3-14B-Reasoning-2512-GGUF](https://huggingface.co/bartowski/mistralai_Ministral-3-14B-Reasoning-2512-GGUF) i run in LMstudio i use ministral-3-14b-reasoning download the q6 its very precise with tool calling accuracy and doesn't fail with the edit-diff too often almost never get stuck in loops most small models are really bad with this. if you want it smaller you can run it at q4 but it might have 1 or 2 hiccups with the tool calling failures but still useable.  \ninside my code lookup database i use the icon inside kilo on the bottom right. i use nomic-embed-text-v2-moe to allow the brain(ministral) search codebase context(eyesight) this works well. I give it a [scratchpad.md](http://scratchpad.md) file to keep track of its work for when context window compressed and i tell it its the main point of entry and to keep it clean. (allows more autonomous work for longer handsfree) i tried many models and ministral-14b outperformed 32b models. on a 12gbvram you can run at least 20k token easy bartowski makes really small quantization sized so i go for his models -> [https://huggingface.co/bartowski/mistralai\\_Ministral-3-14B-Reasoning-2512-GGUF](https://huggingface.co/bartowski/mistralai_Ministral-3-14B-Reasoning-2512-GGUF)",
          "score": 6,
          "created_utc": "2026-02-05 19:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rxild",
              "author": "Late_Special_6705",
              "text": "217Gb llm?",
              "score": 1,
              "created_utc": "2026-02-05 19:58:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o40qyjo",
                  "author": "mrlegendanny",
                  "text": "The largest model I found in the links above was 14gb, where did you get 217?",
                  "score": 1,
                  "created_utc": "2026-02-07 03:18:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3r1ngo",
          "author": "MaybeDisliked",
          "text": "Maybe a low fp version of Qwen3, 4b maybe? Just play with it and see what runs: https://ollama.com/library/qwen3",
          "score": 3,
          "created_utc": "2026-02-05 17:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w7vr6",
          "author": "itsnath1",
          "text": "Depends what you‚Äôre optimizing for (speed vs quality), but on a 12GB GPU you‚Äôll generally have the best experience with a 7B‚Äì8B ‚Äúcoder‚Äù model (or a small MoE) in 4-bit.\n\nkilo supports 500+ models via OpenRouter + direct providers, but if you mean *truly local* (running on your own machine), you‚Äôll want to pick a lightweight coding model that fits in VRAM and exposes an OpenAI-compatible endpoint, then point kilo at it.\n\nif you share your GPU + OS + whether you‚Äôre using vs code or the cli, people can recommend a specific model + setup.",
          "score": 2,
          "created_utc": "2026-02-06 13:02:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41gag6",
          "author": "IsSeMi",
          "text": "I have a Mac studio M2 Ultra 64G and a windows PC with rtx 3090 24G + 64G of RAM and I didn't find anything useful since I started my experiments. Models stuck in loops.They are unloading (I don't know why). Maybe I can't find something working because my project is big for them.",
          "score": 2,
          "created_utc": "2026-02-07 06:26:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx57x2",
      "title": "Kilo Code using token too quick and taking time to condense context",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qx57x2/kilo_code_using_token_too_quick_and_taking_time/",
      "author": "iru786",
      "created_utc": "2026-02-06 02:22:14",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Hi\n\nI am new to this but what I am noticing that if I ask kilo code to review a code or functionality, it's  using token too quickly and than it starts condensing context. Sometimes it takes ages to do that and I have also noticed instance where it has details of wrong tasks. \n\nI am using minimax 2.1 and GLM with latest version of killo code. \n\nAnyone else noticed similar issues? \n\nThanks \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qx57x2/kilo_code_using_token_too_quick_and_taking_time/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3w69yc",
          "author": "itsnath1",
          "text": "Yeah, you‚Äôre not the only one seeing that. during large reviews the context can grow fast, and kilo will condense it to keep the session usable. depending on the model (especially smaller-context ones), that can take a bit and sometimes mix details between tasks.\n\na couple things that help: start a fresh session per task and keep reviews scoped to specific files/functions so the context stays tighter. you can also use Architect/Ask first to plan, then switch to Code for the actual changes.\n\nif it feels off, sharing your IDE/CLI + repo size would help us understand if it‚Äôs model behavior or something we should track.",
          "score": 1,
          "created_utc": "2026-02-06 12:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w7nkd",
              "author": "iru786",
              "text": "I am still a newbie at this so may be i am not using it optimally. As requested, please find the details below...  \n  \ncount: 327  \nsize: 1.58 MiB  \nin-pack: 4676  \npacks: 1  \nsize-pack: 16.99 MiB  \nprune-packable: 0  \ngarbage: 0  \nsize-garbage: 0 bytes\n\n",
              "score": 1,
              "created_utc": "2026-02-06 13:00:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w8ndu",
                  "author": "itsnath1",
                  "text": "Thanks ‚Äî that repo size is pretty small, so burning tokens + slow condensing is more likely coming from the *workflow + model behavior* than raw codebase size. reviews tend to pull in a lot of context (diffs + related files + explanations), and some models will ‚Äúramble‚Äù more, which accelerates context growth and triggers condense.\n\na few practical tweaks:\n\n* keep each session to one task (new session per feature/bug)\n* scope reviews tightly: ‚Äúreview this file / these functions / this diff‚Äù\n* for bigger changes, use Architect/Ask to outline first, then Code to implement (less back-and-forth)\n\nif you can share where you‚Äôre running it (VS Code / JetBrains / CLI) and whether you‚Äôre asking for full-project reviews vs specific files, we can suggest the best flow for minimax/GLM.",
                  "score": 1,
                  "created_utc": "2026-02-06 13:06:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwt163",
      "title": "Codex 5.3 dropped",
      "subreddit": "kilocode",
      "url": "https://i.redd.it/yh0poj38xphg1.jpeg",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-02-05 18:20:14",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwt163/codex_53_dropped/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qxg477",
      "title": "Announcing the Kilo League",
      "subreddit": "kilocode",
      "url": "https://blog.kilo.ai/p/kilo-league",
      "author": "alokin_09",
      "created_utc": "2026-02-06 12:20:14",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qxg477/announcing_the_kilo_league/",
      "domain": "blog.kilo.ai",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qyzdbg",
      "title": "Opus 4.6 1M Context Option",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qyzdbg/opus_46_1m_context_option/",
      "author": "hegemonicht",
      "created_utc": "2026-02-08 04:29:40",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.88,
      "text": "I recently came over from cursor where I have been using Opus 4.6 with the 1M context window quite heavily for some big projects. Unfortunately, it seems like the 1M context window option isn‚Äôt available on Kilo when using my API key. Is there a way to enable it or am I just out of luck?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qyzdbg/opus_46_1m_context_option/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o4qrer7",
          "author": "joshcodeskilo",
          "text": "u/hegemonicht \\- 1M context is available for Opus 4.6! What provider were you using?\n\nhttps://preview.redd.it/tn0w0bgzisig1.png?width=628&format=png&auto=webp&s=2405ade909a87b5be528281d836797e439fa66af",
          "score": 1,
          "created_utc": "2026-02-11 04:10:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzg0b3",
      "title": "Obsessed with terminal after running a huge context window? (GLM 4.7 via Z.ai)",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qzg0b3/obsessed_with_terminal_after_running_a_huge/",
      "author": "Tank_Gloomy",
      "created_utc": "2026-02-08 18:06:32",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 0.99,
      "text": "As I said in the title, I'm currently running GLM 4.7 via [Z.ai](http://Z.ai) (directly from the provider) and I started getting weird issues as the context window grew and a couple of context compactions occurred.\n\nExample:\n\n    Date/time: 2026-02-08T18:01:03.733Z\n    Extension version: 5.4.1\n    Provider: zai\n    Model: glm-4.7\n    \n    Kilo Code tried to use write_to_file without value for required parameter 'path'. Retrying...\n\nThe weirdest thing that happened is that, after going back and forth with the model with messages like \"Try again\" and \"You're having trouble with your tools, why don't you split the task in smaller steps?\" it simply decided to create all the files using the terminal, which works but it's quite slow and had me actually use a wildcard to allow all commands except `rm` and `dd` but eh... not ideal.\n\nI've read about switching to **JSON-based tooling** but the option is missing on my end, and I'm running the latest version of the IDE extension.\n\nLiterally, every single request comes out like this for now:\n\nhttps://preview.redd.it/tdhh8nma9big1.png?width=500&format=png&auto=webp&s=741e8e52bea3c49af11357e2f19dcc7622182764\n\nOnce it switches modes again and grabs the next step from the TODO list, I guess it's gonna get back on track, as the context will reset, but I find it to be a super weird bug and I can't quite pinpoint as to whether this is on Kilo's integration or on Z.ai's model.\n\n**EDIT:** Yes, I can confirm that, after switching contexts, it DOES recover.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qzg0b3/obsessed_with_terminal_after_running_a_huge/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o4bb8rr",
          "author": "mrkillertoast",
          "text": "I try to always make new treads and avoid compaction as much as possible. You always lose some information, or only have partial information which leads to weird things. Whenever possible I try to only use about have of the context. The problem is, you're getting content rott when you have to much in your context window. \n\nHowever apart from that i also had some weird tool calling errors these days... i am not sure where they come from but things that perfectly worked (kimi k2.5) in new threats suddenly breaks. I hope this is only a short time phenomen.. \n\nI hope this comment helps you a bit.",
          "score": 3,
          "created_utc": "2026-02-08 20:35:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bc3hf",
              "author": "Tank_Gloomy",
              "text": "Yeah, I figured it'd probably be because of something lost in summarization, however, at one point it was going alright and randomly switched to using the terminal again (even before compaction), it's really weird.\n\nI'm guessing it's a problem with GLM itself, because I've seen it get stuck in similar loops (not quite the same, but things like weird random replacements of strings and then deciding to rollback by itself) when using it as a Claude Code provider.",
              "score": 1,
              "created_utc": "2026-02-08 20:39:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qwj70l",
      "title": "Codebase Indexing for Teams",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qwj70l/codebase_indexing_for_teams/",
      "author": "LupoZockt",
      "created_utc": "2026-02-05 11:48:17",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "Question:  \nLet's say i set up everything needed for Codebase Indexing (mxbai-embed-large as embedding model + Qdrant database on local dev server)  \nHow would it work if multiple devs want to use the same database? Won't the differences in the working copies of the different devs result in drifts that are constantly changed through automatic indexing?  \nWould it be somehow be possible to index the database from a master branch and make the rest read only?  \nI am not particular knowledgable in this field and don't have the time to invest in research - Maybe someone has a quick answer   \nThanks ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwj70l/codebase_indexing_for_teams/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qz5xkp",
      "title": "Pony alpha",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qz5xkp/pony_alpha/",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-02-08 10:48:26",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.71,
      "text": "Is pony alpha glm5?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qz5xkp/pony_alpha/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o48jskc",
          "author": "Barafu",
          "text": "No, glm5 is omegahorse.",
          "score": 2,
          "created_utc": "2026-02-08 11:31:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48nxn6",
              "author": "ReasonableReindeer24",
              "text": "This model is frontier model , I don't know where model comes from but it's have 200,000 context window",
              "score": 2,
              "created_utc": "2026-02-08 12:07:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48st07",
          "author": "Mayanktaker",
          "text": "Everyone is saying that this is GLM 5. Personality I think its not GLM. Its deepseek.",
          "score": 2,
          "created_utc": "2026-02-08 12:46:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o496slb",
              "author": "FireGuy324",
              "text": "So, deepseek distilled from GLM?",
              "score": 1,
              "created_utc": "2026-02-08 14:16:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4czh4d",
          "author": "yuyuyang1997",
          "text": "I believe it is.",
          "score": 1,
          "created_utc": "2026-02-09 02:02:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ehf1u",
          "author": "Bennie_Pie",
          "text": "Ask it for it's knowledge cut off date, it won't know it but it will answer \"I'm GLM, a large language model developed by Z.ai\"",
          "score": 1,
          "created_utc": "2026-02-09 08:24:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwsbi7",
      "title": "Opus 4.6",
      "subreddit": "kilocode",
      "url": "https://i.redd.it/jlyvimvrsphg1.jpeg",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-02-05 17:55:18",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwsbi7/opus_46/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3uuh5p",
          "author": "MaybeDisliked",
          "text": "It was just released, just try it out",
          "score": 2,
          "created_utc": "2026-02-06 06:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uz1on",
              "author": "ReasonableReindeer24",
              "text": "It's s good with 1 milion context window",
              "score": 1,
              "created_utc": "2026-02-06 06:39:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qynm4h",
      "title": "What of my AI subs can I use with Kilo?",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qynm4h/what_of_my_ai_subs_can_i_use_with_kilo/",
      "author": "Orinks",
      "created_utc": "2026-02-07 19:54:57",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I'm just curious. I have a Claude Max sub and an OpenAI sub. I have about $5 on my Kilo account now, but don't feel like topping it up at the moment. Is there any way to utalize my Claude Max/Codex subs with Kilo? Preparing for the release of Kilo Claw and possibly joining this Kilo League for a bit of fun.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qynm4h/what_of_my_ai_subs_can_i_use_with_kilo/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o44yam2",
          "author": "zekusmaximus",
          "text": "I think Minimax, ChatGPT, Kimi, GLM, there may be more but not Claude.",
          "score": 2,
          "created_utc": "2026-02-07 20:14:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45auxm",
          "author": "uxkelby",
          "text": "I use my GLM pro coding plan with Kilo, very happy with it.",
          "score": 1,
          "created_utc": "2026-02-07 21:23:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46kpm0",
          "author": "alexkgold",
          "text": "Unfortunately, Anthropic decided to restrict Claude Code CLI to official Claude Code clients in January 2026. Claude Code credentials cannot be used in Kilo Code or other third-party harnesses.\n\nFor continued use of Anthropic models in Kilo Code, please use the¬†[Anthropic API provider](https://kilo.ai/docs/ai-providers/anthropic)¬†with an API key instead.",
          "score": 1,
          "created_utc": "2026-02-08 01:53:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qykceq",
      "title": "Honest Developer Question",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qykceq/honest_developer_question/",
      "author": "ItchyAttorney5796",
      "created_utc": "2026-02-07 17:50:05",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": " \nHonest Question.  I see many casually speak of  investors when it comes to app developers.  How and why would an investor invest in our ideas when they can just take your idea and have it built themselves? Am I being an unjustified skeptical? Please explain.  If I'm wrong please provide details how it's done that benefits the developer..",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qykceq/honest_developer_question/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o447e1a",
          "author": "Solonotix",
          "text": "Because they get to invest for a fraction of the total cost. Let's say they agree to a 10% share of profits by investing $1M early on. Then the product does well and becomes valued at $100M. That investor just made $9M profit off a $1M gamble.\n\nThe problem with these things is that they are inherently high risk. You never know for sure that it'll be good, or that there isn't a competitor that will beat you to market. So, investors distribute that risk to improve the odds of getting at least one success.\n\nA developer can only write code for the thing they do. A business owner is strictly linked to their current business. But an investor can have hundreds of companies in their portfolio.",
          "score": 2,
          "created_utc": "2026-02-07 17:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44bmgh",
              "author": "ItchyAttorney5796",
              "text": "Thank You. I've been gifted. Original visionary ideas come extremely easy for me and i produce using vibe and kilo coding.  How would one get ideas on front of investors.",
              "score": 1,
              "created_utc": "2026-02-07 18:19:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44ctc5",
                  "author": "Solonotix",
                  "text": "That's the reason groups like Y Combinator exist. Rich people know \"the poors\" want their money. As such, they don't socialize with them. You need to already have connections to these rich people (see Bill Gates, Jeff Bezos, Elon Musk, etc.). Y Combinator is a group of investors that invite new startups. If you can convince them of your ideas, then you might just get funding.\n\nHowever, investors rarely let you do your own thing without comment. They will likely put constraints on what you can do with the funding, or require that you achieve some specific feature. So it isn't just a free money offer either.",
                  "score": 1,
                  "created_utc": "2026-02-07 18:25:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47th5o",
          "author": "sand_scooper",
          "text": "They are investing in the customers.\n\nWhy did Mark Zuckerberg buy Instagram back then?\n\nWhy did Google buy YouTube?\n\nI can easily list thousands of examples\n\nThey could easily build their own version.\n\nThey have all the money and talent in the world.\n\nThey are investing in the PEOPLE who already use the app.\n\nIt's exactly why Kilo Code is getting investments.\n\nPeople know about it and use it.",
          "score": 1,
          "created_utc": "2026-02-08 07:24:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwmgt6",
      "title": "Problem with Kimi Code plan?",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qwmgt6/problem_with_kimi_code_plan/",
      "author": "thehedonistvagabond",
      "created_utc": "2026-02-05 14:17:54",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "It keeps throwing 400 error codes when I try to load the Kimi Code plan into Kilo Code. Is this expected?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwmgt6/problem_with_kimi_code_plan/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3qwqxw",
          "author": "hlacik",
          "text": "its a known bug, there is already a merge request in kilocode repository that is fixing it. you will just have to wait till they will release it (unless you want to compile it yourself)\n\n[https://github.com/Kilo-Org/kilocode/pull/5544](https://github.com/Kilo-Org/kilocode/pull/5544)",
          "score": 1,
          "created_utc": "2026-02-05 17:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qzsw1",
              "author": "hlacik",
              "text": "update: 5.3.3 has been released with a fix.",
              "score": 1,
              "created_utc": "2026-02-05 17:22:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w5o5l",
          "author": "itsnath1",
          "text": "not expected. 400s usually mean the request is getting rejected during validation, not that the plan itself is unsupported.\n\ncan you share where this is happening (VS Code / JetBrains / CLI / web) and the exact error text? that‚Äôll help narrow down if it‚Äôs a config/provider issue or something we need to track.",
          "score": 1,
          "created_utc": "2026-02-06 12:48:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}