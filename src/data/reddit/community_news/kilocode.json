{
  "metadata": {
    "last_updated": "2026-02-08 08:47:11",
    "time_filter": "week",
    "subreddit": "kilocode",
    "total_items": 13,
    "total_comments": 37,
    "file_size_bytes": 45668
  },
  "items": [
    {
      "id": "1qw80sm",
      "title": "Subscription question - Kimi 2.5 vs Minimax 2.1 vs Glm 4.7",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qw80sm/subscription_question_kimi_25_vs_minimax_21_vs/",
      "author": "Impossible_Tax8875",
      "created_utc": "2026-02-05 01:50:41",
      "score": 25,
      "num_comments": 36,
      "upvote_ratio": 0.97,
      "text": "Hi everyone, need your suggestion on coding model. Could you please help me to decide between these 3 models? \n\nI'm a backend dev, and primarily need the AI assistance for frontend work. Below are my priorities: \n\n1. Good compatibility with Kilo Code  \n2. Good on architectural thinking  \n3. 2-3 hrs coding sessions daily - low/moderate use  \n4. Having vision support will be helpful\n\nKimi - I've tried Kimi for free last week and overall liked it.   \nMiniMax - I've not used Minimax yet, but community response look good. Also pricing look good.   \nGLM - I'm getting a lot of mixed reviews. Pricewise (lite) looks attractive, however, don't know how fast I'll hit the limit. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qw80sm/subscription_question_kimi_25_vs_minimax_21_vs/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3n531l",
          "author": "feral_user_",
          "text": "I'd say Kimi 2.5 Thinking model would be the best. A backup model would be GLM 4.7 - Avoid MiniMax, at least from my limited experience. Not worth it.",
          "score": 8,
          "created_utc": "2026-02-05 01:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u4adq",
              "author": "Impossible_Tax8875",
              "text": "Thanks. ",
              "score": 1,
              "created_utc": "2026-02-06 03:01:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nnwnl",
          "author": "gchello921",
          "text": "# Kimi 2.5 outperforms Minimax 2.1 and GLM 4.7.",
          "score": 7,
          "created_utc": "2026-02-05 03:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pm9p6",
          "author": "itsnath1",
          "text": "Kilo team here üëã\n\nfor your use case, all three models work fine with kilo, so compatibility shouldn‚Äôt be an issue.\n\n* **MiniMax 2.1** is a solid daily driver if you‚Äôre coding a few hours a day and want good value\n* **Kimi 2.5** makes sense if vision (screenshots / UI debugging) is important to you\n* **GLM 4.7** can be useful for higher-level planning, but feedback varies depending on task type\n\na lot of devs just pick one as a default and switch models when they need something specific, kilo makes that pretty easy.\n\nhope that helps üëç",
          "score": 7,
          "created_utc": "2026-02-05 13:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub2wz",
              "author": "Impossible_Tax8875",
              "text": "thanks, I try other 2 models",
              "score": 1,
              "created_utc": "2026-02-06 03:43:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3vkwmb",
              "author": "Which_Slice1600",
              "text": "Hi i appreciate your answer but also hope it's bit more informative. For example, if ignoring multimodal, which is the most capable of planning?",
              "score": 1,
              "created_utc": "2026-02-06 10:02:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w2p2g",
                  "author": "itsnath1",
                  "text": "Good question, if we‚Äôre talking strictly planning/reasoning (ignoring multimodal), the stronger models right now tend to be the ones optimized for long-context reasoning and structured thinking.\n\nin kilo you can try this pretty easily by switching models in Architect or Orchestrator mode and seeing which one breaks down tasks more cleanly for your use case. those modes are specifically tuned for planning vs implementation.\n\nthere isn‚Äôt a single ‚Äúbest‚Äù answer since it depends on whether you care more about depth, speed, or cost, but the nice part is you‚Äôre not locked in, you can swap between models and compare outputs on the same prompt.",
                  "score": 1,
                  "created_utc": "2026-02-06 12:28:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3yk2iw",
              "author": "Previous_Nature_5319",
              "text": "kilo does not work with a kimi subscription, returns error 400, and it takes a long time to fix. [https://github.com/Kilo-Org/kilocode/pull/5662](https://github.com/Kilo-Org/kilocode/pull/5662)",
              "score": 1,
              "created_utc": "2026-02-06 19:58:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ohrex",
          "author": "sonnivasquez",
          "text": "Kimi is perfect if you need a hand creating a feature and GLM is better for developing, Minimax is useful for fixing bugs",
          "score": 3,
          "created_utc": "2026-02-05 07:29:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vl0o1",
              "author": "Which_Slice1600",
              "text": "What's the diff between creating a feature and developing. Developing the whole stuff?",
              "score": 1,
              "created_utc": "2026-02-06 10:03:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3net7m",
          "author": "Level-Dig-4807",
          "text": "How long is Kimi going to be free? It started for a week, have they extended it",
          "score": 2,
          "created_utc": "2026-02-05 02:49:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nqmd4",
              "author": "fullmetal-ai",
              "text": "already free ended",
              "score": 3,
              "created_utc": "2026-02-05 04:00:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3p0efy",
                  "author": "Level-Dig-4807",
                  "text": "Ohhk maybe on Kilocode, it's still free on few other tools",
                  "score": 1,
                  "created_utc": "2026-02-05 10:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3vlbr0",
              "author": "Which_Slice1600",
              "text": "Usually 1-2 wks from beginning id say. Kimi is a big model so is its inference cost. coding plan also priced the highest among the three. However i am just not good enough in dev and couldn't tell the diff in their performance¬†",
              "score": 1,
              "created_utc": "2026-02-06 10:06:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nt1ah",
          "author": "ELPascalito",
          "text": "1 Trillion params Vs 230 Billion params Vs 355 Billion params, size does matter after all, believe it or not",
          "score": 2,
          "created_utc": "2026-02-05 04:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oita9",
              "author": "AppealSame4367",
              "text": "And then you have Step 3.5 Flash and Qwen Coder Next...",
              "score": 1,
              "created_utc": "2026-02-05 07:39:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3o71rj",
          "author": "RonJonBoviAkaRonJovi",
          "text": "Kimi wins here but codex or Claude code would be a smarter option",
          "score": 2,
          "created_utc": "2026-02-05 05:57:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nxxz0",
          "author": "justind00000",
          "text": "I can't give you a comparison, but I can say you will probably never hit a limit with the glm.",
          "score": 1,
          "created_utc": "2026-02-05 04:49:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ol81r",
          "author": "sand_scooper",
          "text": "Minimax and GLM is free now in Kilo Code.\n\nJust try it out.\n\nThey're both sometimes good and sometimes bad.\n\nIt's hard to really tell which one is better overall.  \nBenchmarks don't really mean anything.  \nAnd reddit is full of fake comments and bots hyping up their own model.  \nSometimes it's people who wants to earn referral rewards so they'll tell you it's great.  \nSometimes it's newbies who have no idea what they're talking about.\n\nThat's the problem with these open source Chinese AI models.\n\nIt's cheaper but not as consistent and powerful as GPT 5.2 or Opus 4.5.",
          "score": 1,
          "created_utc": "2026-02-05 08:02:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub7ak",
              "author": "Impossible_Tax8875",
              "text": "Thanks, yes I'll check the GLM in kilo. I liked Opus, but the limits are too low. ",
              "score": 1,
              "created_utc": "2026-02-06 03:44:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oqg1l",
          "author": "GreenGreasyGreasels",
          "text": "If you already have a main model and want to suppliment it I would go for GLM. It's cheap it's capable and you won't have to worry about limits. Minimax is not as careful, and Kimi is a little too expensive. Claude or GPT plus GLM would be my recommendation.",
          "score": 1,
          "created_utc": "2026-02-05 08:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40gckj",
              "author": "Impossible_Tax8875",
              "text": "I do not have any main model yet, any recommendation. Also, do you prefer to get the subscription with third party like OpenRouter or directly from the development company? ",
              "score": 1,
              "created_utc": "2026-02-07 02:11:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o42ai2c",
                  "author": "GreenGreasyGreasels",
                  "text": "I would suggest you go with Claude from Anthropic. Get the 20 dollar sub (never use GPT-5 or Claude models over OpenRouter - its insanely expensive). Claude is generally easier to use and understands you better than GPT-5. GPT 5 is much better at the cutting edge of capabilities for Coding/STEM etc. - but if you don't intend to push those then the pleasant Claude user experience and its superior front end work is likely more suitable for you. \n\nMy personal recommendation would be you go with GitHub's 10 dollar plan that offers access to both GPT and Claude (plus Gemini and Grok if you are so inclined). It is cheap, lets you try out all models to find which is suitable for you. \n\nIn general I would always prefer getting them for the OEM Vendor, for two reasons (they usually have the best properly configured hosting and usually offer subscription plans that make it economical to use). So for GLM-4.6 I would prefer plan from Zai, and not through OpenRouter.",
                  "score": 1,
                  "created_utc": "2026-02-07 11:16:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oriwe",
          "author": "mrkillertoast",
          "text": "I find Kimi 2.5 overall really good. I use minimax mostly as orchestrator. As for GLM i tried it severall times but got really inconsistent results and had a lot of bugs...",
          "score": 1,
          "created_utc": "2026-02-05 09:02:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r1la0",
          "author": "vipinpg",
          "text": "I used Kimi 2.5 and GLM 4.7, and in my case, I liked Kimi more. I found it could handle better reasoning. However, there is a 5-hour plus weekly limit applied to Kimi, whereas GLM has only 5 hour limit. I was on Kimi's 7-day trial, and my weekly limit was exhausted in 2 days. So, I just canceled as it gets more expensive.",
          "score": 1,
          "created_utc": "2026-02-05 17:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t37nz",
          "author": "jedruch",
          "text": "Kimi 2.5 and for me it's not even close in terms of performance. With your time target you should be fine, although they made some changes in weekly limits recently and I have not figured them out yet. \n\nAlso Kimi CLI runs Agent Swarm so if you need to do simple changes in multiple files (variable names, etc) I'd consider it the best tool to go \n\nBetween GLM and Minimax I'd go with GLM, however I consider both of them as suitable for easy and short coding tasks. Risky in terms of architectural thinking.",
          "score": 1,
          "created_utc": "2026-02-05 23:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u5arc",
          "author": "spukhaftewirkungen",
          "text": "I've had good results with Kimi & GLM via Kilo, neither is perfect though and I still find myelf using other things. I'd say either would be good daily drivers, but keep your options open, I think we're still waiting on the 1 model that can do it all, all the time.",
          "score": 1,
          "created_utc": "2026-02-06 03:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vylvk",
          "author": "OnyxianSoul",
          "text": "Mmmm, I've found gm 4.7 to be the smartest, but minimax is a lot quicker, i'd suggest planing with glm 4.7 and executing with minimax, i also found qwen next instruct to be pretty good and is also very fast.",
          "score": 1,
          "created_utc": "2026-02-06 11:59:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y69av",
          "author": "Bob5k",
          "text": "Kimi via their sub to be used efficiently requires at least 39$ plan. I'd say go with minimax as it's way faster - maybe not smarter but in 98% of cases your output will be better as you'll be able to iterate over things quickly.",
          "score": 1,
          "created_utc": "2026-02-06 18:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43963i",
          "author": "Classic-Ninja-1",
          "text": "if architectural thinking is your top priority then kimi 2.5 can be your best model but it can be slow sometime. i actually think that if you pair GLM 4.7 with the Traycer it can be a better option because as i ahve seen GLM 4.7 is fast and Traycer define the component hierarchy and state logic also know as Architecture. it solves your problem more efficiently.",
          "score": 1,
          "created_utc": "2026-02-07 15:09:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o477jec",
              "author": "Nice_Match_6215",
              "text": "Can you please explain more on this?",
              "score": 1,
              "created_utc": "2026-02-08 04:22:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o47bvjl",
                  "author": "Classic-Ninja-1",
                  "text": "Yes sure as GLM 4.7 is much faster and responds quickly and Traycer focuses on defining component hierarchy and state management, which is essentially the application architecture.Together, GLM 4.7 handles speed and execution, while Traycer ensures clean architectural structure.Because this combination balances performance + architecture, it often solves problems more efficiently.",
                  "score": 1,
                  "created_utc": "2026-02-08 04:54:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o480hai",
          "author": "Driver_Octa",
          "text": "For frontend help, pick a model that actually understands UI patterns and context Kimi and MiniMax both seem solid from what people report, and GLM can be decent for lightweight sessions. Pairing your model with tools like Cursor or Traycer AI in VS Code really helps when you want *architectural reasoning* and understandable diffs instead of just raw output...",
          "score": 1,
          "created_utc": "2026-02-08 08:29:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nxtl5",
          "author": "mcowger",
          "text": "Go signup with synthetic.new and get all 3",
          "score": 1,
          "created_utc": "2026-02-05 04:48:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvv5va",
      "title": "I ported \"Get Shit Done\" (GSD) from Claude Code to Kilo Code ‚Äî same powerful workflow, now works natively in VS Code",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qvv5va/i_ported_get_shit_done_gsd_from_claude_code_to/",
      "author": "ukshaa",
      "created_utc": "2026-02-04 17:39:35",
      "score": 15,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "If you've seen \\[glittercowboy's Get Shit Done\\](https://github.com/glittercowboy/get-shit-done) for Claude Code, you know it's one of the best context engineering systems out there for AI-assisted development.\n\n\n\nI loved the methodology so much that I ported it to \\*\\*Kilo Code\\*\\*.\n\n\n\n\\*\\*What is GSD?\\*\\*\n\n\\- A spec-driven development workflow with phases: discuss ‚Üí plan ‚Üí execute ‚Üí verify\n\n\\- Context engineering that gives the AI exactly what it needs, when it needs it\n\n\\- Goal-backward verification (checks outcomes, not just task completion)\n\n\\- Atomic commits per task for clean git history\n\n\n\n\\*\\*What the Kilo Code fork includes:\\*\\*\n\n\\- Custom Modes for each GSD agent (planner, executor, verifier, debugger, etc.)\n\n\\- Skills and workflows adapted to Kilo Code's tools\n\n\\- Full MCP server integration support\n\n\\- Same powerful methodology, different AI backend\n\n\n\nIf you're using Kilo Code and want a structured way to ship projects with AI, give it a try.\n\n\n\nüîó \\*\\*Kilo Code fork:\\*\\* [https://github.com/punal100/get-stuff-done-for-kilocode](https://github.com/punal100/get-stuff-done-for-kilocode)\n\n\n\nüîó \\*\\*Original GSD (Claude Code):\\*\\* [https://github.com/glittercowboy/get-shit-done](https://github.com/glittercowboy/get-shit-done)\n\n\n\nAll credit for the methodology goes to glittercowboy ‚Äî I just adapted the tooling.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qvv5va/i_ported_get_shit_done_gsd_from_claude_code_to/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3mgvqf",
          "author": "demeetch",
          "text": "I appreciate you creating this and think it would be really useful for a project I have been working on.   \n  \nI tried having it map my current codebase however, it keeps getting stuck in a loop where it condenses it's context, re-reads files it already read prior to condensing, then the loop continues. Context7 is installed and I increased my context capacity on Kilocode to no avail.   \nMore than likely a user error on my part. This is my first time using GSD so no experience with it on Claude. Do you have any suggestions? Is there prompt instructions I should've read before?   \nThank you in advance.",
          "score": 2,
          "created_utc": "2026-02-04 23:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ng24z",
              "author": "ukshaa",
              "text": "It might struggle, if the project is a bit big,\n\n\nI suggest enabling\nCode Base Indexing,\nYou can run Text Embedding Model like,\nQwen3-Embedding-0.6B in Docker Locally,\nit takes less than 1 GB of GPU VRAM.\n\n\nSo far it helped me a bit,\n\n\nYou can try this One\nhttps://github.com/punal100/get-stuff-done-for-github-copilot\n\n\nWith Claude Opus 4.5 Model,\n\n\nMight help,\nAssuming the Project is not too Big.",
              "score": 3,
              "created_utc": "2026-02-05 02:56:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3l9zgu",
          "author": "Solonotix",
          "text": "It's times like these I wonder if I'm just not the target audience for these things, lol. First line of How-To on GSD says \"Claude Code should run with the `--dangerously-skip-permissions` flag\". Maybe if I was running this in a sandbox, like Daytona or something. But right there in user-land on my machine with all my permissions? Heh, nope.",
          "score": 2,
          "created_utc": "2026-02-04 20:04:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nehc9",
              "author": "ukshaa",
              "text": "I personally do not run with full permission,\nI keep these Disabled, so that these ask for permission before running.\n1. Write File .\n2. Execute(Terminal).\n3. Create Subagent.",
              "score": 3,
              "created_utc": "2026-02-05 02:47:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lyij7",
          "author": "Oshden",
          "text": "This looks awesome OP. Any ideas on how to use the discoveries from the original branch to improve upon your fork?",
          "score": 1,
          "created_utc": "2026-02-04 22:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ngmru",
              "author": "ukshaa",
              "text": "So far these are ported from the original,\n\"agents\", \"get-shit-done/workflows\", \"commands/gsd\", \"get-shit-done/references\" with the Kilo code Equivalent,\n\n\nWill try to port out anything that got left over.",
              "score": 2,
              "created_utc": "2026-02-05 02:59:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvmgao",
      "title": "What We Learned from a Week of Free Kimi K2.5",
      "subreddit": "kilocode",
      "url": "https://blog.kilo.ai/p/what-we-learned-from-a-week-of-free",
      "author": "alokin_09",
      "created_utc": "2026-02-04 11:48:19",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qvmgao/what_we_learned_from_a_week_of_free_kimi_k25/",
      "domain": "blog.kilo.ai",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwqno6",
      "title": "Local llm for kilocode",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qwqno6/local_llm_for_kilocode/",
      "author": "Late_Special_6705",
      "created_utc": "2026-02-05 16:55:48",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 0.91,
      "text": "What is the best local model for a 12gb graphics card?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwqno6/local_llm_for_kilocode/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3r1ngo",
          "author": "MaybeDisliked",
          "text": "Maybe a low fp version of Qwen3, 4b maybe? Just play with it and see what runs: https://ollama.com/library/qwen3",
          "score": 3,
          "created_utc": "2026-02-05 17:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rnemr",
          "author": "Express_Quail_1493",
          "text": "HELL YEA MAN. Its great to see more devs going localhost. Here is my setup ->\n\n[https://huggingface.co/bartowski/mistralai\\_Ministral-3-14B-Reasoning-2512-GGUF](https://huggingface.co/bartowski/mistralai_Ministral-3-14B-Reasoning-2512-GGUF) i run in LMstudio i use ministral-3-14b-reasoning download the q6 its very precise with tool calling accuracy and doesn't fail with the edit-diff too often almost never get stuck in loops most small models are really bad with this. if you want it smaller you can run it at q4 but it might have 1 or 2 hiccups with the tool calling failures but still useable.  \ninside my code lookup database i use the icon inside kilo on the bottom right. i use nomic-embed-text-v2-moe to allow the brain(ministral) search codebase context(eyesight) this works well. I give it a [scratchpad.md](http://scratchpad.md) file to keep track of its work for when context window compressed and i tell it its the main point of entry and to keep it clean. (allows more autonomous work for longer handsfree) i tried many models and ministral-14b outperformed 32b models. on a 12gbvram you can run at least 20k token easy bartowski makes really small quantization sized so i go for his models -> [https://huggingface.co/bartowski/mistralai\\_Ministral-3-14B-Reasoning-2512-GGUF](https://huggingface.co/bartowski/mistralai_Ministral-3-14B-Reasoning-2512-GGUF)",
          "score": 3,
          "created_utc": "2026-02-05 19:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rxild",
              "author": "Late_Special_6705",
              "text": "217Gb llm?",
              "score": 1,
              "created_utc": "2026-02-05 19:58:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o40qyjo",
                  "author": "mrlegendanny",
                  "text": "The largest model I found in the links above was 14gb, where did you get 217?",
                  "score": 1,
                  "created_utc": "2026-02-07 03:18:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3w7vr6",
          "author": "itsnath1",
          "text": "Depends what you‚Äôre optimizing for (speed vs quality), but on a 12GB GPU you‚Äôll generally have the best experience with a 7B‚Äì8B ‚Äúcoder‚Äù model (or a small MoE) in 4-bit.\n\nkilo supports 500+ models via OpenRouter + direct providers, but if you mean *truly local* (running on your own machine), you‚Äôll want to pick a lightweight coding model that fits in VRAM and exposes an OpenAI-compatible endpoint, then point kilo at it.\n\nif you share your GPU + OS + whether you‚Äôre using vs code or the cli, people can recommend a specific model + setup.",
          "score": 2,
          "created_utc": "2026-02-06 13:02:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41gag6",
          "author": "IsSeMi",
          "text": "I have a Mac studio M2 Ultra 64G and a windows PC with rtx 3090 24G + 64G of RAM and I didn't find anything useful since I started my experiments. Models stuck in loops.They are unloading (I don't know why). Maybe I can't find something working because my project is big for them.",
          "score": 1,
          "created_utc": "2026-02-07 06:26:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx57x2",
      "title": "Kilo Code using token too quick and taking time to condense context",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qx57x2/kilo_code_using_token_too_quick_and_taking_time/",
      "author": "iru786",
      "created_utc": "2026-02-06 02:22:14",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Hi\n\nI am new to this but what I am noticing that if I ask kilo code to review a code or functionality, it's  using token too quickly and than it starts condensing context. Sometimes it takes ages to do that and I have also noticed instance where it has details of wrong tasks. \n\nI am using minimax 2.1 and GLM with latest version of killo code. \n\nAnyone else noticed similar issues? \n\nThanks \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qx57x2/kilo_code_using_token_too_quick_and_taking_time/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3w69yc",
          "author": "itsnath1",
          "text": "Yeah, you‚Äôre not the only one seeing that. during large reviews the context can grow fast, and kilo will condense it to keep the session usable. depending on the model (especially smaller-context ones), that can take a bit and sometimes mix details between tasks.\n\na couple things that help: start a fresh session per task and keep reviews scoped to specific files/functions so the context stays tighter. you can also use Architect/Ask first to plan, then switch to Code for the actual changes.\n\nif it feels off, sharing your IDE/CLI + repo size would help us understand if it‚Äôs model behavior or something we should track.",
          "score": 1,
          "created_utc": "2026-02-06 12:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w7nkd",
              "author": "iru786",
              "text": "I am still a newbie at this so may be i am not using it optimally. As requested, please find the details below...  \n  \ncount: 327  \nsize: 1.58 MiB  \nin-pack: 4676  \npacks: 1  \nsize-pack: 16.99 MiB  \nprune-packable: 0  \ngarbage: 0  \nsize-garbage: 0 bytes\n\n",
              "score": 1,
              "created_utc": "2026-02-06 13:00:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w8ndu",
                  "author": "itsnath1",
                  "text": "Thanks ‚Äî that repo size is pretty small, so burning tokens + slow condensing is more likely coming from the *workflow + model behavior* than raw codebase size. reviews tend to pull in a lot of context (diffs + related files + explanations), and some models will ‚Äúramble‚Äù more, which accelerates context growth and triggers condense.\n\na few practical tweaks:\n\n* keep each session to one task (new session per feature/bug)\n* scope reviews tightly: ‚Äúreview this file / these functions / this diff‚Äù\n* for bigger changes, use Architect/Ask to outline first, then Code to implement (less back-and-forth)\n\nif you can share where you‚Äôre running it (VS Code / JetBrains / CLI) and whether you‚Äôre asking for full-project reviews vs specific files, we can suggest the best flow for minimax/GLM.",
                  "score": 1,
                  "created_utc": "2026-02-06 13:06:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwt163",
      "title": "Codex 5.3 dropped",
      "subreddit": "kilocode",
      "url": "https://i.redd.it/yh0poj38xphg1.jpeg",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-02-05 18:20:14",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwt163/codex_53_dropped/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qvc7sz",
      "title": "qwen coder next",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qvc7sz/qwen_coder_next/",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-02-04 02:31:21",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.86,
      "text": "i have not seen this model on kilocode yet \n\nhttps://preview.redd.it/qp6lk7e03ehg1.png?width=1915&format=png&auto=webp&s=1f49f577b8be2a2f8f4ce2923622d2fc9afb5411\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qvc7sz/qwen_coder_next/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3i7ym7",
          "author": "MaybeDisliked",
          "text": "It's in kilo! Maybe restart your IDE/CLI",
          "score": 3,
          "created_utc": "2026-02-04 10:09:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w7n93",
          "author": "itsnath1",
          "text": "yep, looks like it‚Äôs already available. if it‚Äôs not showing for you, try restarting the IDE/CLI so the model list refreshes.\n\nkilo pulls models dynamically from providers, so sometimes new ones don‚Äôt appear until a reload.",
          "score": 1,
          "created_utc": "2026-02-06 13:00:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxg477",
      "title": "Announcing the Kilo League",
      "subreddit": "kilocode",
      "url": "https://blog.kilo.ai/p/kilo-league",
      "author": "alokin_09",
      "created_utc": "2026-02-06 12:20:14",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qxg477/announcing_the_kilo_league/",
      "domain": "blog.kilo.ai",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwj70l",
      "title": "Codebase Indexing for Teams",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qwj70l/codebase_indexing_for_teams/",
      "author": "LupoZockt",
      "created_utc": "2026-02-05 11:48:17",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Question:  \nLet's say i set up everything needed for Codebase Indexing (mxbai-embed-large as embedding model + Qdrant database on local dev server)  \nHow would it work if multiple devs want to use the same database? Won't the differences in the working copies of the different devs result in drifts that are constantly changed through automatic indexing?  \nWould it be somehow be possible to index the database from a master branch and make the rest read only?  \nI am not particular knowledgable in this field and don't have the time to invest in research - Maybe someone has a quick answer   \nThanks ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwj70l/codebase_indexing_for_teams/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qwsbi7",
      "title": "Opus 4.6",
      "subreddit": "kilocode",
      "url": "https://i.redd.it/jlyvimvrsphg1.jpeg",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-02-05 17:55:18",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwsbi7/opus_46/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3uuh5p",
          "author": "MaybeDisliked",
          "text": "It was just released, just try it out",
          "score": 2,
          "created_utc": "2026-02-06 06:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uz1on",
              "author": "ReasonableReindeer24",
              "text": "It's s good with 1 milion context window",
              "score": 1,
              "created_utc": "2026-02-06 06:39:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qynm4h",
      "title": "What of my AI subs can I use with Kilo?",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qynm4h/what_of_my_ai_subs_can_i_use_with_kilo/",
      "author": "Orinks",
      "created_utc": "2026-02-07 19:54:57",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I'm just curious. I have a Claude Max sub and an OpenAI sub. I have about $5 on my Kilo account now, but don't feel like topping it up at the moment. Is there any way to utalize my Claude Max/Codex subs with Kilo? Preparing for the release of Kilo Claw and possibly joining this Kilo League for a bit of fun.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qynm4h/what_of_my_ai_subs_can_i_use_with_kilo/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o44yam2",
          "author": "zekusmaximus",
          "text": "I think Minimax, ChatGPT, Kimi, GLM, there may be more but not Claude.",
          "score": 2,
          "created_utc": "2026-02-07 20:14:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45auxm",
          "author": "uxkelby",
          "text": "I use my GLM pro coding plan with Kilo, very happy with it.",
          "score": 1,
          "created_utc": "2026-02-07 21:23:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46kpm0",
          "author": "alexkgold",
          "text": "Unfortunately, Anthropic decided to restrict Claude Code CLI to official Claude Code clients in January 2026. Claude Code credentials cannot be used in Kilo Code or other third-party harnesses.\n\nFor continued use of Anthropic models in Kilo Code, please use the¬†[Anthropic API provider](https://kilo.ai/docs/ai-providers/anthropic)¬†with an API key instead.",
          "score": 1,
          "created_utc": "2026-02-08 01:53:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qykceq",
      "title": "Honest Developer Question",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qykceq/honest_developer_question/",
      "author": "ItchyAttorney5796",
      "created_utc": "2026-02-07 17:50:05",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": " \nHonest Question.  I see many casually speak of  investors when it comes to app developers.  How and why would an investor invest in our ideas when they can just take your idea and have it built themselves? Am I being an unjustified skeptical? Please explain.  If I'm wrong please provide details how it's done that benefits the developer..",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qykceq/honest_developer_question/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o447e1a",
          "author": "Solonotix",
          "text": "Because they get to invest for a fraction of the total cost. Let's say they agree to a 10% share of profits by investing $1M early on. Then the product does well and becomes valued at $100M. That investor just made $9M profit off a $1M gamble.\n\nThe problem with these things is that they are inherently high risk. You never know for sure that it'll be good, or that there isn't a competitor that will beat you to market. So, investors distribute that risk to improve the odds of getting at least one success.\n\nA developer can only write code for the thing they do. A business owner is strictly linked to their current business. But an investor can have hundreds of companies in their portfolio.",
          "score": 2,
          "created_utc": "2026-02-07 17:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44bmgh",
              "author": "ItchyAttorney5796",
              "text": "Thank You. I've been gifted. Original visionary ideas come extremely easy for me and i produce using vibe and kilo coding.  How would one get ideas on front of investors.",
              "score": 1,
              "created_utc": "2026-02-07 18:19:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44ctc5",
                  "author": "Solonotix",
                  "text": "That's the reason groups like Y Combinator exist. Rich people know \"the poors\" want their money. As such, they don't socialize with them. You need to already have connections to these rich people (see Bill Gates, Jeff Bezos, Elon Musk, etc.). Y Combinator is a group of investors that invite new startups. If you can convince them of your ideas, then you might just get funding.\n\nHowever, investors rarely let you do your own thing without comment. They will likely put constraints on what you can do with the funding, or require that you achieve some specific feature. So it isn't just a free money offer either.",
                  "score": 1,
                  "created_utc": "2026-02-07 18:25:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47th5o",
          "author": "sand_scooper",
          "text": "They are investing in the customers.\n\nWhy did Mark Zuckerberg buy Instagram back then?\n\nWhy did Google buy YouTube?\n\nI can easily list thousands of examples\n\nThey could easily build their own version.\n\nThey have all the money and talent in the world.\n\nThey are investing in the PEOPLE who already use the app.\n\nIt's exactly why Kilo Code is getting investments.\n\nPeople know about it and use it.",
          "score": 1,
          "created_utc": "2026-02-08 07:24:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwmgt6",
      "title": "Problem with Kimi Code plan?",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1qwmgt6/problem_with_kimi_code_plan/",
      "author": "thehedonistvagabond",
      "created_utc": "2026-02-05 14:17:54",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "It keeps throwing 400 error codes when I try to load the Kimi Code plan into Kilo Code. Is this expected?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1qwmgt6/problem_with_kimi_code_plan/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "o3qwqxw",
          "author": "hlacik",
          "text": "its a known bug, there is already a merge request in kilocode repository that is fixing it. you will just have to wait till they will release it (unless you want to compile it yourself)\n\n[https://github.com/Kilo-Org/kilocode/pull/5544](https://github.com/Kilo-Org/kilocode/pull/5544)",
          "score": 1,
          "created_utc": "2026-02-05 17:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qzsw1",
              "author": "hlacik",
              "text": "update: 5.3.3 has been released with a fix.",
              "score": 1,
              "created_utc": "2026-02-05 17:22:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w5o5l",
          "author": "itsnath1",
          "text": "not expected. 400s usually mean the request is getting rejected during validation, not that the plan itself is unsupported.\n\ncan you share where this is happening (VS Code / JetBrains / CLI / web) and the exact error text? that‚Äôll help narrow down if it‚Äôs a config/provider issue or something we need to track.",
          "score": 1,
          "created_utc": "2026-02-06 12:48:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}