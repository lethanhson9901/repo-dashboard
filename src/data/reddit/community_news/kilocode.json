{
  "metadata": {
    "last_updated": "2026-01-02 12:35:57",
    "time_filter": "week",
    "subreddit": "kilocode",
    "total_items": 7,
    "total_comments": 28,
    "file_size_bytes": 32997
  },
  "items": [
    {
      "id": "1pumfvr",
      "title": "GLM-4.7 is now available in Kilo Code",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1pumfvr/glm47_is_now_available_in_kilo_code/",
      "author": "alokin_09",
      "created_utc": "2025-12-24 12:37:33",
      "score": 26,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "The model features upgrades in two key areas: enhanced programming capabilities and more stable multi-step reasoning/execution, especially around executing complex agent tasks.\n\n[](https://substackcdn.com/image/fetch/$s_!Fn5Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aefc6bf-030f-452e-aa8e-6b84ab0899d8_4426x3128.png)\n\n[Z.AI](http://Z.AI) is going head to head with the major SOTA model providers withÂ [GLM-4.7](https://docs.z.ai/guides/llm/glm-4.7). Two overall improvements stood out to us:\n\n* **Enhanced Programming Capabilities**: Substantially improves model performance in multi-language coding and terminal agent applications. GLM-4.7 now implements a â€œthink before actingâ€ mechanism within programming frameworks like Kilo, delivering more stable performance on complex tasks\n* **Enhanced reasoning capabilities**: Significantly improved mathematical and reasoning skills, achievingÂ **42.8% on the HLE**Â (â€œHuman Last Examâ€) benchmarkâ€”aÂ **41% increase**Â over GLM-4.6 and surpassing GPT-5.1 \n\nMore details -> [https://blog.kilo.ai/p/new-in-kilo-code-glm-47](https://blog.kilo.ai/p/new-in-kilo-code-glm-47) \n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1pumfvr/glm47_is_now_available_in_kilo_code/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "nvpnu0n",
          "author": "FriendlyUser_",
          "text": "great news!",
          "score": 2,
          "created_utc": "2025-12-24 13:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpvi2z",
          "author": "ianxiao",
          "text": "Don't break thinking mode for several months this time.",
          "score": 2,
          "created_utc": "2025-12-24 14:04:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqwlrt",
          "author": "jugac64",
          "text": "Awesome! Thanks to the z.ai and Kilo Teams!",
          "score": 1,
          "created_utc": "2025-12-24 17:29:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt470l",
          "author": "Federal_Spend2412",
          "text": "Thanks!! Has your team conducted any in-depth testing of the glm4.7 ? Is it truly close to Sonnet 4.5? Thank you.",
          "score": 1,
          "created_utc": "2025-12-25 01:43:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtkdrt",
              "author": "ShitShirtSteve",
              "text": "Curious about this as well!",
              "score": 1,
              "created_utc": "2025-12-25 03:47:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvudy5y",
          "author": "UmpireBorn3719",
          "text": "I can see glm 4.7 in dropdown but the max context windows size still limited in 128k",
          "score": 1,
          "created_utc": "2025-12-25 08:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvefdz",
          "author": "Deikku",
          "text": "Now gimme 4.7v and it will be (almost (probably)) my daily driver",
          "score": 1,
          "created_utc": "2025-12-25 14:12:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxr70s",
          "author": "neggbird",
          "text": "Working for me, but it I have found 4.7 thinks much longer than 4.6. However itâ€™s working more reliably, fewer tool errors",
          "score": 1,
          "created_utc": "2025-12-25 22:52:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyr4s0",
      "title": "GLM 4.7 vs Gemini: Architecture and Cost Trade-offs",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1pyr4s0/glm_47_vs_gemini_architecture_and_cost_tradeoffs/",
      "author": "Unfair-Tie2631",
      "created_utc": "2025-12-29 16:44:38",
      "score": 23,
      "num_comments": 31,
      "upvote_ratio": 0.91,
      "text": "I tested several model combinations in Kilo Code. Before the release of GLM 4.7, I mainly used Gemini 3 Flash for all modes, with solid results. I was highly enthusiastic about the release of GLM, but after many cycles of fixing bug after bug, it became clear that GLM 4.7 was not on par with Gemini in terms of architecture. Gemini was able to produce a functional prototype with just a few iterations.\n\nHowever, since GLM 4.7 is significantly more economical, especially with its API subscription plan, I tested a configuration that proves very effective: Gemini 3 Flash for all modes except code, where GLM 4.7 performs perfectly. For small feature architecture, Gemini 3 Flash is sufficient. For full project architecture or large features, I switch to Opus 4.5, but costs escalate rapidly.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1pyr4s0/glm_47_vs_gemini_architecture_and_cost_tradeoffs/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "nwltx3f",
          "author": "sand_scooper",
          "text": "Why would anyone use Gemini or Opus through Kilo Code?  \nThere's Claude Pro/Max for a ton of Opus 4.5 usage that resets weekly and Google AI Pro/Ultra for unlimited Gemini 3 and Opus 4.5 through Antigravity.\n\nKilo Code is not meant to use SOTA models for coding. It's too expensive. Nobody in their right mind will pay for API pricing when there's subscription plans.",
          "score": 11,
          "created_utc": "2025-12-29 20:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoqfjt",
              "author": "Mayanktaker",
              "text": "All other models sucks. Only gpt, gemini and claude works. Other models are just time pass in kilo. If you are serious about projects then don't waste time with other models.",
              "score": 3,
              "created_utc": "2025-12-30 06:07:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwows2r",
                  "author": "Unfair-Tie2631",
                  "text": "That was true not long ago, but now GLM 4.7 is a serious competitor at a much lower cost.\n\nhttps://preview.redd.it/fztr91q6iaag1.jpeg?width=1206&format=pjpg&auto=webp&s=2c307cb69ae42e68082d16706c2416ed07f03bda",
                  "score": 0,
                  "created_utc": "2025-12-30 07:00:25",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpemhe",
              "author": "RMCPhoto",
              "text": "Opus in kilocode is seriously wild...   As far as I'm concerned kilocode is not really for primary development.   I use it for data extraction, searching code, documentation or messing around a bit, but with the number of errors compared to the proper agentic coders it may end up being more expensive to use flash or glm than just springing for the real deal. \n\nEnd of the day, of you're using this for professional work or even to save time - just do the very useful math of \"what would I pay myself per hour\".    This ends up being helpful in all sorts of decision making, but to me it felt pretty clear here.",
              "score": 2,
              "created_utc": "2025-12-30 09:45:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpv6q8",
                  "author": "sand_scooper",
                  "text": "Yeah the saddest thing about Kilo Code is the lack of accountability. Everyone who uses it knows it regularly gets errors. But they spend all their time claiming how it shouldn't happen and asking you to write an email. Then they spend all their time on new features that are inferior copies of other existing companies.",
                  "score": 1,
                  "created_utc": "2025-12-30 12:10:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwnu1pq",
              "author": "yuyuyang1997",
              "text": "Just gotten used to Kilo as my AI agent.",
              "score": 1,
              "created_utc": "2025-12-30 02:41:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlcdue",
          "author": "uxkelby",
          "text": "GLM 4.7 coding plan is doing a good job at architecture my app, I mix it with GPT 5 api",
          "score": 3,
          "created_utc": "2025-12-29 18:57:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwopgs5",
              "author": "MorningFew1574",
              "text": "Agreed",
              "score": 1,
              "created_utc": "2025-12-30 05:59:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwpews9",
              "author": "RMCPhoto",
              "text": "What do you see as the biggest strengths of GLM 4.7?   And what about weaknesses?  \n\nArchitecture?  It could be.  Definitely good for an alternative perspective at least.   But then what about longer contexts and fixing bugs?  Imo it has the classic llm issue of suffering a lot from any incorrect code in the context whether it wrote it in a prior iteration or even when tasked to resolve a bug.",
              "score": 1,
              "created_utc": "2025-12-30 09:47:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1g7ol",
                  "author": "thingswhatnot",
                  "text": "Iâ€™m finding the structure of prompts etc making a big difference. Claude code withÂ https://github.com/solatis/claude-configÂ to provide a frameworkÂ are making a big difference.Â ",
                  "score": 1,
                  "created_utc": "2026-01-01 05:44:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwl3ina",
          "author": "Hisma",
          "text": "Sad to hear that assessment of GLM 4.7. It seems to be the consensus that while GLM 4.7 is a very intelligent model, it goes off the rails in longer multi turn context sessions and breaks stuff / hallucinates. Have you tried Minimax M2. 1 yet? I used M2 for a while and was impressed with how well it could stay on task even over long context. It's great at instruction following and tool calling. I wonder if used M2.1 over GLM 4.7 you'd get better results.",
          "score": 2,
          "created_utc": "2025-12-29 18:17:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl4nzf",
              "author": "Unfair-Tie2631",
              "text": "Before GLM 4.7, I had briefly tested M2.1. It seemed to make fewer architectural mistakes, whereas GLM 4.7 showed a stronger sense for feature ideation. A possible approach would be to use M2.1 as the architect and GLM 4.7 as the coder, but I doubt this setup would reach the level of Gemini 3 Flash. I hope that one day a Chinese model will be trained exclusively for architecture; that would be exceptional.",
              "score": 5,
              "created_utc": "2025-12-29 18:22:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwl5tvu",
                  "author": "Erebea01",
                  "text": "Have you tried deepseek 3.2 for architect mode?",
                  "score": 1,
                  "created_utc": "2025-12-29 18:27:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwoa0wh",
                  "author": "No_Success3928",
                  "text": "I have noticed the same! I am using m2.1 as architect and glm 4.7 as coder and it works really well",
                  "score": 1,
                  "created_utc": "2025-12-30 04:13:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwt4e3v",
          "author": "AppealSame4367",
          "text": "I think its very telling that the big OSS / Chinese models are now better than Claude, GPT, Gemini 1 year ago but they are still \"not enough\" to be considered useful, although they are cheaper.\n\nJust emphasizes where AI will go: It must always be the newest, best model. Otherwise people will feel like they are loosing and wasting their time. It's in our nature and that's why this AI revolution will continue to go as fast and crazy as possible.",
          "score": 2,
          "created_utc": "2025-12-30 22:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9nq5",
          "author": "No_Success3928",
          "text": "Gemini is amazing for UI/UX stuff",
          "score": 1,
          "created_utc": "2025-12-30 04:11:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwobusv",
          "author": "Vozer_bros",
          "text": "both of them are good for all features, but dont try to make feature batch hold too many businesses",
          "score": 1,
          "created_utc": "2025-12-30 04:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr1jip",
          "author": "sbayit",
          "text": "The combination of GLM 4.7 and Deepseek 3.2 is the most effective pairing.",
          "score": 1,
          "created_utc": "2025-12-30 16:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr2xkm",
              "author": "Unfair-Tie2631",
              "text": "Tu utilises lequel dans quel mode ?",
              "score": 1,
              "created_utc": "2025-12-30 16:19:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwymy3o",
          "author": "Royal-Huckleberry943",
          "text": "Tried it out with claude code, Its pretty decent, I got the Pro plan, but look out for the concurrency limit per plan for glm coding plans: lite - 2, pro - 5, max - 40 (concurrent requests limit) . Join their discord, discuss with other people then buy if you feel its worth it.   \nYou get an extra 10% discount on top of the 60% Christmas Offer with my referral:   \n[https://z.ai/subscribe?ic=AJJOVACC4X](https://z.ai/subscribe?ic=AJJOVACC4X)",
          "score": 1,
          "created_utc": "2025-12-31 19:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8a572",
              "author": "East-Present-6347",
              "text": "Where are you getting that concurrency limit request information? And, is that for all models?",
              "score": 1,
              "created_utc": "2026-01-02 09:54:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwyqswe",
          "author": "Royal-Huckleberry943",
          "text": "It needs more detailed plan and handholding. Budget worthy. Tried it out with claude code, Its pretty good, I got the Pro plan, mainly use it as the workhorse with other models for planning, reviewing. Its for people who want to conserve their budget, might need a stronger model to break through some complex places where it loops and gets stuck. But look out for the concurrency limit per plan for glm coding plans: lite - 2, pro - 5, max - 40 (concurrent requests limit) . Join their discord, discuss with other people then buy if you feel its worth it.  \nGet an extra 10% discount on top of the 60% Christmas Offer with my referral:  \n[https://z.ai/subscribe?ic=AJJOVACC4X](https://z.ai/subscribe?ic=AJJOVACC4X)",
          "score": 1,
          "created_utc": "2025-12-31 19:33:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyo737",
      "title": "A CodeRabbit Alternative for 2026: Code Reviews in Kilo",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1pyo737/a_coderabbit_alternative_for_2026_code_reviews_in/",
      "author": "alokin_09",
      "created_utc": "2025-12-29 14:51:37",
      "score": 13,
      "num_comments": 4,
      "upvote_ratio": 0.94,
      "text": "Most AI code review tools lock you into their AI stack. They pick the model, they decide when to upgrade, and you get whatever theyâ€™ve configured. \n\nOn the other hand, code reviews in Kilo are built on a different premise.Â \n\nSo, how does Kilo Code reveiws stack against CodeRabbit? \n\n**Pricing**\n\nCodeRabbit chargesÂ $24 per seat per month for code reviews only.\n\nKilo works on per-token pricing at the cost set by model providers. You pay for the reviews you run, which means you donâ€™t leave money on the table, and you donâ€™t hit invisible rate limits or downgrades when you surpass an arbitrary usage limit.\n\n**Workflow**  \n  \nCodeRabbit is a standalone code review product. It does one thing, and you add it to your existing pile of dev tools.\n\nKiloâ€™sÂ Code ReviewsÂ is part of the Kilo platform, the same environment where youâ€™re already writing code, debugging, and deploying. Your reviews happen alongside: \n\n* Agentic Engineering with specialized modes for implementation, architecture, debugging, and orchestration in the IDE or CLI\n* Cloud Agents for accessing Kilo without using your local machine\n* App Builder for prototyping with a live preview\n* Kilo Sessions that sync across VS Code, JetBrains, CLI, and web\n* Kilo Deploy for one-click shipping when the review passes\n* Managed Indexing for context-aware reviews that understand your codebase \n\nIf anyoneâ€™s interested, hereâ€™s a **more detailed breakdown and comparison**:  \n[https://blog.kilo.ai/p/code-review-alternative-2026]() \n\nQuestion: Anyone whoâ€™s tried multiple AI code review tools, what turned out to matter, and what was mostly noise?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1pyo737/a_coderabbit_alternative_for_2026_code_reviews_in/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "nwk0wpf",
          "author": "Solonotix",
          "text": "Do you have a proposed workflow for using Kilo Code for code review? I've used it to write a change log before, but haven't quite figured out a good way to prompt for a full code review\n\nEdit: Ah, I see. This is promoting a new feature on the Enterprise side. Cool to see.",
          "score": 1,
          "created_utc": "2025-12-29 15:14:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwogak2",
          "author": "ttreyr",
          "text": "\\> Kilo works on per-token pricing at the cost set by model providers.\n\nthat means u will pay more if your code is more. and cursor also has the review fuction.",
          "score": 1,
          "created_utc": "2025-12-30 04:53:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwcrdt",
          "author": "aviboy2006",
          "text": "Does it work like extension or different ?",
          "score": 1,
          "created_utc": "2025-12-31 11:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7kawh",
          "author": "tdehnke",
          "text": "Iâ€™d like the Kilo reviews to just reply with issues and suggestions.  None of the this is good etc,",
          "score": 1,
          "created_utc": "2026-01-02 05:58:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw4p12",
      "title": "What people really think about Kilo Autocomplete",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1pw4p12/what_people_really_think_about_kilo_autocomplete/",
      "author": "alokin_09",
      "created_utc": "2025-12-26 13:48:07",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "So, a month ago, Kilo's autocomplete feature was enabled by default for new users in VS Code.\n\nWanted to see if people are actually finding it useful. Here's a quick overview of the data from the 157 survey respondents.\n\nTL;DR Most users accept suggestions regularly, and theyâ€™re not just filling in obvious lines; theyâ€™re writing documentation, refactoring code, and generating tests. \n\nThe most common uses were lowâ€“cognitive load tasks where the next step is predictable:\n\n* Filling in obvious lines (81)\n* Fixing errors as I type (52)\n* Boilerplate or scaffolding (47) \n\nBut users also rely heavily on autocomplete for higher-value tasks that require more context and thinking:\n\n* Writing documentation (51)\n* Refactoring or code transformations (49)\n* Writing tests (28)\n* API usage and calling libraries (25) \n\nAutocomplete performs well atÂ contextual suggestions, and users are clearly taking advantage of that capability.\n\nThe spread across both categories shows that people see autocomplete as usefulÂ for much more than just boilerplate.\n\nAutocomplete usage isÂ heavily concentrated in dynamically typed and web-focused languages.Â TypeScript/JavaScript (72) and Python (64) account for most responses, followed by HTML/CSS (37) and Markdown (18). \n\nAll of this data suggests that autocomplete is already being used across a vast number of programming languages and frameworks. \n\nHere's a more detailed breakdown -> [https://blog.kilo.ai/p/what-157-developers-really-think](https://blog.kilo.ai/p/what-157-developers-really-think) \n\nWhat about you? What do you use it for most?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1pw4p12/what_people_really_think_about_kilo_autocomplete/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pwlleh",
      "title": "Chat input is getting de-selected",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1pwlleh/chat_input_is_getting_deselected/",
      "author": "FoldOutrageous5532",
      "created_utc": "2025-12-27 01:57:59",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I'll be typing in a prompt and then notice that what I'm typing isn't showing up in the chat.  I have to click back in and continue typing. It's like I'm accidentally clicking outside of the input box, but I'm not! ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1pwlleh/chat_input_is_getting_deselected/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "nw4mmnq",
          "author": "PowerAppsDarren",
          "text": "Happens to me as well. I hate it. My buddy guess is that it is editing the files sending focus over to the editor window(s)",
          "score": 2,
          "created_utc": "2025-12-27 02:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7xey4",
              "author": "FoldOutrageous5532",
              "text": "Very annoying. Hope they fix it soon.",
              "score": 1,
              "created_utc": "2025-12-27 17:23:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiv39u",
          "author": "LigiaZanchet",
          "text": "Hey u/FoldOutrageous5532   \nCould you share a bit more context about when this happens? Does it occur while the agent is actively working on the code, or while youâ€™re typing the prompt?",
          "score": 2,
          "created_utc": "2025-12-29 10:25:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkrp89",
              "author": "FoldOutrageous5532",
              "text": "When I'm typing a prompt.",
              "score": 1,
              "created_utc": "2025-12-29 17:22:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pv3dby",
      "title": "Why is it every LLM I choose has API errors (in VSCode)",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1pv3dby/why_is_it_every_llm_i_choose_has_api_errors_in/",
      "author": "Tiny-Sink-9290",
      "created_utc": "2025-12-25 02:35:44",
      "score": 4,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Just trying to figure this out. I have $250  in credits. I choose Gemini 2, Gemini 3, ChatGPT 5, ChatGPT 5.2... no matter what I use with my KiloCode gateway in VSCode, with my Kilocode API.. I get errors and forks up my damn task.\n\nIt used to work. I dont understand WTH is wrong. I keep getting API errors with ChatGPT, Gemini, etc. I am using a prompt (about 8 lines long) and instructing it to examine some of my projects (small modular projects not super large) and I just cant get it to work. \n\nIs there some bug/issue with KiloCode right now? I am in my own custom mode, where I have a prompt in it (system prompt) set up for a given model (one of the above) and every time, 100% of the time it fails. I've wasted hours now on trying to get this to work. It USED to work. I was able to do this months ago, just coming back to try it out while claude code seems to fail at a task (React/GUI related). \n\nVERY frustrating. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1pv3dby/why_is_it_every_llm_i_choose_has_api_errors_in/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "nvtyci7",
          "author": "hannesrudolph",
          "text": "Because they merged the bugs from Roo ðŸ˜‚",
          "score": 3,
          "created_utc": "2025-12-25 05:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtl47r",
          "author": "Ordinary_Mud7430",
          "text": "Go into Kilo settings and choose a supplier",
          "score": 1,
          "created_utc": "2025-12-25 03:53:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtyy0z",
              "author": "Tiny-Sink-9290",
              "text": "I did so. Still get it. ChatGPT 5 is working now.. Gemini 3 cant get to work at all.",
              "score": 1,
              "created_utc": "2025-12-25 05:48:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu78jd",
          "author": "Mayanktaker",
          "text": "Happens with me too. I stopped using kilo and moved to Zed editor. No errors like this and agents can work hours unstoppable.. using glm subscription there.",
          "score": 1,
          "created_utc": "2025-12-25 07:06:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvm3xv",
          "author": "LigiaZanchet",
          "text": "Hi u/Tiny-Sink-9290   \nThat shouldnâ€™t be happening. Please email [**hi@kilocode.ai**]() with the exact API error messages, your Kilo version. Weâ€™ll take a closer look. Thank you",
          "score": 1,
          "created_utc": "2025-12-25 15:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxezo4",
              "author": "Tiny-Sink-9290",
              "text": "I may do that. I am trying with GLM.. hoping GLM 4.7 is available soon as I hear its gotten better scores than Claude Code in many coding challenges.",
              "score": 1,
              "created_utc": "2025-12-25 21:37:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvykxwu",
                  "author": "Hisma",
                  "text": "I'm using GLM 4.7 right now via the z.ai API. Works great.",
                  "score": 1,
                  "created_utc": "2025-12-26 02:08:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwg9yk5",
              "author": "righteousdonkey",
              "text": "I just signed up with kilocode and am getting this as well with",
              "score": 1,
              "created_utc": "2025-12-28 23:38:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0k74f",
      "title": "Trying a 2B model in local",
      "subreddit": "kilocode",
      "url": "https://www.reddit.com/r/kilocode/comments/1q0k74f/trying_a_2b_model_in_local/",
      "author": "Affectionate_Plant57",
      "created_utc": "2025-12-31 18:40:44",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I saw the new **lfm2-2.6b-exp** model and was amazed to know it has the same intelligence as gpt-4. I run it in LM studio with 4GB context window. However, it gives me these errors so I guess it's not exactly the same. Can someone explain what's happening exactly? I'm very curious to understand the differences and the whys. Thanks\n\nhttps://preview.redd.it/gmkijgai3lag1.png?width=607&format=png&auto=webp&s=28dca8e8f80f78a1ef6514aa960c8620fe067b7f\n\n    Kilo Code tried to use apply_diff without value for required parameter 'path'. If this error keep reoccuring, please disable Editing Through Diffs in Advanced Settings. Retrying...",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/kilocode/comments/1q0k74f/trying_a_2b_model_in_local/",
      "domain": "self.kilocode",
      "is_self": true,
      "comments": [
        {
          "id": "nwyij3q",
          "author": "inevitabledeath3",
          "text": "The answer is in the screenshot to be honest. You need to use a smarter model. No matter what the marketing claims a 2.6B parameter model is not going to be good enough for most coding tasks or comparable to GPT-4, and it looks like it's failing to execute tool calls correctly. The context window for that model is fairly limited and you are likely using a quantized (dumber) version as well. You *might* get some use out of it by switching from XML to JSON/native tool calling or vice versa, but chances are it's not going to work well. You really need at least a 24B parameter model like Devstral 2 or larger like Qwen 3 Coder 30B A3B for agentic coding, ideally bigger and smarter. If you don't have powerful enough hardware you can get good models for cheap or even free through a variety of providers including synthetic, openrouter, or minimax. Minimax M2 model is currently free on open router and would work better, or even Grok Code Fast 1 which is also free I believe.",
          "score": 5,
          "created_utc": "2025-12-31 18:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx795o5",
          "author": "Mayanktaker",
          "text": "I get this error frequently and with model like glm, minimax, qwen code etc and stopped using kilo. My glm subscription is working fine with cline, sadly not with kilo.",
          "score": 1,
          "created_utc": "2026-01-02 04:38:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}