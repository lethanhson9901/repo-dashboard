{
  "metadata": {
    "last_updated": "2026-01-24 16:49:57",
    "time_filter": "week",
    "subreddit": "comfyui",
    "total_items": 20,
    "total_comments": 310,
    "file_size_bytes": 318596
  },
  "items": [
    {
      "id": "1qflmgy",
      "title": "LTX 2 is amazing : LTX-2 in ComfyUI on RTX 3060 12GB",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/h4ugm53kiydg1",
      "author": "tanzim31",
      "created_utc": "2026-01-17 18:58:23",
      "score": 316,
      "num_comments": 59,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qflmgy/ltx_2_is_amazing_ltx2_in_comfyui_on_rtx_3060_12gb/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o06afb5",
          "author": "Frogy_mcfrogyface",
          "text": "Ugh. Why must I test out every new workflow I find lol üòÇ¬†\n\n\nThese results are incredible. Nice work. I'm always looking for a better and faster workflow for my 5060ti. Yeah the default templates are kinda useless unless you have a monster of a GPU.¬†",
          "score": 9,
          "created_utc": "2026-01-17 21:15:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06b3hx",
              "author": "tanzim31",
              "text": "Yeah. I was getting memory error constantly with the default workflow",
              "score": 4,
              "created_utc": "2026-01-17 21:18:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o06d7zk",
                  "author": "Frogy_mcfrogyface",
                  "text": "Oh, this is Yanokusnir's wf :)\n\nDo you know of a good text to video wf?",
                  "score": 1,
                  "created_utc": "2026-01-17 21:29:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o099ctj",
          "author": "juli3n_base31",
          "text": "Thanks man I will try to run this on my rtx 3060 12 gb",
          "score": 4,
          "created_utc": "2026-01-18 07:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06cm58",
          "author": "juanpablogc",
          "text": "Pretty amazing, I have tested several TTS and the best in English is Vibevoice. And why not add some background music, it is the only thing left! If you need I can give you one with Producer ai, just tell me the prompt.",
          "score": 2,
          "created_utc": "2026-01-17 21:26:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06fivy",
              "author": "tanzim31",
              "text": "I quickly cut together all the clips in 20 min. I'm don't understand audio that well. My idea was spy thriller in Dhaka.",
              "score": 1,
              "created_utc": "2026-01-17 21:40:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o06wb0q",
          "author": "SpaceNinjaDino",
          "text": "I'm impressed with this audio. It came out clean except for the sigh. Many examples have had terrible audio. (I'm glad that Lightricks is making this a top priority fix.) The visuals are mostly good too (except the zoom in face color shift).",
          "score": 2,
          "created_utc": "2026-01-17 23:04:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08xy9m",
              "author": "tanzim31",
              "text": "all those audio had a squeaky sound. i cleaned up with audio noise remover in capcut desktop",
              "score": 2,
              "created_utc": "2026-01-18 06:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0a0a5s",
                  "author": "Free-_-Yourself",
                  "text": "Thanks for being transparent",
                  "score": 1,
                  "created_utc": "2026-01-18 11:58:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0b5r9z",
                  "author": "thisiztrash02",
                  "text": "have you tried ltx2 new latent node they dropped like 3 days ago, it really improved audio takes away alot of hiss in the audio and artifacts voices sound silky smooth without having to edit",
                  "score": 0,
                  "created_utc": "2026-01-18 16:06:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05wl9c",
          "author": "intermundia",
          "text": "Great stuff. Top tier indeed. The fact you got this high quality  and consistency from 12 gig is insane.  I've got a 5090 and struggle to get a decent cohesive 15 second clip.  I think the prompt and workflow makes a massive difference as well.  Anyway just wanted to say good job. I'm always looking for ways to optimise so I'll give your workflow a shot.",
          "score": 2,
          "created_utc": "2026-01-17 20:03:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05zkdj",
              "author": "tanzim31",
              "text": "My suggestion is to keep it under 8 seconds if you have dialogues in the prompt, as it tends to ghost some monologue parts and then goes silent. also no action scene. it doen't understand that. at least for illustration. also read the official prompt guide\n\n[https://ltx.io/model/model-blog/prompting-guide-for-ltx-2](https://ltx.io/model/model-blog/prompting-guide-for-ltx-2)",
              "score": 1,
              "created_utc": "2026-01-17 20:19:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o060wlv",
                  "author": "intermundia",
                  "text": "Yeah i usually get gpt to go over the official prompt guide then load the starting image to gpt and get it to output a prompt based on the prompt give the image and what i want it to do.  I then run that a few times and tweak things in the prompt to see how that effects the gen.  Once I have the prompt dialed in i start running mutiple gens at random seeds. Rince repeat.",
                  "score": 1,
                  "created_utc": "2026-01-17 20:25:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0661vk",
          "author": "RevvelUp",
          "text": "I also have 12gb of vram. Have you encountered the message in comfy saying that you ran out of memory? What were your settings when generating this in comfy?",
          "score": 1,
          "created_utc": "2026-01-17 20:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o066xek",
              "author": "tanzim31",
              "text": "Absolutely. All the time. Default workflow didn't work that well. It kept getting error. I thought best I could do would be 480p until this workflow I got that someone shared in a comment. Try the attracted workflow in post with all the loras enabled (important). I was able to generate 720p easily.",
              "score": 2,
              "created_utc": "2026-01-17 20:56:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o07i9n7",
                  "author": "RevvelUp",
                  "text": "Ah so that workflow works better than the default workflow on comfy?",
                  "score": 1,
                  "created_utc": "2026-01-18 01:00:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o09srs6",
              "author": "Risky-Trizkit",
              "text": "i started with comfy not long ago but i noticed there were things called garbage collection nodes - those may be of interest. They dump things from memory that are not needed after they are executed.\n\nalso check out your run.bat file, you can append things to your initial startup command like --low vram and other things depending on your video card specifics. (ask chatgpt/gemini if unsure)\n\nmy setup is \".\\\\python\\_embeded\\\\python.exe -s ComfyUI\\\\main.py --windows-standalone-build --lowvram --bf16-unet --cuda-malloc\" for example",
              "score": 1,
              "created_utc": "2026-01-18 10:52:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o07f32o",
          "author": "Exotic_Researcher725",
          "text": "Are you using the --novram argument?",
          "score": 1,
          "created_utc": "2026-01-18 00:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ft9y6",
              "author": "tanzim31",
              "text": "yes",
              "score": 1,
              "created_utc": "2026-01-19 07:21:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0892h2",
          "author": "MHIREOFFICIAL",
          "text": "sorry im a wan guy, i must be missing a lot of custom nodes because isee:\n\nWhen loading the graph, the following node types were not found:\n\n* [\\[object Object\\]](https://github.com/search?q=[object%20Object]&type=code)\n\nCannot execute because a node is missing the class\\_type property.: Node ID '#161:154'\n\nwhich in the json has:\n\n    \"inputs\": [\n    ¬† ¬† ¬† ¬† {\n    ¬† ¬† ¬† ¬† ¬† \"label\": \"image\",\n    ¬† ¬† ¬† ¬† ¬† \"name\": \"image\",\n    ¬† ¬† ¬† ¬† ¬† \"type\": \"IMAGE\",\n    ¬† ¬† ¬† ¬† ¬† \"link\": 356\n    ¬† ¬† ¬† ¬† },\n    ¬† ¬† ¬† ¬† {\n    ¬† ¬† ¬† ¬† ¬† \"label\": \"frame_count\",\n    ¬† ¬† ¬† ¬† ¬† \"name\": \"value\",\n    ¬† ¬† ¬† ¬† ¬† \"type\": \"INT\",\n    ¬† ¬† ¬† ¬† ¬† \"widget\": {\n    ¬† ¬† ¬† ¬† ¬† ¬† \"name\": \"value\"\n    ¬† ¬† ¬† ¬† ¬† },\n    ¬† ¬† ¬† ¬† ¬† \"link\": null\n    ¬† ¬† ¬† ¬† },\n    ¬† ¬† ¬† ¬† {\n    ¬† ¬† ¬† ¬† ¬† \"label\": \"frame_rate\",\n    ¬† ¬† ¬† ¬† ¬† \"name\": \"Value\",\n    ¬† ¬† ¬† ¬† ¬† \"type\": \"INT\",\n    ¬† ¬† ¬† ¬† ¬† \"widget\": {\n    ¬† ¬† ¬† ¬† ¬† ¬† \"name\": \"Value\"\n    ¬† ¬† ¬† ¬† ¬† },\n    ¬† ¬† ¬† ¬† ¬† \"link\": null\n    ¬† ¬† ¬† ¬† }",
          "score": 1,
          "created_utc": "2026-01-18 03:25:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ftde1",
              "author": "tanzim31",
              "text": "please update comfyui first then install missing links. Otherwise you'll get errors",
              "score": 1,
              "created_utc": "2026-01-19 07:22:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o089r20",
          "author": "No_Conversation9561",
          "text": "T2V is amazing but I2V changes face a lot. I don‚Äôt know if it‚Äôs just me.",
          "score": 1,
          "created_utc": "2026-01-18 03:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08xia5",
              "author": "tanzim31",
              "text": "i had terrible luck with t2v. yeah. in i2v it changes the face slightly.",
              "score": 1,
              "created_utc": "2026-01-18 06:09:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09o272",
          "author": "MaximilianPs",
          "text": "Thank you for sharing the workflow but I absolutely disagree with --novram parameter. Also:\n\nThe GGUF format has three huge advantages compared to safetensor models!\n\n1. Lighter Quantization   \nGGUF uses aggressive quantizations (Q4, Q5, Q8, etc.) which:\n\n\\- reduce model size,  \n\\- reduce the required VRAM  \n\\- reduce the weight of tensor loads\n\n2. More Efficient Loading\n\nMany GGUF loaders are optimized for:  \n\\- streaming  \n\\- memory mapping  \n\\- reducing VRAM spikes\n\n3. Less overhead than safetensors\n\nSafetensor models are larger, require more VRAM to load (that's why --novram), have fewer quantization/ optimizations and often use higher precisions (fp16, bf16, fp8)\n\nResult: GGUF = more stable, longer workflow, fewer crashes.\n\nThat's why I prefer it and you should too. ¬Ø\\\\\\_(„ÉÑ)\\_/¬Ø",
          "score": 1,
          "created_utc": "2026-01-18 10:09:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09p9kc",
              "author": "tanzim31",
              "text": "isn't `--novram` flag are runtime strategies like offloading or CPU fallback used when VRAM is tight. why would safetensor matter in that case? i have powerful enough cpu and ssd to handle that. You're right about GGUF. since it's a new model having correct and efficient gguf is tricky. also GGUF **‚â†** more stable, fewer crashes.",
              "score": 1,
              "created_utc": "2026-01-18 10:19:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0mmary",
                  "author": "MaximilianPs",
                  "text": "From Comfy UI documentation \nnovram : No VRAM usage, runs entirely on system memory;",
                  "score": 1,
                  "created_utc": "2026-01-20 07:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0bd9eu",
          "author": "spidyrate",
          "text": "How fast can it generate on 32gb ram and rtx 5050 8gb vram ?",
          "score": 1,
          "created_utc": "2026-01-18 16:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ftkij",
              "author": "tanzim31",
              "text": "i fear 32 gb ram is gonna get filled quickly with text encoders and vl, you'll need more rams to get comfortably generate videos. also try the workflow with 480P then increase the res.",
              "score": 1,
              "created_utc": "2026-01-19 07:24:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0k0or9",
          "author": "Internal_Meaning7116",
          "text": "i am using comfyui standalone version.   \nVHS\\_VideoCombine\n\nLTXVSpatioTemporalTiledVAEDecodein subgraph 'Image 2 Video Workflow'\n\nImageResizeKJv2in subgraph 'Image 2 Video Workflow'\n\nInstall these nodes to run this workflow, or replace them with installed alternatives. Missing nodes are highlighted in red on the canvas.",
          "score": 1,
          "created_utc": "2026-01-19 21:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l74w3",
              "author": "tanzim31",
              "text": "- install the missing nodes\n- expand the subgraph. See what's missing. Set the frames, parameters",
              "score": 1,
              "created_utc": "2026-01-20 01:44:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kh291",
          "author": "Whole_Disk3247",
          "text": "That's great !  \nDo you used a specific workflow with image start and image end ?",
          "score": 1,
          "created_utc": "2026-01-19 23:23:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l6s0l",
              "author": "tanzim31",
              "text": "Hi, workflow is attached in the post. The Google drive link",
              "score": 1,
              "created_utc": "2026-01-20 01:42:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lbfxe",
                  "author": "tanzim31",
                  "text": "no, just start frame and prompt",
                  "score": 1,
                  "created_utc": "2026-01-20 02:07:50",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0na3uk",
          "author": "One_Turnip_8989",
          "text": "Nice it can be whole Netflix series",
          "score": 1,
          "created_utc": "2026-01-20 10:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13ezo7",
          "author": "Hollow_Himori",
          "text": "LTXVImgToVideoInplace\n\ndivision by zero\n\nhow to fix this error?",
          "score": 1,
          "created_utc": "2026-01-22 18:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15s9xt",
              "author": "tanzim31",
              "text": "- my first guess would be unpack the subgraph and see if your models are correct it not.\n- prolly update comfyui and all custom nodes",
              "score": 1,
              "created_utc": "2026-01-23 01:50:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19c184",
                  "author": "Hollow_Himori",
                  "text": "did all that :(",
                  "score": 1,
                  "created_utc": "2026-01-23 16:05:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15lod7",
          "author": "Conscious-Citzen",
          "text": "Hey! Loved the results! Great stuff! How do you upscale it? I'm able to generate consistent videos on ltx 2, but never learned to upscale. I'm old (40) and whenever I have the time to mess with comfyui I just generate stuff, never had enough time to learn to upscale.\nVideos I generate are usually 1376 x 768... I use gguf cause I have a 3060ti (8gb).\n\nThx and congratulations!",
          "score": 1,
          "created_utc": "2026-01-23 01:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dgsm6",
          "author": "null_nerd",
          "text": "You achieved amazing results with the RTX 3060 12GB VRAM. I think it's such a difficult process to input prompts and wait for the results after RUN. In the case of image generation, you can see the intermediate step with a sampler, so if it is being generated differently from the intended purpose, you can cancel it. This result seems to be entirely the result of your efforts.",
          "score": 1,
          "created_utc": "2026-01-24 04:47:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dtqs2",
              "author": "tanzim31",
              "text": "reduce the frames to 20-30. See the results > fix the seed > increase the frames > regenerate",
              "score": 1,
              "created_utc": "2026-01-24 06:24:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1duyf1",
                  "author": "null_nerd",
                  "text": "That‚Äôs a great way! I will try this method too.",
                  "score": 1,
                  "created_utc": "2026-01-24 06:34:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05vahl",
          "author": "mikethehunterr",
          "text": "Love the style",
          "score": 1,
          "created_utc": "2026-01-17 19:57:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05pwcx",
          "author": "Yattagor",
          "text": "Anyway, it's truly incredible. Congratulations, and I'll be following everything with interest.",
          "score": 1,
          "created_utc": "2026-01-17 19:31:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0652uc",
          "author": "Powerful_Spring948",
          "text": "The result is incredible; now it's our turn to study film, scene composition, making anime, movies, and short films. It's within reach and cheap; we just need to understand how to produce them.",
          "score": 1,
          "created_utc": "2026-01-17 20:47:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o065qu8",
              "author": "tanzim31",
              "text": "Yeah absolutely!",
              "score": 1,
              "created_utc": "2026-01-17 20:50:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o08cmeo",
              "author": "Gigglegambler",
              "text": "Indeed. I'm glad this medium is getting more people into film. Instead of \"cheap\", maybe, \"attainable\". There is already enough race to the bottom in VFX unfortunately. I would love to see this mixed with better VO (since you already have a scratch track). Add some music to it as well. For 2 days of work this is impressive, good luck with revisions though.",
              "score": 1,
              "created_utc": "2026-01-18 03:45:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjmzm5",
      "title": "Flux 2 Klein has decent built-in face swapping ability",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qjmzm5",
      "author": "slpreme",
      "created_utc": "2026-01-22 06:20:13",
      "score": 301,
      "num_comments": 57,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qjmzm5/flux_2_klein_has_decent_builtin_face_swapping/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o10akpf",
          "author": "inagy",
          "text": "This model is a real surprise to me. Even the 4B (which is really fast) is good at isolating parts, replacing elements, \"understanding\" controlnet images.",
          "score": 17,
          "created_utc": "2026-01-22 07:19:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13fk77",
              "author": "ChicoTallahassee",
              "text": "Is the 4b base better than 9b distilled?",
              "score": 2,
              "created_utc": "2026-01-22 18:46:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1416zi",
                  "author": "inagy",
                  "text": "**For others reading this comment for the first time**: looks like I was mislead by that [ComfyUI blog](https://blog.comfy.org/p/flux2-klein-4b-fast-local-image-editing), as the 9b distilled model is available here [https://huggingface.co/black-forest-labs/FLUX.2-klein-9B](https://huggingface.co/black-forest-labs/FLUX.2-klein-9B)\n\n\\---\n\nI've only tested the 9b base, because the distilled is API only, and I'm only interested in local models. 9b is more capable of course, it also uses Qwen3 8b instead of 4b, so the prompt understanding is obviously better, because the text encoder is more capable. But it's much slower. At the same time 9b base renders an image with 20 or so steps, I can do 2-3 smaller edits with 4b distilled, multiple edits chained after another or just do quicker edit-generate turns overall. And if you don't overload 4b with too many editing instructions at once, it's pretty capable as I see. 4b seems to struggle with text rendering a bit though.",
                  "score": 0,
                  "created_utc": "2026-01-22 20:24:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o10cgu7",
          "author": "Snoo20140",
          "text": "I tried it already and had issues with lighting mismatching. It doesn't seem to handle low resolution faces or conflicting lighting from my tests. \n\nMaybe it was just my run/prompts/images tho.",
          "score": 8,
          "created_utc": "2026-01-22 07:36:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10hs9m",
              "author": "slpreme",
              "text": "prompt is still important i barely wrote it in a minute. maybe adding a prompt about the lighting would help",
              "score": 6,
              "created_utc": "2026-01-22 08:24:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o10r8ff",
              "author": "Bronzeborg",
              "text": "use the relight template to fix old images first :)",
              "score": 3,
              "created_utc": "2026-01-22 09:54:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o16qt1e",
                  "author": "Snoo20140",
                  "text": "That isn't a bad idea. Will give it a go. I appreciate it.",
                  "score": 1,
                  "created_utc": "2026-01-23 05:15:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o134e13",
              "author": "yoomiii",
              "text": "indeed, I had this issue as well",
              "score": 1,
              "created_utc": "2026-01-22 17:57:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o114p73",
          "author": "huaweio",
          "text": "Playing around with the sigma\\_max values ‚Äã‚Äã(\\~0.95-0.98) yields some interesting results. The main problem I see is the size difference between the heads. It usually makes the head larger than the body. I've tried adding some extra instructions to the prompt, but without success. I don't know if anyone has been able to find a solution.",
          "score": 8,
          "created_utc": "2026-01-22 11:49:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1152dv",
              "author": "slpreme",
              "text": "yeah i really tried but it loves making bobble heads",
              "score": 8,
              "created_utc": "2026-01-22 11:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o11kyh2",
                  "author": "huaweio",
                  "text": "Great job anyways!",
                  "score": 1,
                  "created_utc": "2026-01-22 13:34:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11vjxx",
              "author": "Expicot",
              "text": "So far my best trick is to photoshop the new head over the old one (so human control about the proportions), and make the \"swap\" using the same new face as reference. Works pretty well. Better than all previous checkpoints and edit version I had tried so far (Qwen, Kontext).",
              "score": 4,
              "created_utc": "2026-01-22 14:29:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15ctsr",
                  "author": "slpreme",
                  "text": "hmm maybe i can make a node to do this automatically",
                  "score": 3,
                  "created_utc": "2026-01-23 00:25:17",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o170l40",
                  "author": "orangeflyingmonkey_",
                  "text": "hey! do you have an example maybe you can share? I am having the same problem.",
                  "score": 1,
                  "created_utc": "2026-01-23 06:29:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11f7ps",
          "author": "D0wly",
          "text": "I found that making an (single) image in photoshop with multiple faces of the reference subject helps a ton.",
          "score": 7,
          "created_utc": "2026-01-22 13:01:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o104kxy",
          "author": "Ok-Page5607",
          "text": "this is great! thanks for sharing",
          "score": 4,
          "created_utc": "2026-01-22 06:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104za6",
              "author": "slpreme",
              "text": "of course:)",
              "score": 2,
              "created_utc": "2026-01-22 06:31:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10hkn5",
          "author": "eidrag",
          "text": "I found that for better headswap you have to make the face straight forward 1st, before swapping",
          "score": 3,
          "created_utc": "2026-01-22 08:22:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10wh58",
          "author": "diffusion_throwaway",
          "text": "What about facial expressions? I‚Äôve had good luck cloning faces, but what if you want the expression from face 1 applied to face 2? As far as I can tell this isn‚Äôt possible. I hope someone can prove me wrong though.",
          "score": 3,
          "created_utc": "2026-01-22 10:41:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10qqfb",
          "author": "Ok-Internal9317",
          "text": "Compared to Qwen?",
          "score": 3,
          "created_utc": "2026-01-22 09:49:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1601hi",
              "author": "Swimming_Dragonfly72",
              "text": "Flux klein much much better in face swap than qwen. And faster ~ x6 times.¬†\nBut qwen has absolute domination on anatomy and 3D space understanding.¬†\nThey complement each other perfectly",
              "score": 2,
              "created_utc": "2026-01-23 02:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18gb2l",
                  "author": "Ok-Internal9317",
                  "text": "I'll try it out, good insight!",
                  "score": 1,
                  "created_utc": "2026-01-23 13:28:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o104s24",
          "author": "ton89y2k",
          "text": "9B or 4B  or Both",
          "score": 2,
          "created_utc": "2026-01-22 06:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104xpu",
              "author": "slpreme",
              "text": "can work with both but 9b is a lot better",
              "score": 3,
              "created_utc": "2026-01-22 06:31:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10srtl",
          "author": "Mugaluga",
          "text": "I think it's pretty good for faces and portraits, upper bodies and even standing in neutral poses. But not much beyond that.\n\nQwen Image Edit is a better tool IMO. And Qwen-Rapid-AIO is even better (because it's based on Qwen Image Edit) but it's also NSFW.\n\nThey will handle everything Klein will but won't fall apart with anatomy nearly as often.\n\nLet's just be honest here. Klein is not great at anatomy and VERY commonly screws up hands, arms and legs. Not every time, but very often.\n\nAlso I am using the default Klein edit workflow from Comfy but for some reason it doesn't let you choose the output resolution. You can open the subgraph and choose a megapixel value, like 1mp or 2mp but it doesn't let you directly choose your output resolution. Anyone know why? Is this a limitation of Klein editing or just a limitation of the template workflow?\n\nMy Qwen Image Edit WF lets me choose the exact output resolution - I prefer that.",
          "score": 5,
          "created_utc": "2026-01-22 10:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11slx0",
              "author": "mnmtai",
              "text": "You can change resolution like with any other workflow out there. Modify the latent node.",
              "score": 2,
              "created_utc": "2026-01-22 14:14:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o11k3g2",
              "author": "bhasi",
              "text": "\"My Qwen Image Edit WF lets me choose the exact output resolution - I prefer that.\"\n\nJust use the same node? Skill issue",
              "score": 3,
              "created_utc": "2026-01-22 13:29:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o14bgfp",
              "author": "Own-Quote-2365",
              "text": "Does the Qwen editor also work with the same model, text encoder, and VAE installation, along with basic CompyNode requirements? And what are the system requirements? When I tried using Klein, I was able to use it with only the basic downloads. It ran quickly and smoothly even with low VRAM.",
              "score": 1,
              "created_utc": "2026-01-22 21:12:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12x63q",
          "author": "tempedbyfate",
          "text": "The first image looks like the face was photoshopped onto the image, head looks too big to me and doesn't really fit overall.",
          "score": 2,
          "created_utc": "2026-01-22 17:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12wbyh",
          "author": "msixtwofive",
          "text": "first example is horrible - insanely huge head. The rest are very good.\n\n\nJust tried this with 4b vs 9b and from what I can tell 9b doesn't suffer with the head size issue as much.",
          "score": 1,
          "created_utc": "2026-01-22 17:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dlbl",
              "author": "slpreme",
              "text": "true ngl üòÇ",
              "score": 1,
              "created_utc": "2026-01-23 00:29:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1370kr",
          "author": "yoomiii",
          "text": "Are you masking the face on the target image?",
          "score": 1,
          "created_utc": "2026-01-22 18:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13mo33",
          "author": "TechnologyGrouchy679",
          "text": "I noticed in your workflow you are using the \\`BasicScheduler\\` node as opposed to the \\`Flux2Scheduler\\` node that is used in the official Flux.2 Klein workflows.  Any particular reason for this?",
          "score": 1,
          "created_utc": "2026-01-22 19:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dhs8",
              "author": "slpreme",
              "text": "i find it to get slightly better results (with 5 shift) in my testing. you should test it yourself to know for sure :)",
              "score": 1,
              "created_utc": "2026-01-23 00:28:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o160lmv",
          "author": "krsnt8",
          "text": "Better than Qwen faceswap???",
          "score": 1,
          "created_utc": "2026-01-23 02:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16lqvs",
          "author": "orangeflyingmonkey_",
          "text": "tried using it but all the 'body' images are being cropped to 432x1280 so the head swap fails.",
          "score": 1,
          "created_utc": "2026-01-23 04:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o179il1",
          "author": "Temporary_Mud_6722",
          "text": "–ø–ª–æ—Ö–∞—è...",
          "score": 1,
          "created_utc": "2026-01-23 07:45:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o180mqd",
          "author": "imaginationking",
          "text": "my issue with it, is the size of the head usually not realistic... not sure if there is a custom node that can fix that",
          "score": 1,
          "created_utc": "2026-01-23 11:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c3svd",
              "author": "slpreme",
              "text": "prompting can be improved. i would also get a reference photo around the same size of head as the target photo",
              "score": 1,
              "created_utc": "2026-01-23 23:57:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19kenu",
          "author": "Upper_Basis_4208",
          "text": "Where is the workflow",
          "score": 1,
          "created_utc": "2026-01-23 16:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o114m8j",
          "author": "reyzapper",
          "text": "With 3 references images will do a lot better, no need to train loras haha, it reminds me using IPadapter faceID.\n\nsubject : Younger Angelina Jolie (2000s),\n\nIt seems higher dimension of ref images will produce better likeliness, this is 512x512 cropped\n\nklein 9B, 4 steps\n\nhttps://preview.redd.it/xh9a0znm1weg1.png?width=1024&format=png&auto=webp&s=e9d96a3945be7ae466618e3626e48a80f3d82fd5\n\nThose fingers tho üò•üò•",
          "score": 1,
          "created_utc": "2026-01-22 11:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o114y70",
              "author": "slpreme",
              "text": "good idea i didnt think of that",
              "score": 1,
              "created_utc": "2026-01-22 11:51:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o13dyym",
              "author": "Mirandah333",
              "text": "https://preview.redd.it/0yevgsqq3yeg1.png?width=1451&format=png&auto=webp&s=b0138f7db72a83be146f1c0e8110affeb5a6132c\n\n5 reference images, sometimes get close, sometimes better, sometimes bad",
              "score": 1,
              "created_utc": "2026-01-22 18:39:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11plm1",
          "author": "Xp_12",
          "text": "So does Nicky Cage.\n\n![gif](giphy|hwZ51FKy98qv6)",
          "score": 0,
          "created_utc": "2026-01-22 13:58:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o105zvo",
          "author": "Practical-Nerve-2262",
          "text": "Everyone uses Banana now, but this open-source version is great too!",
          "score": -20,
          "created_utc": "2026-01-22 06:40:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg0q52",
      "title": "ComfyUI-RMBG v3.0.0 Released! Major Update ‚Äì New Florence-2, YOLOv8, SAM3 Improvements, Batch Tools & More",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qg0q52",
      "author": "Narrow-Particular202",
      "created_utc": "2026-01-18 06:05:25",
      "score": 214,
      "num_comments": 16,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qg0q52/comfyuirmbg_v300_released_major_update_new/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o09oiiu",
          "author": "alexmmgjkkl",
          "text": "i already have 10 otehr versions of the same nodes, lol.. why not git pull ONE fully featured resize node to comfy core and everybody else deletes the resize nodes in their own pack ?\n\nI guess when people acted like that , those times are over forever",
          "score": 15,
          "created_utc": "2026-01-18 10:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a2ad2",
          "author": "dirtybeagles",
          "text": "i need sleep!",
          "score": 15,
          "created_utc": "2026-01-18 12:15:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0953io",
          "author": "Used-Ear-8780",
          "text": "Installed ok but SAM3 Segmentation node not exist",
          "score": 3,
          "created_utc": "2026-01-18 07:14:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0982ng",
              "author": "Narrow-Particular202",
              "text": "Don‚Äôt forget install requirements.txt and restart Comfyui",
              "score": 8,
              "created_utc": "2026-01-18 07:41:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0dfg7h",
                  "author": "Riroh_bcn",
                  "text": "Have the same error even after installing requirements here",
                  "score": 1,
                  "created_utc": "2026-01-18 22:41:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o09xwn4",
          "author": "remarkedcpu",
          "text": "Cheers. I was getting OOM left and right using RMBG SAM3 and had to switch back to original SAM3 nodes. Was there an issue there?",
          "score": 2,
          "created_utc": "2026-01-18 11:38:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0sfl24",
              "author": "Several_Honeydew_250",
              "text": "Is your vram stacking with each sam3 run? That's what mine is doing, it's not releasing the sam3 model... and loads a new instance every time :(",
              "score": 1,
              "created_utc": "2026-01-21 03:05:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0si4zm",
                  "author": "remarkedcpu",
                  "text": "I don‚Äôt think so, cuz it would not crash and the workflow continues, and I‚Äôm not seeing VRAM stress at all. Here‚Äôs not the right place to report bugs so maybe try GitHub. TBG works fine with me, for now.",
                  "score": 1,
                  "created_utc": "2026-01-21 03:20:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a5dip",
          "author": "Illynir",
          "text": "Great job, will test today many of the nodes as a replacement for old nodes from other GitHubs.. Thanks",
          "score": 2,
          "created_utc": "2026-01-18 12:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09dy28",
          "author": "shinigalvo",
          "text": "Awesome!",
          "score": 1,
          "created_utc": "2026-01-18 08:34:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fwqvj",
          "author": "FitContribution2946",
          "text": "greast stuff!",
          "score": 1,
          "created_utc": "2026-01-19 07:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hwr8m",
          "author": "Then_Gas712",
          "text": "Bring back urgently our  Qwen3-Vl loaders ... rest update we don\"t care less!",
          "score": 1,
          "created_utc": "2026-01-19 16:11:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p6y7u",
              "author": "Narrow-Particular202",
              "text": "If you encounter any issues with [ComfyUI-QwenVL](https://github.com/1038lab/ComfyUI-QwenVL), please feel free to submit an [issue](https://github.com/1038lab/ComfyUI-QwenVL/issues) on our GitHub page.",
              "score": 1,
              "created_utc": "2026-01-20 17:21:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09e7as",
          "author": "edwios",
          "text": "Awesome works, thank you! Is it Apple Silicon friendly?",
          "score": 1,
          "created_utc": "2026-01-18 08:37:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qim6km",
      "title": "Complete FLUX.2 Klein Workflow",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qim6km",
      "author": "No_Percentage1138",
      "created_utc": "2026-01-21 02:57:58",
      "score": 203,
      "num_comments": 21,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qim6km/complete_flux2_klein_workflow/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0tp86o",
          "author": "oeufp",
          "text": "upvoted just for adding \".json\" to the file name on pastebin",
          "score": 25,
          "created_utc": "2026-01-21 08:56:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tjuo3",
          "author": "Relevant_Eggplant180",
          "text": "Thanks for sharing. Looks very useful. I will give it a go.",
          "score": 5,
          "created_utc": "2026-01-21 08:05:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tfa7v",
          "author": "Upset-Virus9034",
          "text": "pixaroma robot :)",
          "score": 4,
          "created_utc": "2026-01-21 07:23:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uifqt",
              "author": "No_Percentage1138",
              "text": "Yeah, and easy-install\nI just learn to use it this days",
              "score": 2,
              "created_utc": "2026-01-21 12:58:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0uj2ex",
                  "author": "Upset-Virus9034",
                  "text": "Thanks for sharing üôè",
                  "score": 2,
                  "created_utc": "2026-01-21 13:01:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0w3825",
          "author": "FvMetternich",
          "text": "that looks like a brillilant idea with the reference numbers! thanks for sharing that gold nugget.",
          "score": 4,
          "created_utc": "2026-01-21 17:35:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zm97e",
          "author": "wjc_5",
          "text": "This reference method is very convenient",
          "score": 3,
          "created_utc": "2026-01-22 04:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uzkpk",
          "author": "BarkLicker",
          "text": "I'm so sad that the images aren't zero-indexed. I don't care either way, but I wish the world was consistent in this.",
          "score": 2,
          "created_utc": "2026-01-21 14:33:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v04zr",
              "author": "No_Percentage1138",
              "text": "Yeah, but the node I use to preview the Images start from 1, that make the things harder",
              "score": 3,
              "created_utc": "2026-01-21 14:36:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v9tx0",
          "author": "sir-bantzalot",
          "text": "This is very interesting. How do you use the references in the second field? It says 2, 3 6",
          "score": 2,
          "created_utc": "2026-01-21 15:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w1v3f",
              "author": "No_Percentage1138",
              "text": "Well it loads all the images from the folder and in the field where I chose the images to use I put the names of the files, in the examples are 1,2,3,4,5,6 to make it easier to understand and more consistent with the Index. And if the name of the files apear in the list then it will load the images in the CLIP",
              "score": 4,
              "created_utc": "2026-01-21 17:29:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0y68lz",
          "author": "FreezaSama",
          "text": "What a boss thanks",
          "score": 2,
          "created_utc": "2026-01-21 23:20:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xzrc1",
          "author": "RayHell666",
          "text": "Is the bug with \"control after generate\" in subgraph fixed yet ?",
          "score": 1,
          "created_utc": "2026-01-21 22:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y2c6q",
              "author": "No_Percentage1138",
              "text": "If you mean that the file search range doesn‚Äôt update when the folder gets updated: no, I didn‚Äôt find a way to fix it. My temporary workaround is to rename the folder where the reference images are stored, and then update the path in the workflow.",
              "score": 2,
              "created_utc": "2026-01-21 23:00:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0y5ly9",
                  "author": "RayHell666",
                  "text": "no just talking about the seed control.",
                  "score": 1,
                  "created_utc": "2026-01-21 23:17:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ybqqn",
          "author": "Whole_Disk3247",
          "text": "Multi-image doesn't work as great as it seems. I had a few bad anatomical ones.",
          "score": 1,
          "created_utc": "2026-01-21 23:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ynssd",
              "author": "No_Percentage1138",
              "text": "Yeah, Flux.2 Klein isn't so good with anatomy. Maybe with some Lora or ControlNet you will have a better resoult",
              "score": 2,
              "created_utc": "2026-01-22 00:55:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yzlxg",
          "author": "juandann",
          "text": "can klein fix messed up anatomy?",
          "score": 1,
          "created_utc": "2026-01-22 02:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e2zmc",
          "author": "TheRealAncientBeing",
          "text": "Mine ends up in a \n\nmat1 and mat2 shapes cannot be multiplied (512x4096 and 12288x4096)\n\nany idea? tried with different, smaller background and images?",
          "score": 1,
          "created_utc": "2026-01-24 07:44:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e5mco",
              "author": "TheRealAncientBeing",
              "text": "Layer 8 problem: Misclicked the matching text encoder for the 9B.\n\nResults: It uses the background image but there is ZERO resemblance to the (people) reference pics, really nothing, except perhaps hair color.",
              "score": 1,
              "created_utc": "2026-01-24 08:07:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qh9sff",
      "title": "Flux2 Klein performs exceptionally well, surpassing the performance of the trained LoRA in many aspects, whether for image editing or text-to-image conversion. I highly recommend testing it out. Tutorial link: https://youtu.be/F7gokUkzSnc",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qh9sff",
      "author": "Daniel81528",
      "created_utc": "2026-01-19 17:01:22",
      "score": 197,
      "num_comments": 29,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qh9sff/flux2_klein_performs_exceptionally_well/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0kv0jd",
          "author": "admajic",
          "text": "https://preview.redd.it/3yepuxe8heeg1.jpeg?width=1376&format=pjpg&auto=webp&s=a247a5aaaa006305c9764c907c2be96cecfcb952\n\nJust did this for a mate. In a few steps 1.5 mins work\n\nCrazy can get same car same number plate same wheels",
          "score": 7,
          "created_utc": "2026-01-20 00:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qxhzr",
              "author": "ANR2ME",
              "text": "Is it just me, or the numbers in the plate reflection didn't get inverted vertically ü§î\n\nAre you using 4B or 9B model?",
              "score": 1,
              "created_utc": "2026-01-20 22:08:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0qzpvz",
                  "author": "admajic",
                  "text": "9b. Took a few goes just to get the number plate legible",
                  "score": 2,
                  "created_utc": "2026-01-20 22:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jlyn3",
          "author": "pepitogrillo221",
          "text": "Tell the prompt for every image gen, looks great!",
          "score": 6,
          "created_utc": "2026-01-19 20:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kj0xa",
              "author": "Daniel81528",
              "text": "The video explains the precautions for using prompt words.",
              "score": 5,
              "created_utc": "2026-01-19 23:33:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lf7gu",
                  "author": "pepitogrillo221",
                  "text": "Watching it, thanks.",
                  "score": 1,
                  "created_utc": "2026-01-20 02:28:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0lx83w",
          "author": "Eydahn",
          "text": "First of all, I just wanted to thank you so much for this awesome workflow. I watched the video to see how it works, and everything is super clear. I honestly find it really practical and effective.\n\nThat said, I wanted to ask you something: do you think it‚Äôs possible to use two images (Image 1 and Image 2) and do something similar to OpenPose, but for facial expressions? Basically, I‚Äôd like to change the facial expression of Image 2 while keeping the same facial features/identity, using the expression from Image 1 as the reference.",
          "score": 3,
          "created_utc": "2026-01-20 04:09:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mfepr",
              "author": "admajic",
              "text": "yeah you can do it with two image but even better with one. maybe get everything together then prompt the facial expression",
              "score": 5,
              "created_utc": "2026-01-20 06:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mfzg4",
                  "author": "Eydahn",
                  "text": "I tried these approaches with some anime closeup face images: \n\nInpaint: it changes the expression, but it doesn‚Äôt faithfully preserve the character‚Äôs facial features/identity.\n\nUsing Image 1 and Image 2 (taking the expression from Image 1): in my case it didn‚Äôt work at all.\n\t\nI haven‚Äôt tried a simple edit yet though",
                  "score": 1,
                  "created_utc": "2026-01-20 06:20:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0n6y6x",
          "author": "Kuldeep_music",
          "text": "Just a little advice, put links in the description section so it can be clickable üòÄ",
          "score": 2,
          "created_utc": "2026-01-20 10:24:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uvp34",
              "author": "Daniel81528",
              "text": "Haha, I was lazy, but even if I put the link in the article, people would still ask where the link is, haha.",
              "score": 0,
              "created_utc": "2026-01-21 14:12:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kvjyd",
          "author": "admajic",
          "text": "For the first image try dark soul purple crystal looks way sicker",
          "score": 1,
          "created_utc": "2026-01-20 00:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n5wbq",
          "author": "Professional_Test_80",
          "text": "Thank you for the tutorial! What was your prompt for the first image? The one with the car body paint transfer.",
          "score": 1,
          "created_utc": "2026-01-20 10:15:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uvsru",
              "author": "Daniel81528",
              "text": "The video explains it in detail; you can take a closer look.",
              "score": 1,
              "created_utc": "2026-01-21 14:13:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0sehd3",
          "author": "TomatoInternational4",
          "text": "Test more of the human body in potentially difficult poses and with multiple people.",
          "score": 1,
          "created_utc": "2026-01-21 02:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tt3z0",
          "author": "Relevant_Eggplant180",
          "text": "Thank you for sharing this workflow and tutorial!",
          "score": 1,
          "created_utc": "2026-01-21 09:34:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ybk7p",
          "author": "spacemidget75",
          "text": "Hey Daniel, I'm having trouble getting the masking to work with your WF. I did swap the filll holes and gausion mask nodes with \"Grow Mask With Blur\" from KJNodes, but I've also tried without it. Sometimes it either doesn't create anything in the mask or it does but it's a very blured image.",
          "score": 1,
          "created_utc": "2026-01-21 23:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18avhs",
              "author": "Daniel81528",
              "text": "I've never encountered this problem before.",
              "score": 1,
              "created_utc": "2026-01-23 12:56:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0k5j9o",
          "author": "YMIR_THE_FROSTY",
          "text": "Advertising photography is dead and buried.",
          "score": 2,
          "created_utc": "2026-01-19 22:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mk3bp",
              "author": "protector111",
              "text": "Commercial photography is dead for a long time. Iv been selling ai ‚Äúphotos‚Äù on adobe stock for 3 years now. Real photos sales dropped to almost zero for lots of ppl including me.",
              "score": 3,
              "created_utc": "2026-01-20 06:54:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0opiez",
                  "author": "addandsubtract",
                  "text": "Does Adobe have an AI section or are you selling them as \"photos\"?",
                  "score": 2,
                  "created_utc": "2026-01-20 16:00:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0kj2av",
              "author": "Daniel81528",
              "text": "üòÇ",
              "score": 2,
              "created_utc": "2026-01-19 23:33:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0lbgew",
              "author": "ReasonablePossum_",
              "text": "Not in 2 years more at least (maybe one if things advance fast enough tho). Ad photography needs exact depiction of a product and quite big resolutions and control over the flow, which we¬¥re still a bit far to achieve.\n\nWhat will be dead soon, is all the backend of editing and VFX to create artsy contexts for images and stuff, lot of technical assistants will have to find a new job, but photo studios and ads will be just outputting a lot better commercial imagery (at least technically, conceptually, i very much doubt as lots of people with bad taste are about to get big possibilities to make stuff lmao..... the slop age is coming)",
              "score": 1,
              "created_utc": "2026-01-20 02:07:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0lc6pc",
                  "author": "YMIR_THE_FROSTY",
                  "text": "Yea I meant more or less not actual taking picture of product, but everything else. But actual advert and product photography was a lot more than just picture of product.\n\nYou basically make few pics of product and then you can put it anywhere you want, re-light it, literally whatever you want.",
                  "score": 0,
                  "created_utc": "2026-01-20 02:11:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0kivbs",
          "author": "Winter_unmuted",
          "text": "Low effort post that is basically youtube linkspam?",
          "score": -6,
          "created_utc": "2026-01-19 23:32:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ko768",
              "author": "pepitogrillo221",
              "text": "You are talking about Daniel81528 the man who made the best qwen edit loras... Please respect...",
              "score": 11,
              "created_utc": "2026-01-20 00:01:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfpnbc",
      "title": "Inspyrenet is absolute magic for background removal. Simple, clean, and effective workflow.",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/pan4jlov7zdg1.png",
      "author": "Otherwise_Ad1725",
      "created_utc": "2026-01-17 21:38:08",
      "score": 192,
      "num_comments": 30,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qfpnbc/inspyrenet_is_absolute_magic_for_background/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o08dlyf",
          "author": "gweilojoe",
          "text": "Does this add an alpha channel to the image file for the isolated image? If not, it really doesn‚Äôt solve much.",
          "score": 16,
          "created_utc": "2026-01-18 03:51:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a1dot",
              "author": "inagy",
              "text": "There's the \"Join Image with Alpha\" node in Comfy for that, it joins the mask as an alpha channel. (you might need to invert the mask with \"Invert Mask\")",
              "score": 9,
              "created_utc": "2026-01-18 12:07:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0vk7ro",
                  "author": "lolxdmainkaisemaanlu",
                  "text": "I tried this and when I opened the saved image in Photoshop, it still didn't have an alpha in channels tab. Is it showing up for you ?",
                  "score": 1,
                  "created_utc": "2026-01-21 16:10:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o09w7mp",
              "author": "lolxdmainkaisemaanlu",
              "text": "I would appreciate this feature, looking for a custom node which can do this.. please let me know if u are aware of a node which can do this.",
              "score": 1,
              "created_utc": "2026-01-18 11:23:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0a9zty",
              "author": "diffusion_throwaway",
              "text": "Good question. I see the node has an output for ‚Äúmask‚Äù so‚Ä¶hopefully?",
              "score": 1,
              "created_utc": "2026-01-18 13:13:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o07m08o",
          "author": "infearia",
          "text": "Have you included BiRefNet-HR in your tests? Because I'd be really surprised if InSPyReNet turned out to perform better...",
          "score": 5,
          "created_utc": "2026-01-18 01:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cltdo",
              "author": "Otherwise_Ad1725",
              "text": "Based on the workflow provided, only InSPyReNet was used (Node ID 2)\n\nI haven't added BiRefNet-HR to this specific comparison yet\n\nbut I agree it would be a strong contender especially for high-resolution details",
              "score": 1,
              "created_utc": "2026-01-18 20:11:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o06sfa0",
          "author": "Illynir",
          "text": "Interesting, I needed exactly this kind of thing to perform mass background removal on a few datasets.",
          "score": 3,
          "created_utc": "2026-01-17 22:45:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06hiz4",
          "author": "Otherwise_Ad1725",
          "text": "https://preview.redd.it/rwxp7z7hdzdg1.png?width=2663&format=png&auto=webp&s=f81f33cc1943df96f551bdca6cb39c140819d6fd",
          "score": 4,
          "created_utc": "2026-01-17 21:50:54",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o087hbg",
          "author": "Ciprianno",
          "text": "Thanks for sharing !",
          "score": 1,
          "created_utc": "2026-01-18 03:16:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cm3dg",
              "author": "Otherwise_Ad1725",
              "text": "You are welcome",
              "score": 2,
              "created_utc": "2026-01-18 20:13:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09tusy",
          "author": "Powerful_Evening5495",
          "text": "Works great, thanks. \n\nI did not work with a mask before, but this is giving me some good ideas.",
          "score": 1,
          "created_utc": "2026-01-18 11:02:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cmh9b",
              "author": "Otherwise_Ad1725",
              "text": "thanks",
              "score": 1,
              "created_utc": "2026-01-18 20:15:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09ybp9",
          "author": "janosibaja",
          "text": "Very good, thank you! I wonder what would be the most effective way to add a new background afterwards?",
          "score": 1,
          "created_utc": "2026-01-18 11:42:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cmo2u",
              "author": "Otherwise_Ad1725",
              "text": "thank you!",
              "score": 1,
              "created_utc": "2026-01-18 20:15:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09zw80",
          "author": "FreezaSama",
          "text": "How is this different from photoshop? As in can we then chain this together with other nodes for some comfy magic?",
          "score": 1,
          "created_utc": "2026-01-18 11:55:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cn6gd",
              "author": "Otherwise_Ad1725",
              "text": "free",
              "score": 4,
              "created_utc": "2026-01-18 20:18:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ftg0c",
                  "author": "FreezaSama",
                  "text": "Sure... but so is GIMP it doesn't do the same thing at all. People that use these tools professionally want to keep control, precision, resolution etc.",
                  "score": 1,
                  "created_utc": "2026-01-19 07:23:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ckpz1",
          "author": "boi-the_boi",
          "text": "How much better is this than BEN2?",
          "score": 1,
          "created_utc": "2026-01-18 20:06:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0co71k",
              "author": "Otherwise_Ad1725",
              "text": "In terms of pure edge quality and handling complex details (like hairŸÑÿß- fur - or transparency) **InSPyReNet is significantly better**",
              "score": 2,
              "created_utc": "2026-01-18 20:23:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0d3pp2",
          "author": "Temporary-Roof2867",
          "text": "I used \"Image Remove Bg\" but it left holes with gray! ü§™ but this \"Inspyrenet Rembg\" is a hundred times better! üíñ\n\nThanks so much Bro!",
          "score": 1,
          "created_utc": "2026-01-18 21:46:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0dh3pf",
              "author": "Otherwise_Ad1725",
              "text": "thanks",
              "score": 2,
              "created_utc": "2026-01-18 22:49:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0g6988",
          "author": "Gnarlsko",
          "text": "I get a weird error, that numpy uses an outdated alias.. Anyone know how to fix this and might share their knowledge?\n\n\"Alias 'bool8' was removed in NumPy 2.0. Use a name without a digit at the end.\"",
          "score": 1,
          "created_utc": "2026-01-19 09:21:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ycvg5",
          "author": "_magvin",
          "text": "clean edges without stacking ten nodes is always a win in comfyui, so this setup makes sense. inspyrenet seems to handle semi-transparent areas more gracefully than most. once the background is out, i‚Äôve noticed people often jump into uniconverter for quick batch resizing or format cleanup without reopening comfyui.",
          "score": 1,
          "created_utc": "2026-01-21 23:56:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11swqh",
          "author": "Ann7kbell",
          "text": "Works great, or at least better than some alternatives like workflows bloated with nodes.\n\nPros:\n\n\\- Really fast.  \n\\- Clean removal of background (much better if it was a simple color).  \n\\- Being 1 node really makes it easy to integrate in other workflows.\n\nCons:\n\n\\- It doesn't handle well complex or multi-subject images (I used an image of a golf ball on a sand mount in a white background and in the result only the golf ball appeared, however there were also bits of sand that made the result very bad. I also tested cartoonish images on white backgrounds and sometimes it removes important things, for example, a flower on a vase with a simple color background it should leave the flower AND the vase, but sometimes it removes the flower, other times the vase and so on)\n\nIn this case it shows a bit of what I mean. The grass is part of the whole image, but you can see that the grass was about to be removed but not fully.\n\nhttps://preview.redd.it/t8zc5pebsweg1.png?width=1024&format=png&auto=webp&s=3224f777cc685d993369a07d1c765765ed3cccdc\n\nIn any case, it's a great option with some details.",
          "score": 1,
          "created_utc": "2026-01-22 14:16:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qipxhx",
      "title": "I ported my personal prompting tool into ComfyUI - A visual node for building cinematic shots",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qipxhx/i_ported_my_personal_prompting_tool_into_comfyui/",
      "author": "shamomylle",
      "created_utc": "2026-01-21 05:59:49",
      "score": 167,
      "num_comments": 45,
      "upvote_ratio": 0.99,
      "text": "https://reddit.com/link/1qipxhx/video/jqr07t0smneg1/player\n\nhttps://preview.redd.it/2u6d7as9iueg1.png?width=1524&format=png&auto=webp&s=42e4b9a7c6e09ec1362e3a2f4e097f36c6a39d04\n\nHi everyone,\n\nI wanted to share my very first custom node for ComfyUI. I'm still very new to ComfyUI (I usually just do 3D/Unity stuff), but I really wanted to port a personal tool I made into ComfyUI to streamline my workflow.\n\nI originally created this tool as a website to help me self-study cinematic shots, specifically to memorize what different camera angles, lighting setups (like Rembrandt or Volumetric), and focal lengths actually look like (link to the original tool : [https://yedp123.github.io/](https://yedp123.github.io/)).\n\n**What it does:** It replaces the standard CLIP Text Encode node but adds a visual interface. You can select:\n\n* Camera Angles (Dutch, Low, High, etc.)\n* Lighting Styles\n* Focal Lengths & Aperture\n* Film Stocks & Color Palettes\n\nIt updates the preview image in real-time when you hover over the different options so you can see a reference of what that term means before you generate. You can also edit the final prompt string if you want to add/remove things. It outputs the string + conditioning for Stable Diffusion, Flux, Nanobanana or Midjourney.\n\nLike I mentioned above, I just started playing with ComfyUI so I am not sure if this can be of any help to any of you or if there are flaws with it, but here's the link if you want to give it a try. Thanks, Have a good day!\n\n**Links:** [https://github.com/yedp123/ComfyUI-Cinematic-Prompt](https://github.com/yedp123/ComfyUI-Cinematic-Prompt)\n\n***-----------------------------------------------------------------------------------------***\n\n***UPDATE: added \"Cinematic Reference Loader\", an Image Loader node which lets the user select an image among the image assets to use for Image-to-Image workflows***",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qipxhx/i_ported_my_personal_prompting_tool_into_comfyui/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o0thf5z",
          "author": "BarGroundbreaking624",
          "text": "Can you output the image to use as depth map or for image to image ? I‚Äôve been prototyping that but not close to how this looks.",
          "score": 10,
          "created_utc": "2026-01-21 07:42:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tkwnu",
              "author": "shamomylle",
              "text": "Oh, I see what you mean! currently, the images in the node are just static reference jpg to help visualize the prompt terms (like a dictionary). The node only outputs the Text and Conditioning data, not the actual image shown in the preview, sorry for the confusion !  \n  \nFor now, the preview images are just static assets stored in the node's folder. The node doesn't currently have an `IMAGE` output to send them directly to other nodes via wires.\n\nHowever, if you really like a specific reference image (like the \"Low Angle\" shot for instance) and want to use it for ControlNet, you can find all the source images in your `custom_nodes/ComfyUI_Cinematic_Prompt/web/assets/` folder and load them into a standard 'Load Image' node",
              "score": 5,
              "created_utc": "2026-01-21 08:15:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0tn8c5",
                  "author": "Gilgameshcomputing",
                  "text": "agree that getting that image output would be really useful - I suspect Klein would make short work of turning it into a decent image.",
                  "score": 5,
                  "created_utc": "2026-01-21 08:37:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0u9cl7",
                  "author": "SvenVargHimmel",
                  "text": "Unfortunately i don't know much about the web/\\*\\* folder when building a node. Is there a way to keep the choices made in the UI in-sync with the node backend. \n\nI think I can make the image out change but I don't know enough about the frontend and backend sync designs.  Any pointers ?",
                  "score": 1,
                  "created_utc": "2026-01-21 11:55:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tl6ra",
              "author": "noyart",
              "text": "Would be cool if you could do a simple 3D scene in comfyui. But you can do scenes in either blender and daz3D.¬†\n\n\nMy last try I made a scene in DaZ3D and than renderd it(dont need to be perfect), put that image in comfyui, convert to depth map with depth anything v3. It worked really good.",
              "score": 2,
              "created_utc": "2026-01-21 08:17:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tniw0",
                  "author": "shamomylle",
                  "text": "I hear you! I also do 3D block out that I feed to Nanobanana Pro at the moment, but I would eventually like to move to ComfyUI, I still have to learn the basics! Building a full 3D viewport inside a node would be an awesome project, but definitely a massive undertaking compared to this!",
                  "score": 5,
                  "created_utc": "2026-01-21 08:40:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o106xld",
              "author": "shamomylle",
              "text": "Just included an Image Loader node to let you select and use these preview pictures for Image-to-Image, you just need to redownload and replace your current *ComfyUI\\_Cinematic\\_Prompt* folder :)",
              "score": 2,
              "created_utc": "2026-01-22 06:48:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0towua",
          "author": "Past_Ad6251",
          "text": "Thank for sharing! One tip for guys using Qwen Image, if you put camera brand in the prompt, you will find the camera itself in the generated image, which may not be what you needed.",
          "score": 5,
          "created_utc": "2026-01-21 08:53:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tpsd5",
              "author": "shamomylle",
              "text": "Thanks a lot for the feedback, that's good to know!   \nI haven't played much with Qwen yet but this is a Chinese model, you might have to go through some prompt translation to get it to work, it is most likely a lot better at adherence when it is written in Chinese.",
              "score": 1,
              "created_utc": "2026-01-21 09:01:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0tbvcn",
          "author": "Momkiller781",
          "text": "This looks fantastic!  I'll try it today. Thank you for sharing!",
          "score": 2,
          "created_utc": "2026-01-21 06:52:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tkbjx",
          "author": "Zakki_Zak",
          "text": "I wish I had your self discipline!",
          "score": 2,
          "created_utc": "2026-01-21 08:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tj1js",
          "author": "pharaohfx",
          "text": "Wow",
          "score": 1,
          "created_utc": "2026-01-21 07:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tkffy",
          "author": "Substantial_Aid",
          "text": "Will give it a try later today. Thank you for this!",
          "score": 1,
          "created_utc": "2026-01-21 08:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tldzn",
          "author": "jumpinthewatersnice",
          "text": "Having worked in film and tv for over 20 years im looking forward to testing this",
          "score": 1,
          "created_utc": "2026-01-21 08:19:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tmsp2",
              "author": "shamomylle",
              "text": "You would probably find some of the preview pictures highly inaccurate or find the tool incomplete, but I look forward to your feedbacks and if you think it can be a decent educational tool, thanks! :)",
              "score": 1,
              "created_utc": "2026-01-21 08:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0vq57a",
                  "author": "oodelay",
                  "text": "Doesn't matter, I also worked in the domain and words follow trends and locality. A pan shot can be explained twelve ways by twelve experts.",
                  "score": 1,
                  "created_utc": "2026-01-21 16:37:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tm7cx",
          "author": "mrgonuts",
          "text": "Looks intresting thanks",
          "score": 1,
          "created_utc": "2026-01-21 08:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tv2x3",
          "author": "TheTimster666",
          "text": "Looks great, gotta try the node!   \nOne thing: I only tried it on the website so don't know if the node is the same, but after you choose something, eg \"Lighting\"->\"Blue Hour\" you can't deselect/default that option to \"None/Default\" again.",
          "score": 1,
          "created_utc": "2026-01-21 09:52:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tx2sr",
              "author": "shamomylle",
              "text": "You are totally right, I can't believe I missed such an important feature, I will update the node right away! Good catch, thanks a lot!\n\n**EDIT:** Made it so that everything can be selected manually and added a \"Reset\" button to unselect everything.",
              "score": 2,
              "created_utc": "2026-01-21 10:11:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0txwr4",
                  "author": "TheTimster666",
                  "text": "Awesome :-)",
                  "score": 1,
                  "created_utc": "2026-01-21 10:19:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0txvsy",
          "author": "KeyTumbleweed5903",
          "text": "gettign error - any idea why\n\nhttps://preview.redd.it/0f32l22phoeg1.png?width=1235&format=png&auto=webp&s=2d768ab135ec87627ea22bff54bb1d8244c7fdc8",
          "score": 1,
          "created_utc": "2026-01-21 10:19:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzrdg",
              "author": "shamomylle",
              "text": "it is hard to tell from the picture, it seems everything is properly connected, can you show the exact error message?",
              "score": 2,
              "created_utc": "2026-01-21 10:36:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u1wsd",
                  "author": "KeyTumbleweed5903",
                  "text": "https://preview.redd.it/up2b55d5ooeg1.png?width=943&format=png&auto=webp&s=4f181480b64d178fe697bd62a72cb47886e48270",
                  "score": 1,
                  "created_utc": "2026-01-21 10:55:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uwj93",
          "author": "Mysterious_Pride_858",
          "text": "Excellent node. It allows for intuitive viewing of examples for each prompt. Are there any plans to add Z-image or Flux2 klein? I tested the Flux prompts on Flux klein2, and there were significant issues with generating the structure of human figures.",
          "score": 1,
          "created_utc": "2026-01-21 14:17:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uyw4a",
              "author": "shamomylle",
              "text": "Thanks for the feedback, ~~I never tried it with these models, I would have to do some tests with various prompt styles to understand how it behaves, did you try with a simple text encode before or simpler prompt too? Does it manage to generate good pictures? If yes, I would be interested in knowing what prompt you used, thanks!~~ Also you can try adding things like negative prompting, such as \"bad anatomy, missing limbs, deformed hands/face\" and see if there is any improvement.\n\n**UPDATE:** make sure you have your KSampler denoise set to 1.0 if you aren't doing any editing and just trying to generate pictures, it should avoid any kind of \"creative\" liberties.\n\nhttps://preview.redd.it/l8bnns4jlteg1.png?width=371&format=png&auto=webp&s=abf538391fe3679f5846afce2f85f13917db2878",
              "score": 1,
              "created_utc": "2026-01-21 14:29:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wle9w",
          "author": "Sad-Investigator-81",
          "text": "this is really cool !  for some reason though midjourny ignores the camera angle part of the prompt",
          "score": 1,
          "created_utc": "2026-01-21 18:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xy2z1",
              "author": "shamomylle",
              "text": "Thanks for the feedback, this is weird, I do not use midjourney but I will check if there is an issue with the final prompt formatting. Is the camera angle the only issue you have or do you also have any other issues when it comes to prompt adherence?\nFor now (if you want), you can try to use another model format or try to copy the generated prompt in a regular CLIP text encode to see if the issue really comes with my node or the prompt itself",
              "score": 1,
              "created_utc": "2026-01-21 22:38:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zcj3j",
          "author": "SheepherderNo6921",
          "text": "very interesting! Congratulations, it looks fantastic :D",
          "score": 1,
          "created_utc": "2026-01-22 03:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o128jjz",
              "author": "shamomylle",
              "text": "Thanks for the kind words :)",
              "score": 1,
              "created_utc": "2026-01-22 15:33:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16u09l",
          "author": "Kaoru-Kun",
          "text": "Hey firstly, thank you so much! I manually do similar inputs so I know this tool will save me so much time!!\n\nIn your video you can scroll through the options. When I added it to my workflow, I don‚Äôt get the scroll bar (just one super large list) and therefore I can‚Äôt see the preview when I am selecting some of the options at the bottom of the list.¬†\nThe whole node seems to be locked in on its size, how can that be fixed?¬†",
          "score": 1,
          "created_utc": "2026-01-23 05:38:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o176zqh",
              "author": "shamomylle",
              "text": "Hello, thank you for your feedback!  \nSorry to hear you are getting an issue with the scrolling, are both nodes acting the same way? Did you try to rescale the node, delete them and add them again in the project? Also if you install/update the nodes while your ComfyUI session is open it might mess with the cache files or not work properly.  \nJust for test I redownloaded and reinstalled the node from the github repository and it behaves normally.\n\nLet me know if anything worked.",
              "score": 2,
              "created_utc": "2026-01-23 07:23:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17a5tj",
                  "author": "Kaoru-Kun",
                  "text": "Hi, thanks for the quick feedback. Sorry, giving input to what I have tried would have helped the discussion (thought in hindsight).¬†\n\nI installed the node directly by cloning the GitHub repository into my folder which worked fine.¬†\n\nAfter inserting the node into the workflow, I can expand the node to make it bigger, but somehow it cannot be resized to anything smaller than the list.¬†\n\nThis is the same for both nodes that come with the tool.¬†\n\nAn update and restart to comfy don‚Äôt fix it.¬†\n\nI‚Äôm running Comfy 3.39.2.¬†\n\nAnyway, no big deal. I love the tool a lot and am using it from the website now to help design the prompt.¬†\nPerhaps this weekend I‚Äôll try to look into the issue more and if I can work something out I‚Äôll post my solution here.¬†",
                  "score": 1,
                  "created_utc": "2026-01-23 07:51:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qljtix",
      "title": "üéôÔ∏è A New Voice Has Arrived ‚Äî Qwen3-TTS Custom Node for ComfyUI Is Here",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qljtix",
      "author": "Narrow-Particular202",
      "created_utc": "2026-01-24 10:38:44",
      "score": 145,
      "num_comments": 20,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/comfyui/comments/1qljtix/a_new_voice_has_arrived_qwen3tts_custom_node_for/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1erxk1",
          "author": "SubstantialYak6572",
          "text": "Looks interesting but those Huggingface hub files are just annoying with all those extra json files. Hopefully someone will package it into a single safetensors or gguf file so it doesn't clutter the model folder up.",
          "score": 4,
          "created_utc": "2026-01-24 11:31:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f8crl",
              "author": "ANR2ME",
              "text": "GGUF for the 1.7b model https://huggingface.co/mradermacher/Qwen3-1.7B-Multilingual-TTS-GGUF\n\nBut not sure whether this is the same model or not üòÖ since the files were uploaded on September 2025.",
              "score": 5,
              "created_utc": "2026-01-24 13:33:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f4cu4",
          "author": "RatioTheRich",
          "text": "cann't isntall requirements.txt because of depedency conflicts with huggingface\\_hub",
          "score": 5,
          "created_utc": "2026-01-24 13:08:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f4l60",
              "author": "RatioTheRich",
              "text": "ERROR: Cannot install -r requirements.txt (line 1), -r requirements.txt (line 11), huggingface\\_hub>=1.3.2 and transformers==4.57.3 because these package versions have conflicting dependencies.\n\n\n\nThe conflict is caused by:\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.3 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.6 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.5 depends on huggingface-hub<1.0 and >=0.34.0\n\nThe user requested huggingface\\_hub>=1.3.2\n\naccelerate 1.12.0 depends on huggingface\\_hub>=0.21.0\n\ntransformers 4.57.4 depends on huggingface-hub<1.0 and >=0.34.0",
              "score": 4,
              "created_utc": "2026-01-24 13:09:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1favaj",
          "author": "MortgageOutside1468",
          "text": "This guy also made a comfyui node:  \n[https://github.com/DarioFT/ComfyUI-Qwen3-TTS](https://github.com/DarioFT/ComfyUI-Qwen3-TTS)\n\nFound it from here:  \n[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice/discussions/7](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice/discussions/7)\n\nDon't ask me about the implementation though i haven't checked the node.",
          "score": 2,
          "created_utc": "2026-01-24 13:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eo44v",
          "author": "Atmey",
          "text": "Would be nice to hear an example or sample",
          "score": 5,
          "created_utc": "2026-01-24 10:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1erb7v",
              "author": "Coloniaman",
              "text": "You can use the example on huggingface and use your own text!",
              "score": 5,
              "created_utc": "2026-01-24 11:26:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eyvev",
          "author": "Nokai77",
          "text": "I think he's missing the ability to inject emotion into a cloned voice... FAILURE",
          "score": 4,
          "created_utc": "2026-01-24 12:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1g2ihr",
              "author": "ronbere13",
              "text": "you're right",
              "score": 1,
              "created_utc": "2026-01-24 16:11:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fu3wn",
          "author": "Ecstatic_Sale1739",
          "text": "I can‚Äôt find it in comfyui manager..üò¢üò¢",
          "score": 1,
          "created_utc": "2026-01-24 15:32:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fx0b4",
          "author": "GravitationalGrapple",
          "text": "It‚Äôs going to take a lot for me to move away from IndexTTS2. Voice cloning, emotional control, good cadence‚Ä¶ I just wish I could get the nodes from snicolast to work, the UI it comes with is just okay.\n\nhttps://github.com/snicolast/ComfyUI-IndexTTS2",
          "score": 1,
          "created_utc": "2026-01-24 15:46:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fx17l",
          "author": "NoBuy444",
          "text": "Oh yeah !!!  And by 1038lab. Boy, we're spoiled :-D",
          "score": 1,
          "created_utc": "2026-01-24 15:46:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1g6fj8",
          "author": "RazsterOxzine",
          "text": "One more key component for AI world domination, but in a friendly voice you recognize, because it will already know everything about you. Well done. gg wp.",
          "score": 1,
          "created_utc": "2026-01-24 16:29:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1exnaj",
          "author": "Key_Highway_8728",
          "text": "Apparently needs shitloads of vram?  \"If your machine has less than 96GB of RAM...\"",
          "score": 1,
          "created_utc": "2026-01-24 12:19:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f205x",
              "author": "BarGroundbreaking624",
              "text": "Is that for training because I‚Äôve been running it and the 1.7b  models only seem to use half my 24gb VRAM \n\n\nThe clone seems pretty good and voice design too. \nI don‚Äôt know why I always try to do something awkward - you can‚Äôt tell a cloned voice to use emotion in the same was as custom voice so it has to infer it from the spoken words‚Ä¶",
              "score": 7,
              "created_utc": "2026-01-24 12:51:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1f7zfc",
              "author": "aeroumbria",
              "text": "I think that is for manually compiling flash attention if you have like 64GB of RAM but a 9950. In this case you don't have enough RAM to run 16 or 32 threads of compilation, so you have to cap the jobs to a smaller number.",
              "score": 2,
              "created_utc": "2026-01-24 13:31:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fciyc",
              "author": "raz0099",
              "text": "Or just give it a try with wangp.",
              "score": 2,
              "created_utc": "2026-01-24 13:58:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fcapa",
          "author": "EnvironmentalDust229",
          "text": "This is amazing!",
          "score": 1,
          "created_utc": "2026-01-24 13:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ffq3r",
          "author": "SnooPuppers4132",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-24 14:16:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkowfo",
      "title": "Creative Code",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/yoec80at83fg1",
      "author": "skbphy",
      "created_utc": "2026-01-23 11:58:30",
      "score": 138,
      "num_comments": 12,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkowfo/creative_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o184mdx",
          "author": "LeKhang98",
          "text": "ELI5 please? Is it a node that I can put simple code directly into it to process input (image, latent, audio, etc)? As a non-coder I'd love to be able to do that.",
          "score": 5,
          "created_utc": "2026-01-23 12:15:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o186h4q",
              "author": "skbphy",
              "text": "It‚Äôs a creative coding node (GLSL/ShaderToy + p5.js) that renders images/animation frames in ComfyUI. Even if you don‚Äôt code, you can use built-in animations and the AI(ollama) helper. The focus is visuals/animation, not general audio processing(You *can* use the rendered image in latent workflows by encoding it back to latent), but the node itself doesn‚Äôt directly process latents.).  English isn‚Äôt my first language, but I hope I explained it clearly.",
              "score": 7,
              "created_utc": "2026-01-23 12:28:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19ji8t",
                  "author": "LeKhang98",
                  "text": "Thank you I understand it now.",
                  "score": 2,
                  "created_utc": "2026-01-23 16:38:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18lz63",
          "author": "FreezaSama",
          "text": "I would love to try this or similar IF it takes the input image in consideration. This node doesn't seem to do that all the times?",
          "score": 4,
          "created_utc": "2026-01-23 13:59:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18rxg1",
              "author": "skbphy",
              "text": "[https://github.com/SKBv0/ComfyUI\\_CreativeCode/blob/main/HOW\\_TO\\_USE.md#using-textures](https://github.com/SKBv0/ComfyUI_CreativeCode/blob/main/HOW_TO_USE.md#using-textures) check this out",
              "score": 1,
              "created_utc": "2026-01-23 14:29:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18lsd5",
          "author": "bingobongo3001",
          "text": "I just love the way it looks!",
          "score": 2,
          "created_utc": "2026-01-23 13:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bpehf",
          "author": "pwillia7",
          "text": "Dude COOL! -- Does this work fine with the API?",
          "score": 2,
          "created_utc": "2026-01-23 22:40:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c5m5z",
              "author": "skbphy",
              "text": "No idea. I haven't tested it.",
              "score": 1,
              "created_utc": "2026-01-24 00:06:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o186xf5",
          "author": "ANR2ME",
          "text": "Btw, what's the output string (source code)blooked like? in which language is the output source code? ü§î",
          "score": 1,
          "created_utc": "2026-01-23 12:31:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18800c",
              "author": "skbphy",
              "text": "It simply passes the code written in the editor or settings node as a string. It‚Äôs not important on its own. I added it for automation.",
              "score": 2,
              "created_utc": "2026-01-23 12:38:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19cj0l",
          "author": "35point1",
          "text": "So basically this is a code snippet generator node with a built in preview and customized widget ui‚Ä¶\n\nI‚Äôve never used those libraries you mentioned but does your node just spit out a js function or similar to produce a canvas animation? I really like the customization to accomplish this!",
          "score": 1,
          "created_utc": "2026-01-23 16:07:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c5hz9",
              "author": "skbphy",
              "text": "Pretty much but it‚Äôs more than a snippet generator. it actually runs the code with a live preview and you can even turn // uniform ... comments into UI sliders / color pickers.  here  [https://github.com/SKBv0/ComfyUI\\_CreativeCode?tab=readme-ov-file#interactive-uniforms](https://github.com/SKBv0/ComfyUI_CreativeCode?tab=readme-ov-file#interactive-uniforms)",
              "score": 1,
              "created_utc": "2026-01-24 00:06:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhdrg6",
      "title": "ZSampler Turbo: new sampler for Z-Image with high prompt adherence and good level of details.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qhdrg6",
      "author": "FotografoVirtual",
      "created_utc": "2026-01-19 19:18:55",
      "score": 136,
      "num_comments": 19,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhdrg6/zsampler_turbo_new_sampler_for_zimage_with_high/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0j7tpu",
          "author": "MomentTimely8277",
          "text": "Looks good but prompt adherence is not always accurate :(. I like the simplicity of the setting, but changing cfg would be a plus. Any way nice work. here's a picture (no lora and bf16 z-image turbo + ultraflux vae, 9steps, no style applied)\n\nhttps://preview.redd.it/652it0s40deg1.png?width=1312&format=png&auto=webp&s=a6757f168efdf59152c1a8ec2842c6fdb31dd4d5",
          "score": 13,
          "created_utc": "2026-01-19 19:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jh779",
              "author": "MaxDaClog",
              "text": "Love those glass beer cans üòÅ was it a mistake or prompted?",
              "score": 7,
              "created_utc": "2026-01-19 20:25:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0kddxq",
                  "author": "wh33t",
                  "text": "I actually wanna buy that.",
                  "score": 4,
                  "created_utc": "2026-01-19 23:03:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0ju45a",
                  "author": "MomentTimely8277",
                  "text": "50/50 üòÅ I asked for beer cans with the \"nervous breakdown\" logo on it",
                  "score": 3,
                  "created_utc": "2026-01-19 21:27:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0jayh6",
              "author": "FotografoVirtual",
              "text": "Thanks for the feedback! That image looks fantastic, haha.\n\nWould you be willing to share the workflows where it struggled the most? I've been using the failed attempts as a base for refining the process.\n\nRegarding enabling CFG adjustments, I'm still hesitant about that feature for a couple of reasons: generation times would roughly double, and it would add another variable to manage during testing, which significantly complicates development. But your input is really valuable and allows me to consider all angles.",
              "score": 4,
              "created_utc": "2026-01-19 19:56:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jcyuq",
                  "author": "MomentTimely8277",
                  "text": "The problem is the more characters the more it goes wrong, normaly this photo is supposed to have 4  staff members but it mixed them. So i lower it to 2. The workflow is your \"photo-style-prompt-encoder\\_GGUF\"  but with these models instead.  \n\nhttps://preview.redd.it/2eex9yza4deg1.png?width=899&format=png&auto=webp&s=3a0339b5de6fc0b9c56bae1ee3ed1aad7e7742b7",
                  "score": 3,
                  "created_utc": "2026-01-19 20:05:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0lhms3",
          "author": "HareMayor",
          "text": "If it's only the sampler, just make it installable as such that is shows up in the Ksampler dropdowns, so users will have other K sampler options too...",
          "score": 6,
          "created_utc": "2026-01-20 02:41:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ki4ee",
          "author": "Whole_Disk3247",
          "text": "That's why this sub exists. Kudos !",
          "score": 3,
          "created_utc": "2026-01-19 23:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ja7jy",
          "author": "Yasstronaut",
          "text": "I‚Äôm not clear from the docs what sampler it uses in each phase",
          "score": 2,
          "created_utc": "2026-01-19 19:53:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jf9kz",
              "author": "FotografoVirtual",
              "text": "All three phases use Euler.\n\nPhases 1 and 2 combined function similarly to a standard denoising process, but with two key conditions:\n\n* Phase 1 always has 2 steps and fixed sigmas, regardless of the total number of steps used.\n* There's a jump in sigmas between Phase 1 and Phase 2, there's no continuity. I'm not sure why it works, but after hundreds of tests where the final image consistently had better quality, I had to accept that this was a rule and it needed to be this way.\n\nOnce Phases 1 and 2 are complete (acting as a standard denoising process), Phase 3 begins. This is essentially a refining stage, reversing the sigmas and adding the corresponding noise.",
              "score": 3,
              "created_utc": "2026-01-19 20:16:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0khuow",
                  "author": "Yasstronaut",
                  "text": "Makes complete sense. This approach feels very familiar to a node for something else released a while ago and I appreciate this! I‚Äôll give this a few tests",
                  "score": 1,
                  "created_utc": "2026-01-19 23:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0k0r30",
          "author": "ANR2ME",
          "text": "The text looks good after 8 steps, that is because ZIT was designed for 8-step instead of 4-step, based on their github.\n\n> Decoupled-DMD is the core few-step distillation algorithm that empowers the 8-step Z-Image model.",
          "score": 1,
          "created_utc": "2026-01-19 22:00:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kbngh",
          "author": "Illynir",
          "text": "Interesting, i will test later and make feedback. Thanks for the work.",
          "score": 1,
          "created_utc": "2026-01-19 22:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n9coj",
          "author": "etupa",
          "text": "Star deserved, that's really a good implementation of z-image, results are really cool :))",
          "score": 1,
          "created_utc": "2026-01-20 10:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0o6f7c",
          "author": "ThiagoAkhe",
          "text": "I loved it! Props!!",
          "score": 1,
          "created_utc": "2026-01-20 14:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ug2x2",
          "author": "simplephoneuser",
          "text": "why does it look so fake z image",
          "score": 1,
          "created_utc": "2026-01-21 12:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15d728",
          "author": "SEOldMe",
          "text": "As often, you create great things ...Thanks for all",
          "score": 1,
          "created_utc": "2026-01-23 00:27:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkog35",
      "title": "Colour shift is not caused by the VAE",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/0lhl6roc43fg1.png",
      "author": "Luke2642",
      "created_utc": "2026-01-23 11:33:33",
      "score": 127,
      "num_comments": 53,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkog35/colour_shift_is_not_caused_by_the_vae/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o18mt0i",
          "author": "afinalsin",
          "text": ">If you pass it through six times you get a slight fading effect, that is all\n\nNo, that is not all. The VAE will degrade the details of the image. Below is a bunch of comparisons between input image and 8x trips through a VAE encode/decode cycle (scroll to zoom in):\n\n[Here is a robot](https://imgsli.com/NDQ0NjY3).\n\n[Here is an anime creature.](https://imgsli.com/NDQ0NjY4)\n\n[Here is a random demon woman](https://imgsli.com/NDQ0Njcy).\n\n[Here is a Fernando Alonso](https://imgsli.com/NDQ0Njcx).\n\n[Here is a lower resolution Fernando Alonso](https://imgsli.com/NDQ0Njgy).\n\nYour methodology isn't great because you are running a single 1080 x 1935 image through the VAE so the loss of detail is much less noticeable. When you run a 1 megapixel image through the same process (like you would with image editing, for example) the degradation is much more apparent. Even if the the colors remain basically the same, the destruction of details is too much to be dismissed as a \"slight fading effect\". \n\n---\n\n>It's some sort of groupthink, when no-one actually tested it.\n\nOkay, let's test it. I'll create an image from two inputs, and run 8 edits on consecutive outputs. \n\n[Here](https://i.postimg.cc/dJvcwHqW/grid-00027.png) I make all edits directly with latents, so it was only one encode/decode cycle. 0 Degradation.\n\n[Here](https://i.postimg.cc/R472s7cq/grid-00028.png) I make all edits with images in an encode/decode cycle, changing the seed each prompt. Nothing super noticeable, if at all. There might be if I had static elements that are supposed to remain untouched by the model. Test that next.\n\n[Here](https://i.postimg.cc/dwYxyhz0/grid-00029.png) I make all edits with images in an encode/decode cycle and keep the seed static. There are tons of errors, and they compound from one gen to the next. The wings and midsection are the most noticeable, but there are errors all over the place. So, I can confidently say leaving the seed static from gen to gen will fuck up your image. Now lets test that VAE cycle.\n\n---\n\n[Here](https://i.postimg.cc/rcC97QPW/vae-00001.png) is a run of edits using the VAE.\n\n[Here](https://i.postimg.cc/tytz82M6/vaeless-00001.png) is a run of edits using only latents.\n\nBoth of these runs are the exact same seed, exact same prompts. The differences are subtle, but the result is clear. [Here is the final output](https://imgsli.com/NDQ0NzAw). \n\nCompare the detail on the house and the tree above it on the mug. Every edit had a clear instruction: \"Leave the coffee mug exactly as it is.\" And it did, but the VAE got in the way and degraded the details.\n\nSo yeah, it was some sort of \"groupthink\", but the group was thinking based on knowledge gained over years of doing this. The color degradation was likely based on an unchanging seed, but passing into and out of latent space will still fuck up your image so it's best to avoid it as much as possible. \n\nEven a VAE as good as the Flux2 Vae will fuck up your image with enough cycles, because the tiny mistakes compound with each other. If I ran that mug through ten more edits the results would be even more stark. That's also why you mask when you inpaint, otherwise your final result ends up like mud.",
          "score": 30,
          "created_utc": "2026-01-23 14:03:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18o6fq",
              "author": "Luke2642",
              "text": "You make some good points, but I have no idea what your workflow is for each of those edit images or what you're trying to prove. There is a slight quality/detail loss each you encode/decode with the VAE, I never said there wasn't.\n\nThe OP was doing 3 edits and seeing colour shift on that exact image. The colour shift was not caused by the VAE, as demonstrated.",
              "score": 3,
              "created_utc": "2026-01-23 14:10:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18rwcv",
                  "author": "afinalsin",
                  "text": "Just reread that thread, and we both overlooked something very simple: \n\n>\"I am using the default workflow. Just removed the \"ImageScaleToTotalPixels\" node to keep the output reso same as input.\"\n\nThe input resolution of OP's image is 1536 x 2752. Homie was trying to gen 2x base resolution. \n\nEven with that full resolution, [I couldn't replicate the errors](https://i.postimg.cc/WsgrJ7Jt/Comfy-UI-0003.webp). The image deteriorated, as to be expected, but the color wasn't one of them, so I'm stumped.\n\nEdit to answer edit:\n\n>what you're trying to prove\n\nJust countering your statement that \"you only get a slight fading\". \n\n>There is a slight quality/detail loss each you encode/decode with the VAE, I never said there wasn't.\n\nI see now that you were referring strictly to color, but that statement can easily be read as the fading being the only detrimental effect at being passed in and out of the VAE. My reading of that made it seem like you did indeed say there wasn't a quality/detail loss, which is what I was countering. My bad. On a completely related note, I hate English. \n\nStill, if someone reads this and gets something out of it, job done, I don't mind arguing against a point I've misinterpreted.",
                  "score": 6,
                  "created_utc": "2026-01-23 14:29:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o180u4y",
          "author": "Most_Way_9754",
          "text": "Can you explain why the laten multiply removes the fade? Should we include this after the vae decode on a regular generation?",
          "score": 11,
          "created_utc": "2026-01-23 11:48:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o183x1o",
              "author": "Luke2642",
              "text": "There must be something in the encode/decode that is squashing the weights or pixel values slightly each time, or, it could be a cumulative rounding error, but that seems unlikely. Latent space geometry is hard to conceptualise, but I guess that the absolute magnitudes somehow control the overall contrast, and the relative magnitudes control the features.\n\nIf you want to boost the contrast, yeah, multiply is a quick and dirty way to do it?",
              "score": 6,
              "created_utc": "2026-01-23 12:10:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o190kqt",
                  "author": "RickyRickC137",
                  "text": "Is there a cleaner way, then?\n\nAlso, does it work with qwen edit?",
                  "score": 1,
                  "created_utc": "2026-01-23 15:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o183kqt",
              "author": "Character-Bend9403",
              "text": "I wanna know aswell i am a beginner and would love to know  the answer to this , if you feel explaining it.",
              "score": 1,
              "created_utc": "2026-01-23 12:08:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18ilex",
          "author": "Justify_87",
          "text": "https://github.com/Jelosus2/comfyui-vae-reflection\n\n\nThis may be interesting for you",
          "score": 9,
          "created_utc": "2026-01-23 13:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18pg1u",
              "author": "Luke2642",
              "text": "That is very interesting to me, thank you!",
              "score": 1,
              "created_utc": "2026-01-23 14:17:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o184lpz",
          "author": "infearia",
          "text": "I don't claim to understand fully what you're talking about, but it seems you're onto something. There are times I notice a HUGE increase in brightness after only ONE single edit operation. Maybe you could open an issue or a discussion on ComfyUI's official GitHub repo?",
          "score": 7,
          "created_utc": "2026-01-23 12:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o187wbo",
          "author": "TBG______",
          "text": "Pixel space ranges from 0 to 255, while latent space is normalized to 0‚Äì1 float32 in best cases and compressed by a factor of 8. Rounding errors are common, but they do not cause image saturation like in these examples.\n\nSure, I didn‚Äôt test this with Flux 2, but it looks like the Flux model tends to add a bit more saturation to the input image on each pass. My suspicion is that Flux has a built-in corrector that works ‚Äúwell‚Äù on the first pass but then accumulates over subsequent passes. The problem is that it seems to be applied only to unchanged areas, so ideally your correction pass should also target only those areas.\n\nHave you tried whether the same behavior happens with img2img without inpainting? Did you keep the same seed on each pass, or did you change it? In any case, it‚Äôs usually best to do your generation first, then crop and stitch the original background back into the image using the same mask, and only then proceed with the next pass.",
          "score": 3,
          "created_utc": "2026-01-23 12:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18bigi",
              "author": "TheSquirrelly",
              "text": "SwarmUI has some color correction options when using flux inpainting.  I wonder if it's a similar issue at all.  Though it looks like it applies this through a custom \"SwarmImageCompositeMaskedColorCorrecting\" node.\n\nhttps://preview.redd.it/ld6ri81rj3fg1.png?width=1229&format=png&auto=webp&s=2dbcc1be2748357a97259927089c74c88bf90f8c",
              "score": 1,
              "created_utc": "2026-01-23 13:00:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18s54n",
              "author": "Formal-Exam-8767",
              "text": "> while latent space is normalized to 0‚Äì1 float32\n\nAre you sure? I thought latent tensor values are unbound, with approximately zero‚Äëmean.",
              "score": 1,
              "created_utc": "2026-01-23 14:30:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1azkx7",
              "author": "Geekn4sty",
              "text": "Isn't the latent tensor autocast as the same dtype of the model weights? They must undergo matrix multiplication in the forward pass, I'm pretty sure torch will not allow them to have mismatched precision.",
              "score": 1,
              "created_utc": "2026-01-23 20:38:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ecllm",
                  "author": "TBG______",
                  "text": "That‚Äôs correct. \n\n* **Latent ‚Üí pixel conversion happens** ***after*** **decoding via the VAE**\n* The VAE decoder output is:\n   * float (usually fp16/fp32)\n   * then clamped / scaled to `[0,1]`\n   * *then* converted to 8-bit (0‚Äì255) for images (bottleneck)",
                  "score": 1,
                  "created_utc": "2026-01-24 09:11:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o184q4b",
          "author": "Minimum-Let5766",
          "text": "When inpainting flux1.dev, I get color fading in the final image of the non-masked area .  But the masked area which was manipulated remains at original saturation, so it never looks quite right.  Hopefully this latent multiply can help.",
          "score": 2,
          "created_utc": "2026-01-23 12:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18ge1r",
          "author": "ReasonablePossum_",
          "text": "Tried with the latentmultiply at 2, it gives me an oversaturated and degraded output\n\nhttps://preview.redd.it/0emk7lnmp3fg1.png?width=293&format=png&auto=webp&s=4de3a2264f64303ab542ea289274a7ccf70f5bbc",
          "score": 2,
          "created_utc": "2026-01-23 13:29:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18qg1m",
              "author": "TBG______",
              "text": "If you multiply 0.2% of any color by 2, you simply get 0.4%. That‚Äôs just post-processing, not an actual solution.",
              "score": 2,
              "created_utc": "2026-01-23 14:22:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18ix6a",
              "author": "Luke2642",
              "text": "I have no idea what your workflow is. Are simply replicating encode-decode 6 times for fun?",
              "score": 1,
              "created_utc": "2026-01-23 13:42:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o18oorc",
              "author": "Cute_Ad8981",
              "text": "It depends on the vae. For example I had to use 0.75 with the latent multiply for wan 2.2 5b. OP is using 2.0 after 3-4 encodings/decodings. maybe test 1.25 or 0.75 and adjust.",
              "score": 1,
              "created_utc": "2026-01-23 14:13:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18sy7i",
              "author": "Formal-Exam-8767",
              "text": "That's expected since it's a hack. There is no semantic explanation what multiplying every value by 2 in latent space means.",
              "score": 1,
              "created_utc": "2026-01-23 14:34:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18jctv",
          "author": "pwillia7",
          "text": "This guy diffuses",
          "score": 2,
          "created_utc": "2026-01-23 13:45:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18p8oq",
              "author": "Luke2642",
              "text": "Technically this is just encoding and decoding üòú",
              "score": 1,
              "created_utc": "2026-01-23 14:15:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18pbu4",
                  "author": "pwillia7",
                  "text": "ha -- fair play",
                  "score": 1,
                  "created_utc": "2026-01-23 14:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18564l",
          "author": "ANR2ME",
          "text": "Nice finding üëç\n\nDo we need lower multiplier value if it was for the 2nd image? ü§î since 2.0 is for the 3rd image, which already faded twice.",
          "score": 1,
          "created_utc": "2026-01-23 12:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o185e05",
          "author": "Formal-Exam-8767",
          "text": "Do VAE encode, modify part of the image (while in latent space), do VAE decode, whole image tone changes/shifts.",
          "score": 1,
          "created_utc": "2026-01-23 12:21:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o185tzr",
              "author": "Luke2642",
              "text": "You're confusing local and global features in the latent space. I mentioned in the other post that latent geometry is complex!",
              "score": 1,
              "created_utc": "2026-01-23 12:24:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18bmej",
                  "author": "Formal-Exam-8767",
                  "text": "Yes, and you can't replicate color tone shift without changing the image in latent space. Same thing happens with tiled VAE decode.",
                  "score": 1,
                  "created_utc": "2026-01-23 13:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o187m7i",
          "author": "TidalFoams",
          "text": "I've always thought about it like the ksampler is making a copy of a very slightly altered copy (like a game of telephone). Any problems it introduces (color change in this case) get amplified in the next pass. It's not just the color that changes but subtle detail gets lost over iterations through a ksampler. If you do it enough times you get monster people.",
          "score": 1,
          "created_utc": "2026-01-23 12:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18biw5",
          "author": "Cute_Ad8981",
          "text": "Your post is interesting, because I'm searching for an easy solution for that problem.  \nI'm using latent multiply too and it helped, but are you sure this is fully lossless and can be applied to all different vaes (wan for example)? I often chained multiple samplers for video extension and it was pretty hard to pinpoint the exact value for latent multiply.",
          "score": 1,
          "created_utc": "2026-01-23 13:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bw9c4",
              "author": "physalisx",
              "text": "It is absolutely not and OP is completely wrong. VAE encode/decode will absolutely degrade your image, there is no \"lossless\" way around that.",
              "score": 2,
              "created_utc": "2026-01-23 23:16:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o18jdiz",
              "author": "Luke2642",
              "text": "It's going to be trial and error.\n\nComfyUI will auto-detect the model type in the ksampler and apply the correct latent space scale and shift before doing the model denoising, then rescale and shift it afterwards. So there's lots of opportunity for shifts to creep in.",
              "score": 1,
              "created_utc": "2026-01-23 13:45:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18o17i",
                  "author": "ZenEngineer",
                  "text": "I wonder if there's some way to test that. What happens if you run a few steps with 0 denoise? Would ksampler still scale and shift?",
                  "score": 1,
                  "created_utc": "2026-01-23 14:09:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1a43jo",
          "author": "Spare_Ad2741",
          "text": "would same methodology apply to video creation also?",
          "score": 1,
          "created_utc": "2026-01-23 18:12:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1agx3k",
          "author": "Simple-Variation5456",
          "text": "its crazy to see what people come up with, struggle with and take minutes to correct and edit in their workflows while oldboi photoshop just did it better 20 years ago.  \nI generate in comfyui (or directly in photoshop with firefly fill / flux / nano) and copy the output into photoshop as a layer, can copy already made masks and can refine and edit everything on seperate layers whenever i want.  \nHow you even precisely pick a color for certain things?  \nCan you even mask stuff above 4-8k?  \n  \nDo people actually edit their outputs over and over?  \nI'm already getting mad when NanoPro/Flux2Pro/Seedream 4.5 just slightly change the image even tho telling or masking areas to keep and only change x.  \nThis sounds worse than people had to cut out stuff and glue them together physically (analog) before photoshop was a thing.",
          "score": 1,
          "created_utc": "2026-01-23 19:10:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bh215",
          "author": "MidSolo",
          "text": "Could you have made it any more difficult to figure out what is going on in your workflow?",
          "score": 1,
          "created_utc": "2026-01-23 21:59:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1biv7d",
              "author": "Luke2642",
              "text": "Load image\nEncode\nDecode\nEncode\nDecode\nEncode\nDecode\nPreview\nEncode\nDecode\nEncode\nDecode\nEncode\nDecode\nPreview¬†",
              "score": 1,
              "created_utc": "2026-01-23 22:08:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bpd94",
          "author": "superstarbootlegs",
          "text": "VAE will degrade your images coming out of Latent Space though. You do it enough (put it through another wf) and still apply color shift you will have very shit results which is why color shift alone is not enough to solve it.\n\nOr has something new been involved to resolve that because last year it was the bane of extending WAN based workflow results. Trying to swap multiple characters out with VACE got nasty and would require compositing to resolve mostly due to passing in and out of Latent Space from the VAE encode decode.\n\nMaybe I misunderstood what you are trying to show here. (I work mostly in video which is where I see this issue).",
          "score": 1,
          "created_utc": "2026-01-23 22:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bqn0p",
              "author": "Luke2642",
              "text": "Yes, small loss of detail each time. I was showing that the vae doesn't damage the latent much, even 6 encodes 6 decodes. The ksampler damages it a lot more, and causes colour shift.",
              "score": 1,
              "created_utc": "2026-01-23 22:47:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1cj07c",
                  "author": "superstarbootlegs",
                  "text": "isnt that its job - making pixels.",
                  "score": 1,
                  "created_utc": "2026-01-24 01:20:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1brsdn",
          "author": "YMIR_THE_FROSTY",
          "text": "Colors are defined by matrix thats in some ComfyUI file, bit lazy to find it, so ask AI.",
          "score": 1,
          "created_utc": "2026-01-23 22:53:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1c5gy4",
          "author": "Several_Honeydew_250",
          "text": "Anytime you pass to latent space and back, you lose quality.  This is why if your not shift your model class (XL/PONY/FLUX/KRAE etc..) leave it in LATENT, don't decode until the end.  Also, you're using the same VAE on all of them... so, it should maintain color space.",
          "score": 1,
          "created_utc": "2026-01-24 00:06:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ej5fx",
          "author": "TBG______",
          "text": "https://preview.redd.it/150m7twbs9fg1.png?width=379&format=png&auto=webp&s=8c49da4c2aae3e221801fd845fa6cd82c21b32a3\n\nThis is a difference overlay showing all the changes made by the edit sampling with the prompt: *‚Äúchange the color of the shirt to black, keep the background as is.‚Äù* Flux2 Klein 9B tends to alter everything in the image, which is typical behavior and not unique to Flux2 Klein.\n\nAfter observing the results, the changes made by the model are not what I would expect in a real-world scenario. For instance, if a large portion of the image is occupied by a person wearing a white T-shirt and you change it to black, the overall color balance of the image shifts. Reflections, lighting, and even the camera‚Äôs exposure or white balance adapt to the new mid-gray of the scene.   As example looking at the arm, the upper part should show a different light reflection, but it doesn‚Äôt. For me, that means using masking along with a low-percentage blend of the unmasked areas could be sufficient.",
          "score": 1,
          "created_utc": "2026-01-24 10:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eltbb",
              "author": "TBG______",
              "text": "This is after a straightforward fix\n\nhttps://preview.redd.it/7864o8wlz9fg1.png?width=373&format=png&auto=webp&s=049a7892e898d8d119adb38f49b17b338aa52f73\n\nwill post wf later",
              "score": 1,
              "created_utc": "2026-01-24 10:37:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ema1k",
                  "author": "TBG______",
                  "text": "https://preview.redd.it/x4v68k210afg1.png?width=1634&format=png&auto=webp&s=35bb23d10d6dc113c749fe22bc948a5306fe8248\n\nUse Sam to select the element, then expand and blur the mask, apply compositing with the mask and unmasked area, and if you like blend old over new as an optional intermediate solution.",
                  "score": 1,
                  "created_utc": "2026-01-24 10:41:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1f292l",
              "author": "TBG______",
              "text": "https://preview.redd.it/1y5gvizmgafg1.png?width=1433&format=png&auto=webp&s=c0688fe805d0c9b2ba7c43a8de06970bab26907b\n\nAfter testing it with SAM, which requires manual input of elements, I‚Äôve created a new node that can automatically detect the affected areas without SAM, allowing the process to be fully automated.\n\nHowever, I noticed that many contour edges are affected by the mode changes, even with slight repositioning of the edges. This could make the issue a bit tricky to resolve. I will uplode the node to the TBG Takeaways : The TBG Difference Mask node compares two images, mixes SSIM (structure) and RGB difference (color), then thresholds and area-filters to isolate real, coherent changes (like a shirt swap) while ignoring tiny model artifacts and compression noise. \n\nTBG Takeaways with the new TBG Difference Mask node are now uploaded and accessible from the Manager. The workflow for the SAM and Diff nodes can be found here: [https://www.patreon.com/posts/149003920](https://www.patreon.com/posts/149003920) (free access).",
              "score": 1,
              "created_utc": "2026-01-24 12:53:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fqush",
          "author": "Clasyc",
          "text": "Commenting not specifically to OP, but with some general points for other commenters. I see a lot of people missing a key piece of information when talking about moving forward with VAEs.\n\nLatent ‚Üí Image (VAE Decode) is a deterministic operation and does not lose information relative to the latent. Image ‚Üí Latent (VAE Encode) is a lossy operation.\n\nSo it‚Äôs kinda obvious that if you repeatedly chain multiple VAE encode + VAE decode operations, you will eventually lose more and more information. At the same time, you start ‚Äúsaturating‚Äù specific patterns that the VAE is good at resolving and encoding. As a result, overall quality degrades, and the image becomes more and more sharp, contrasty, and ‚ÄúAI-looking‚Äù in the end.\n\nIt doesn‚Äôt matter what tricks you do between those operations - once you keep re-encoding, you are inevitably losing original information.",
          "score": 1,
          "created_utc": "2026-01-24 15:16:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhppi7",
      "title": "What are the best NSFW Models?",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qhppi7/what_are_the_best_nsfw_models/",
      "author": "Firm-Struggle8183",
      "created_utc": "2026-01-20 03:16:18",
      "score": 126,
      "num_comments": 50,
      "upvote_ratio": 0.84,
      "text": "Trying to create some custom NSFW image to video workflows, and struggling to figure out what models are best for this.\n\nIs there a list somewhere?",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhppi7/what_are_the_best_nsfw_models/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o0m7l3h",
          "author": "GregBahm",
          "text": "I think Civitas is basically that list.\n\nAI smut is a pretty broad category. You got your photorealistic smut. You got your anime smut. You got your smut that does boring shit well. You got your smut that does prompt adherence well (but doesn't beat the boring models at boring shit.)\n\nThe video part is easy. Smutty fine-tunes of Wan2.2 will get any image moving and grooving. Wan 2.2 Enhanced NSFW is number 1 on civitas for a reason. It's the best. Image-to-Image will get you to where ever you want to go, so the problem is thus reduced to an image (de)generation problem.\n\nFor photoreal image smut, you can use Flux or Qwen or Z-Image probably. They will all give you exactly what you want as long as you plug in the right LoRA and are willing to settle for something that is mostly 1girl.\n\nFor anime image smut, a lot of people seem to go for Illustrious or Pony. If you're willing to settle for something mostly 1girl, these models will also trivially do anything you'd ever want.\n\nThe hardest thing is getting 2+ characters interacting without it looking like some horror show dreamed up by the worshippers of slaanesh. The state of the AI art today is that, to get two characters interacting with a lot of prompt adherence, people start sprouting extra fingers and legs and dicks. So you can either wrestle your way through that (with control nets and in-painting and a lot of random chance) or settle for something more generic.",
          "score": 186,
          "created_utc": "2026-01-20 05:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mk7yi",
              "author": "kirmm3la",
              "text": "The this guy knows his smut.",
              "score": 97,
              "created_utc": "2026-01-20 06:55:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mw8hi",
                  "author": "Pope_Phred",
                  "text": "And his 1girls, apparently.",
                  "score": 31,
                  "created_utc": "2026-01-20 08:43:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15d8vk",
                  "author": "NoLiterature4575",
                  "text": "This guy smuts",
                  "score": 1,
                  "created_utc": "2026-01-23 00:27:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0ms3mi",
              "author": "meoli",
              "text": "\"worshippers of slaanesh\" made me chuckle there.",
              "score": 24,
              "created_utc": "2026-01-20 08:05:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0o73mu",
              "author": "uniquelyavailable",
              "text": "I feel that the available NSFW checkpoints and loras are still a far cry of where we should be in the NSFW space for Ai generation. The amount of work it takes to produce a fantastic and anatomically correct netheryea with even the best workflow is on par with building a cabin in the woods and still leaves a lot to be desired.",
              "score": 11,
              "created_utc": "2026-01-20 14:30:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0opqzy",
              "author": "clwill00",
              "text": "Flux is a no-go below the waist. Barbie and Ken have more realism. Qwen and Zimage are better choices there.",
              "score": 3,
              "created_utc": "2026-01-20 16:02:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0mrpve",
              "author": "TheDudeWithThePlan",
              "text": "let's not forget Chroma for photo realistic",
              "score": 11,
              "created_utc": "2026-01-20 08:01:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0mtxen",
              "author": "tugiz1004",
              "text": "That's why adeptus mechanicus exist",
              "score": 3,
              "created_utc": "2026-01-20 08:22:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0nbhx0",
              "author": "Jygglewag",
              "text": "For a Godrick fan the extra hands are part of the fun",
              "score": 3,
              "created_utc": "2026-01-20 11:05:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0qzrbv",
              "author": "FrolickingFox4",
              "text": "I've only played around with it a little so far (all Wan2.2 based) but it seems like you can really tell what sort of smut it was trained on. The NSFW LoRAs I've tried have no idea what a normal human penis size is. I haven't been able to get anything less than about 9\" out of it.",
              "score": 2,
              "created_utc": "2026-01-20 22:19:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ms22c",
              "author": "Ok_Camera2445",
              "text": "You guys mean Civiatai? And do you refer to a specific \"Wan 2.2 Enhance NSFW\"? I am not able to find this one.",
              "score": 3,
              "created_utc": "2026-01-20 08:04:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0negp3",
                  "author": "MysteriousMongoose92",
                  "text": "https://civitai.com/models/2053259/wan-22-enhanced-nsfw-or-svi-or-camera-prompt-adherence-lightning-edition-i2v-and-t2v-fp8-gguf",
                  "score": 16,
                  "created_utc": "2026-01-20 11:30:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0niq6o",
                  "author": "liquidtensionboy",
                  "text": "civitai . It's a checkpoint, I found the checkpoint just fine. Check your civitai browsing level perhaps?",
                  "score": 1,
                  "created_utc": "2026-01-20 12:03:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0piit1",
              "author": "hugo4711",
              "text": "civarchive",
              "score": 1,
              "created_utc": "2026-01-20 18:14:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0oa5vk",
              "author": "Firm-Struggle8183",
              "text": "Much appreciated.",
              "score": 0,
              "created_utc": "2026-01-20 14:46:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0muu78",
          "author": "Mugaluga",
          "text": "For Photorealism there are a few that stand out.\n\nIf you want to put real people (so the face and likeness matters) start with....\n\n[Phr00t/Qwen-Image-Edit-Rapid-AIO at main](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/tree/main)\n\nYou can put a photo of anyone in there to easily remove their clothes or get them \"into position\"\n\nZ-Image Turbo will get you photorealistic images too, but won't give you an exact person unless you use a lora.\n\nThen you move on to WAN 2.2 plus loras. There are plenty available at Civit but IMO many of the best ones were deleted. Fortunately, you can still find most of them here...\n\nIt's the loras that turn Wan2.2 from PG-13/softcore into XXX hardcore.\n\n[https://civitaiarchive.com/](https://civitaiarchive.com/)\n\nThe only problem with the photorealistic models currently is that none of them go as easily hardcore as Pony or Illustrious. There ARE mixes for realism and they'll get you most of the way there, but they do not look 100% real or like photos.\n\nBut they will create the most hardcore scenes you can probably imagine. Excellent at anatomy in complex positions of \"interactions\" that none of the truly photorealistic models can manage. They weren't trained specifically on porn - Pony was. Skip Pony 7 entirely. In fact you don't want to use ANY of the Pony base models. You want a Pony finetune.\n\nI'd recommend CyberRealisticPony and there are a few others. You can create hardcore XXX with this model. It's fast, has great anatomy and is HIGHLY knowledgeable about practically every sexual act or position you could want, and I love it! But it's NOT quite photorealism.\n\nChroma is an interesting one. It's a photorealistic model that WAS trained on porn. It exists as a halfway point. Not quite as hardcore as Pony can get but way more \"knowledgeable\" than Qwen or Z-Image Turbo. Its main drawback is sketchy anatomy compared to the others. It CAN do amazing things, but you'll ALSO get a lot of deformities. A lot of almost perfect pics ruined by that deformed foot or extra hand etc. And then BAM... exactly what you were going for.\n\nFor anime smut, you'll have to ask someone else :)",
          "score": 40,
          "created_utc": "2026-01-20 08:30:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nkrz4",
              "author": "Mother-Food4084",
              "text": "Can you please share workflow to use run with Phr00t/Qwen-Image-Edit-Rapid-AIO.",
              "score": 2,
              "created_utc": "2026-01-20 12:18:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0nsp7o",
                  "author": "Whipit",
                  "text": "I'm not OP but here's the link to the \"official\" WF. I use it. It definitely works.\n\n[https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/blob/main/Qwen-Rapid-AIO.json](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/blob/main/Qwen-Rapid-AIO.json)",
                  "score": 6,
                  "created_utc": "2026-01-20 13:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0qlqmr",
              "author": "beragis",
              "text": "Z-Image does tend to give the same or very similar person in the same environment as long as the description of the person remains consistent.",
              "score": 1,
              "created_utc": "2026-01-20 21:14:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o15dyxr",
              "author": "NoLiterature4575",
              "text": "This guy smuts too",
              "score": 1,
              "created_utc": "2026-01-23 00:31:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0naul2",
          "author": "zxcvbnm_mnbvcxz",
          "text": "Lustify is still my favorite",
          "score": 12,
          "created_utc": "2026-01-20 10:59:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ocz9i",
              "author": "NoElephant5027",
              "text": "i can even run it on my macbook pro m3, really like that model",
              "score": 3,
              "created_utc": "2026-01-20 15:00:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nwkzz",
          "author": "Whipit",
          "text": "I'll throw these in here. I'm not listing any hidden gems. These are well known staples.\n\nThe first two are Pony finetunes that focus on realism. The third is RedCraft, they make all kinds of finetunes and always tend to lean towards realism.\n\n[https://civitai.com/models/443821/cyberrealistic-pony?modelVersionId=2581228](https://civitai.com/models/443821/cyberrealistic-pony?modelVersionId=2581228)\n\n[https://civitai.com/models/372465/pony-realism?modelVersionId=914390](https://civitai.com/models/372465/pony-realism?modelVersionId=914390)\n\n[https://civitai.com/models/958009?modelVersionId=2462789](https://civitai.com/models/958009?modelVersionId=2462789)\n\nAnd I'll echo my own endorsement of \n\n[https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/tree/main](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/tree/main)\n\nMaybe this is the first and only editing model that is being designed to be very NSFW friendly. It's fucking great.\n\nIt just makes what we used to have to do so much easier. In the past if I wanted someone naked I'd use an inpainting model. It works but can be finicky and is slower. You can undress people using WAN 2.2 as well. But Qwen-Image-Edit-Rapid-AIO **is the easiest**. And it's very high quality too. Just load in a photo of the person you want naked and prompt \"Her clothes are now gone\" - DONE\n\nI feel like this is an ability all men have wanted since the dawn of time - and now we have it! lol",
          "score": 11,
          "created_utc": "2026-01-20 13:33:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12zwku",
              "author": "Pitiful_Season4294",
              "text": "Hey there, I'm fairly new to this and after seeing your review, I downloaded the Qwen's AIO model by Phroot but it was freezing up my system (16GB VRAM+16GB RAM), so I went with the Q5 KM gguf versions listed by Phil2Sat here: \n\n[https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/tree/main/v90](https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/tree/main/v90)\n\n  \nI was unable to get the output to match the face and even the body proportions in the input pic, is there a particular set of configurations you'd recommend (cfg, steps, scheduler etc.)? Or could you please share or direct me to the workflow which works well. Many thanks!",
              "score": 1,
              "created_utc": "2026-01-22 17:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17ikez",
                  "author": "Mugaluga",
                  "text": "I have some advice for you. The default workflow works perfectly. So 4 steps is all you need. In fact increasing the steps decreases the likeness of the person.\n\nIt's best if your input image is high resolution (higher than 1024x1024) and it's also good if your output resolution is high too. At first I was trying with 1024x1024 and was getting some good results and some bad. I upped my output resolution to 1440x1440 or 1920x1080 and found the likeness was matching a LOT more often.\n\nIt's possible that a Q5 quant is degrading the likeness but I don't know that. Also, the Q5 quants you linked to are of version 9, so quite a bit older than the latest v22.\n\nI can say that v20 works very well and some have said that v18 maintains likeness the best.\n\nI was able to find Q5 quants of the newer versions for you. Try one of these, and good luck...\n\n[https://huggingface.co/Arunk25/Qwen-Image-Edit-Rapid-AIO-GGUF/tree/main](https://huggingface.co/Arunk25/Qwen-Image-Edit-Rapid-AIO-GGUF/tree/main)",
                  "score": 3,
                  "created_utc": "2026-01-23 09:08:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0meajr",
          "author": "tinsin3479",
          "text": "Indeed, you should go to Civitas. There's everything you want there, nsfw.",
          "score": 6,
          "created_utc": "2026-01-20 06:07:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pfpb4",
          "author": "Zealousideal_Roof_96",
          "text": "So much good info. Thank you all from a newbie to ai.",
          "score": 3,
          "created_utc": "2026-01-20 18:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ptt1a",
          "author": "TekaiGuy",
          "text": "Models in the same family can regress, so an earlier version of a model can perform better at a particular task than a later iteration. Keep that in mind and look at the comments/ratings on Civit as they give you an idea of which versions perform better.\n\nIf you're looking for true customization, I would recommend a pipeline of samplers, not just one model. Since each model excels at something unique, you should start with the model that gives you the pose/camera you want, then send the image to the next ksampler/advanced ksampler to give you color/lighting, and hook the depth map of the first image into it so it keeps the same shape, then send that image with its depth map to the next ksampler which can provide the texture/details (I like Z-image for this step).\n\nThat's just an example, you can invent any pipline you want with each step adding something the previous model couldn't.",
          "score": 3,
          "created_utc": "2026-01-20 19:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nkmuv",
          "author": "imakeboobies",
          "text": "For image to vid assuming you mean Wan2.2 and not ltxv2, and assuming you mean checkpoints and not Lora‚Äôs your options are fairly limited. Personally I use the enhanced lightning models, but the SFW works and then add the right NSFW Lora‚Äôs in. Smoothmix is another option but I personally find it gives me horrible gens.",
          "score": 1,
          "created_utc": "2026-01-20 12:17:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0orxym",
          "author": "etupa",
          "text": "No idea why wan2.2Remix last i2v models aren't more liked. With the painter node and a few LoRA it's just perfect... With the last face swap LoRA using Klein9b you're pretty much covered for any image input x)))",
          "score": 1,
          "created_utc": "2026-01-20 16:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qy1wh",
              "author": "South_Prize_2350",
              "text": "Could you share your workflow and the LoRa you mentioned?",
              "score": 1,
              "created_utc": "2026-01-20 22:11:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0u4mn0",
          "author": "trainity_lab",
          "text": "The secret is to have good image quality. Then with WAN2.2 is amazing. Go check my videos. Of course sometimes is kind of weird so just change the picture",
          "score": 1,
          "created_utc": "2026-01-21 11:18:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n8k2c",
          "author": "Adept_Internal_9443",
          "text": "What's about the Originals? They call themselves \"girls\" oder \"boys\".",
          "score": 1,
          "created_utc": "2026-01-20 10:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mvjzy",
          "author": "SimplyBlue09",
          "text": "If ever you are also into NSFW fic or eroticas, I highly suggest redquill. The platform has been great for not only creating and generating fics, but also looking at the works of others since it also has a forum for those.",
          "score": -1,
          "created_utc": "2026-01-20 08:37:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nsvaa",
          "author": "bfume",
          "text": "I hear there‚Äôs a lot of them on only fans¬†",
          "score": -1,
          "created_utc": "2026-01-20 13:11:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhuqwr",
      "title": "Big thanks to the ComfyUI community! Just wrapped a national TV campaign (La Centrale) using a hybrid 3D/AI workflow.",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qhuqwr/big_thanks_to_the_comfyui_community_just_wrapped/",
      "author": "cheerldr_",
      "created_utc": "2026-01-20 07:35:46",
      "score": 123,
      "num_comments": 30,
      "upvote_ratio": 0.91,
      "text": "Hey everyone,\n\nI wanted to share a quick win and, more importantly, a **huge thank you to this community**. I‚Äôve been lurking and learning here for a while, and I honestly couldn't have pulled this off without the incredible nodes, workflows, and troubleshooting tips shared by everyone here.\n\nI recently had the chance to integrate ComfyUI into a \"real-world\" professional production for **La Centrale** (a major French automotive marketplace), working alongside agencies BETC and Bloom.\n\n**The challenge:** We had to bring a saga of 25 custom-designed cars to life for over 10 different commercials in a very tight 4-week window.\n\nhttps://reddit.com/link/1qhuqwr/video/vhhgg7rajgeg1/player\n\n**The process:** To meet the brand's high standards, I deployed a hybrid pipeline: **3D for the structure/consistency and ComfyUI for the design, textures, and realism.** This allowed us to stay incredibly agile while maintaining a level of detail that traditional 3D alone wouldn't have reached in that timeframe.\n\nIt‚Äôs definitely not \"perfect,\" and there‚Äôs always room for improvement, but it‚Äôs a solid proof of concept that our workflows are ready for high-stakes professional advertising.\n\nThanks again for being such an inspiring hub of innovation. This is only the beginning! üçøüí•\n\n*(If anyone is curious about the specific nodes or how I handled the 3D-to-AI pass to keep the cars consistent, I‚Äôm happy to answer questions in the comments!)*\n\nmore details about this project : [https://www.surrendr.studio/work/la-centrale-ai](https://www.surrendr.studio/work/la-centrale-ai)  \n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhuqwr/big_thanks_to_the_comfyui_community_just_wrapped/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o0nbzj6",
          "author": "MisundaztoodMiller",
          "text": "How did you maintain accuracy per vehicle design?how did you transpose the design of the vehicle onto the 3d model block out?",
          "score": 9,
          "created_utc": "2026-01-20 11:09:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ncexn",
              "author": "cheerldr_",
              "text": "I mainly used it for the animation to keep it consistent. AI is still a bit random when trying to animate 25 vehicles, and this method allowed me to get exactly the animation I wanted.",
              "score": 6,
              "created_utc": "2026-01-20 11:13:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0nlhwe",
                  "author": "MisundaztoodMiller",
                  "text": "Yeah I get using the 3d model for the animation. I'm just wondering how you got the car design on to it. Did you have a side view photo graph of the car manufacturer/model of the car and got comfy to transpose it on to the animation?",
                  "score": 9,
                  "created_utc": "2026-01-20 12:23:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0orxwy",
                  "author": "TekaiGuy",
                  "text": "u/MisundaztoodMiller is asking about the \"texture\". Most current os models don't support generating the texture alongside the mesh so it's sort of a holy grail. Trellis can do it but it's not officially supported on comfy yet.",
                  "score": 1,
                  "created_utc": "2026-01-20 16:12:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0o9pq8",
          "author": "blastcat4",
          "text": "Great stuff! This is the kind of work that AI can be legitimately useful for, especially with tools like comfyui. Thanks for sharing this with us!",
          "score": 2,
          "created_utc": "2026-01-20 14:44:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0oj4q5",
              "author": "cheerldr_",
              "text": "Exactly ! üí•üî•",
              "score": 1,
              "created_utc": "2026-01-20 15:30:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pt1x4",
          "author": "jefharris",
          "text": "Congrats!",
          "score": 2,
          "created_utc": "2026-01-20 19:01:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n1sdq",
          "author": "Used-Ear-8780",
          "text": "Great job",
          "score": 5,
          "created_utc": "2026-01-20 09:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nc5yp",
              "author": "cheerldr_",
              "text": "Thx !",
              "score": 3,
              "created_utc": "2026-01-20 11:10:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0n5spr",
          "author": "SEOldMe",
          "text": "Excellent!!! Bravo ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ",
          "score": 4,
          "created_utc": "2026-01-20 10:14:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ndz3h",
          "author": "_CreationIsFinished_",
          "text": "Some jealous \\*someone\\* had gone through and downvoted every post in the thread. What is up with some ppl here? XD",
          "score": 4,
          "created_utc": "2026-01-20 11:26:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0np97p",
              "author": "Grid421",
              "text": "Probably one of those \"AI steals and destroys the planet\" people. \n\nLoved seeing what OP did. It shows what's possible in the hands of talented people.",
              "score": 6,
              "created_utc": "2026-01-20 12:49:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nkixx",
          "author": "Excellent_Koala769",
          "text": "Woah, that looks soo good.  I just started running comfyui workflows locally on my DGX Spark.  I have a couple questions if you don't mind.\n\n  \n1.  What hardware do you run your workflows on?\n\n2.  The cars look so real and high quality.  I would love to be able to generate a moving hyper realistic logo.  Kind of how you made the cars drop down.  Could you imagine making this logo flying in from the left side?  \n\n3.  Do you have any example workflows I could attempt to do this on?\n\n  \nThanks!\n\nhttps://preview.redd.it/mhgh02fhxheg1.png?width=1563&format=png&auto=webp&s=69742b1490fa8dfb61c2da86475390fede692026",
          "score": 3,
          "created_utc": "2026-01-20 12:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0obqa5",
          "author": "PestBoss",
          "text": "I'm confused about the \"real-world\" bit of the project, and what the brief was.\n\nIn the end ComfyUI is production ready now, it was 12 months ago or longer for all I know.\n\n\nI'm not sure a client cares how you eventually get to your end product. I've spent 20 years working professionally in businesses and for myself and I'd say not one said \"how\", they just said get it done.\n\nNow I'm sure some big companies might be weird about trying new ideas and stuff, and just like to have a standardised template and workflows, but an absolute ton of businesses will be as creative with their tools and workflows as they will the actual outputs :D\n\n\nNice work in any case :D",
          "score": 1,
          "created_utc": "2026-01-20 14:54:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0oj0mc",
              "author": "cheerldr_",
              "text": "Haha, I totally hear you! After 14 years in the game, I‚Äôve realized that working directly with agencies and clients is a whole different beast compared to running in post-production studio.\nWhen you‚Äôre in the driver‚Äôs seat of a studio, you have your own flow, but with direct clients, it‚Äôs all about hitting that budget without sacrificing the 'wow' factor.\n\n For this project, the real challenge was rock-solid consistency, keeping the same lighting, the same energy, and the same art direction across every single frame and asset.\n\n Honestly, even with a full-blown 3D pipeline, that‚Äôs a mountain to climb!\n\nSo, I turned to AI primarily as a smart way to bridge that budget gap. My goal was to take those tools and 'tame' them until the output was 100% production-ready in my little studio\n\nIt‚Äôs all about finding that sweet spot between creative tech and real-world constraints",
              "score": 4,
              "created_utc": "2026-01-20 15:30:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0oxmb0",
                  "author": "PestBoss",
                  "text": "Right, yeah the consistency factor is certainly a challenge even with classic workflows/tools.\n\nOK so from that point of view I expect you've succeeded well enough given the examples.\n\nI often find things like seed alone can kinda give 'similar' looks/styles, even with changing prompts. I wonder if that's been a thing you've noticed on this project?",
                  "score": 1,
                  "created_utc": "2026-01-20 16:38:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ohmtp",
          "author": "divyanshii03",
          "text": "This is soo awesome!! Finally someone is getting some awesome results on 3D side on node based platform.",
          "score": 1,
          "created_utc": "2026-01-20 15:23:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0oiar0",
          "author": "Enashka_Fr",
          "text": "Pas mal du tout ! So if I understand correctly, the cars don't exist at all in the real world, and were entirely json prompted? Wan 2.2 and Qwen for subsequent edit? Would you say json format was essential at least for that pipe (I hear diverging views). Bravo ;)",
          "score": 1,
          "created_utc": "2026-01-20 15:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ood4k",
          "author": "themilush",
          "text": "great job on this project and can't wait to learn more about it... what intregues me is how you've used the 3d animation from blender to guide the AI generation (if that was the workflow), can you elaborate more on that part of the process?",
          "score": 1,
          "created_utc": "2026-01-20 15:55:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0opyxd",
              "author": "themilush",
              "text": "I've noticed in the video showcase you've lost the reflection animation on the car from comfy to after effects, just curious on what happened there :)",
              "score": 1,
              "created_utc": "2026-01-20 16:03:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nm0kc",
          "author": "76vangel",
          "text": "Congratulations, well done. I‚Äôm interested in the blender 3d to refine with comfy phase. What model, technique did you use here? First frame + Animate/Vace? Z from blender? Could you please elaborate? Great work!",
          "score": 1,
          "created_utc": "2026-01-20 12:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0oprz9",
              "author": "cheerldr_",
              "text": "Used only wan 2.2 for img generation, hyuan for 3d meshing and wan2.2 for video generation : img to video driven by video",
              "score": 2,
              "created_utc": "2026-01-20 16:02:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0oigsm",
          "author": "divyanshii03",
          "text": "curious how you approached it. Would you be open to sharing your workflow or a detailed explanation on node setup?",
          "score": 1,
          "created_utc": "2026-01-20 15:27:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0opgzy",
              "author": "cheerldr_",
              "text": "I'd love to, but it's a bit of a complex beast because it relies heavily on local custom scripts that bridge several tools together\n\nHere‚Äôs the high-level breakdown of the pipeline:\n\nInitial Gen: I start with Qwen Edit to generate the base images.\n\nClient Approval: Once the client validates the look, my script takes over.\n\n3D Mesh: The script calls ComfyUI and Huyuan to generate a 3D mesh directly from that validated 2D image.\n\nBlender Integration: My script then pulls that 3D model into Blender and applies a custom auto-rig we built.\n\nMotion Approval: We get the animation/movement approved by the client within Blender first.\n\nFinal Render: Once the motion is locked, it goes back into ComfyUI via Wan 2.1 for the final video render.\n\nSince it involves a lot local scripts ( simple in .bat)  handle the handoffs between Blender and Comfy, the node setup itself is actually the simple part, the magic is really in the automation! :D\"",
              "score": 8,
              "created_utc": "2026-01-20 16:00:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ot327",
                  "author": "divyanshii03",
                  "text": "Super helpful, I wasn‚Äôt sure about the blender integration. Thanks a lot! :)",
                  "score": 1,
                  "created_utc": "2026-01-20 16:17:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0ye87g",
                  "author": "Queasy-Carrot-7314",
                  "text": "How did you clean up and separate the 3d mesh generated by hunyuan. It generally creates a single mesh for the whole scene. That's not really usable for animation.\n\nI see in your video that you have a much nicer mesh separated into parts. How did you achieve that ?",
                  "score": 1,
                  "created_utc": "2026-01-22 00:03:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pu4l1",
          "author": "Neither-Apricot-1501",
          "text": "Absolutely incredible work! The hybrid workflow sounds like a game-changer for tight deadlines. Would love to hear more about your AI-to-3D integration techniques!",
          "score": 1,
          "created_utc": "2026-01-20 19:06:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhlvmb",
      "title": "Flux.2-[Klein]: Lucy MacLean (Ella Purnell) Multiple points of view from the same image",
      "subreddit": "comfyui",
      "url": "https://i.redd.it/9jy00bl4deeg1.png",
      "author": "supermaramb",
      "created_utc": "2026-01-20 00:27:15",
      "score": 121,
      "num_comments": 27,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "No workflow",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhlvmb/flux2klein_lucy_maclean_ella_purnell_multiple/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0n3vyp",
          "author": "RIP26770",
          "text": "You are just using my WF LMFAO!\n\n[https://www.reddit.com/r/StableDiffusion/comments/1qg5ph5/flux\\_2\\_klein\\_4b\\_vs\\_9b\\_multi\\_camera\\_angles\\_one/](https://www.reddit.com/r/StableDiffusion/comments/1qg5ph5/flux_2_klein_4b_vs_9b_multi_camera_angles_one/)",
          "score": 32,
          "created_utc": "2026-01-20 09:56:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mbbx1",
          "author": "Violent_Walrus",
          "text": "Without a workflow or explanation, this is just somebody‚Äôs random internet story.",
          "score": 29,
          "created_utc": "2026-01-20 05:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n3xpl",
              "author": "RIP26770",
              "text": "here the OG Post with WF\n\n[https://www.reddit.com/r/StableDiffusion/comments/1qg5ph5/flux\\_2\\_klein\\_4b\\_vs\\_9b\\_multi\\_camera\\_angles\\_one/](https://www.reddit.com/r/StableDiffusion/comments/1qg5ph5/flux_2_klein_4b_vs_9b_multi_camera_angles_one/)",
              "score": 12,
              "created_utc": "2026-01-20 09:56:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nhte9",
          "author": "Achaeminuz",
          "text": "https://preview.redd.it/q20al5f5uheg1.jpeg?width=1206&format=pjpg&auto=webp&s=bced69c5b3e4fff3dcb9dfbe82106a041009c4b7",
          "score": 9,
          "created_utc": "2026-01-20 11:56:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v3qc3",
              "author": "ColloidalSuspenders",
              "text": "Appreciate different angles of cow human centipede",
              "score": 5,
              "created_utc": "2026-01-21 14:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l0860",
          "author": "Mirandah333",
          "text": "Wow, all from the 1st image?",
          "score": 4,
          "created_utc": "2026-01-20 01:06:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0l9c2i",
          "author": "NessLeonhart",
          "text": "None of those look real though. \n\nI think we‚Äôre still a couple generations away from realism",
          "score": 13,
          "created_utc": "2026-01-20 01:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ndesf",
              "author": "blownawayx2",
              "text": "I know I may be in the minority, but before AI image generation existed, if people were to show me these images, I‚Äôd say they looked great and wouldn‚Äôt question if they were real or not. Magazines forever have been modifying photos beyond what we‚Äôd find in reality. So, what am I missing here in these photos that immediately implies for you that they don‚Äôt look real? They look real enough to me‚Ä¶",
              "score": 5,
              "created_utc": "2026-01-20 11:21:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0o7mcb",
                  "author": "NessLeonhart",
                  "text": "That‚Äôs shocking to me. This isn‚Äôt passable at all, to me. Not the entire image, but the face, mostly. It is not her. It‚Äôs like a wax statue of her; looks like her but you know that it‚Äôs not somehow.\n\nThe lifeless pose, also.",
                  "score": 2,
                  "created_utc": "2026-01-20 14:33:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0r41i8",
                  "author": "Eriane",
                  "text": "They look real enough for 99.9% of the world for sure. She gained weight in some pics but most people won't notice. There are other issues as well but the reality is that phones all take fake photos now unless you buy a real camera and people are used to filters.",
                  "score": 1,
                  "created_utc": "2026-01-20 22:41:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ljr1h",
          "author": "maxtablets",
          "text": "the portrait on at least 3 of those is way off, unfortunately. Still cool attempt though",
          "score": 3,
          "created_utc": "2026-01-20 02:53:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0l2m0j",
          "author": "Libertalias",
          "text": "Workflow?",
          "score": 6,
          "created_utc": "2026-01-20 01:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l8cri",
              "author": "PeterTheMeterMan",
              "text": "Someone on discord said this morning, \"9B even works with the muti-angle node with NO LORAS! unlike qwen-edit which i am officially retiring it by deleting it from my drive\"  \n  \nSo, I would say try it with this node: https://github.com/jtydhr88/ComfyUI-qwenmultiangle  \n  \nDoes look cool, so I shall try it myself later when I have time.",
              "score": 7,
              "created_utc": "2026-01-20 01:51:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0l4hzi",
              "author": "RigelXVI",
              "text": "The tag says No Workflow",
              "score": -5,
              "created_utc": "2026-01-20 01:29:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l4tkd",
          "author": "Cautious_Schedule849",
          "text": "Nice work!\nWith Lora or just prompt ?",
          "score": 2,
          "created_utc": "2026-01-20 01:31:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lre26",
          "author": "GravitationalGrapple",
          "text": "Close, but middle row left image has 2 vials next to her holster that are not in the other images. As others have said it feels ‚Äúoff‚Äù too.",
          "score": 2,
          "created_utc": "2026-01-20 03:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m1u1a",
          "author": "FitContribution2946",
          "text": "I haven't had anywhere near this quality with Klein. Klein has been basically garbage as far as I've seen. This must be a different workflow than what company was putting out just a day ago",
          "score": 2,
          "created_utc": "2026-01-20 04:38:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n4yp9",
          "author": "Yattagor",
          "text": "Not bad for a first attempt! It must be said that Ella Purnell looks prettier in real life! Here she looks a little ugly!",
          "score": 2,
          "created_utc": "2026-01-20 10:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lglun",
          "author": "InternationalOne2449",
          "text": "Qwen can do the same.",
          "score": 1,
          "created_utc": "2026-01-20 02:36:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ly8e7",
          "author": "nutrunner365",
          "text": "Still doesn't capture the likeness perfectly. The from above one is even quite bad.",
          "score": 1,
          "created_utc": "2026-01-20 04:15:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mdbp0",
          "author": "elvinjoker",
          "text": "Wait rtx 2080? Do u have huge vram?",
          "score": 1,
          "created_utc": "2026-01-20 05:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mkde1",
          "author": "No-Location6557",
          "text": "is it better than qwen 2511 multi angle?",
          "score": 1,
          "created_utc": "2026-01-20 06:56:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mtjer",
          "author": "JohnSnowHenry",
          "text": "Not as good as qwen multi but dammm it‚Äôs also really good!\n\nPlease please release Omni so actually good Lora‚Äôs can be made",
          "score": 1,
          "created_utc": "2026-01-20 08:18:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qk0ri",
          "author": "superstarbootlegs",
          "text": "how long to finish? that is best I seen from Klein so far. QWEN 2511 also does a good one with camera angle nodes how did you ask for the angles?",
          "score": 1,
          "created_utc": "2026-01-20 21:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m5873",
          "author": "Upset-Virus9034",
          "text": "Can you share your wf please",
          "score": 1,
          "created_utc": "2026-01-20 05:00:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ms8oj",
          "author": "TheTimster666",
          "text": "Most of them are still  off. The human eye and brain is optimized to spot familiar faces, and the smallest deviation will alert us.",
          "score": 1,
          "created_utc": "2026-01-20 08:06:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj8yx1",
      "title": "Blender Soft Body Simulation + ComfyUI (flux)",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/h0pza820dreg1",
      "author": "bingobongo3001",
      "created_utc": "2026-01-21 20:18:26",
      "score": 98,
      "num_comments": 24,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Show and Tell",
      "permalink": "https://reddit.com/r/comfyui/comments/1qj8yx1/blender_soft_body_simulation_comfyui_flux/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0xh5ao",
          "author": "ChilouXx",
          "text": "Is it a frame by frame rendering since you mention flux?  \nIt looks nice.  \nRemindes me of the vintage Animatediff workflow with a modern quality.",
          "score": 3,
          "created_utc": "2026-01-21 21:19:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y0owv",
              "author": "bingobongo3001",
              "text": "With FLUX, I did it frame by frame, right. But then I started using WAN as well, it‚Äôs a bit faster and more stable because it uses a depth video as input.  \nSorry for the confusion about FLUX, I was supposed to mention WAN too.",
              "score": 2,
              "created_utc": "2026-01-21 22:51:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o109aqi",
                  "author": "CertifiedTHX",
                  "text": "So you do the style with flux then throw it in wan to animate, ya? Cuz for a sec i was going to ask how to keep consistency across time heh",
                  "score": 2,
                  "created_utc": "2026-01-22 07:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xey9p",
          "author": "Mintfriction",
          "text": "How do you do that: style transfer ?",
          "score": 2,
          "created_utc": "2026-01-21 21:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y2tan",
              "author": "bingobongo3001",
              "text": "1. I did a static render of the first frame of video and then used it as a style input image. I used different AI to do it ‚Äì something in comfy, adobeFF, something on higgsfield. Doesnt matter where, just anywhere you can provide a depth image and get a render based on it.  \n2. input image > resize image (depicted) > start image of WAnFUnControlToVideo. And then it goes as a positive, negrative, and latent image to ksampler  \nBut also it goes as a latent image from WanFunControlToVideo > wanVideo enhance > as a model to Ksampler.",
              "score": 3,
              "created_utc": "2026-01-21 23:02:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0y42iv",
                  "author": "Mintfriction",
                  "text": "Thanks for the answer",
                  "score": 2,
                  "created_utc": "2026-01-21 23:09:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o10twsr",
                  "author": "jeebiuss",
                  "text": "How does it take to generate each video",
                  "score": 1,
                  "created_utc": "2026-01-22 10:18:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0y39zp",
          "author": "bingobongo3001",
          "text": "Guys, just wanted to clarify that I **also used WAN** for these videos. Sorry for confusing title, it is not just flux.",
          "score": 2,
          "created_utc": "2026-01-21 23:05:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0y4l50",
          "author": "Necessary-Froyo3235",
          "text": "Really like the last one",
          "score": 2,
          "created_utc": "2026-01-21 23:11:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y4v1r",
          "author": "desertstudiocactus",
          "text": "How does blender integrate into this process?",
          "score": 2,
          "created_utc": "2026-01-21 23:13:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ybhhe",
              "author": "bingobongo3001",
              "text": "I made a model of the Nike logo and did a ‚Äúbubble‚Äù simulation (soft body or rigid body) in Blender, then rendered it there (Cycles) in mist mode, or whatever it‚Äôs called. Mist mode, also known as depth, gives the AI information about how deep the image is, haha.",
              "score": 2,
              "created_utc": "2026-01-21 23:49:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ybotw",
                  "author": "desertstudiocactus",
                  "text": "Very clever, cool!",
                  "score": 2,
                  "created_utc": "2026-01-21 23:50:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o12t91i",
                  "author": "michael-65536",
                  "text": "It's usually called a depth pass or a z-buffer.",
                  "score": 2,
                  "created_utc": "2026-01-22 17:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0z2y3z",
          "author": "Sareth324",
          "text": "Nice, I did something similar but just used 1st and last frame of a basic lighting and materials. End result was refined lighting through bubbles / refractions.\n\n\nWhat was your thought of only using a depth pass?",
          "score": 2,
          "created_utc": "2026-01-22 02:21:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zkdyh",
              "author": "bingobongo3001",
              "text": "Good question.\n\nThe reason is that I knew I could use depth as a ‚Äúcheap‚Äù input for a render engine.  \n  \nI worked at Snapchat before, so we did experiments with our engine (Lens Studio). The team found that we could transform camera input (selfie) into a depth map rendered in real time, and then send this depth map to a 3D mesh or something else. It gave us some cool effects, and we figured out how to approximately calculate the distance between the camera and the user‚Äôs face = we got new face lenses.  \nSo that‚Äôs how I got the idea of using a depth map.  \n  \nAnd obviously I also saw some youtubers using it as an input, but they usually used a couple more maps as well (outlines, color masks, etc)",
              "score": 2,
              "created_utc": "2026-01-22 04:03:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zfhzp",
          "author": "TanguayX",
          "text": "Really cool!",
          "score": 2,
          "created_utc": "2026-01-22 03:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zlcbd",
              "author": "bingobongo3001",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 04:09:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16rtks",
          "author": "No_Damage_8420",
          "text": "Looks great!  \nIndeed Comfy + Wan = Ultra powerful GLOBAL ILLUMINATION renderer of all times (anyone from old days - Vray, Maxwell Render, MentalRay?)\n\n\"GI\" and \"AI\" finally met.",
          "score": 2,
          "created_utc": "2026-01-23 05:22:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18l1mb",
              "author": "bingobongo3001",
              "text": "I‚Äôll never forget all the pain I went through during test renders in vray on my baby 8 GB ram and integrated GPU",
              "score": 2,
              "created_utc": "2026-01-23 13:54:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o188jxw",
          "author": "TidalFoams",
          "text": "This is the kind of reproducibility required for actual commercial work. Looking great.",
          "score": 2,
          "created_utc": "2026-01-23 12:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18lnod",
              "author": "bingobongo3001",
              "text": "Thank you!  \nThere's still a lot of room for experiments. I want to try to simulate some particles in houdini and then render it with this approach",
              "score": 2,
              "created_utc": "2026-01-23 13:57:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfnajq",
      "title": "Rant on subgraphs in every single template",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qfnajq/rant_on_subgraphs_in_every_single_template/",
      "author": "1filipis",
      "created_utc": "2026-01-17 20:03:44",
      "score": 88,
      "num_comments": 71,
      "upvote_ratio": 0.8,
      "text": "I'm annoyed as hell from wasting my time on having to unpack and rearrange the nodes every single time I open a workflow.\n\nIt's cool that you have this feature. It's not cool that you've hidden EVERY SINGLE NODE BEHIND IT, including model loaders that sometimes don't even match the names of the files from your own huggingface repo!\n\nThis is not normal. \n\nNo, I don't want less controls.\n\nNo, I don't want your streamlined user experience.\n\nNo, I don't want to make pictures with one click.\n\nIf I wanted to make them with one click, I would choose Nano Banana. Open models are not zero-shot for you to be able to do that. \n\nAnd default workflows always have some weird settings that never produce usable results.\n\nI get it if you packed stuff like custom samplers from LTX or FLUX.2, but no, they are still spaghetti, you've just packed everything.\n\nShow me one person (apart from your designer) who said \"ComfyUI is too complicated, let's dumb it down to one node\". \n\nSomeone had actually invested their time to go through EVERY existing workflow, pack every node, rename the inputs, commit it..\n\nMust have been the same guy who successfully manages to make the UI worse with every update. \n\nStop ignoring what the community says!\n\nI'm out",
      "is_original_content": false,
      "link_flair_text": "No workflow",
      "permalink": "https://reddit.com/r/comfyui/comments/1qfnajq/rant_on_subgraphs_in_every_single_template/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o05xnfh",
          "author": "OnceWasPerfect",
          "text": "I just love how they put the seed value as an input for the subgraph and the setting to make it fixed/random/increment/etc, but if you do that the seed can't actually change because the subgraph expects you to give it a value, its not just showing you what value it is.",
          "score": 56,
          "created_utc": "2026-01-17 20:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o069ozh",
              "author": "broadwayallday",
              "text": "This is the most annoying thing",
              "score": 9,
              "created_utc": "2026-01-17 21:11:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o07txgi",
              "author": "jd641",
              "text": "Is this what changed with Z Image Turbo's template? It used to show the seed now it shows as 0 with a setting of 'Decrement'. If I set it to random it still shows as 0.",
              "score": 3,
              "created_utc": "2026-01-18 02:01:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o081pqz",
                  "author": "Perfect-Campaign9551",
                  "text": "It's broken or something, the randomize doesn't work across graphs",
                  "score": 2,
                  "created_utc": "2026-01-18 02:43:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o081lnw",
              "author": "Perfect-Campaign9551",
              "text": "Yep subgraph randomize doesn't work. Instead you should pass the noise value in entirely from a noise node in the parent",
              "score": 1,
              "created_utc": "2026-01-18 02:43:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0jlnja",
              "author": "deadsoulinside",
              "text": "This. Annoys me, as everytime when I was first messing with this app those subgraph setups caused me to have to close/open the workflow again as it won't update the seed and thus gives me the same exact thing back immediately versus generating a new image or video.",
              "score": 1,
              "created_utc": "2026-01-19 20:46:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o06mgif",
          "author": "Zueuk",
          "text": "100% this. we use ComfyUI to mess with things under the hood, not to hide from them\n\n> Someone had actually invested their time to go through EVERY existing workflow, pack every node, rename the inputs, commit it\n\nand this... this is just ü§¶‚Äç‚ôÇÔ∏è",
          "score": 13,
          "created_utc": "2026-01-17 22:15:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0aabmp",
              "author": "Perfect-Campaign9551",
              "text": "And they didn't even fully test because the seed generators don't work across the subgraph connection",
              "score": 3,
              "created_utc": "2026-01-18 13:15:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o063qgh",
          "author": "Guilty_Emergency3603",
          "text": "Subgraphs are useful in complex and long workflows but yes for simple workflows like in the latest flux Klein templates it's very not necessary.",
          "score": 17,
          "created_utc": "2026-01-17 20:40:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o068erj",
              "author": "neanderthology",
              "text": "Klein is more complicated than qwen image edit, and even the official workflow for qwen has subgraphs, to hide the loaders, prompts, and samplers‚Ä¶ it really is just goofy. For official workflows it should just be the straight up spaghetti. I agree with OP. I download official workflows to see the bare minimum necessary before I modify the workflow for myself. And subgraphs are not necessary.",
              "score": 9,
              "created_utc": "2026-01-17 21:04:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0jlzvh",
              "author": "deadsoulinside",
              "text": "The initial starter Z image template for example. Not a whole lot of nodes there. Which is worse as people end up thinking Z Image cannot do a lot as there are only 2 templates on the Comfy site.",
              "score": 1,
              "created_utc": "2026-01-19 20:48:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o062b9u",
          "author": "sci032",
          "text": "If you click on a subgraph, then click on the split box shaped icon on the upper right of the UI, you can see everything inside of the subgraph and change the settings that you want with out having to actually open the subgraph in the UI.\n\nAlso, if you right click on a subgraph and click on 'edit subgraph widgets'. It will give you a menu similar to the one in the image and you can hide/unhide/move everything to how/where you want it to be.  \n\n\nhttps://preview.redd.it/me2gqr25zydg1.png?width=2560&format=png&auto=webp&s=5f54162dfa969babbe2ef8d0e4f90a8bfc8ded78",
          "score": 21,
          "created_utc": "2026-01-17 20:33:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o062px5",
              "author": "sci032",
              "text": "This is the menu for 'edit subgraph widgets'. If you grab the icon with 6 dots, you can drag the item wherever you want it. The eye icons hide/unhide items.\n\nhttps://preview.redd.it/8vk4miw00zdg1.png?width=2560&format=png&auto=webp&s=5f9aea5b63c1812ca1d4b1354316f9178ed702f1",
              "score": 6,
              "created_utc": "2026-01-17 20:35:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0824ji",
              "author": "Perfect-Campaign9551",
              "text": "Why isn't that icon right on the \"parent\" \" sub - graph\" node? It should be imo . It should have the expand like it does but it should also have this icon too then because nobody is going to think to look somewhere else for that\n\n\nBroken cohesion there",
              "score": 3,
              "created_utc": "2026-01-18 02:46:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o06abwy",
          "author": "tanoshimi",
          "text": "I'm glad you posted this as I was grumbling about the exact same issue to myself, so glad to know I'm not alone!\nNot only does it make the template examples more obtuse, but in several cases it's broken them (as in not passing seed increment/random values into the subgraph, as previously mentioned). It's a lovely _optional_ feature to use in some workflows, but absolutely shouldn't be the default structure.",
          "score": 13,
          "created_utc": "2026-01-17 21:14:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06g06x",
          "author": "TechnologyGrouchy679",
          "text": "While I do understand the utility of subgraphs, some of default templates are just very badly thought out.   \n  \nI normally put my symlinks of my models in subdirectories, so I know I will need to change them - an extra step.  \n  \nAnd as others have mentioned - fixed seeds. wtf?\n\nSubgraphs are useful when I myself am happy with a workflow and need to compact it down, not when others decide. \n\nokay?",
          "score": 13,
          "created_utc": "2026-01-17 21:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o063qkw",
          "author": "Sarashana",
          "text": "Agreed. Subgraphs have no place in workflows meant to teach and demonstrate.",
          "score": 19,
          "created_utc": "2026-01-17 20:40:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06pdtn",
              "author": "No_Possession_7797",
              "text": "While I understand it to a degree, I functionally see them as no different than nodes. They have inputs, outputs and parameters, just like a node. What do you have to do when you want to understand how a node works? You either find docs, examples, or, if you know how, you read the code.   \n  \nCompartmentalizing things reduces clutter and visual complexity, I don't want to parse everything, everywhere all at once. If you can logically partition what you're doing, it makes it easier to conceptualize. If you make that a habit, then wouldn't it be easier to apply the same reasoning from one place to the next?  \n  \nI think the reality is that people have different preferences and it's for valid reasons, but to demand or dictate that it \"should be done in my preferred way\" just seems out of touch.",
              "score": -4,
              "created_utc": "2026-01-17 22:29:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o06sici",
                  "author": "Sarashana",
                  "text": "I would totally agree with what you said for production workflows. We're still taking about workflows meant to teach techniques, though. It's \"read the code\", except that it's \"look at the workflow\" in this case. And having to zoom in and out of subgraphs still makes that more complicated than necessary.",
                  "score": 9,
                  "created_utc": "2026-01-17 22:45:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o06auv5",
          "author": "zincmartini",
          "text": "I haven't used subgraphs, but I have tried to merge (\"group\"?) nodes to clean up the spaghetti as explained by pixaroma. Unfortunately that seems to break some of the functionality. After grouping/ungrouping until it works my workflow is barely cleaned up. I thought I could use different string inputs with text concatenate to help manage the text input a bit more (LoRA tokens in one, character attributes in another, etc) but once merged the concatenate links broke. Merging my model inputs broke other links if there were any links that weren't perfectly linear. I was trying to get a simplified \"variables\" \"constants\" \"output\" workflow, but it never really worked as intended.\n\nAlso the group boxes are pointless because whenever I'm clicking around to move the canvas I invariably end up accidentally dragging a group of nodes. The groups can't be pinned like the nodes, so the group boxes are kinda pointless. Everyone uses them in their custom workflows though. I have taken to just deleting all the groups, rearranging in a way that makes sense to me, and then pinning the nodes so I can't accidentally drag them around.",
          "score": 5,
          "created_utc": "2026-01-17 21:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06wdab",
              "author": "No_Possession_7797",
              "text": "https://preview.redd.it/hk4s217dqzdg1.png?width=975&format=png&auto=webp&s=4020090c9f6982ab446bd35b1be1373581ce369f\n\nDid you know that you can still pin groups? It's a little more convoluted now, because they removed the button from the quick menu. But, you can either right click the group and bring up the context menu or you can toggle the properties panel using the icon on the top right, then click the settings tab.\n\nThe problem I have is that a group can be pinned but it's nodes aren't. So I can select all of the group nodes and the group, but the nodes move and the group doesn't. It's a minor inconvenience, I have to undo the last change, unpin the group and then make sure it's all selected.",
              "score": 1,
              "created_utc": "2026-01-17 23:04:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05zwz7",
          "author": "Euchale",
          "text": "I am with you 100%. I want to see what the nodes in the template do and understand how the workflow works. I can't do that if there is just a single big subgraph node.   \nSure I can open the subgraph up, but thats a completely unnecessary click.",
          "score": 11,
          "created_utc": "2026-01-17 20:20:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o065g4j",
          "author": "Frogy_mcfrogyface",
          "text": "Yeah not a huge fan of them, and this 100%¬†\n\n\n\"including model loaders that sometimes don't even match the names of the files from your own huggingface repo!\"¬†",
          "score": 8,
          "created_utc": "2026-01-17 20:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06dcr8",
          "author": "a_beautiful_rhind",
          "text": "I loved grouping. It cleaned up messes and was simple to understand. Last time I tried subgraphs, they opened over the whole workflow and I had no idea how to move shit outside of them.\n\nThis functionality would be fine if you could just graph/de-graph easily.",
          "score": 3,
          "created_utc": "2026-01-17 21:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07xzg1",
              "author": "ZenWheat",
              "text": "It's because you unpacked the subgraph",
              "score": 1,
              "created_utc": "2026-01-18 02:23:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0aj0ku",
          "author": "ScrotsMcGee",
          "text": "Damn straight.\n\nHonestly, just installed the recent version of ComfyUI, and started testing newer templates, and realised that they've turned it into a bucket of shit.",
          "score": 3,
          "created_utc": "2026-01-18 14:09:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o060jtf",
          "author": "uniquelyavailable",
          "text": "The model loader is so dumb it couldn't find its way out of a paper bag",
          "score": 2,
          "created_utc": "2026-01-17 20:24:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06f3h2",
          "author": "ANR2ME",
          "text": "Yeah, why did they hide the model/lora nodes ü§¶üèª with so many different VRAM amount out there, it's normal for people to use different models.",
          "score": 2,
          "created_utc": "2026-01-17 21:38:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06hb5r",
          "author": "FernDiggy",
          "text": "Umm‚Ä¶guys, what are sub graphs?",
          "score": 2,
          "created_utc": "2026-01-17 21:49:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o076np3",
          "author": "Sabbath196",
          "text": "I agree",
          "score": 2,
          "created_utc": "2026-01-17 23:58:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o089dhu",
          "author": "und3rtow623",
          "text": "I'm usually not one to throw shade, but damn I am with you on this. Shit drives me bonkers",
          "score": 2,
          "created_utc": "2026-01-18 03:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08cxyg",
          "author": "pto2k",
          "text": "That's annoying. In my opinion, many aspects of ComfyUI are counterintuitive.",
          "score": 2,
          "created_utc": "2026-01-18 03:47:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09w5xn",
          "author": "spacemidget75",
          "text": "Hard agree. It's driving me mad, especially with the number of new models/templates hitting at the moment.",
          "score": 2,
          "created_utc": "2026-01-18 11:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06alld",
          "author": "Samurai_zero",
          "text": "Thank god. I downloaded a new portable zip so I would have separate stuff for LTX and one for my old image stuff and I thought I was going mad. Those workflow/templates are the worst shit I have ever seen in ComfyUI.",
          "score": 3,
          "created_utc": "2026-01-17 21:15:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o065kf9",
          "author": "AwakenedEyes",
          "text": "This! So much. What an annoying design feature.\n\nI hate feature creep.",
          "score": 2,
          "created_utc": "2026-01-17 20:49:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o060xbg",
          "author": "bobatoms",
          "text": "Ive only been using comfy for 4 days and even I know you can modify the workload and save it your way",
          "score": 4,
          "created_utc": "2026-01-17 20:26:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o060ux5",
          "author": "Yasstronaut",
          "text": "Model loaders are fine in there IF THEY EXPOSE THE INPUT TO THR SUBGRAPH. So annoyed that people don‚Äôt do that sometimes, even official workflows. It‚Äôs super easy to do‚Ä¶ any attribute of any node can be dragged to the left side to make it a config outside of the subgraph",
          "score": 2,
          "created_utc": "2026-01-17 20:25:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o062mah",
              "author": "IONaut",
              "text": "I kind of like subgraphs but they do have their issues. Like if you try to expose the inputs of the audioloader including the audio player interface the audio player does not update in the main graph if you change the file in the main graph.",
              "score": 2,
              "created_utc": "2026-01-17 20:34:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o082scx",
              "author": "Perfect-Campaign9551",
              "text": "The bad thing about subgraph is advanced nodes like fast bypasser can't cross over them (fast GROUP bypasser is fine but if you want more fine grained control you can't put a regular fast bypass or relay in the parent and have it control an item in the sub)",
              "score": 1,
              "created_utc": "2026-01-18 02:49:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o061syc",
              "author": "vibrantLLM",
              "text": "But exposing the input can only be done as a string not a selectbox right?",
              "score": 1,
              "created_utc": "2026-01-17 20:30:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05zgt8",
          "author": "SearchTricky7875",
          "text": "don't unpack, instead click on the arrow at the top right corner, it ll open the sub graph, update and save, that will keep the workflow sane, unpacking sometimes breaks the workflow.",
          "score": 2,
          "created_utc": "2026-01-17 20:18:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o060ntl",
              "author": "1filipis",
              "text": "But image output is outside of that subgraph. And whatever you can open through Assets doesn't let you zoom in",
              "score": 5,
              "created_utc": "2026-01-17 20:24:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o061z20",
                  "author": "anekii",
                  "text": "You could always have a preview output in the subgraph if you wanted to.",
                  "score": 1,
                  "created_utc": "2026-01-17 20:31:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o06dckg",
          "author": "redstej",
          "text": "Amen.",
          "score": 1,
          "created_utc": "2026-01-17 21:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06hb8p",
          "author": "GraftingRayman",
          "text": "Use my nodes, theres a \"Arrange nodes\" right click option, once unpacked, one click and its rearranged in a nice way",
          "score": 1,
          "created_utc": "2026-01-17 21:49:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06idp5",
          "author": "mallibu",
          "text": "I was searching for a working ltx2 workflow i2v with gguf clip loader and normal .safetensors checkpoint and everything I found in civit workflows was full of needless custom nodes and tons upon tons of unnecessary bs, or the simple official one who couldnt do shit because they loaded everything in one single bs node. I just gave up and returned to wan",
          "score": 1,
          "created_utc": "2026-01-17 21:55:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08no8p",
          "author": "Ok-Page5607",
          "text": "https://preview.redd.it/1ydqa36ng1eg1.png?width=2303&format=png&auto=webp&s=faf9f4ef4b6f36888c5e244d209eaa7cf76074c3\n\nI love this feature!!! It makes everything much simpler because you can view it like modules. If you label it properly and connect the links, everyone will understand it. By that, I really mean labeling every single connection meaningfully. And this is where I see the biggest problem with shared workflows: things get chaotically packed into subgraphs, and things like loaders are hidden but not routed externally to the subgraph.",
          "score": 1,
          "created_utc": "2026-01-18 04:56:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a6h7m",
          "author": "kharzianMain",
          "text": "Subgraphs make comfyui so much more confusing for me to use. At least they need to have an undo subgraph button that just unpacks the whole subgraph there and then into actually usable nodes.¬†",
          "score": 1,
          "created_utc": "2026-01-18 12:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a96wa",
          "author": "iandigaming",
          "text": "Preach.",
          "score": 1,
          "created_utc": "2026-01-18 13:07:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0abs17",
          "author": "ThiagoAkhe",
          "text": "My problem isn‚Äôt subgraphs, I kinda like it, but my biggest hatred is the **pin** on the nodes, because I like to make changes and when it‚Äôs time to see what‚Äôs connected to where‚Ä¶ dude‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-18 13:25:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0czn0m",
          "author": "Old_System7203",
          "text": "I think the problem is a difference in understanding of what templates are for.\n\nHiding everything makes perfect sense if you think of templates as being finished products, aimed at the one-click market (that, I suspect, is what the comfy team see as their future in $ terms).\n\nIt makes zero sense if templates are intended to teach you how to use a feature, which is what most of us here seem to want.",
          "score": 1,
          "created_utc": "2026-01-18 21:23:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0d00j3",
          "author": "SuddenInitial890",
          "text": "Subgraphs are like folders on your hard drive. Instead of having all 50,000 waifu pics in one great big pile, you can organize them by blonde, brunette, etc.¬†\n\n\nIf you have 15 nodes that you never touch once you get the workflow set up, pack them into a subgraph so they don't clutter the workflow. Or pack the loaders into one, samplers into one, etc. Just prompting those few widgets you need access to.¬†\n\n\nIf you load up a workflow and don't like the subgraphs in it, just unpack them. It only takes a single click and the subgraph is no more.¬†",
          "score": 1,
          "created_utc": "2026-01-18 21:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0d2wts",
          "author": "King_Salomon",
          "text": "i create my own workflows. i can‚Äôt stand 99.9% of the workflows i see out there especially the ones (which is the majority i assume) who pack the nodes on top of each other so you don‚Äôt see the connection lines. I WANT TO SEE THE LINES, I WANT IT NEATLY ORGANIZED SO I CAN SEE THE FLOW! and that is why i make my own workflows",
          "score": 1,
          "created_utc": "2026-01-18 21:42:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0io7b0",
          "author": "Woisek",
          "text": "Just use groups, get/set nodes and forget that subgraph shit...\n\nhttps://preview.redd.it/9ce90s6okceg1.jpeg?width=1313&format=pjpg&auto=webp&s=0975b76018ece5246f2dca927cde198be5e4a89e",
          "score": 1,
          "created_utc": "2026-01-19 18:14:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0j9rgv",
          "author": "Robo-420_",
          "text": "Subgraphs should function no differently than any other workflow, I don't know how they screwed this up so much.\n\nThey should literally just make it a regular workflow and only change its appearance.",
          "score": 1,
          "created_utc": "2026-01-19 19:51:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0o5oag",
          "author": "kburoke",
          "text": "I hate subgraphs.",
          "score": 1,
          "created_utc": "2026-01-20 14:23:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o063inx",
          "author": "Momkiller781",
          "text": "Yes!!! You tell them bro!!!",
          "score": 1,
          "created_utc": "2026-01-17 20:39:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o069o37",
          "author": "artdude41",
          "text": "yeah its annoying , but i guess it's meant for these blokes who like to turn this shit into a spaghetti string theory equation lol",
          "score": 1,
          "created_utc": "2026-01-17 21:11:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o073gfx",
          "author": "ramonartist",
          "text": "üôÑ All I'm saying if you are newbie and only now getting into ComfyUI, be prepared for a rough ride because putting everything behind a subgraph and subgraphs within subgraphs, helps no one!",
          "score": 1,
          "created_utc": "2026-01-17 23:41:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0euceu",
          "author": "Herr_Drosselmeyer",
          "text": "OK, take a chill pill. Subgraphs are actually very useful to streamline workflows. Yes, there are some small issues with them, and they're annoying,¬† but they're also easily fixed.¬†\n\n\nWorst case, you spend a couple of minutes on new workflows to have them the way you like them,¬† but that was already the case, at least for me.",
          "score": 1,
          "created_utc": "2026-01-19 03:11:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0658d6",
          "author": "iliark",
          "text": "Guy getting free stuff complains it's not exactly like he wants it. \n\nPosts like these (being whiny, not constructive) just lead to people keeping workflows for themselves.",
          "score": -6,
          "created_utc": "2026-01-17 20:48:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o067s1n",
          "author": "MrWeirdoFace",
          "text": "While you are venting, what else really grinds your gears?",
          "score": -3,
          "created_utc": "2026-01-17 21:01:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06k9l0",
          "author": "Kal315",
          "text": "Then make your own worflows.",
          "score": -5,
          "created_utc": "2026-01-17 22:04:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06nsh0",
          "author": "No_Possession_7797",
          "text": "You do realize that you can expose any of the controls for nodes contained within a Subgraph as controls accessible from the Subgraph element itself? For example, if still want to pick your model of choice from the top level, then make that select/combo visible as part of the element for the subgraph.   \n  \nPersonally I hate the sphagettification of workflows, and yet I am sure there are ways of managing that (Get Node, Set Node, Pipes, Reroutes, etc.) But, the fact of the matter is, that no matter which strategy you employ, you will have to make adjustments and adapt. How often do you use a workflow out of the box, with no alterations? I find they never 100% suit my needs, whether they're default or made by someone else. (The point being, I don't think you'll ever be satisfied to the point that you have to do nothing.)\n\nI think you're right about developers and designers constantly moving goal posts and creating shifts where the users didn't necessarily demand changes. But, there really are considerations beyond just what you want as an individual. Hopefully they can be more open in advance about changes and willing to make compromises and adjustments to what they're doing. But that's not a one way street, it means you have to be open too.",
          "score": -4,
          "created_utc": "2026-01-17 22:21:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o082zjv",
              "author": "Perfect-Campaign9551",
              "text": "It doesn't work for everything actually quite a few nodes don't work and can't cross the subgraph boundary",
              "score": 1,
              "created_utc": "2026-01-18 02:50:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o09o8pn",
          "author": "Justify_87",
          "text": "Oh nooo. Can this free software I didn't pay a penny for please be the way I want it to be",
          "score": -6,
          "created_utc": "2026-01-18 10:10:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09ze3q",
              "author": "spacemidget75",
              "text": "\"It's free\\* software so you can't critique it\" is right up there with \"it's a fictional movie so literally anything can exist and make sense\" in terms of low IQ thought processes.\n\n\\* It's open-source which means \\[Wikipedia\\]: *The¬†open source model¬†is a decentralized software development model that encourages open* ***collaboration****.*  This \"collaboration\" includes product feedback. That's what this is.",
              "score": 6,
              "created_utc": "2026-01-18 11:51:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0bfayz",
                  "author": "Justify_87",
                  "text": "Oh yeah. I can feel your and OPs contribution. Like a hearty fart",
                  "score": -1,
                  "created_utc": "2026-01-18 16:51:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o07w1o6",
          "author": "TekaiGuy",
          "text": "Maybe you're underestimating how many people don't like the complexity, or would start using the program if the complexity was removed. Templates are just a way to get beginners up and running. If you understand enough to be able to unpack them at all, then you're in a high class of users and not really representative of the bulk of the user base.",
          "score": -2,
          "created_utc": "2026-01-18 02:12:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkgc4y",
      "title": "Flux.2 Klein 9B (Distilled) Image Edit - Image Gets More Saturated With Each Pass",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/gallery/1qkgc4y",
      "author": "eagledoto",
      "created_utc": "2026-01-23 03:53:08",
      "score": 85,
      "num_comments": 72,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qkgc4y/flux2_klein_9b_distilled_image_edit_image_gets/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o171yk9",
          "author": "afinalsin",
          "text": "Everyone is correct, it's the VAE encoding/decoding that trashes it, and it's an issue with every image editing model. Even nanobanana pro will bake an image if you pass it back and forth a couple times. Unfortunately everything in the image degrades, not just the colors, so a simple color correct pass won't fix it.\n\nThere is a way to make multiple successive edits without the severe degradation, you just need to work with latents instead of images, but at the moment it's an extremely clunky process. You need to use the \"SaveLatent\" and \"LoadLatent\" nodes. \n\n[Here is a grid showing a multi-image edit](https://i.postimg.cc/c6yNgdy7/grid-00001.png) to swap a character's clothes. It is input/input/output.\n\n[And here is another grid](https://i.postimg.cc/HH5TVFFR/grid-00008.png) showing ~~7~~ 6 consecutive edits with minimal degradation.  \n\nTo use this technique, first run the first edit you want to do and save the latent and image to the same folder. This will make it easier to track which latent is which image, which will be important. It's pretty simple, and [will look like this](https://i.postimg.cc/Q8zgwFXM/Screenshot-2026-01-23-171025.png).\n\nNow the annoying part is you need to copy the latent from your/output/folder to Comfy's input folder. Make sure you copy instead of move, otherwise your image/latent name pairing will be out of sync. You'll need to do this for every successive edit you make, so if you're using windows 11 just middle click both folders to open them in new tabs in windows explorer, it will make it much easier to transfer the files.\n\nNow unpack the \"Reference conditioning\" subgraph and delete the vae encode node, and plug a \"LoadLatent\" node straight into both ReferenceLatent nodes. [It will look like this](https://i.postimg.cc/fTyNLqG3/Screenshot-2026-01-23-171144.png).\n\nImportant: You need to manually set the actual latent width and height, and the width and height of the flux2scheduler to match your input. If you feel like automating the math, Derfuu has a get latent size node, and you can combine that with math nodes to x2 the latent size to get the correct resolution. [It'll look like this](https://i.postimg.cc/nhMB3x6s/Screenshot-2026-01-23-172416.png). Then just plug the outputs into the width/height inputs.\n\nMaybe important: Change the noise seed every edit. In older models, running the same noise on the same image can burn the image extremely badly. I'm unsure if that's an issue with Klein, but better safe than sorry. \n\nNow, make your edit and save the image and latent as before. If you're happy with the image, congratulations, you're done. If you want to make further edits, simply copy-paste the correct latent from your output folder to your input, refresh comfy (just press r), and select the new input latent. Make your edit, save both image and latent, and continue, making as many edits as you want.\n\n---\n\nBefore now you've never really needed to use latents instead of images, so the user experience is awful. There's currently no preview on the latents so you're relying on explorer to see which latent corresponds to which image. The latent also has to be in the input folder, which makes it clunky to immediately switch to the new latent. \n\nI'll have a look and see if I can find a custom node pack that makes working with latents a better experience and whip up an actual workflow. I might try my hand at vibecoding a solution if there isn't one to be found because while this technique produces infinitely better results than the out of the box workflow it's just such a pain in the dick to actually use.",
          "score": 29,
          "created_utc": "2026-01-23 06:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o173m2d",
              "author": "eagledoto",
              "text": "Damn! Thank you so much for the detailed explanation and the work around. Will definitely try it out just out of curiosity and the time out took to write it all, and it might even come in handy later on. Thanks again man.",
              "score": 5,
              "created_utc": "2026-01-23 06:54:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18vb8s",
                  "author": "afinalsin",
                  "text": "Ayy, no worries.\n\nSo, I just downloaded your image and couldn't replicate. I did notice you said you removed the scaletototalpixels node. Your input image is 1536 x 2752, and the Flux2 Klein base resolution is 1 megapixel. 1536 x 2752 is 4,227,072 pixels, so you are trying to generate 2x the base resolution. \n\nFor now, bring back the scale to total pixels node and set it to 2 megapixels max. Also make sure to change the seed between generations, and that should set you up right for small edits like these. If you need to make bigger edits like putting a character in a different pose, you will likely need to drop the resolution down closer to base res. \n\nIf you want to work on the full resolution image, you'd be better off running an inpainting workflow on it, where you crop bits out of the image, make your edits on those cropped bits, and reattach. There must be a Klein inpainting workflow around somewhere, let me know if you can't find one and I'll throw one together for you.",
                  "score": 3,
                  "created_utc": "2026-01-23 14:46:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o19nf8p",
              "author": "roxoholic",
              "text": "You don't have to manually save/load latents. There are `Latent Sender`/`Latent Receiver` nodes in Impact Pack which make iterating a lot faster.\n\nhttps://github.com/ltdrdata/ComfyUI-extension-tutorials/blob/Main/ComfyUI-Impact-Pack/tutorial/sender_receiver.md",
              "score": 3,
              "created_utc": "2026-01-23 16:56:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19uurh",
                  "author": "afinalsin",
                  "text": "Fuck yes, I knew someone was gonna come through with the goods. That looks incredibly useful, and way better than the hatchet job nodes I vibecoded. Thank you.",
                  "score": 3,
                  "created_utc": "2026-01-23 17:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o19gwvq",
              "author": "Sgsrules2",
              "text": "Use this node instead: https://github.com/Velour-Fog/comfy-latent-nodes\nIt lets you specify the location so you don't have to copy the latents. Just setup a temp file and read from it.",
              "score": 1,
              "created_utc": "2026-01-23 16:27:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19veht",
                  "author": "afinalsin",
                  "text": "Hell yeah. I already vibed my way through a similar node group, but good to have options. Between this and the other guy's send/receive nodes, I'll pretty much be set. Thank you.",
                  "score": 1,
                  "created_utc": "2026-01-23 17:33:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16ihoz",
          "author": "BathroomEyes",
          "text": "It‚Äôs called color shifting and it‚Äôs a consequence of the vae encode/decode nodes. They‚Äôre lossy and can result in noticeable loss of sharpness and color inaccuracy. There‚Äôs a few ways you can address this. You could color correct with post processing software. If you need to do multiple sampler passes you can keep the successive outputs in latent space and only decode after the last sampler pass. TBG-Takeaways has a vae decode color shift fixer for Flux1.D so I‚Äôm hoping u/TBG______ releases one for Flux.2.",
          "score": 31,
          "created_utc": "2026-01-23 04:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16uet5",
              "author": "eagledoto",
              "text": "\"If you need to do multiple sampler passes you can keep the successive outputs in latent space and only decode after the last sampler pass\"\n\ncan you explain this part a bit more please? I am new to comfy so learning",
              "score": 3,
              "created_utc": "2026-01-23 05:41:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16vj4y",
                  "author": "Haiku-575",
                  "text": "After your denoised latent comes out of the KSampler, instead of passing it to a VAE Decode node to turn it into pixels, pass it straight into another KSampler with different conditioning. Does that make sense?",
                  "score": 17,
                  "created_utc": "2026-01-23 05:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17663a",
              "author": "JoelMahon",
              "text": "is there simply a less lossy version of the vae encode/decode nodes?",
              "score": 1,
              "created_utc": "2026-01-23 07:15:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cvx2n",
                  "author": "Winter_unmuted",
                  "text": "no. it isn't the node's fault, but rather a consequence of vae encoding itself. latent space, which is what an image is vae encoded into, has less information than the input image. It's lossy. \n\nThe reconstructed image has the same amount of info as the input image did, but it doesn't \"remember\" what the input was. It just knows the latent, and what general informatino can be used to reconstruct an image from such a latent. The missing info between those two formats is made up. The data are constrained randomness. \n\nthat randomness manifests as essentially what we call degraded images. Nonsensical things like extra fingers. eyes that don't line up right or irises that aren't circular. teeth that are lopsided or the wrong number. Letters that look letter-ish but are actually nonsense. etc.",
                  "score": 2,
                  "created_utc": "2026-01-24 02:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17ldoz",
              "author": "TheSquirrelly",
              "text": "The multiple samplers sounds like a good idea.  Though if you have a number of edits to do like the OP you have to hope each of the edits get it right the first time.  If you find you have to regen for each edit and get success 1 in 4 (just to make up a number) and have 3 edits, now you have a 1 in 64 chance.  But if it usually hits it the first time then should be good.",
              "score": 1,
              "created_utc": "2026-01-23 09:34:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17z1d5",
                  "author": "Aromatic-Somewhere29",
                  "text": "You can chain multiple KSamplers and place a VAE decoder with image preview after each one. Bypass all but the first KSampler and keep regenerating only the first edit until you get the desired result. Once satisfied, lock the seed of the first KSampler, enable the second one, and repeat the process.",
                  "score": 5,
                  "created_utc": "2026-01-23 11:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16f231",
          "author": "marres",
          "text": "vae encode/decode and whatever other stuff the model is doing with the image degrades it.",
          "score": 8,
          "created_utc": "2026-01-23 03:59:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16gvz5",
              "author": "eagledoto",
              "text": "what if i give it a detailed prompt to keep the rest of the image as it is and just make the change that i tell it to? you think that will help?",
              "score": 1,
              "created_utc": "2026-01-23 04:10:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16ihsi",
                  "author": "marres",
                  "text": "No, you can't control that with the prompt. What you would need to do is do all the editing in one pass",
                  "score": 6,
                  "created_utc": "2026-01-23 04:20:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16l41u",
          "author": "Most_Way_9754",
          "text": "Create a mask of the area you want to edit by using:\n\n[https://github.com/adambarbato/ComfyUI-Sa2VA](https://github.com/adambarbato/ComfyUI-Sa2VA)\n\nthen use: \n\n[https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch](https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch)",
          "score": 10,
          "created_utc": "2026-01-23 04:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16tzz6",
              "author": "eagledoto",
              "text": "What about using the inbuilt mask editor?",
              "score": 3,
              "created_utc": "2026-01-23 05:38:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17700i",
                  "author": "Most_Way_9754",
                  "text": "i'm not skilled at using a paintbrush to paint the ear-rings, people in the background or her clothes. so i just use something like Sa2VA to automatically create the mask. you can try to mask manually using the in built tools and use the crop and stitch nodes that way.",
                  "score": 2,
                  "created_utc": "2026-01-23 07:23:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1710y8",
          "author": "_VirtualCosmos_",
          "text": "Same with Qwen Edit",
          "score": 4,
          "created_utc": "2026-01-23 06:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17xe5o",
          "author": "Luke2642",
          "text": "GROUPTHINK ALERT - THIS IS NOT CAUSED BY VAE.\n\nThe colour shift is caused by the ksampler applying a STD + MEAN shift to move the distribution across the channels from being more like the noise to more like the distribution statistics of the VAE.\n\nIf you pass it through six times you get a slight fading effect, that is all. No colour shift.\n\nIf you add a latent multiply, the fading effect vanishes. No colour shift.\n\nhttps://preview.redd.it/v6zdrrp343fg1.png?width=3234&format=png&auto=webp&s=8fd5da77168a7727d09bff6209f1e766089799ac",
          "score": 8,
          "created_utc": "2026-01-23 11:21:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19bs7f",
              "author": "BathroomEyes",
              "text": "You really should be using the Flux2 vae in your example if you‚Äôd like to make an apples to apples demonstration. Unless that‚Äôs just what you‚Äôve renamed your flux2 vae.",
              "score": 1,
              "created_utc": "2026-01-23 16:04:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19i7ro",
                  "author": "Luke2642",
                  "text": "yeah, they named it ae.safetensors originally, which I think was an error. So at least I should call it flux\\_vae.safetensors!\n\n[https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main](https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main)",
                  "score": 2,
                  "created_utc": "2026-01-23 16:32:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o185l2z",
              "author": "Luke2642",
              "text": "That is absolutely not all. The details degrade with each successive trip in and out of latent space, depding on the VAE.¬†[Comparison between input and 8x encode/decode cycles](https://imgsli.com/NDQ0NjY3).",
              "score": 0,
              "created_utc": "2026-01-23 12:22:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18eyej",
                  "author": "eagledoto",
                  "text": "So the latent multiply works?",
                  "score": 1,
                  "created_utc": "2026-01-23 13:21:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o16ormt",
          "author": "krigeta1",
          "text": "Same thing happened with nano banana pro gemini app too, when I am keeping asking for edits the quality degraded by each step, have you tried with an actual image? I guess it could be a thing with nano banana pro images and its watermark. But is it just amu assumption.",
          "score": 3,
          "created_utc": "2026-01-23 05:01:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u9wt",
              "author": "eagledoto",
              "text": "Havent tested it on a real image, but i believe once its processed by ai, it would start to have the same issue of one tries to edit it again and again.",
              "score": 1,
              "created_utc": "2026-01-23 05:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16r0nm",
          "author": "TechnologyGrouchy679",
          "text": "kijai's \"match color\" node might tame the saturation",
          "score": 3,
          "created_utc": "2026-01-23 05:17:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u6b4",
              "author": "eagledoto",
              "text": "https://preview.redd.it/wklxr9kod1fg1.png?width=1173&format=png&auto=webp&s=fb38e808cfd8f51f581b11f6e44b63257052ebca\n\nThis correct?",
              "score": 3,
              "created_utc": "2026-01-23 05:39:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16v9oh",
                  "author": "TechnologyGrouchy679",
                  "text": "yes but in this case it doesn't seem all that effective",
                  "score": 2,
                  "created_utc": "2026-01-23 05:47:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ed7n3",
                  "author": "TBG______",
                  "text": "A mask is needed so the black shirt doesn‚Äôt inherit colors from the white shirt. And try to use lab or wavelet for cc.",
                  "score": 1,
                  "created_utc": "2026-01-24 09:17:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18ax3o",
              "author": "sevenfold21",
              "text": "Color matching sucks for editing, because you're adding or removing pixels, and these colors can be completely different from the original.",
              "score": 2,
              "created_utc": "2026-01-23 12:57:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o17cuiw",
          "author": "TBG______",
          "text": "Colorshifts in Diffusion + VAE workflows in ComfyUI mainly originate from three technical sources.\n\n\n1. Model capacity and reconstruction error\n\nEvery diffusion model has limited capacity to perfectly reconstruct image content. During iterative, these reconstruction errors accumulate. This becomes most visible in uniform, low-entropy regions such as pure black, gray, or white areas. The observed color shift is therefore not random noise, but the model‚Äôs inability to exactly reproduce flat tonal regions across generations.\n\n2. Inpainting and differential diffusion leakage\n\nInpainting introduces unavoidable leakage, even when differential diffusion is implemented directly within the model. Color and sharpness changes are not confined to the masked region, they also affect unmasked areas.\nEven with a fully black (0) mask, subtle changes can be observed outside the intended edit region. Increasing the number of inpainting steps amplifies this effect, causing gradual drift in color and detail both inside and near the mask boundaries.\n\n3. VAE encoding and decoding shifts\n\nVAE-induced color shifts are a well-known issue and have already been thoroughly documented. Any pixel-to-latent and latent-to-pixel conversion introduces small but cumulative deviations in color and contrast.\n\nUsing tiled VAE encoding/decoding generally produces better local color stability compared to full-frame VAE passes, especially at high resolutions. However, tiled VAEs introduce small rounding and boundary errors at tile borders. More details here : https://www.patreon.com/posts/147809146\n\nThere is only one reliable method to exactly maintain image content:\nDo not pass it through the sampler.\n\nThis makes crop-and-stitch workflows essential. Ideally, these operations should happen entirely in pixel space, using the original image data. Even a single VAE encode/decode pass alters the image, so avoiding unnecessary latent conversions is critical when preservation is required.\n\nIn the TBG ETUR Enhanced Tiled Upscaler and Refiner, these principles are fully automated:\n\t‚Ä¢\tCrop-and-stitch handling\n\t‚Ä¢\tVAE correction\n\t‚Ä¢\tLanpaint\n\t‚Ä¢\tMulti-object editing in a single pass\n\nThis allows you to modify many separate objects while keeping the background fully intact.\n\nHow it works\n\nYou can either:\n\t‚Ä¢\tUse the SAM segmentation nodes, or\n\t‚Ä¢\tManually mask all target elements\n\nPass the masks and the input image through the Upscaler and Tiler node with:\n\t‚Ä¢\tUpscale = None\n\t‚Ä¢\tPreset = Full Image\n\nThis configuration converts the workflow into an advanced inpainting pipeline, rather than a tiled upscaler.\n\nThe ‚ÄúETUR Tile Overrides‚Äù node enables:\n\t‚Ä¢\tAutomatic prompt generation\n\t‚Ä¢\tPer-segment prompt assignment\n\t‚Ä¢\tAdditional conditioning per selected element\n\nThe Refiner then applies all modifications while preserving the background. Optionally, the background itself can be refined in the same pass if desired.\n\nThis workflow has been tested with:\n\t‚Ä¢\tFlux, Qwen,ZIT,SD,SDXL,CHROMA\n\t‚Ä¢\tFlux Inpaint\n\t‚Ä¢\tFlux Kontext\n\nIt has not yet been tested with Flux2 Klein, but it should work similarly. Alternatively, a manual crop-and-stitch approach can be used to achieve comparable results.\n\nCore rule\n\nNever sample content that must remain unchanged.",
          "score": 3,
          "created_utc": "2026-01-23 08:15:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17fjnq",
              "author": "TBG______",
              "text": "This gives me an idea: I could add an inpainting-only mode switch to TBG ETUR. Let me know if this would be useful for you.",
              "score": 2,
              "created_utc": "2026-01-23 08:40:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19cckj",
                  "author": "BathroomEyes",
                  "text": "I would find this very useful, thank you!",
                  "score": 1,
                  "created_utc": "2026-01-23 16:06:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18elho",
              "author": "eagledoto",
              "text": "Will test with ZIT, thank you!",
              "score": 1,
              "created_utc": "2026-01-23 13:19:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16gj3n",
          "author": "alsshadow",
          "text": "Looks funny",
          "score": 2,
          "created_utc": "2026-01-23 04:08:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16r5js",
          "author": "admajic",
          "text": "I use the two image workflow. Put both images in. In image 2 I color the parts I don't want it to touch in red. Then prompt what I want it to do",
          "score": 2,
          "created_utc": "2026-01-23 05:17:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16tlek",
          "author": "mac404",
          "text": "You should be able to fairly easily make all 3 of those changes in a single prompt.  \n\nDoing multiple VAE Encode/Decode passes will degrade the quality over time, but not necessarily to the degree you see here. You will see a color shift with every gen, but it should be only noticeable when doing more intense back-and-forth comparisons (whereas it's quite obvious here).  \n\nIn your example, after the first change you also already had some shifting / squishing of the image. This can happen sometimes, I've found it's usually a good idea to try 2-4 seeds with the same prompt and then pick the best one. You are also running at a resolution that is getting too high for Klein to handle well (1536 x 2752), and it will generally be much less stable because of that. I have generally found (although I haven't tested overly scientifically) that keeping the longest side below about 2k resolution will improve stability significantly when making changes. The model itself tends to output images that are so sharp / clear that I don't find the resolution limitation to actually be all that limiting.  \n\nNot perfect, but [here was the very first image I got](https://imgsli.com/NDQ0NTU4) when I tried with this prompt (after downloading the original PNG of your first image):  \n\n> Subject's shirt is black. Remove the subject's earrings. Remove the people from the background. Keep the subject‚Äôs pose and framing unchanged.  \n\nBecause the res is so high, you still get a little bit of squashing/stretching that's noticeable in the face. Maybe it would be perfect in a different seed if you tried a few. Hair color is slightly darker and the coffee cup also darkens slightly, but skin color stayed basically the same. There's a random out-of-focus person that got added into the background and a few other random changes, too. But not bad for literally the first try with a simple multi-change prompt.",
          "score": 2,
          "created_utc": "2026-01-23 05:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16zoud",
              "author": "eagledoto",
              "text": "yep definitely, I could do all that with a single prompt, but that was not my main focus, i was just playing around and noticed it as i didn't know about the color shifting.\n\nRegarding resolution, yes i believe its odd and pretty high.\n\nThe output you got is pretty decent yes.\n\nThank you for the detailed comment. Means alot!",
              "score": 1,
              "created_utc": "2026-01-23 06:22:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1725sj",
                  "author": "mac404",
                  "text": "Sure! And gotcha, makes sense.  \n\nAnother random note on prompting - basically anything you mention in the prompt will get changed in some way. You can always try variations on \"Keep ___ unchanged\" and it will often work pretty well.  And as weird as it feels, I find just puttring \"[object] is [color]\" can work better than \"change [color] to [other color]\". \"Remove\" and \"replace\" are pretty good prompt words, though.",
                  "score": 2,
                  "created_utc": "2026-01-23 06:42:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1728ai",
          "author": "Downtown-Bat-5493",
          "text": "*Quality degradation with each pass. \n\n\nJust compare her face in first image to the last.",
          "score": 2,
          "created_utc": "2026-01-23 06:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1880od",
          "author": "Aromatic-Somewhere29",
          "text": "You can disable all groups except the first one and rerun the workflow until you get the result you want. Then, lock the seed, enable the second group, and repeat the process. You can copy and paste the last group to extend the chain. \n\nhttps://preview.redd.it/nh8qo9dwo3fg1.png?width=6121&format=png&auto=webp&s=e15e5c3ded9cad14674dd1acf0058b57adc7550c",
          "score": 2,
          "created_utc": "2026-01-23 12:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18eh8m",
              "author": "eagledoto",
              "text": "Thank you will try it out",
              "score": 1,
              "created_utc": "2026-01-23 13:18:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18gv12",
                  "author": "Aromatic-Somewhere29",
                  "text": "I'm not sure if the workflow saved properly, here's a second attempt with small refinements. Looks like Reddit converts .png to .webp and removes the embedded workflow. I‚Äôve uploaded the .json file here: [https://pastebin.com/pqQfeHLD](https://pastebin.com/pqQfeHLD)\n\nhttps://preview.redd.it/2lpuhok6r3fg1.png?width=5874&format=png&auto=webp&s=5176e79bbee7a784170182a93318d76452b0a6a4",
                  "score": 2,
                  "created_utc": "2026-01-23 13:31:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ee9vm",
                  "author": "Aromatic-Somewhere29",
                  "text": "Have you had a chance to try it? I‚Äôm curious whether it helped streamline your workflow.\n\nThe main idea here is to avoid the kind of manual overhead described in that long comment - things like saving intermediate latent files, digging through folders to find the right one, copying and pasting them back into the graph, and reloading just to make a tiny adjustment. That approach works in a pinch, but it‚Äôs not how latent-space editing was really meant to be used within a single, continuous generation task.\n\nIn ComfyUI the standard and most efficient approach for iterative refinement is chaining KSamplers and passing the latent directly from one to the next. This keeps everything in memory, avoids file clutter, and lets you isolate each edit step cleanly.\n\nThat‚Äôs exactly what this workflow does: each KSampler handles one specific change (e.g., pose, lighting, expression), and the latent flows sequentially down the chain. You can disable all but the first group, iterate until you‚Äôre happy, lock the seed, then move on to the next stage - all without ever touching the filesystem.\n\nIt‚Äôs a bit wordy, I know, but maybe my take on this gets missed because it hasn‚Äôt been explained in enough detail.",
                  "score": 1,
                  "created_utc": "2026-01-24 09:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1f2vzy",
          "author": "TBG______",
          "text": "I propose a potential solution that replaces the unedited areas with the corresponding regions from the original image to prevent degradation: [https://www.reddit.com/r/comfyui/comments/1qkog35/comment/o1f292l/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/comfyui/comments/1qkog35/comment/o1f292l/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) \\- After testing it with SAM, which requires manual input of elements, I‚Äôve created a new node that can automatically detect the affected areas without SAM, allowing the process to be fully automated.\n\nHowever, I noticed that many contour edges are affected in the output of the sampler. This could make the issue a bit tricky to resolve. The new TBG Difference Mask node compares two images, mixes SSIM (structure) and RGB difference (color), then thresholds and area-filters to isolate real, coherent changes (like a shirt swap) while ignoring tiny model artifacts and noise.\n\nTBG Takeaways with the new TBG Difference Mask node are now uploaded and accessible from the Manager. The workflow for the SAM and Diff nodes can be found here:¬†[https://www.patreon.com/posts/149003920](https://www.patreon.com/posts/149003920)¬†(free access). SAM version works always but Diff Mask only if diff is significant.\n\nhttps://preview.redd.it/tvi98fzloafg1.png?width=1336&format=png&auto=webp&s=3b28ffe8b7e40aee43b6dd595bc72961dea63253",
          "score": 2,
          "created_utc": "2026-01-24 12:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o173w2b",
          "author": "Rude_Dependent_9843",
          "text": "In my experience, I've found that this depends on the seed. Since the distilled model, according to FL Studio, already exhibits less variability, what I do is add a seed randomization module: I use Easy Seed (I've also added RGthree Seed, but for some reason it makes the flow heavier). Because generation is fast, I run some tests, and let's say 1 out of every 3 results has a balanced color correction.",
          "score": 1,
          "created_utc": "2026-01-23 06:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1747f3",
              "author": "eagledoto",
              "text": "Regarding seed, the seed doesnt change every time I generate something, it's set to randomized but I believe I am looking at a different seed? It's the noise seed something, sorry I am on my phone so I can't tell you what it was exactly",
              "score": 1,
              "created_utc": "2026-01-23 06:59:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1742jb",
          "author": "Amirferdos",
          "text": "Don‚Äôt think about the VAE, it‚Äôs working good in 4B model.\nSo the problem is in 9B model",
          "score": 1,
          "created_utc": "2026-01-23 06:57:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17a5sa",
          "author": "magik111",
          "text": "for me sometimes help add to prompt \"Strictly preserve all original colors from the photo, maintain exact color tones, saturation, and hues without any changes. Be completely consistent with the photo's color palette throughout the entire image\"",
          "score": 1,
          "created_utc": "2026-01-23 07:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17ap59",
          "author": "StableLlama",
          "text": "My main observation is that it gives the image warmer colors.\n\nAnyway, when the colors and saturation isn't kept, actually when the values of the not edited pixels isn't kept it's bad.\n\nIMHO there are two ways to solve it:\n\n* Train the model better. (My guess is, that the VAE doesn't keep the error function. I.e. a pixel change resulting in a Delta E and luminosity change of \"err\" should create after the same change in latent space also an error of \"err\". Most likely the VAE wasn't trained with that constraint). But that's nothing you can do.\n* Use masking. Only allow the model to change the pixels you are allowing it to, the others are prevent from changing with a mask.\n\nWith tools like Krita AI it's very simple to mask the area that is allowed to change. With Comfy itself you need to adapt your workflow.",
          "score": 1,
          "created_utc": "2026-01-23 07:55:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o191o87",
          "author": "TekaiGuy",
          "text": "Edit models are always destructive which is why I don't even bother with them.",
          "score": 1,
          "created_utc": "2026-01-23 15:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19lgxg",
          "author": "Comfortable_Swim_380",
          "text": "Your scheduler can be causing that I would try something more advanced",
          "score": 1,
          "created_utc": "2026-01-23 16:47:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dc20p",
          "author": "tarkansarim",
          "text": "That‚Äôs why I always use photoshop to only adopt the parts I‚Äôve changed onto the original image for local edits. Obviously that doesn‚Äôt work for global edits like lighting but that‚Äôs also not something you would do repeatedly on the same image.",
          "score": 1,
          "created_utc": "2026-01-24 04:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16hvpt",
          "author": "Key-Tension1528",
          "text": "All models have some sort of bias and do this. If you repeat enough times the bias will make it drift into nonsense. You could try passing it through a color match node with the original image.",
          "score": 0,
          "created_utc": "2026-01-23 04:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16u5cx",
              "author": "eagledoto",
              "text": "https://preview.redd.it/r94ndihld1fg1.png?width=1173&format=png&auto=webp&s=093afeebace3bfb939581678389af7bac1b18606\n\nAm i supposed to do it like this? sorry m a noob at this, started a few days ago.",
              "score": 1,
              "created_utc": "2026-01-23 05:39:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16q9ku",
          "author": "TomorrowNeverKnowss",
          "text": "Other than the increased saturation, that actually looks really good. Can you share your workflow?",
          "score": 1,
          "created_utc": "2026-01-23 05:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16tr2w",
              "author": "eagledoto",
              "text": "I am using the default workflow from here, everything should be there if you want to download.\n\n[https://docs.comfy.org/tutorials/flux/flux-2-klein](https://docs.comfy.org/tutorials/flux/flux-2-klein)",
              "score": 2,
              "created_utc": "2026-01-23 05:36:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgsk0o",
      "title": "Is there an NSFW AI community for chatting, sharing, and helping - like this but for NSFW?",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qgsk0o/is_there_an_nsfw_ai_community_for_chatting/",
      "author": "trollkin34",
      "created_utc": "2026-01-19 03:03:16",
      "score": 79,
      "num_comments": 38,
      "upvote_ratio": 0.8,
      "text": "I've finally started to get some really good results, but I need some more focused help from people with experience. For example, I'd like to post my  prompt and get feedback. I think it's not working 100% and I'm not experienced enough to tell why. \n\nBut respect to this and other subs: I'm not going to post NSFW where it's not wanted. So what's a good place? Most of the stuff I see is just for posting pics and they don't talk or help each other at all.",
      "is_original_content": false,
      "link_flair_text": "Help Needed",
      "permalink": "https://reddit.com/r/comfyui/comments/1qgsk0o/is_there_an_nsfw_ai_community_for_chatting/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o0exnw3",
          "author": "Choowkee",
          "text": "Discord servers are your go to for that. Unstable Diffusion / Furry Diffusion / NSFW AI.\n\nThere are invite links posted if you search for older posts.",
          "score": 41,
          "created_utc": "2026-01-19 03:30:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ezqsm",
              "author": "trollkin34",
              "text": "Nice ideas! Thank you!",
              "score": 6,
              "created_utc": "2026-01-19 03:43:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ewz4k",
          "author": "Shppo",
          "text": "maybe /r/unstable_diffusion",
          "score": 24,
          "created_utc": "2026-01-19 03:26:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fbexs",
              "author": "Prudent-Struggle-105",
              "text": "I quit the sub because of Grok's spamming stuffs.\nI wanted to create a sub but someone already  created a Comfyui_nsfw sub with no post.\nLet's give it a try [Comfyui_nsfw ](http://r/comfyui_nsfw)",
              "score": 31,
              "created_utc": "2026-01-19 05:00:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0g2j0n",
                  "author": "Mylaptopisburningme",
                  "text": "Your link is missing the reddit. /r/comfyui_nsfw/",
                  "score": 12,
                  "created_utc": "2026-01-19 08:46:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0fz0cc",
                  "author": "iCreatedYouPleb",
                  "text": "Agree. The grok spam is getting annoying.",
                  "score": 9,
                  "created_utc": "2026-01-19 08:13:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0jyc3u",
                  "author": "StuccoGecko",
                  "text": "Yeah that‚Äôs the challenge of trying to find NSFW communities. They easily get overrun by mega-gooners just looking for a place to spam their low effort slop for some reason.\n\nIt would be awesome to find a place that does not allow users to upload their spam outside of maybe one, dedicated channel that users must opt in to.",
                  "score": 3,
                  "created_utc": "2026-01-19 21:48:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1c2nl6",
                  "author": "iCreatedYouPleb",
                  "text": "Good news lol. With the big nerf grok just updated, hopefully we get less fake grok spam now. They can‚Äôt even make bikini pic anymore, so much less nude.",
                  "score": 1,
                  "created_utc": "2026-01-23 23:50:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0fxsys",
                  "author": "jingtianli",
                  "text": "haha",
                  "score": -4,
                  "created_utc": "2026-01-19 08:02:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0im4w2",
              "author": "necrophagist087",
              "text": "good for gooning, but there is little technical discussion about the works (how it was made ect.)",
              "score": 3,
              "created_utc": "2026-01-19 18:05:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ezo6r",
              "author": "trollkin34",
              "text": "Thank you.",
              "score": 1,
              "created_utc": "2026-01-19 03:42:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zp097",
          "author": "ellensrooney",
          "text": "Most people struggle because there aren‚Äôt many NSFW spaces that encourage learning instead of flexing. I usually look for communities that talk openly about prompt logic, conditioning, and consistency rules rather than just posting images.\n\nPersonally, I ended up doing a lot of learning inside kalon ai since you can experiment with NSFW AI chat and generation in one place and quickly see how prompt tweaks affect outcomes. Big advice is to save versions of prompts so you can compare what changed instead of guessing.",
          "score": 6,
          "created_utc": "2026-01-22 04:33:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g03y2",
          "author": "Sandzaun",
          "text": "r/sdnsfw",
          "score": 5,
          "created_utc": "2026-01-19 08:23:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g98vr",
          "author": "ThexDream",
          "text": "‚ÄúSo what's a good place? Most of the stuff I see is just for posting pics and they don't talk or help each other at all.‚Äù\nLike what kind of feedback? Whether it helps them goon or not? Whether they will help you to goon? You just have to keep rolling the seed until you hit bliss.",
          "score": 5,
          "created_utc": "2026-01-19 09:50:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0g9r1o",
              "author": "guai888",
              "text": "Most of the posters are actually pretty helpful if you ask. Gooner are nice people too. All of my question regarding workflow and tool do get answered when I asked in r/unstablediffusion",
              "score": 8,
              "created_utc": "2026-01-19 09:54:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0i0rxv",
                  "author": "Fast_Situation4509",
                  "text": "Gooners are people tooTM",
                  "score": 3,
                  "created_utc": "2026-01-19 16:29:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0k8l2a",
                  "author": "CertifiedTHX",
                  "text": "Huh its been banned in the last day?",
                  "score": 2,
                  "created_utc": "2026-01-19 22:38:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0gfkj9",
              "author": "Choowkee",
              "text": "I learned more about lora training on an NSFW discord than on Reddit or Civit.",
              "score": 5,
              "created_utc": "2026-01-19 10:48:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0i38et",
              "author": "Crafty_Republic_2486",
              "text": "Deliberately obtuse?  Just curious.",
              "score": 0,
              "created_utc": "2026-01-19 16:40:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0h27j0",
          "author": "10minOfNamingMyAcc",
          "text": "Copy of my reply:\n\nYou could look at some Discord servers where both chub and janitor people share their works at:  \nThe workshop:  \nContent from AI Gen Resources to Botjams.  \nWould really recommend it if you're into character creation as well for RP/Story writing for example\n\nYou find it on chub @ ZypherMakesBots",
          "score": 1,
          "created_utc": "2026-01-19 13:38:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hidiy",
          "author": "OtherProfessional433",
          "text": "Have you tried CivitAI? I saw people comment on models or workflows and the creators answer their questions. Also some big creators post links to their Discord.",
          "score": 1,
          "created_utc": "2026-01-19 15:04:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0honu6",
          "author": "EitherKaleidoscope06",
          "text": "How about a telegram group",
          "score": 1,
          "created_utc": "2026-01-19 15:34:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0i1i3w",
          "author": "paulatheprogrammer",
          "text": "There's plenty of NSFW content on Civitai, and it's a fairly large community.",
          "score": 1,
          "created_utc": "2026-01-19 16:32:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iau5z",
          "author": "Neither-Apricot-1501",
          "text": "Check out niche Discord servers or some AI forums focused on NSFW content for chatting.",
          "score": 1,
          "created_utc": "2026-01-19 17:14:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jye3q",
          "author": "3DNZ",
          "text": "Isn't that all of these genAi subs?",
          "score": 1,
          "created_utc": "2026-01-19 21:48:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0how97",
          "author": "zxcvbnm_mnbvcxz",
          "text": "Check out my profile and DM me if you‚Äôd like",
          "score": 1,
          "created_utc": "2026-01-19 15:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0etvum",
          "author": "darkbit1001",
          "text": "r/JanitorAI_Official",
          "score": -12,
          "created_utc": "2026-01-19 03:09:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0h1k5j",
              "author": "10minOfNamingMyAcc",
              "text": "I hate janitor, but you could look at some Discord servers where both chub and janitor people share their works at:  \nThe workshop:  \nContent from AI Gen Resources to Botjams.  \nWould really recommend it if you're into character creation as well for RP/Story writing for example",
              "score": 2,
              "created_utc": "2026-01-19 13:34:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ew76s",
              "author": "Savings-Cry-3201",
              "text": "Definitely not this",
              "score": 1,
              "created_utc": "2026-01-19 03:21:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ewyk6",
                  "author": "darkbit1001",
                  "text": "You know what? I heard.I know they're pretty comfortable on the topic of NSFW and AI, as I thought that would help you... but whats wrong with it in your respect - current controversy notwithstanding?",
                  "score": 1,
                  "created_utc": "2026-01-19 03:26:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0g7h1q",
          "author": "ItsZerone",
          "text": "If only reddit had a search bar",
          "score": -11,
          "created_utc": "2026-01-19 09:33:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0eyhdw",
          "author": "eatTheRich711",
          "text": "Fuckn Gooners ruin everything",
          "score": -80,
          "created_utc": "2026-01-19 03:35:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0g2wid",
              "author": "Mylaptopisburningme",
              "text": "Like it or not smut moves technology.  I doubt there would be as much interest in Comfy/SD if you were limited to making puppies chase butterflies or old ladies baking pies.",
              "score": 17,
              "created_utc": "2026-01-19 08:49:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0gf5nw",
                  "author": "minecraft_fam",
                  "text": "/rule34",
                  "score": 2,
                  "created_utc": "2026-01-19 10:44:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0gf3ms",
              "author": "minecraft_fam",
              "text": "Yeah, but on the other hand, the evolution of the internet owes a lot of its growth to porn and porn-related technologies. Think of it like fertlilizer; if you come into contact with it then you want to wash your hands, but it helps you get some *amazing* flowers or vegetables.",
              "score": 5,
              "created_utc": "2026-01-19 10:44:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhnbwq",
      "title": "Drum pad but for Comfy.",
      "subreddit": "comfyui",
      "url": "https://v.redd.it/1xysutl5qeeg1",
      "author": "skbphy",
      "created_utc": "2026-01-20 01:29:57",
      "score": 77,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/comfyui/comments/1qhnbwq/drum_pad_but_for_comfy/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mx22i",
          "author": "NoYogurtcloset4090",
          "text": "Cool\\~",
          "score": 4,
          "created_utc": "2026-01-20 08:51:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nx058",
          "author": "aastle",
          "text": "Thanks for creating this node pack, very clever. However, it's not displaying correctly if I enable ComfyUI's latest user interface,, \"Nodes 2.0\" . I'm seeing a multi-line text box with different parameter value pairs.  The drum pad node displays correctly in the \"legacy\" ui.",
          "score": 4,
          "created_utc": "2026-01-20 13:36:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ofi8i",
              "author": "skbphy",
              "text": "I‚Äôm not personally using or targeting Nodes 2.0 yet. I want to see where ComfyUI‚Äôs UI direction actually settles before adapting custom nodes to it. Nodes 2.0 is still changing, so UI behavior isn‚Äôt stable.",
              "score": 3,
              "created_utc": "2026-01-20 15:13:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0oj7w3",
                  "author": "aastle",
                  "text": "I agree. Nodes 2.0 introduced more bugs than it did quality of life features. This is especially true for ComfyUI's cloud based service.",
                  "score": 2,
                  "created_utc": "2026-01-20 15:31:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0o2idu",
          "author": "deadsoulinside",
          "text": "This looks nice. Now I really have to work on an older plan that was dicing up Stems from Suno into single note hits for pads lol.",
          "score": 2,
          "created_utc": "2026-01-20 14:06:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pkek4",
          "author": "SuicidalFatty",
          "text": "![gif](giphy|5xaOcLGvzHxDKjufnLW)",
          "score": 2,
          "created_utc": "2026-01-20 18:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qlx65",
          "author": "superstarbootlegs",
          "text": "love it!",
          "score": 2,
          "created_utc": "2026-01-20 21:15:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m3rqa",
          "author": "ReasonablePossum_",
          "text": "people at r/MiXXX will appretiate something like this way more lel.",
          "score": 4,
          "created_utc": "2026-01-20 04:50:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0l7vvh",
          "author": "Practical-Nerve-2262",
          "text": " It looks like a game, kind of cute, but your video is way too blurry!",
          "score": 1,
          "created_utc": "2026-01-20 01:48:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0lbzo2",
              "author": "skbphy",
              "text": " There's a video in the link too. Does it look blurry there?",
              "score": 1,
              "created_utc": "2026-01-20 02:10:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lcurc",
                  "author": "Practical-Nerve-2262",
                  "text": "Much better, thank you",
                  "score": 2,
                  "created_utc": "2026-01-20 02:15:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ppc1i",
          "author": "ycFreddy",
          "text": "And are you playing with your mouse?",
          "score": 1,
          "created_utc": "2026-01-20 18:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uxfvs",
              "author": "skbphy",
              "text": "Mouse or keyboard. We can set the order in which the pads are played using the sequencer.",
              "score": 1,
              "created_utc": "2026-01-21 14:22:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjzdr8",
      "title": "I tested Microsoft Trellis 2 for real VFX work ‚Äî honest thoughts from a 15-year 3D artist",
      "subreddit": "comfyui",
      "url": "https://www.reddit.com/r/comfyui/comments/1qjzdr8/i_tested_microsoft_trellis_2_for_real_vfx_work/",
      "author": "ArcticLatent",
      "created_utc": "2026-01-22 16:38:04",
      "score": 76,
      "num_comments": 26,
      "upvote_ratio": 0.96,
      "text": "I just released a new YouTube video where I take a **realistic, production-focused look** at **Trellis 2**, a 3D generation model from **Microsoft** that creates textured 3D assets from images.\n\nFrom a technology standpoint, this is genuinely impressive‚Äîespecially for an open-source model. Generating geometry and textures together, quickly, is a big step forward for AI-driven 3D workflows.\n\nThat said, after **15 years working as a 3D modeling and texturing artist in the VFX industry**, it‚Äôs also easy to see the current limitations.  \nTopology quality, clean shapes, and UVs are still not at a level where these assets can be dropped directly into production without additional work.\n\nIn the video, I cover:  \n‚Ä¢ Where Trellis 2 already provides real value (prototyping, base meshes, background assets, 3D printing)  \n‚Ä¢ Why fundamental 3D principles still matter  \n‚Ä¢ How I see AI 3D models evolving for real production use\n\nI‚Äôm also currently **building a bridge plugin for Autodesk Maya and Blender** to make it easier to move AI-generated 3D assets from ComfyUI into real DCC workflows‚Äîfocusing on practical, artist-friendly integration rather than hype.\n\nI‚Äôm **not trying to hype or dismiss AI**‚Äîjust share an **honest, experience-driven perspective** for artists and studios navigating this space.\n\n‚ñ∂Ô∏è Watch the full video here: [https://www.youtube.com/watch?v=r8KEuOEudlI](https://www.youtube.com/watch?v=r8KEuOEudlI)  \nüîß Workflows used in the video: [https://github.com/ArcticLatent/ComfyUI-TRELLIS2/tree/main/workflows](https://github.com/ArcticLatent/ComfyUI-TRELLIS2/tree/main/workflows)  \nüé¨ My professional VFX work (if you are interested): [https://vimeo.com/1044521891](https://vimeo.com/1044521891)",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/comfyui/comments/1qjzdr8/i_tested_microsoft_trellis_2_for_real_vfx_work/",
      "domain": "self.comfyui",
      "is_self": true,
      "comments": [
        {
          "id": "o12oqx2",
          "author": "an80sPWNstar",
          "text": "I appreciate this. I don't plan on taking the time to learn to become an experienced 3d artist like yourself, it's just not an option for me. I do however see the value in tools that can help bridge that gap because I love art, computers and learning. I will check out your video.",
          "score": 9,
          "created_utc": "2026-01-22 16:46:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12p22o",
              "author": "ArcticLatent",
              "text": "Thank you for your thoughts!",
              "score": 2,
              "created_utc": "2026-01-22 16:47:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1324om",
          "author": "FugueSegue",
          "text": "I have heard of this but haven't investigated it yet. Thank you for your work. I worked in 3D many years ago and I sometimes return to it when I need a quick reference for illustration or painting. I skimmed your video and I can clearly see how this is a huge boon to 3D animation. In some ways, I'm glad that I left that career.\n\nWhen I was a student in the 1990s, 3D was new. We constantly worked very hard trying to do the simplest things like flowing water or animating a figure. Countless hours were spent trying to achieve an effect. And then the following year a new app or update was released and what used to take an extreme amount of time would now take no time at all. The innovations were both welcome and frustrating. I think back on the months of time that are lost forever. Time I could have spent doing ANYTHING else.\n\nI realized that trying to do cutting edge animation was pointless unless I worked with a massive crew of people. I never liked the idea of working for a company and work for peanuts making animated toothbrushes. I had my own ideas. And I could see the trajectory: what once took a crew of a dozen could eventually be done by one person in the same amount of time. I became disillusioned. I no longer wanted to spend all my time trying to make animations when production would be so much easier in the near future. So I quit and returned to traditional fine art, sketching, and illustration.\n\nAs time went by, the 3d production pipeline didn't get much simpler. Everything else except for modelling got simpler and better. I still had to spend massive amounts of time designing and building models. It looks like generative AI is proving to be useful for streamlining this aspect of animation production.\n\nWhen I first started experimenting with gen AI in 2022, I instantly saw its value and I knew I would use it as a medium for the rest of my life. It is extremely obvious to me that it is a massive upgrade to digital art and animation production. What you've demonstrated is the latest example.",
          "score": 6,
          "created_utc": "2026-01-22 17:47:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13aoas",
              "author": "ArcticLatent",
              "text": "Thank you very much for your thoughts! There is a funny section in the video that I am saying 'It is 2026 and we are still doing uv unwrapping! why??!' :D",
              "score": 2,
              "created_utc": "2026-01-22 18:24:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o16vkjp",
          "author": "turbosmooth",
          "text": "take a look at ultrashape as well, really impressive! Even on low VRAM gpus.\n\nhttps://github.com/jtydhr88/ComfyUI-UltraShape1\n\nHunyuan3D-2.2 has a PBR texture generator as well, its just a pain to setup but its good to have alternatives. \n  \nAs a technical artist myself, I have to admit these models do help quite a bit, and because I know how to retopo and clean the asset, its been a breeze to prototype iteratively and quickly. \n  \nWith all that being said, what are future 3d artists actually going to learn moving forward? Will this just be a pay to create or subscription model in a few years time? \n  \nLook at how poorly adobe have implemented genAI content fill, should we expect the same for substance? I have no idea, but maybe I'm just turning into a jaded creative telling kids to get off my virtual lawn!",
          "score": 2,
          "created_utc": "2026-01-23 05:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cymyu",
              "author": "Erik_malkavia",
              "text": "If I were you turbosmooth I would look at how you can profit from this technology and perhaps that is a way to stay positive about the changes?\n\nI am a musician and I see what is being done in that space is a bit discouraging for me as well.",
              "score": 1,
              "created_utc": "2026-01-24 02:52:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14sk57",
          "author": "Neex",
          "text": "Sick ChatGPT post bro",
          "score": 2,
          "created_utc": "2026-01-22 22:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15dyvr",
              "author": "AnOnlineHandle",
              "text": "Yeah I'm really baffled and kind of sad that people can't immediately spot these posts which always have the same length, same weird formatting with excessive text bolding etc, always using emoticons as dot points (who the hell is going to really spend their time hunting down each of these weird emoticons?), the same little point lists, and always citation links at the end.",
              "score": 1,
              "created_utc": "2026-01-23 00:31:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o136xyx",
          "author": "RobotToaster44",
          "text": "How does it compare to hunyuan?",
          "score": 1,
          "created_utc": "2026-01-22 18:08:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13gn99",
              "author": "ArcticLatent",
              "text": "I only tested the open source hunyuan (2.0) and I like the results of trellis 2 better.",
              "score": 1,
              "created_utc": "2026-01-22 18:51:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13q2tz",
          "author": "PolyBend",
          "text": "AI for 3D still has a long way to go. I do think it will get there. There are just so many hills it has to climb still.\n\nThe fact we can still barely get 2d generation to do exactly what we want... And to even get close is a LOT of work, proves this.\n\nLike you said, topology, UVs, and multiple map types are still so far away.\n\nIt doesn't help either that artists in the industry are so underpaid and so oversaturated. So the cost to benefit ratio isn't there yet.\n\nStill, what a lot of these companies need to be focusing on first is tools, not all in 1 solutions.\n\nFirst make an AI tool that UVs for me\n\nThen make one that retopos for me.\n\nNow combine those....",
          "score": 1,
          "created_utc": "2026-01-22 19:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15fnht",
          "author": "phunkaeg",
          "text": "Also a VFX artist - I've just completed a project partially using InfiniteTalk to fix some lipsync issues in a short film where the creature prosthetic the actor was using was interfering with how they were speaking. \n\nI've used deepfaking process before to do a similar effect but that involved a lot more work. And before deepfaking you'd need to either manipulate each frame by hand, or recreate the mouth as a 3d model and animate the changes that way. \n\nAi is definitely another very power tool in a VFX artists toolbox.",
          "score": 1,
          "created_utc": "2026-01-23 00:40:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16d052",
          "author": "pixel8tryx",
          "text": "I wanted to install the last 3D generator I heard about but it needed a different attention pkg and I'd just gotten Triton/Sage working.  What does this use?\n\nFor me, if it's simple, it's better for me to make it myself in Cinema 4D.  If it's a complex object, it might be interesting to try.   What I've tried on HuggingFace for past models worked surprisingly well considering the complexity of my test vehicles and odd viewing angle.  I was surprised it managed to approximate the basic shape.  But the detail was lacking, the mesh was a mess and awful to try to texture map.  I don't work with a lot of UV maps usually and find C4D's UV support ok when everything works but hair-pulling when it doesn't.   \n\nIt looks like this generates photogrammetry-style maps?  Packed, Atlas, whatever they're called?  No other option?  If it would actually make a decent model from some complex novel style space ship or something, it might be worth the effort to retexture.  But I have no interest in making Yet Another Generic vehicle, or device or whatever. That's why I'm using AI image generation tools in the first place.  If one needs to crank out a dozen low poly plant zombies or something maybe it's useful.",
          "score": 1,
          "created_utc": "2026-01-23 03:47:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16ghu8",
          "author": "1Neokortex1",
          "text": "![gif](giphy|hVYVYZZBgF50k)\n\nThat would be phenomenal to have a connection from comfyui to blenderü´°",
          "score": 1,
          "created_utc": "2026-01-23 04:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o177cku",
          "author": "fisj",
          "text": "Nice breakdown from a professional point of view. I subbed your channel, and also crossposted this to r/aigamedev",
          "score": 1,
          "created_utc": "2026-01-23 07:26:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o196vgf",
          "author": "Lil_Twist",
          "text": "This is ALWAYS what we need, no hype, trusted source, honest evaluations. Can't fix or progress if there isn't a truth of source, multiple for that matter, and you are a strong valid data point we should lean on for reference. Many thanks!",
          "score": 1,
          "created_utc": "2026-01-23 15:42:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1332ax",
          "author": "optimisticalish",
          "text": "Nice, but it's my understanding that Trellis 2 needs a $25k NVIDIA H100. If a studio has that kind of cash to spare, just to get videogame assets... \"not at a level where these assets can be dropped directly into production without additional work\", then they really should be hiring a proper asset maker.\n\nAre there any ComfyUI solutions for impressive 3D model generation with textures, which can be run on standard gaming graphics cards? I'm not talking about static single-mesh garden gnomes. The ideal for animators would be to prompt for multi-mesh and ready- rigged 3D figures that you can drop into something like the $29 Bondware Poser 12.",
          "score": 0,
          "created_utc": "2026-01-22 17:51:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1376ur",
              "author": "jib_reddit",
              "text": "Or you can rent one for $2-$3 an hour when you need it...",
              "score": 5,
              "created_utc": "2026-01-22 18:09:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o13b7x6",
                  "author": "optimisticalish",
                  "text": "Ah I see. I didn't realise that.",
                  "score": 1,
                  "created_utc": "2026-01-22 18:27:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o137rz6",
              "author": "ArsInvictus",
              "text": "There are examples of Trellis 2 running on a 5060 out there, though it sounds tight, but I'm guessing a 5090 will run it with room to spare.",
              "score": 3,
              "created_utc": "2026-01-22 18:12:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o13csj7",
              "author": "Sudden-Variation-660",
              "text": "This is not true at all, i‚Äôm running it on a 3090.",
              "score": 2,
              "created_utc": "2026-01-22 18:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o16wkjl",
                  "author": "turbosmooth",
                  "text": "agreed, running it on a 3080ti mobile (16gig VRAM), I would guess 12gig VRAM is bare minimum from my testing.",
                  "score": 1,
                  "created_utc": "2026-01-23 05:57:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o13p9aj",
              "author": "Smile_Clown",
              "text": ">Nice, but it's my understanding that Trellis 2 needs a $25k NVIDIA H100\n\nYour understanding is wrong.  I run it with a 4090 just fine and I know many people can run it with much, much less.\n\nHopefully you look into things a little more before you dismiss things.\n\nwhich makes your later analysis silly:\n\n>If a studio has that kind of cash to spare, just to get videogame assets...\n\nDon't do this... don't make assumptions based on something else, especially when you're not up to speed with the subject. projecting is just silly.\n\n>Are there any ComfyUI solutions for impressive 3D model generation with textures, which can be run on standard gaming graphics cards?\n\nYes... ffs... trellis 2 runs in comfyui there are several vids on YT explain the process easy peasy. There are also nodes for cleanup, patching and making it virtually perfect (from an amateur stance anyway)\n\n\nYour entire comment hinges on something you were absolutely wrong about. Crazy huh?\n\n>The ideal for animators would be to prompt for multi-mesh and ready- rigged 3D figures that you can drop into something like the $29 Bondware Poser 12.\n\nThat's the kicker. That's YOU and YOUR ideal, not \"ideal for animators\" as animators all have different processes.  and yes, you can do some of this with trellis and some basic understanding and learning about the process.\n\nYou're not an animator, you want to drop in rigged setups from a prompt. That's lazy and not creative. There is NOTHING wrong with it, I would love to see and enjoy your work, but when  you slap the \"I am an animator\" label on it, it's a bit icky. (but lol, I saw someone do with with trellis and mixamo)",
              "score": 1,
              "created_utc": "2026-01-22 19:29:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o13rfhu",
                  "author": "optimisticalish",
                  "text": "I was just quoting the official Microsoft page on the matter. It recommends a H100. Nothing else",
                  "score": 2,
                  "created_utc": "2026-01-22 19:39:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o144cvl",
                  "author": "Botoni",
                  "text": "Will it run on 8gb 3070 with offloading?",
                  "score": 1,
                  "created_utc": "2026-01-22 20:39:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}