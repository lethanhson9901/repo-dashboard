{
  "metadata": {
    "last_updated": "2026-02-02 16:57:30",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 11,
    "total_comments": 57,
    "file_size_bytes": 54854
  },
  "items": [
    {
      "id": "1qo5y54",
      "title": "DeepSeek releases DeepSeek-OCR 2. üêã",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/rjkucnsh3ufg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-27 06:15:19",
      "score": 346,
      "num_comments": 31,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qo5y54/deepseek_releases_deepseekocr_2/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1zg0su",
          "author": "GosuGian",
          "text": "Downloading now..",
          "score": 13,
          "created_utc": "2026-01-27 08:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20copo",
              "author": "danielhanchen",
              "text": "Let us know how it does!",
              "score": 2,
              "created_utc": "2026-01-27 13:10:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z7ocu",
          "author": "larrytheevilbunnie",
          "text": "Outperforming Gemma 3 pro is crazy",
          "score": 6,
          "created_utc": "2026-01-27 07:31:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z8wv5",
              "author": "Inflation_Artistic",
              "text": "Gemma 3 pro?",
              "score": 4,
              "created_utc": "2026-01-27 07:42:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z8ywk",
                  "author": "larrytheevilbunnie",
                  "text": "Gemini* oops\n\nI want Gemma 4 tho",
                  "score": 8,
                  "created_utc": "2026-01-27 07:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20cs28",
              "author": "danielhanchen",
              "text": "Ye it's pretty good for the reduction in edit distance!\n\nI'm sure Gemini 3 Pro does better on other image benchmarks though!",
              "score": 1,
              "created_utc": "2026-01-27 13:10:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zixpy",
          "author": "PaceZealousideal6091",
          "text": "Lcpp support?",
          "score": 3,
          "created_utc": "2026-01-27 09:15:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zjrin",
              "author": "yoracale",
              "text": "Not yet unfortunately. there was a PR back in nov 2025 but it never got merged",
              "score": 5,
              "created_utc": "2026-01-27 09:22:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o20f6ku",
          "author": "Cautious-Raccoon-364",
          "text": "This is completely insane, wow. Gonna try it now.",
          "score": 3,
          "created_utc": "2026-01-27 13:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22m80d",
              "author": "Cautious-Raccoon-364",
              "text": "Ok, tried it, worked well. I just hate the pdf to image conversion needed? Why not natively support pdf? I‚Äôm sure u could fine tune it further for your datasets, but still finding Claude a little better for bank document due diligence.",
              "score": 1,
              "created_utc": "2026-01-27 19:22:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zqgsg",
          "author": "Intelligent-Form6624",
          "text": "ROCm or vulkan?",
          "score": 2,
          "created_utc": "2026-01-27 10:24:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o207bbc",
              "author": "yoracale",
              "text": "Should work",
              "score": 2,
              "created_utc": "2026-01-27 12:36:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21lc80",
          "author": "arman-d0e",
          "text": "This feels like fate. I just started using deepseek ocr yesterday üò≠",
          "score": 2,
          "created_utc": "2026-01-27 16:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eop15",
          "author": "Late_Special_6705",
          "text": "How to launch this model? I don't want to write a python code. Maybe he works in ollama or vlm?",
          "score": 2,
          "created_utc": "2026-01-29 13:33:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21si3w",
          "author": "vertigo235",
          "text": "Does it handle checkboxes on forms?",
          "score": 1,
          "created_utc": "2026-01-27 17:15:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27zxqm",
              "author": "yoracale",
              "text": "Yes it should be able to, due to its new reading ability",
              "score": 1,
              "created_utc": "2026-01-28 14:44:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2aeyaa",
                  "author": "vertigo235",
                  "text": "Yeah seems promising",
                  "score": 1,
                  "created_utc": "2026-01-28 21:07:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21z6dl",
          "author": "AntiqueAndroid0",
          "text": "How is this compared to mistral ocr3?",
          "score": 1,
          "created_utc": "2026-01-27 17:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2800gi",
              "author": "yoracale",
              "text": "Unsure but it 'should' be better.",
              "score": 1,
              "created_utc": "2026-01-28 14:45:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o221xb6",
          "author": "samplebitch",
          "text": "Anyone know if this model knows how to generate bounding boxes?",
          "score": 1,
          "created_utc": "2026-01-27 17:55:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28586w",
              "author": "yoracale",
              "text": "I'm not sure if it'll generate bounding boxes, but it will detect them yes",
              "score": 1,
              "created_utc": "2026-01-28 15:10:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o224v6m",
          "author": "eagledoto",
          "text": "Is it a vlm? Can we use it instead of qwen 3 vl in comfy to generate prompts from images?",
          "score": 1,
          "created_utc": "2026-01-27 18:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2851bx",
              "author": "yoracale",
              "text": "Yes it is a vision model but cannot replace Qwen 3 vl",
              "score": 1,
              "created_utc": "2026-01-28 15:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o285cgk",
                  "author": "eagledoto",
                  "text": "But can we use it in ComfyUI?",
                  "score": 1,
                  "created_utc": "2026-01-28 15:10:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2a7tx8",
          "author": "Outrageous-Phase-786",
          "text": "I wonder if it is also well performing with handwriting... any result on that?",
          "score": 1,
          "created_utc": "2026-01-28 20:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c2ypz",
          "author": "6969its_a_great_time",
          "text": "Vllm example doesn‚Äôt work",
          "score": 1,
          "created_utc": "2026-01-29 02:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ccvan",
              "author": "yoracale",
              "text": "Oh whoops it doesn't work because the PR is still open. You'll need to use transformers v5 or the Unsloth guide",
              "score": 1,
              "created_utc": "2026-01-29 03:00:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dui2c",
          "author": "someone383726",
          "text": "Anyone get this working in vLLM?  I tried nightly and a PR version that had a fix and all I got were garbage outputs.",
          "score": 1,
          "created_utc": "2026-01-29 09:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zo5rt",
          "author": "loutishgamer",
          "text": "Does it have new coding and new texting generation coding and knowledge? Like when you're giving it a prompt does it give more new knowledge or coding or truth?",
          "score": 0,
          "created_utc": "2026-01-27 10:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o205cb0",
              "author": "Smilysis",
              "text": "That's a ocr model, not llm",
              "score": 2,
              "created_utc": "2026-01-27 12:23:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpbmrt",
      "title": "You can now run Kimi K2.5 locally!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/nwp8ammpf3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 13:40:33",
      "score": 249,
      "num_comments": 22,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qpbmrt/you_can_now_run_kimi_k25_locally/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o281v90",
          "author": "m98789",
          "text": "On a single H100 (80GB) VM with 256 GB of ram and 1TB of ssd and plenty of cpu cores, how fast in tokens / sec can we expect?",
          "score": 11,
          "created_utc": "2026-01-28 14:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284n8h",
              "author": "hudimudi",
              "text": "That would be very interesting to know",
              "score": 5,
              "created_utc": "2026-01-28 15:07:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o284ovk",
              "author": "yoracale",
              "text": "You can run the 2bit one possible with that. Maybe like 20 tokens?",
              "score": 6,
              "created_utc": "2026-01-28 15:07:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o28c5jm",
                  "author": "m98789",
                  "text": "So if this set up works, it‚Äôs about $3/hour on demand as an instance in a major cloud. So say we get Claude Opus level for $3/hour. And at 20 tokens a second, that‚Äôs 72,000 output tokens per hour. using the api for opus for that amount of tokens costs about $2. If using a reserved instance of the cloud vm, the cost goes to about $2/hour. So it‚Äôs a wash on cost effectiveness. But on privacy and control, it‚Äôs a win. Assuming quality is effectively the same.",
                  "score": 9,
                  "created_utc": "2026-01-28 15:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28ew1j",
          "author": "illkeepthatinmind",
          "text": "I mean.... \\*I\\* can't...",
          "score": 4,
          "created_utc": "2026-01-28 15:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2j647h",
          "author": "BeginningReveal2620",
          "text": "Cool",
          "score": 3,
          "created_utc": "2026-01-30 02:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28h6dn",
          "author": "joninco",
          "text": "Did you apply any unsloth fixes? I ran the K2.5 from moonshot's hf page and was unimpressed with the coding results for creating a single page html tetris app. I used 1.0 temp, 0.95 topp.",
          "score": 2,
          "created_utc": "2026-01-28 16:03:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d0d8h",
              "author": "yoracale",
              "text": "There's no unsloth fixes for this model :(. Have you tried their API?",
              "score": 2,
              "created_utc": "2026-01-29 05:30:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2evu8w",
                  "author": "joninco",
                  "text": "Only locally with sglang. I haven't tried their API yet, but I will give it a shot and compare with my results to see if they are consistent!\n\nFor SGLANG on RTX PRO 6000s, disabling DEEPGEMM and ensuring temp 1.0, top p 0.95 and min p 0.01 made a material difference. \n\nMy only problem now is that it's materially slowing down as context grows -- assuming triton attn isn't optimal for this model.",
                  "score": 3,
                  "created_utc": "2026-01-29 14:11:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28lhoq",
          "author": "nomorebuttsplz",
          "text": "Hi -- why is the K.2.5 Q 3 XL UD quite a bit larger than the same quant for K2 thinking?",
          "score": 2,
          "created_utc": "2026-01-28 16:22:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28q2mv",
          "author": "Nooddlleee",
          "text": "Does anyone test the code quality? I am curious how it performs on the complex tasks and big projects",
          "score": 2,
          "created_utc": "2026-01-28 16:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28sxci",
          "author": "davidl002",
          "text": "Cannot imagine running it locally anytime soon but thanks for the work!\n\nHope in 5 years hardware can be affordable enough to run this locally cheaply.",
          "score": 2,
          "created_utc": "2026-01-28 16:54:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xywgu",
              "author": "Certain_Cricket7958",
              "text": "I‚Äôm afraid in five years, it won‚Äôt be as interesting",
              "score": 1,
              "created_utc": "2026-02-01 09:17:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o29lpma",
          "author": "emperorofrome13",
          "text": "That is the flux 2 dev of coding ai. Way too big to be useful.",
          "score": 2,
          "created_utc": "2026-01-28 18:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28p0fc",
          "author": "maxtheman",
          "text": "Would be VERY interested in the vision support, but already awesome work.",
          "score": 1,
          "created_utc": "2026-01-28 16:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r6ij8",
          "author": "Significant-Taro409",
          "text": "Is there any info on fine-tuning available",
          "score": 1,
          "created_utc": "2026-01-31 07:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2skoz9",
              "author": "yoracale",
              "text": "The model is gigantic. You'll need at least 8x B200's to finetune it",
              "score": 2,
              "created_utc": "2026-01-31 14:27:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xgec7",
          "author": "No-Intention-5521",
          "text": "Huhh i only have one A100 .... seems i have to buy apissss",
          "score": 1,
          "created_utc": "2026-02-01 06:29:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xtgny",
              "author": "yoracale",
              "text": "How much RAM do you have? If it adds to 240gb then itll work you dont need api",
              "score": 1,
              "created_utc": "2026-02-01 08:26:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o32gr45",
          "author": "texasdude11",
          "text": "I could not get it to do function calling. Any sample command for that?",
          "score": 1,
          "created_utc": "2026-02-02 00:18:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqc06x",
      "title": "How to Run Local LLMs with Claude Code & OpenAI Codex!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/6mhzmpzd6bgg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-29 15:42:38",
      "score": 118,
      "num_comments": 27,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qqc06x/how_to_run_local_llms_with_claude_code_openai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2fnep4",
          "author": "__Maximum__",
          "text": "Fine-tune?",
          "score": 3,
          "created_utc": "2026-01-29 16:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2frhwe",
              "author": "yoracale",
              "text": "Yep fine-tune! We use glm flash to autonomously fine-tune an LLM with unsloth",
              "score": 3,
              "created_utc": "2026-01-29 16:38:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2fr3lz",
              "author": "moonflowerseed",
              "text": "On Mac/Apple Silicon?",
              "score": 1,
              "created_utc": "2026-01-29 16:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2fre76",
                  "author": "yoracale",
                  "text": "We're working on Mac support for real. Optimizations are done, only thing next is checking, benchmarking and Integra tion",
                  "score": 9,
                  "created_utc": "2026-01-29 16:38:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2j0zdt",
          "author": "PixelatedCaffeine",
          "text": "Is there a way to change the Claude Code limit to match the model‚Äôs limit? It always seems to default to 200k, and I would love to use the auto compact feature based on that",
          "score": 2,
          "created_utc": "2026-01-30 02:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g6qtr",
          "author": "ethereal_intellect",
          "text": "They lose web search capability when linked to local models right?",
          "score": 1,
          "created_utc": "2026-01-29 17:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kd5p2",
              "author": "admajic",
              "text": "Not is you ask it to build you a mcp search tool.",
              "score": 1,
              "created_utc": "2026-01-30 07:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gahui",
          "author": "Glittering-Call8746",
          "text": "Prompt \"You can only work in the cwd project/. Do not search for CLAUDE.md - this is it. Install Unsloth via a virtual environment via uv. See https://unsloth.ai/docs/get-started/install/pip-install on how (get it and read). Then do a simple Unsloth finetuning run described in https://github.com/unslothai/unsloth. You have access to 1 GPU.\" What's the model it's finetuning..",
          "score": 1,
          "created_utc": "2026-01-29 18:04:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrpqc",
              "author": "yoracale",
              "text": "Llama most likely",
              "score": 1,
              "created_utc": "2026-01-29 22:14:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gu6cv",
          "author": "creminology",
          "text": "Has Anthropic ever given any indication that they view this as a breach of terms of service? Asking because they have come down on hard on using Claude Code subscriptions in other environments, although this is doing the reverse.",
          "score": 1,
          "created_utc": "2026-01-29 19:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrmg0",
              "author": "yoracale",
              "text": "Oh no, they allow this because Claude Code was meant to be used locally!",
              "score": 3,
              "created_utc": "2026-01-29 22:13:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2luumi",
              "author": "Otherwise-Way1316",
              "text": "They don‚Äôt like their models being used in other platforms, like OpenCode.\n\nAll indications are that they are ok with Claude Code being used with other models.",
              "score": 2,
              "created_utc": "2026-01-30 14:20:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kpq0d",
          "author": "No-Weird-7389",
          "text": "Still waiting for nvfp4",
          "score": 1,
          "created_utc": "2026-01-30 09:34:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l4ryc",
              "author": "yoracale",
              "text": "We're working on it! :) Might not be for this model but for future ones",
              "score": 1,
              "created_utc": "2026-01-30 11:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l3ars",
          "author": "SatoshiNotMe",
          "text": "Last I checked, running glm-4.7-flash with CC on my M1 Pro Max 64GB MacBook via llama-server got me an abysmal 3 tok/s, for less than the 20 tok/s I got with Qwen3-30B-A3B. I use this setup to hook up CC with local models:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nCurious what llama-server settings you recommend to get good performance with GLM-4.7-flash",
          "score": 1,
          "created_utc": "2026-01-30 11:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2x6voc",
              "author": "yoracale",
              "text": "When was the last time you tried it? A week ago llamacpp was updated to imrpove speed a lot for it",
              "score": 1,
              "created_utc": "2026-02-01 05:15:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oegdi",
          "author": "stuckinmotion",
          "text": "Does this work for anyone? I followed the steps, set ANTHROPIC\\_BASE\\_URL to my llama-server instance, but I'm getting \"Missing API key\"\n\nedit: Ok so exporting ANTHROPIC\\_API\\_KEY=sk-1234 got it working. Maybe the guide can be updated",
          "score": 1,
          "created_utc": "2026-01-30 21:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pq0sy",
              "author": "yoracale",
              "text": "Ooo ok interesting we'll update it in our guide then thanks for the feedback",
              "score": 1,
              "created_utc": "2026-01-31 01:30:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2x752j",
              "author": "yoracale",
              "text": "We just added it to our guide: [https://unsloth.ai/docs/basics/claude-codex](https://unsloth.ai/docs/basics/claude-codex)\n\nThanks so much for your feedback!",
              "score": 1,
              "created_utc": "2026-02-01 05:17:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xb6bx",
                  "author": "stuckinmotion",
                  "text": "Hey nice! Thanks for the work. It's been interesting playing with Claude code locally though it makes it obvious how much worse it is without their models",
                  "score": 1,
                  "created_utc": "2026-02-01 05:47:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2vuwel",
          "author": "JonatasLaw",
          "text": "Can I run it in a rtx 3090 + 64gb RAM?",
          "score": 1,
          "created_utc": "2026-02-01 00:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2x6zfd",
              "author": "yoracale",
              "text": "Yes ofc, it'll be fast for you. You can even run the 8-bit one",
              "score": 1,
              "created_utc": "2026-02-01 05:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fnjti",
          "author": "toreobsidian",
          "text": "This is awesome. I'll Test this with Just a dataset I'm currently preparing that Features content of a famous german political figure. Too bad I have so little time for this nonsens Project but this should be a nice boost üòÖ",
          "score": 1,
          "created_utc": "2026-01-29 16:21:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qohf9o",
      "title": "Kimi-K2.5 Prelim Dynamic 2bit 4bit GGUFs out!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qohf9o/kimik25_prelim_dynamic_2bit_4bit_ggufs_out/",
      "author": "danielhanchen",
      "created_utc": "2026-01-27 15:46:41",
      "score": 45,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hey everyone! we made some dynamic imatrix 1bit to 4bit ~~preliminary GGUFs~~ (now final release) for Kimi-K2.5! Currently they're text only (no vision yet) and the Dynamic 2bit, 4bit and normal 8bit quants are out at [https://huggingface.co/unsloth/Kimi-K2.5-GGUF](https://huggingface.co/unsloth/Kimi-K2.5-GGUF)\n\nHow to run dynamic 1bit:\n\n    LLAMA_SET_ROWS=1 ./llama.cpp/llama-cli \\\n        --model unsloth/Kimi-K2.5-GGUF/UD-TQ1_0/Kimi-K2.5-UD-TQ1_0-00001-of-00005.gguf \\\n        --temp 1.0 \\\n        --min_p 0.01 \\\n        --top-p 0.95 \\\n        --ctx-size 16384 \\\n        --seed 3407 \\\n        --fit on \\\n        --jinja\n\nGuide to run quants at [https://unsloth.ai/docs/models/kimi-k2.5](https://unsloth.ai/docs/models/kimi-k2.5)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qohf9o/kimik25_prelim_dynamic_2bit_4bit_ggufs_out/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o21lnfq",
          "author": "m98789",
          "text": "Is this possible to run on a single H100 server with plenty of RAM, CPU and disk?",
          "score": 2,
          "created_utc": "2026-01-27 16:45:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22dfeg",
              "author": "sjoerdmaessen",
              "text": "Yes, its literally in the link ‚ÄúYou need 247GB of disk space to run the 1bit quant!\n\nThe only requirement is disk space + RAM + VRAM ‚â• 247GB. That means you do not need to have that much RAM or VRAM (GPU) to run the model, but it will be much slower.‚Äù",
              "score": 4,
              "created_utc": "2026-01-27 18:44:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24gd29",
                  "author": "danielhanchen",
                  "text": "Yes the only requirement is (RAM + VRAM >= disk_space(GGUF)) then using `--fit on` in llama.cpp will optimally allocate space",
                  "score": 2,
                  "created_utc": "2026-01-28 00:33:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24ugxz",
          "author": "danielhanchen",
          "text": "Update: The imatrix dynamic quants are out! 1bit, 2bit are out",
          "score": 2,
          "created_utc": "2026-01-28 01:46:57",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2g1xp3",
          "author": "TheRiddler79",
          "text": "https://preview.redd.it/rp5ihtc4pbgg1.jpeg?width=1440&format=pjpg&auto=webp&s=06ce423d636bac406e0400bbc05bf843f3e8b750\n\nI'm not breaking records in speed, but it's clear that the intelligence is not degraded by compression",
          "score": 1,
          "created_utc": "2026-01-29 17:25:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt7q4c",
      "title": "I bullied my dual 3060s into ruinning GLM-4.7-Flash  500+ T/s @ 70k Context on a Ryzen 2500 Potato. (Two Configs: \"Daily Driver\" vs. \"The Diesel Factory\")",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/gallery/1qt7q4c",
      "author": "MohammedGomaa",
      "created_utc": "2026-02-01 19:15:29",
      "score": 32,
      "num_comments": 36,
      "upvote_ratio": 0.73,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qt7q4c/i_bullied_my_dual_3060s_into_ruinning_glm47flash/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o310uy4",
          "author": "mukz_mckz",
          "text": "Please stop writing posts with AI.",
          "score": 20,
          "created_utc": "2026-02-01 19:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o312ok4",
              "author": "MohammedGomaa",
              "text": "i am not a native english speaker , i used ai to polish it",
              "score": -1,
              "created_utc": "2026-02-01 20:03:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31gwup",
                  "author": "--Spaci--",
                  "text": "Id rather struggle to read your paragraph, than have to read an AI's attempt at conveying information",
                  "score": 14,
                  "created_utc": "2026-02-01 21:13:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o34s89y",
              "author": "marko_mavecki",
              "text": "Huh? Maybe he can't? I am here for information. Not for formatting or the beauty of words. Besides, in few months you will no longer be able to differentiate between human and machine. Will it then be acceptable for you?",
              "score": 0,
              "created_utc": "2026-02-02 10:19:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3277zm",
          "author": "i_wayyy_over_think",
          "text": "Thanks for the detailed run configs. What do you use to actually use 64 concurrent quest at once besides running a benchmark? Can moltbot use that many concurrent requests effectively to get actual useful work done? I‚Äôve not tried it yet.",
          "score": 3,
          "created_utc": "2026-02-01 23:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o329d4z",
              "author": "MohammedGomaa",
              "text": "A swarm of AI agents plus multiple instance open code and the other coding agents",
              "score": 1,
              "created_utc": "2026-02-01 23:38:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31gezn",
          "author": "alhinai_03",
          "text": "no one will bother reading AI produced text.",
          "score": 2,
          "created_utc": "2026-02-01 21:10:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32d7hy",
              "author": "MohammedGomaa",
              "text": "You know you are on an AI focused subreddit almost 99% of AI mentioned here argued towards Text generation",
              "score": 2,
              "created_utc": "2026-02-01 23:59:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32gt6c",
                  "author": "alhinai_03",
                  "text": "Brother, this doesn't mean I'm okay with reading AI generated text. I get it if it's not your first language but I'd rather read human written content no matter how bad the grammar is. I'm sure you have good intentions but people are generally getting tired of it.",
                  "score": 4,
                  "created_utc": "2026-02-02 00:19:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31mixg",
          "author": "AbheekG",
          "text": "How do you handle the cache going stale? Doesn‚Äôt that happen fairly frequently with a codebase?",
          "score": 1,
          "created_utc": "2026-02-01 21:40:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31nnv4",
              "author": "MohammedGomaa",
              "text": "Sglang finds previously computed tokens and reuses it , very useful for ai agents  play books , system prompts and skills, i ran scheduled script    that removes chunks not accessed in the last 7 days",
              "score": 2,
              "created_utc": "2026-02-01 21:45:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31nsth",
                  "author": "AbheekG",
                  "text": "Very cool, thanks!",
                  "score": 1,
                  "created_utc": "2026-02-01 21:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33h8sb",
          "author": "Soft_Syllabub_3772",
          "text": "Can modify to use with rtx3090 x 2 and 192gb ram and 1tb nvme?",
          "score": 1,
          "created_utc": "2026-02-02 03:47:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33hp44",
              "author": "MohammedGomaa",
              "text": "Go fo it , the sky is the limit",
              "score": 1,
              "created_utc": "2026-02-02 03:50:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o348hst",
          "author": "Previous_Nature_5319",
          "text": "good job, I think it will be useful for many people. thank you!",
          "score": 1,
          "created_utc": "2026-02-02 07:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34ef1f",
          "author": "Aggravating_Bee3757",
          "text": "what model did you use? can i use it in my single 5060 ti 16/16?",
          "score": 1,
          "created_utc": "2026-02-02 08:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34lc83",
              "author": "MohammedGomaa",
              "text": "**GLM-4.7-Flash**¬†(the¬†`QuantTrio-AWQ`¬†flavor) , i dont think so , not without c\\[u offloading tanking performance",
              "score": 1,
              "created_utc": "2026-02-02 09:12:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34u3r2",
          "author": "jannemansonh",
          "text": "impressive setup for agent swarms... curious though - for production agent workflows, ended up using needle app since you just describe what you want vs configuring cuda graphs and cache layers. kept the self-hosted setup for experimenting but way easier when agents need to understand code/docs",
          "score": 1,
          "created_utc": "2026-02-02 10:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o355g99",
              "author": "MohammedGomaa",
              "text": "sorry , i dont realy get what you are asking about , BTW setting \n\n    --cuda-graph-bs 4 16 32  # makes sure that single requist get 20-70 t/s , depending on context cach , ie single  agent get 20-70 t/s depending on caching context and concurancy , with 450 + t/s on max concurancy",
              "score": 1,
              "created_utc": "2026-02-02 12:12:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o316ecp",
          "author": "heaven00",
          "text": "Interesting stuff will have to try out",
          "score": 1,
          "created_utc": "2026-02-01 20:21:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr4i8a",
      "title": "Experimental DeepSeek-V3.2 Dynamic GGUFs",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/",
      "author": "danielhanchen",
      "created_utc": "2026-01-30 12:46:49",
      "score": 28,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "We made some experimental DeepSeek-V3.2 GGUFs for those interested! [https://huggingface.co/unsloth/DeepSeek-V3.2-GGUF](https://huggingface.co/unsloth/DeepSeek-V3.2-GGUF)\n\nThey **don't need any llama.cpp updates or special forks** \\- these should work in llama.cpp, LM Studio, Ollama (UD-TQ1\\_0).\n\nDeepSeek Sparse Attention (DSA) is disabled for now, and this mostly acts like a normal DeepSeek V3.1 model. However, we had to cook up the chat\\_template.jinja from scratch.\n\nUse [https://unsloth.ai/docs/models/tutorials/deepseek-v3.1-how-to-run-locally](https://unsloth.ai/docs/models/tutorials/deepseek-v3.1-how-to-run-locally) and replace \"DeepSeek-V3.1\" with \"DeepSeek-V3.2\"\n\nAn example Flappy Bird game in HTML with the UD-Q2\\_K\\_XL quant:\n\nhttps://preview.redd.it/a5d7sugrfhgg1.png?width=1547&format=png&auto=webp&s=26f2c96289c84fe8cace79c30a633f7a8e3b5a62\n\nLet us know how it goes!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qs8gcp",
      "title": "Should I use UnslothTrainer or SFTTrainer for Continued Pre-training (Raw Text) to create a LoRA for later merging?",
      "subreddit": "unsloth",
      "url": "https://arxiv.org/abs/2507.18294",
      "author": "choco132134",
      "created_utc": "2026-01-31 17:28:47",
      "score": 14,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qs8gcp/should_i_use_unslothtrainer_or_sfttrainer_for/",
      "domain": "arxiv.org",
      "is_self": false,
      "comments": [
        {
          "id": "o2ujqvz",
          "author": "Educational_Rent1059",
          "text": "UnslothTrainer is the same thing with additional support for these internal params which are needed for continued pre training  args = UnslothTrainingArguments(\n        ....\n        learning_rate = 5e-5,\n        embedding_learning_rate = 5e-6, # 2-10x smaller than learning_rate",
          "score": 1,
          "created_utc": "2026-01-31 20:13:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xti8b",
          "author": "yoracale",
          "text": "Use unsloth trainer if it works as it has continued pretraining benefits. If it doesn't work then use SFTTrainer",
          "score": 1,
          "created_utc": "2026-02-01 08:27:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o323ker",
          "author": "de4dee",
          "text": "I do CPT on Instruct model, using UnslothTrainer and it is looking ok..",
          "score": 1,
          "created_utc": "2026-02-01 23:06:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qocvl6",
      "title": "How to develop using Apple Sillicon?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qocvl6/how_to_develop_using_apple_sillicon/",
      "author": "growndemon",
      "created_utc": "2026-01-27 12:46:14",
      "score": 13,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hi,  \nI'm developing my codebase on my macbook and afterwards submit trainingjobs to a gpu cluster. However I can't create a virtual env with unsloth and thus don't have any ide support and also can't have a dry run with a small model to test my code.\n\nIs there any workflow / workaround that is recommended or widely used by apple users working with unsloth?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qocvl6/how_to_develop_using_apple_sillicon/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o20cj6g",
          "author": "danielhanchen",
          "text": "We're working on it as we speak! :) Unsloth on Mac will come!",
          "score": 9,
          "created_utc": "2026-01-27 13:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20f4pn",
              "author": "pokemonplayer2001",
              "text": "‚ù§Ô∏è",
              "score": 2,
              "created_utc": "2026-01-27 13:23:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20qde1",
                  "author": "A-Rahim",
                  "text": "u/pokemonplayer2001   \nMeanwhile, you may try this: [https://github.com/ARahim3/unsloth-mlx](https://github.com/ARahim3/unsloth-mlx)",
                  "score": 0,
                  "created_utc": "2026-01-27 14:22:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq0m1x",
      "title": "Guidance Needed: GPT-OSS 20B Fine-Tuning with Unsloth ‚Üí GGUF ‚Üí Ollama ‚Üí Triton (vLLM / TensorRT-LLM)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qq0m1x/guidance_needed_gptoss_20b_finetuning_with/",
      "author": "Double_Tourist3600",
      "created_utc": "2026-01-29 06:07:11",
      "score": 13,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I am currently fine-tuning the **GPT-OSS 20B** model using **Unsloth** with **HuggingFace TRL (SFTTrainer)**.\n\n**Long-term goal**\n\n* Serve the model in production using **Triton** with either **vLLM** or **TensorRT-LLM** as the backend\n* **Short-term / initial deployment** using **Ollama (GGUF)**\n\n**Current challenge**  \nGPT-OSS uses a **Harmony-style chat template**, which includes:\n\n* `developer` role\n* Explicit EOS handling\n* `thinking` / `analysis` channels\n* Tool / function calling structure\n\nWhen converting the fine-tuned model to **GGUF** and deploying it in **Ollama** using the **default GPT-OSS Modelfile**, I am running into ambiguity around:\n\n1. Whether the **default Jinja chat template** provided by GPT-OSS should be **modified** for Ollama compatibility\n2. How to correctly handle:\n   * EOS token behavior\n   * Internal reasoning / analysis channels\n   * Developer role alignment\n3. How to do this **without degrading the model‚Äôs default performance or alignment**\n\n**Constraints / Intent**\n\n* I already have training data prepared strictly in **system / user / assistant** format\n* I want to:\n   * Preserve GPT-OSS‚Äôs native behavior as much as possible\n   * Perform **accurate, non-destructive fine-tuning**\n   * Avoid hacks that work short-term but break compatibility with **vLLM / TensorRT-LLM** later\n\n**What I‚Äôm looking for**\n\n* Has anyone successfully:\n   * Fine-tuned GPT-OSS\n   * Converted it to GGUF\n   * Deployed it with **Ollama**\n   * While preserving the Harmony template behavior?\n* If yes:\n   * Did you modify the **chat template / Modelfile**?\n   * How did you handle EOS + reasoning channels?\n   * Any pitfalls to avoid to keep it production-ready for Triton later?\n\nAny concrete guidance, references, or proven setups would be extremely helpful.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qq0m1x/guidance_needed_gptoss_20b_finetuning_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2d9qb5",
          "author": "max6296",
          "text": "https://github.com/openai/harmony",
          "score": 1,
          "created_utc": "2026-01-29 06:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2df7hg",
              "author": "Double_Tourist3600",
              "text": "can you please elaborate, as per my understanding you are suggesting to use this library to prepare data my question is then which chat template to use?",
              "score": 1,
              "created_utc": "2026-01-29 07:30:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2lclhf",
                  "author": "danielhanchen",
                  "text": "For Ollama maybe use the modelfile here: https://ollama.com/library/gpt-oss:20b/blobs/fa6710a93d78",
                  "score": 1,
                  "created_utc": "2026-01-30 12:39:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2j85ai",
          "author": "Phaelon74",
          "text": "I would skip GGUF entirely then, and roll directly from full model, to INT4/8/FP4/8 as vllm and TensorRT do absolutely HORRIBLE at GGUF serving.  Train your base model, and skip GGUF quanting.",
          "score": 1,
          "created_utc": "2026-01-30 02:56:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qruejs",
      "title": "cerebras MiniMax M2.1 REAP gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qruejs/cerebras_minimax_m21_reap_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-31 06:03:05",
      "score": 12,
      "num_comments": 6,
      "upvote_ratio": 0.88,
      "text": "[https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B](https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B)\n\nmradermacher GGUF dont work, only Unsloth has best chat template fixes ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qruejs/cerebras_minimax_m21_reap_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2qzkik",
          "author": "StardockEngineer",
          "text": "I use the mrader reap daily.  Didn‚Äôt do anything special to make it work.",
          "score": 2,
          "created_utc": "2026-01-31 06:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ric8p",
              "author": "LegacyRemaster",
              "text": "me too.",
              "score": 1,
              "created_utc": "2026-01-31 09:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2u41ym",
                  "author": "ClimateBoss",
                  "text": "M2 not M2.1 dat came out like yday bruh",
                  "score": 1,
                  "created_utc": "2026-01-31 18:58:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qs6esv",
      "title": "Are Usnloth planning to provide a notebook for the Ministral 3 text?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qs6esv/are_usnloth_planning_to_provide_a_notebook_for/",
      "author": "kompania",
      "created_utc": "2026-01-31 16:13:01",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "I tried tuning the Ministral 3 3B model by swapping the training sets provided by Unsloth notebook with my own. I tried tuning the VL and Sudoku versions using the Alpaca dataset.\n\nUnfortunately, I was unsuccessful. Both Gemini and ChatGPT claim that this is currently impossible due to the lack of MistralAI support.\n\nDoes Unsloth plan to provide notebooks for Colab for tuning Ministral 3 using text?\n\nI also want to thank the people behind this system/library. I'm 63, and thanks to their extensive guides, I've made some very satisfying tweaks for Gemma 3. Thank you, Unsloth, for your work!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qs6esv/are_usnloth_planning_to_provide_a_notebook_for/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2w5qy7",
          "author": "yoracale",
          "text": "You need to disable the vision component, it is quite simple. Use the same notebook and just follow the vision finetuning guide: https://unsloth.ai/docs/basics/vision-fine-tuning\n\nE.g. turn fine-tune vision layers = off in the guide",
          "score": 1,
          "created_utc": "2026-02-01 01:19:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y15vy",
              "author": "kompania",
              "text": "Unfortunately, this solution doesn't work.\n\nI tried it myself, but unfortunately, despite entering the correct name, the model doesn't download.\n\nI get the error:\n\nRuntimeError: Unsloth: No config file found \"unsloth/Ministral-3-3B-Instruct-2512-bnb-4bit\"\n\nAnalysis of the configuration files (config.json) reveals that the model architecture is defined as Mistral3ForConditionalGeneration. This is a significant departure from the standard MistralForCausalLM or LlamaForCausalLM architectures, which are natively and seamlessly supported by the FastLanguageModel class in the Unsloth library. The Mistral3ForConditionalGeneration class implies the presence of visual projection layers and mechanisms for combining text and visual embeddings, which complicates the process of initializing the model as a pure text generator.\n\nI tried to bypass this, but encountered another problem. The main challenge is that the FastLanguageModel expects an architecture that follows the Causal LM paradigm. Forcing this class to load Ministral-3-3B requires precise dependency management, including installing the unsloth\\_zoo module, which contains definitions for non-standard architectures. The absence of this component leads to import errors or an inability to find the configuration file, which is one of the most frequently reported issues in GitHub repositories related to this model.\n\nI have unsloth\\_zoo, but it doesn't help.\n\nI tried solving the problem with Gemini Pro and ChatGPT, but they also failed.\n\nI'm at a loss here. I'm not an engineer and I can't progress any further.",
              "score": 2,
              "created_utc": "2026-02-01 09:38:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}