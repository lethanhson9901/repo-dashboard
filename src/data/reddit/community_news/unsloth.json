{
  "metadata": {
    "last_updated": "2026-02-15 16:43:41",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 15,
    "total_comments": 66,
    "file_size_bytes": 82002
  },
  "items": [
    {
      "id": "1r10d9l",
      "title": "GLM-4.7-Flash is now the #1 most downloaded model on Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/n9wlycyu2oig1.png",
      "author": "yoracale",
      "created_utc": "2026-02-10 13:12:55",
      "score": 339,
      "num_comments": 45,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r10d9l/glm47flash_is_now_the_1_most_downloaded_model_on/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4m748w",
          "author": "RedKnightRG",
          "text": "*Raises a toast to everyone who bought 128GB of RAM and dual 3090s or similar thousands of dollars ago*",
          "score": 19,
          "created_utc": "2026-02-10 14:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7sdz",
              "author": "yoracale",
              "text": "You don't need that much to run the 30B model. 24GB is enough to get it working well with quality",
              "score": 10,
              "created_utc": "2026-02-10 14:09:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m8f6l",
                  "author": "RedKnightRG",
                  "text": "Oh my B I saw GLM but read Qwen and thought this was the 80B model.  You're right 1 3090 is fine.   So I'll raise a toast to the guys who bought their single 3090 hundreds of dollars ago!  üòÖ",
                  "score": 4,
                  "created_utc": "2026-02-10 14:13:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50kicp",
                  "author": "Lovro1st",
                  "text": "What quant/context are you using?",
                  "score": 1,
                  "created_utc": "2026-02-12 17:34:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ohfii",
              "author": "timbo2m",
              "text": "So mad I didn't stack my machine with 256GB ram when I bought it a couple of years ago and it was like a couple of hundred bucks for 32GB. 20/20 hindsight!",
              "score": 2,
              "created_utc": "2026-02-10 20:35:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4x1hap",
                  "author": "epicskyes",
                  "text": "I‚Äôm so mad i didn‚Äôt stack up on 1tb of ram it was only a couple g‚Äôs now my 256gb costs the same amount as 1tb did fml ü§¶",
                  "score": 1,
                  "created_utc": "2026-02-12 02:57:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o51itbg",
              "author": "R_Duncan",
              "text": "Those are running step-3.5-flash, which is waaaay better than glm-4.7-Flash",
              "score": 2,
              "created_utc": "2026-02-12 20:16:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz42d",
          "author": "Far-Donut-1177",
          "text": "I envy folks with the hardware that could run 100B above. The best I could do is 30 and that's stretching it.",
          "score": 11,
          "created_utc": "2026-02-10 13:21:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m0j7v",
              "author": "Significant_Fig_7581",
              "text": "Heyyy let's just be happy cause this post just proved that most of us are and these download numbers are surely going to be an encouragement for most of them to release good models at that size, Qwen is releasing a 35B MOE btw...",
              "score": 8,
              "created_utc": "2026-02-10 13:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mujj1",
                  "author": "ScoreUnique",
                  "text": "35B MoE sounds like a dream after seeing qwen 3 next coder.",
                  "score": 4,
                  "created_utc": "2026-02-10 16:04:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m3n6z",
                  "author": "ismaelgokufox",
                  "text": "Indeed. Being able to run this at good speeds in 16GB VRAM is great!",
                  "score": 3,
                  "created_utc": "2026-02-10 13:47:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m5rwq",
          "author": "MrMrsPotts",
          "text": "I need a version I can squeeze into 12GB of VRAM.",
          "score": 9,
          "created_utc": "2026-02-10 13:58:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ming1",
              "author": "eesnimi",
              "text": "Unload the expert layers to CPU and it will be a nice squeeze :) I have squeezed a Q6\\_K\\_XL quant with 130k token window to 11GB VRAM with all the expert layers unloaded to CPU, getting around 11-13t/s that's quite usable.",
              "score": 5,
              "created_utc": "2026-02-10 15:06:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mqv58",
                  "author": "MrMrsPotts",
                  "text": "Is there something I can read about how to do that?",
                  "score": 2,
                  "created_utc": "2026-02-10 15:46:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4o7yqv",
                  "author": "ethereal_intellect",
                  "text": "Commenting so I'll remember to try this later. I'm on lm studio out of convenience, which app are you on",
                  "score": 1,
                  "created_utc": "2026-02-10 19:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4thuda",
                  "author": "Confident-Ad-2688",
                  "text": "Can you share the command with all parameters I ,also have 12gb vram but not able to run a good speed .",
                  "score": 1,
                  "created_utc": "2026-02-11 16:04:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4m8sty",
              "author": "NorthEastCalifornia",
              "text": "glm-4.7-flash REAP 50%",
              "score": 1,
              "created_utc": "2026-02-10 14:15:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ma897",
                  "author": "paq85",
                  "text": "I've tried the REAP version and couldn't make it work with open code at all... As if it end in some loop, even though I run it with same llamacpp params as the full version.",
                  "score": 3,
                  "created_utc": "2026-02-10 14:22:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m9zsu",
          "author": "paq85",
          "text": "This model is definitely the best 30b agentic coding model I've seen so far.",
          "score": 6,
          "created_utc": "2026-02-10 14:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mml3u",
          "author": "alfpacino2020",
          "text": "[https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) mucho mejor este mas lento pero   no falla como loco   no se tilda como gml 4.7",
          "score": 3,
          "created_utc": "2026-02-10 15:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mr3bb",
              "author": "MrMrsPotts",
              "text": "I would need the 2 bit quantisation though which might not be good.",
              "score": 1,
              "created_utc": "2026-02-10 15:47:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4msb5x",
                  "author": "alfpacino2020",
                  "text": "https://preview.redd.it/k4kc0it8voig1.png?width=974&format=png&auto=webp&s=5c2f0a31e47adbbb79d2bae65ac01c10de4a04be\n\nejemplo en iq4 me da casi  23 tokens x seg el qwen me da 15 x seg osea no gran diferencia usando Qwen3-Coder-Next-MXFP4\\_MOE.gguf",
                  "score": 1,
                  "created_utc": "2026-02-10 15:53:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4muk1k",
                  "author": "alfpacino2020",
                  "text": "tengo 5070 16gb vram 48ram  aca el qwen \n\nhttps://preview.redd.it/916th7phxoig1.png?width=1024&format=png&auto=webp&s=78fb7ae7e7bc1dd6ca8581429839edb61bc221e0\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 16:04:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4q1vqh",
          "author": "PassageInfamous3639",
          "text": "Everyone‚Äôs chasing 100B+ like it‚Äôs the promised land and I‚Äôm over here just wanting a 7‚Äì30B model that doesn‚Äôt hallucinate my whole repo üò≠\n\nIf GLM-4.7-Flash is actually dependable, that‚Äôs a huge W.",
          "score": 3,
          "created_utc": "2026-02-11 01:31:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qyfsm",
              "author": "yoracale",
              "text": "Very soon there will be more!",
              "score": 1,
              "created_utc": "2026-02-11 05:00:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o51iia0",
          "author": "R_Duncan",
          "text": "If you have 32gb of CPU ram, try qwen3-coder-next REAP 48b3 MXFP4_MOE. Faster (ok, 0-4k context is a tie... But noone keep context that low) and with less context issues (both space and speed) than glm-4.7-Flash. You'll thank me.",
          "score": 2,
          "created_utc": "2026-02-12 20:14:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5217ix",
              "author": "MrMrsPotts",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-12 21:43:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nk2o0",
          "author": "Megalion75",
          "text": "What's the best model for local SWE?  Or is there a model that can run locally that is reliable?",
          "score": 1,
          "created_utc": "2026-02-10 18:01:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50340j",
          "author": "Lesteriax",
          "text": "What is the best available model in could run on rtx pro 6000?",
          "score": 1,
          "created_utc": "2026-02-12 16:12:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2soh5",
      "title": "Run GLM-5 Locally Guide!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/u0l6522992jg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-12 12:55:16",
      "score": 171,
      "num_comments": 25,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r2soh5/run_glm5_locally_guide/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4z3l8b",
          "author": "arm2armreddit",
          "text": "Too large for my potato (2x48 GB VRAM). I dream of getting more money to get more VRAM... you think you are almost there, then bam! 1.5 TB VRAM required.",
          "score": 17,
          "created_utc": "2026-02-12 13:08:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4xwc",
              "author": "danielhanchen",
              "text": "RAM offloading also works via --fit on, but yes more VRAM the better :(",
              "score": 7,
              "created_utc": "2026-02-12 13:16:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4z81p4",
                  "author": "joninco",
                  "text": "Kimi K2.5 really shines here. GLM 5 is slightly better at more than double the size.",
                  "score": 1,
                  "created_utc": "2026-02-12 13:34:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zjegx",
          "author": "false79",
          "text": "Love these guides...\n\n\nDon't love when I don't have the VRAM requirements :(",
          "score": 8,
          "created_utc": "2026-02-12 14:36:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zkadt",
          "author": "fragment_me",
          "text": "Yeah let me just fire up my 1TB VRAM machine",
          "score": 6,
          "created_utc": "2026-02-12 14:41:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o512kcl",
          "author": "joblesspirate",
          "text": "Woot! getting 13 tokens a second. Better than nothing!",
          "score": 2,
          "created_utc": "2026-02-12 18:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59qk35",
          "author": "kripper-de",
          "text": "Please create a version for 128 GB unified RAM devices.\nFocus on agentic coding and prune unnecessary parameters.",
          "score": 2,
          "created_utc": "2026-02-14 01:45:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zitig",
          "author": "kingabzpro",
          "text": "I am running (2-bit model) on the single H200 and getting 9 tps. ",
          "score": 2,
          "created_utc": "2026-02-12 14:33:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zwg3y",
              "author": "fragment_me",
              "text": "Wow, what's the prompt processing speed?",
              "score": 1,
              "created_utc": "2026-02-12 15:41:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zxbip",
                  "author": "kingabzpro",
                  "text": "16 tps. \n\n\\>I think. The full model need to be run on 8 H200 to get 200K context window and also best speed. ",
                  "score": 1,
                  "created_utc": "2026-02-12 15:45:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50m5bb",
          "author": "ShadowIron",
          "text": "How much does something degrade at 1 bit?",
          "score": 1,
          "created_utc": "2026-02-12 17:41:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50mncq",
          "author": "llitz",
          "text": "The most interesting information for me here is the temperature top_p parameters for the swe bench.\n\nGonna give it a try to see how it affect things",
          "score": 1,
          "created_utc": "2026-02-12 17:44:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56wj77",
          "author": "trubbleshoota",
          "text": "Loved it until i hit 256gb VRAM requirement ... then it sucked",
          "score": 1,
          "created_utc": "2026-02-13 16:49:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57bqyo",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-13 18:03:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5bivvw",
                  "author": "tremenza",
                  "text": "How much token / s you can geberate with that spec?",
                  "score": 1,
                  "created_utc": "2026-02-14 10:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5bm2dn",
          "author": "Select-Student-6711",
          "text": "When will the day come when AI (the good kind) no longer uses such enormous amounts of VRAM? We're still using binary to manage networks that are actually more like FPGAs than general-purpose processors. Our brains don't use binary language; they're more like a bunch of transistors in a circuit with analog voltage than with digital logic and precise levels.\n\nI hope that day comes very soon, but at the speed these AIs are advancing, I'm sure the real revolution will arrive in a few months.",
          "score": 1,
          "created_utc": "2026-02-14 11:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bssiy",
              "author": "Silent_Ad_1505",
              "text": "These days has long gone.",
              "score": 1,
              "created_utc": "2026-02-14 12:01:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5fsvwd",
          "author": "redditor0xd",
          "text": "Is 2-bit quantization even worth the effort here?",
          "score": 1,
          "created_utc": "2026-02-15 01:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g1zlc",
              "author": "yoracale",
              "text": "Yes because the model is very large. Even larger than deepseek. For benchmarks see: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs#calibration-dataset-overfitting",
              "score": 1,
              "created_utc": "2026-02-15 02:44:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ggkxs",
          "author": "Oxffff0000",
          "text": "oh wow, that's huge! My 4070 card won't work with that right?",
          "score": 1,
          "created_utc": "2026-02-15 04:30:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gp9m3",
              "author": "yoracale",
              "text": "Do you have a lot of RAM? If not you're better off running glm flash: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)",
              "score": 1,
              "created_utc": "2026-02-15 05:39:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gxx8p",
                  "author": "Oxffff0000",
                  "text": "My machine only has 32gb.",
                  "score": 1,
                  "created_utc": "2026-02-15 06:56:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5h0o7f",
                  "author": "Oxffff0000",
                  "text": "It worked! A little slow but it worked. I asked Claude to analyze the code it generated. It said that it was decent. There were some missing api calls. Maybe if I was descriptive on my prompt, it could have generated it. Thank you so much for the link!",
                  "score": 1,
                  "created_utc": "2026-02-15 07:22:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5hrnal",
          "author": "richardwl",
          "text": "https://preview.redd.it/we991m85bnjg1.png?width=719&format=png&auto=webp&s=e0094dd85406093c771a59cc52a70f8f4ab940a4\n\nat least their dream is achievable. ",
          "score": 1,
          "created_utc": "2026-02-15 11:41:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r13pk4",
      "title": "Faster MoE LLM Training now in Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/qfoq8mirjoig1.png",
      "author": "yoracale",
      "created_utc": "2026-02-10 15:25:44",
      "score": 144,
      "num_comments": 18,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r13pk4/faster_moe_llm_training_now_in_unsloth/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4mp3bh",
          "author": "joninco",
          "text": "Multi gpu?",
          "score": 5,
          "created_utc": "2026-02-10 15:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mqz3p",
              "author": "yoracale",
              "text": "Works already but still preliminary. It's out soon.\n\nGuide for now: [https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth](https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth)",
              "score": 4,
              "created_utc": "2026-02-10 15:47:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4muop1",
          "author": "arman-d0e",
          "text": "Transformers v5 fully optimized? Do the optimizations apply to Qwen3-Coder-Next?",
          "score": 4,
          "created_utc": "2026-02-10 16:04:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mvode",
              "author": "danielhanchen",
              "text": "Not yet - we plan to add support for Qwen3-Next next!",
              "score": 10,
              "created_utc": "2026-02-10 16:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mwszz",
                  "author": "arman-d0e",
                  "text": "Much appreciated, thanks for the updates.",
                  "score": 3,
                  "created_utc": "2026-02-10 16:14:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4qyfj9",
                  "author": "Desperate-Sir-5088",
                  "text": "Would you make 4bit-bnb quant for the Qwen3-Coder-Next model? I expected QLoRA finetuing of this model with only one H100 80GB or RTX PRO 6000 96GB",
                  "score": 1,
                  "created_utc": "2026-02-11 05:00:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n0rp8",
          "author": "sterby92",
          "text": "Will we also get vulkan or rocm support at some point? ",
          "score": 3,
          "created_utc": "2026-02-10 16:32:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n5cn4",
              "author": "yoracale",
              "text": "Yes, we're announcing very soon. As soon as this month. It already works: https://unsloth.ai/docs/get-started/install/amd",
              "score": 6,
              "created_utc": "2026-02-10 16:53:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nobiz",
                  "author": "sterby92",
                  "text": "Amazing, thank you üôå",
                  "score": 1,
                  "created_utc": "2026-02-10 18:21:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sv1cm",
          "author": "Mac_NCheez_TW",
          "text": "What did we do to deserve this work üò≠. You folks are like world heros to poor ram serfs, the Robin hood to us low end peasents stealing from the Nvidia's tax demands and giving us another couple years on our garbage rigs.¬†",
          "score": 3,
          "created_utc": "2026-02-11 14:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qmmrc",
          "author": "Old-Nobody-2010",
          "text": "What is the minimum VRAM required to fine-tune GLM-4.7-Flash with Unsloth      30b a3b model",
          "score": 2,
          "created_utc": "2026-02-11 03:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qs17q",
              "author": "yoracale",
              "text": "I think around 20GB VRAM",
              "score": 1,
              "created_utc": "2026-02-11 04:14:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50umqm",
          "author": "Nazreon",
          "text": "Hey @[yoracale](https://www.reddit.com/user/yoracale/) wondering if Unsloth supports Qwen3 Next 80b finetuning? Is the backwards pass implemented? Thanks!",
          "score": 1,
          "created_utc": "2026-02-12 18:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4q3e6p",
          "author": "PassageInfamous3639",
          "text": "12x faster + less VRAM is actually insane. This is exactly the kind of ‚Äúmake local practical‚Äù progress I love seeing.\n\nAlso shoutout to the RTX 3090 mention ‚Äî my GPU just felt seen üòÇ",
          "score": 1,
          "created_utc": "2026-02-11 01:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pj0od",
          "author": "codeblockzz",
          "text": "Train or fine tune?",
          "score": 0,
          "created_utc": "2026-02-10 23:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ptl9r",
              "author": "yoracale",
              "text": "Both, works for prettaining, FFT and Lora",
              "score": 4,
              "created_utc": "2026-02-11 00:42:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3jvp8",
      "title": "Unsloth is trending on GitHub today!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/c9kgw2yv28jg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-13 08:29:44",
      "score": 130,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r3jvp8/unsloth_is_trending_on_github_today/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54s76e",
          "author": "PaceZealousideal6091",
          "text": "Keep it up guys! You have made local llm possible for everyone. Along with lcpp, you guys have been instrumental in people like me to experiment with ai. Thanks a lot. Looking forward to continue to learn and grow with you guys.",
          "score": 7,
          "created_utc": "2026-02-13 08:38:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54v22a",
              "author": "yoracale",
              "text": "Thank you so much for the support!! <3",
              "score": 4,
              "created_utc": "2026-02-13 09:05:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o55k7cv",
          "author": "m98789",
          "text": "You guys are great. But we worry that you will get bought out by an evil mega corp and all of us who build on your library will be at risk. Getting investment is one thing, but we hope you can pledge to remain open source and independent.",
          "score": 4,
          "created_utc": "2026-02-13 12:38:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56qmun",
              "author": "yoracale",
              "text": "Definitely not, the package will always have an open source license and we will always continue our open source work!!",
              "score": 2,
              "created_utc": "2026-02-13 16:21:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56hsca",
          "author": "segmond",
          "text": "unsloth so important it's pinned on my browser\n\nhttps://preview.redd.it/yn30zpdv7ajg1.png?width=928&format=png&auto=webp&s=7117d0d63336756e9ed726940298744cd2209167\n\n",
          "score": 3,
          "created_utc": "2026-02-13 15:39:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56qg45",
              "author": "yoracale",
              "text": "Oh wow! Well what can I say, you have very good taste!! üëèü¶•",
              "score": 1,
              "created_utc": "2026-02-13 16:20:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bzaf5",
          "author": "Comacdo",
          "text": "And you truly deserve it ! Congrats üëè",
          "score": 1,
          "created_utc": "2026-02-14 12:53:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5f4v8",
      "title": "Run MiniMax-2.5 locally Guide!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/61b97oryxnjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-15 13:51:53",
      "score": 59,
      "num_comments": 11,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r5f4v8/run_minimax25_locally_guide/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5iap76",
          "author": "Status_Contest39",
          "text": "This is super and you are always the best !",
          "score": 6,
          "created_utc": "2026-02-15 14:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5if6vn",
              "author": "yoracale",
              "text": "Thanks a lot! Let us know if you encounter any issues. üôè There were previously some identification issues with Qwen3 coder next where it was detected nor loading.",
              "score": 3,
              "created_utc": "2026-02-15 14:27:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iauwa",
          "author": "Sad-Bat6310",
          "text": "Many thanks folks, you are truly the best !",
          "score": 2,
          "created_utc": "2026-02-15 14:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ibuw7",
          "author": "Local-Cartoonist3723",
          "text": "I cincerely feel as a community we‚Äôve almost been to quiet on this model, this size and that strong is ridiculous.\n\nThanks unsloth team for making this avail!",
          "score": 2,
          "created_utc": "2026-02-15 14:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iqfor",
          "author": "Lesteriax",
          "text": "Will it work on rtx pro 6000 and 96gb ram?",
          "score": 2,
          "created_utc": "2026-02-15 15:27:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j5qvn",
              "author": "yoracale",
              "text": "Yes of course! Run a bigger one and it'll be even faster",
              "score": 1,
              "created_utc": "2026-02-15 16:42:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iqpdu",
          "author": "Porespellar",
          "text": "Amazing job guys! Thanks so much for getting your quants out so quickly. Is there any chance that you guys could start including the quant perplexity information with your releases so it‚Äôs easier to compare your quants with Ubergarm‚Äôs quants and others who include this information?",
          "score": 1,
          "created_utc": "2026-02-15 15:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5imzqm",
          "author": "Mysterious_Value_219",
          "text": "Would it work with the 128GB GX10 or the ryzen ai max+ 395?",
          "score": 1,
          "created_utc": "2026-02-15 15:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j3u4p",
              "author": "etcetera0",
              "text": "I tried smaller quantization models but it doesn't load for me. I'll try again today",
              "score": 1,
              "created_utc": "2026-02-15 16:32:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iu39g",
          "author": "jellydn",
          "text": "Do you know if that's running okay on a Mac M1/M2 laptop? Or what is the recommended spec for the Mac Mini to run it smoothly? I really appreciate any help you can provide.",
          "score": 0,
          "created_utc": "2026-02-15 15:45:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j5l99",
              "author": "yoracale",
              "text": "If it fits it should mostly be ok. I have a M3 and get 20 toks",
              "score": 1,
              "created_utc": "2026-02-15 16:41:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r30q9u",
      "title": "Excited to run GLM-5 on a potato!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/l7sogx6ot3jg1.jpeg",
      "author": "Porespellar",
      "created_utc": "2026-02-12 18:09:22",
      "score": 40,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r30q9u/excited_to_run_glm5_on_a_potato/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r3sqgt",
      "title": "qwen3-coder-next ggufs updated?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r3sqgt/qwen3codernext_ggufs_updated/",
      "author": "Clank75",
      "created_utc": "2026-02-13 15:51:05",
      "score": 32,
      "num_comments": 18,
      "upvote_ratio": 0.97,
      "text": "I just noticed (because llama decided to download the quants all over again) that Qwen3-Coder-Next GGUFs all seem to have been updated (judging by the filetimes on Huggingface, about 13 hours ago.)\n\nAny ideas what's changed?  (Hoping/praying for something that fixes let's-read-this-file-over-and-over-again toolcalling problems ;-).)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r3sqgt/qwen3codernext_ggufs_updated/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o56mahv",
          "author": "yoracale",
          "text": "Some GGUFs weren't dynamic so we've updated some of them to be dynamic.\nYou won't see much improvement unless you use lower bit quants",
          "score": 13,
          "created_utc": "2026-02-13 16:01:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56p0ps",
              "author": "ywis797",
              "text": "But please add some update notes so we won't download again. I am too eager to download newer for better performance.",
              "score": 7,
              "created_utc": "2026-02-13 16:14:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56q993",
                  "author": "yoracale",
                  "text": "You can redownload if you want and you may see some minor improvements at higher bits.",
                  "score": 1,
                  "created_utc": "2026-02-13 16:20:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56pvuu",
              "author": "Clank75",
              "text": "Ahh, OK, so unlikely to be lifechanging at Q8?  Anyway, thanks for the quick reply!",
              "score": 1,
              "created_utc": "2026-02-13 16:18:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o56rrvz",
                  "author": "yoracale",
                  "text": "You can try it and see which is better. Wont be life-changing no but you may see some differences",
                  "score": 1,
                  "created_utc": "2026-02-13 16:27:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ao6ox",
              "author": "UmpireBorn3719",
              "text": "any improvement on MXFP4?",
              "score": 1,
              "created_utc": "2026-02-14 05:40:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o570fcb",
          "author": "Phantasmagoriosa",
          "text": "Ah so its not just me with the toolcalling errors all the time then:  \n`‚Üê Write lambda/src/dispatcher-handler/agentcore.ts Error: The write tool was called with invalid arguments: [   {     \"expected\": \"string\",     \"code\": \"invalid_type\",     \"path\": [       \"content\"     ],     \"message\": \"Invalid input: expected string, received undefined\"   } ]. Please rewrite the input so it satisfies the expected schema.`",
          "score": 2,
          "created_utc": "2026-02-13 17:08:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5738yz",
              "author": "Clank75",
              "text": "Nope.  I have high hopes that they'll eventually get fixed - there are some *probably* relevant fixes happening to prompt template parsing in Llama.cpp at the moment - e.g. https://github.com/ggml-org/llama.cpp/pull/18675 - but for the timebeing it's essentially unusable.  Which is a shame, because it seems smart.\n\nOne of these days I'mma get round to getting vLLM working, but every time I've fallen down that rabbit hole it's been a world of hurt...",
              "score": 2,
              "created_utc": "2026-02-13 17:22:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o58g8ur",
              "author": "__SlimeQ__",
              "text": "i was getting that like crazy using the unofficial ollama release from frob, on the official one i am getting pretty great behavior in both qwen code and openclaw",
              "score": 1,
              "created_utc": "2026-02-13 21:22:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57aizj",
          "author": "klop2031",
          "text": "Sheesh again?",
          "score": 2,
          "created_utc": "2026-02-13 17:57:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bda1l",
          "author": "nunodonato",
          "text": "My server less container was working fine with the previous version and now fails to load üòì (Q6 btw)",
          "score": 1,
          "created_utc": "2026-02-14 09:34:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r330v7",
      "title": "Step-3.5-flash Unlosth dynamic ggufs?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r330v7/step35flash_unlosth_dynamic_ggufs/",
      "author": "GodComplecs",
      "created_utc": "2026-02-12 19:33:05",
      "score": 18,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Any info on this? Works pretty well but I'd like to use Unsloth quants and fixes. It's seems to be a great model (running it in Q4) but I don't know if the hefty reasoning is a bug or what, but the end results are okay. Qwen 3 next coder is much faster still even though both are offloaded the same way, not OOM.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r330v7/step35flash_unlosth_dynamic_ggufs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o51c7o6",
          "author": "yoracale",
          "text": "Yes we're working on them.\n\nEdit: Here: https://huggingface.co/unsloth/Step-3.5-Flash-GGUF",
          "score": 18,
          "created_utc": "2026-02-12 19:44:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51frlk",
              "author": "GodComplecs",
              "text": "Thanks! Hope it turns out great and not too much of an headache.",
              "score": 6,
              "created_utc": "2026-02-12 20:01:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o539bul",
                  "author": "yoracale",
                  "text": "They're going to be uploaded here: [https://huggingface.co/unsloth/Step-3.5-Flash-GGUF](https://huggingface.co/unsloth/Step-3.5-Flash-GGUF)",
                  "score": 1,
                  "created_utc": "2026-02-13 01:48:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o58wban",
              "author": "ravage382",
              "text": "Dang. 404 now.",
              "score": 1,
              "created_utc": "2026-02-13 22:43:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r525jp",
      "title": "Best coding model to use with 48GB vram and 90GB ram?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r525jp/best_coding_model_to_use_with_48gb_vram_and_90gb/",
      "author": "StartupTim",
      "created_utc": "2026-02-15 01:45:32",
      "score": 17,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I have a system with a RTX 5090 32GB vram and a RTX 5070Ti with 16GB vram.\n\nWhich would be the best model to run for doing JS, html (node/react) type of development?  The goal would be as big of a context window as possible as well.\n\nAlso, would you recommend llama.cpp normal or compile with any specific flags?\n\n\nThanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r525jp/best_coding_model_to_use_with_48gb_vram_and_90gb/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5hpzrw",
          "author": "Holiday_Purpose_3166",
          "text": "Reddit seems to error to post my comment, so had to break into 4 parts and LLM condensed it:\n\n# PART 1\n\nIt depends on which pipeline you‚Äôre using (Kilocode, Opencode, Openclaw, etc.) ‚Äî the same model can behave very differently depending on chat templates, tool-call reliability, and context handling.\n\n# Models I‚Äôm actively using\n\nMy current rotation: **Devstral Small 2**, **GLM 4.7 Flash**, **GPT-OSS-20B**, **GPT-OSS-120B**.\n\n# Devstral Small 2 (main workhorse)\n\n* Most reliable for **repo work** overall.\n* Runs well in **Kilocode** and **Mistral Vibe**.\n* **Opencode + Openclaw** won‚Äôt work well out-of-the-box unless you fix the **chat template** (I have the fix on my git listed below).\n* Low latency, **no-reasoning style**, very strong for **Rust + NextJS + ML** repos.\n* Can miss some deeper architectural leaps, but consistently ships working code.\n* Don‚Äôt go below **Q8** if you want it to stay strong; lower quants tend to ‚Äúspend context‚Äù fixing itself.\n* KV cache: **Q8** is great; **Q4** only for lighter tasks.\n\n# GLM 4.7 Flash (fast JS/TS repo helper)\n\n* Great for repo work involving **JS/TS**, but not as consistently strong as Devstral for full repo correctness.\n* Works best in **Opencode**; also works in **Kilocode** but occasionally misses tool calls.\n* Also works in **Openclaw**.\n* Faster than Devstral, reasoning isn‚Äôt verbose, less brittle than GPT-OSS-20B.\n* Main drawback: inference speed drops hard above \\~**40k context** (long-horizon tasks get mildly painful, especially prompt reprocessing).\n* Tested on **Q5**.\n* KV cache: prefers **F16** for speed; Q8 is fine too. Context window processing is bearable up to \\~100k\n\n#",
          "score": 8,
          "created_utc": "2026-02-15 11:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hq33s",
              "author": "Holiday_Purpose_3166",
              "text": "# PART 2\n\n# GPT-OSS-20B (great until complexity spikes)\n\n* Strong for repo work **as long as it‚Äôs not too complex**; can hard-refuse when ingesting very large files (e.g., **900+ LoC**).\n* Works well in **Opencode** and **Openclaw** out-of-the-box.\n* Weaker in **Kilocode** (tool/API calls fail too often for me).\n* Best at **medium reasoning**. High reasoning gives a small edge, but CoT verbosity kills completion time and risks falling in repetition - sampling below helps.\n* Strong speed across full context.\n* I run **MXFP4**, **no KV cache compression**.\n\n# GPT-OSS-120B (planning/docs monster, good complement to 20B)\n\n* Great for repo work; oddly, I‚Äôve seen **20B beat it** in some coding areas.\n* Where 120B shines: **planning + documentation**, and it pairs well with 20B.\n* Works with **Opencode + Kilocode**.\n* Felt a bit ‚Äúdumb‚Äù in **Openclaw**, but I haven‚Äôt used it long enough there.\n* Best at **medium reasoning**; high reasoning helps planning/docs more than code execution.\n* Stronger CoT than 20B and more token-efficient.\n* Also **MXFP4**, **no KV cache compression**.\n* Speed stays excellent even offloaded (activated params keep it lighter than it sounds).\n\n# ",
              "score": 6,
              "created_utc": "2026-02-15 11:27:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5hq4gy",
                  "author": "Holiday_Purpose_3166",
                  "text": "# PART 3\n\n# Models I didn‚Äôt keep using (yet)\n\n**Nemotron 3 Nano**\n\n* Extremely fast, tiny reasoning traces.\n* Not stable for agentic repo work (NVIDIA team themselves noted stability issues in agentic settings).\n* Good for chat-turn tasks, but GPT-OSS-20B can be significantly more intelligent, although Nemotron dataset is more up to date.\n\n**Qwen3 Coder Next**\n\n* Not weak ‚Äî but it breaks during inference on llama.cpp for me despite staying updated.\n* When it runs, it‚Äôs efficient in Kilocode/Opencode, but I can‚Äôt replicate reliably due to inference failures.\n* If someone has a stable llama.cpp setup for it, I‚Äôd love to see it.\n\n# General llama.cpp flags (what‚Äôs worked for me)\n\n* `--numa numactl` can give a small edge on non-server CPUs.\n* llama.cpp default RAM cache \\~7.5GB; I use `--cache-ram 10000`. Bigger sometimes slows prompt processing for full GPU-offload models, but can help offloaded MoE due to shared bandwidth.\n* `--no-mmap` improves stability virtually in all cases.\n* Offload tuning (hybrid GPU/CPU): `-ot \".ffn_(up)_exps.=CPU\"` \\+ `--n-cpu-moe <N>` can help with your specs.\n* Threads: more isn‚Äôt always better as CPU bottlenecks. On **Ryzen 9 9950X**:\n   * full GPU-offload tends to like `-t 8`\n   * GPT-OSS-120B offloaded liked `-t 10` with my flags\n* KV cache: generally **Q8** is the best speed/context tradeoff (higher context return vs. speed sacrifice) ‚Äî **except GPT-OSS** models, where compression hits harder.\n* Batch sizes `-b` / `-ub` are rig/model-specific:\n   * I use `16384` for GPT-OSS-20B\n   * `2048` for GPT-OSS-120B\n   * elsewhere `512` is often a solid baseline\n\n# ",
                  "score": 6,
                  "created_utc": "2026-02-15 11:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5hkan0",
          "author": "loadsamuny",
          "text": "pick your quants wisely\n\nhttps://electricazimuth.github.io/LocalLLM_VisualCodeTest/results/2026.02.04_quant/",
          "score": 3,
          "created_utc": "2026-02-15 10:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fuda0",
          "author": "ClimateBoss",
          "text": "qwen coder next MXFP4 bruh",
          "score": 7,
          "created_utc": "2026-02-15 01:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g2e2a",
              "author": "larrytheevilbunnie",
              "text": "Yeah, qwen3 coder next is best right now, he doesn't even need the 5070ti for that tbh, should reserve that one for other agents",
              "score": 2,
              "created_utc": "2026-02-15 02:47:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ha3nt",
              "author": "StartupTim",
              "text": "Hey thanks, can llama.cpp run that?",
              "score": 1,
              "created_utc": "2026-02-15 08:54:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5j453a",
              "author": "raphh",
              "text": "Is there a noticeable difference between MXFP4 and the UD-Q4\\_K\\_XL variant ?",
              "score": 1,
              "created_utc": "2026-02-15 16:34:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ghhpi",
          "author": "TokenRingAI",
          "text": "GLM 4.7 Flash",
          "score": 3,
          "created_utc": "2026-02-15 04:37:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ivx1r",
          "author": "YearnMar10",
          "text": "Minimax 2.5 probably in q4ish",
          "score": 1,
          "created_utc": "2026-02-15 15:54:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mgfg",
      "title": "Unsloth Model Quantization: When is the MiniMax M2.5 REAP GGUF coming?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r4mgfg/unsloth_model_quantization_when_is_the_minimax/",
      "author": "Leolin7519",
      "created_utc": "2026-02-14 14:47:56",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.95,
      "text": "I know everyone‚Äôs waiting for the GGUF of the older models, but we need to prioritize MiniMax M2.5. This 10B active parameter MoE is already so efficient that even the FP8 version runs like a dream. It‚Äôs SOTA (80.2% SWE-Bench) and acts as a Real World Coworker for $1/hour. The RL scaling they‚Äôve done is more impressive than any simple quantization. If you want a model that actually reasons through a linting error instead of just guessing, M2.5 is the only one in this size category that‚Äôs truly industry-leading.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r4mgfg/unsloth_model_quantization_when_is_the_minimax/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5cr6at",
          "author": "raysar",
          "text": "We need to wait than people working on training this reap gguf quantisation üòÑ\nwe want high quality not just fast delivery üòä",
          "score": 6,
          "created_utc": "2026-02-14 15:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eavni",
          "author": "segmond",
          "text": "IMO, from my experience trying REAP versions, tho shall not REAP, just pick a smaller quant size.",
          "score": 4,
          "created_utc": "2026-02-14 20:24:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4o39t",
      "title": "Is there a Problem with Qwen3 Coder Next Q6_K_XL?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/gallery/1r4o39t",
      "author": "AIMasterChief",
      "created_utc": "2026-02-14 15:54:42",
      "score": 14,
      "num_comments": 9,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r4o39t/is_there_a_problem_with_qwen3_coder_next_q6_k_xl/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5dbch6",
          "author": "Particular-Way7271",
          "text": "I think you should just try using llama.cpp directly...",
          "score": 10,
          "created_utc": "2026-02-14 17:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dpbkt",
          "author": "yoracale",
          "text": "We saw this issue and going to investigate further. Might be a lm studio specific issue since it works fine in llama.cpp",
          "score": 4,
          "created_utc": "2026-02-14 18:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fj8f4",
          "author": "admajic",
          "text": "Cant you override lmstudio by holding down Ctrl key or shift key? When it had red boxes?",
          "score": 1,
          "created_utc": "2026-02-15 00:40:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dfvgh",
          "author": "TaroOk7112",
          "text": "Same here. I used llama.cpp, but fails with opencode with streaming. There is a proxy that converts llama.cpp into a non-streaming api. But I wanted to use LM Studio for now and also is great to test configurations quickly. I tested unsloth's Qwen3-Coder-Next in MXFP4\\_MOE, Q6\\_K and UD-Q8\\_K\\_XL. Only MXFP4 is correctly detected in LM Studio 4.2.2 X86 Linux appimage.\n\nhttps://preview.redd.it/bobnzzi7zhjg1.png?width=754&format=png&auto=webp&s=6cbb5145f831f2d620de537e786dd3b49fd84a3d\n\n",
          "score": 0,
          "created_utc": "2026-02-14 17:45:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dg8ii",
              "author": "TaroOk7112",
              "text": "When I open the model for loading it doesn't know how long is the supported context and doesn't load the model.\n\nhttps://preview.redd.it/6lle6jzizhjg1.png?width=757&format=png&auto=webp&s=de1aba75a59e4539427e33ff16491feb6ecbc8f7\n\n",
              "score": 1,
              "created_utc": "2026-02-14 17:47:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dgvxv",
              "author": "TaroOk7112",
              "text": "Is there a TUI or any other app that helps loading models in plain llama.cpp like LM Studio does? It would be great to select parameters with helpers that warn you if the parameters doesn't make sense or even have a suggestion of the best parameters to use, and from there you tweak and test the best config.",
              "score": 1,
              "created_utc": "2026-02-14 17:50:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dpz52",
              "author": "yoracale",
              "text": "Really it didn't work for you in llama.cpp? What was the error, it worked for us",
              "score": 1,
              "created_utc": "2026-02-14 18:35:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ek8dt",
                  "author": "TaroOk7112",
                  "text": "Is not a fail of the GGUF, is a problem with llama.cpp streaming. Plain llama.cpp works ok, but not with opencode.\n\nEdit: I checked the integrity of the GGUFs with sha256. There are all ok.",
                  "score": 1,
                  "created_utc": "2026-02-14 21:15:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5e1lhl",
          "author": "blackhawk00001",
          "text": "Qwen3 coder next is a great opportunity to explore running llama.cpp server directly.  Lm studio uses it but not always the most recent version.  I noticed a decent speed improvement moving to llama server and it works better with vs code Kline code extension.  Llama.cpp has been having cuda optimization issues with qwen next but a few prs were merged this morning that I can‚Äôt wait to get home and pull in to test this evening.  Finding what works best can be a day or two of trouble but the results are much better.  I‚Äôm also now starting up a temporary llamacpp server from within comfy UI nodes to enhance prompts during the workflow.",
          "score": 0,
          "created_utc": "2026-02-14 19:34:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4jqpn",
      "title": "Updates to Qwen3-Coder-Next broke my setup! :(",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r4jqpn/updates_to_qwen3codernext_broke_my_setup/",
      "author": "nunodonato",
      "created_utc": "2026-02-14 12:44:54",
      "score": 6,
      "num_comments": 10,
      "upvote_ratio": 0.88,
      "text": "Hi guys,\n\nToday my container downloaded the new GGUFs that were recently updated, and since then I haven't been able to use the model.\n\nIt loads fine, but when I try to make a request it crashes\n\n`[2026-02-14T12:33:58.483Z] [zm62x] srv params_from_: Chat format: Qwen3 Coder`  \n`[2026-02-14T12:33:58.483Z] [zm62x] slot get_availabl: id 0 | task -1 | selected slot by LRU, t_last = -1`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot launch_slot_: id 0 | task -1 | sampler chain: logits -> penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> ?temp-ext -> dist`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot launch_slot_: id 0 | task 0 | processing task, is_child = 0`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot update_slots: id 0 | task 0 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 123`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot update_slots: id 0 | task 0 | n_tokens = 0, memory_seq_rm [0, end)`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot update_slots: id 0 | task 0 | prompt processing progress, n_tokens = 123, batch.n_tokens = 123, progress = 1.000000`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot update_slots: id 0 | task 0 | prompt done, n_tokens = 123, batch.n_tokens = 123`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot init_sampler: id 0 | task 0 | init sampler, took 0.03 ms, tokens: text = 123, total = 123`\n\n`[2026-02-14T12:33:58.697Z] [zm62x] /app/ggml/src/ggml-cuda/ggml-cuda.cu:97: CUDA error`\n\n`[2026-02-14T12:33:58.697Z] [zm62x] CUDA error: an illegal memory access was encountered`\n\n`[2026-02-14T12:33:58.697Z] [zm62x] current device: 0, in function launch_mul_mat_q at /app/ggml/src/ggml-cuda/template-instances/../mmq.cuh:3893`\n\n`[2026-02-14T12:33:58.697Z] [zm62x] cudaFuncSetAttribute((mul_mat_q<type, mmq_x, false>), cudaFuncAttributeMaxDynamicSharedMemorySize, nbytes_shared)`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libggml-base.so.0(+0x1826b)[0x7edca2b7926b]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libggml-base.so.0(ggml_print_backtrace+0x21c)[0x7edca2b796cc]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libggml-base.so.0(ggml_abort+0x15b)[0x7edca2b798ab]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(_Z15ggml_cuda_errorPKcS0_S0_iS0_+0xb7)[0x7edc9a963057]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(+0x726e8c)[0x7edc9aec4e8c]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(_Z19ggml_cuda_mul_mat_qR25ggml_backend_cuda_contextPK11ggml_tensorS3_S3_PS1_+0xb63)[0x7edc9a991ba3]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(+0x1d6af4)[0x7edc9a974af4]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(+0x1db507)[0x7edc9a979507]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(+0x1ddd2e)[0x7edc9a97bd2e]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libggml-base.so.0(ggml_backend_sched_graph_compute_async+0x817)[0x7edca2b95e37]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libllama.so.0(_ZN13llama_context13graph_computeEP11ggml_cgraphb+0xa1)[0x7edca2cd7dc1]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libllama.so.0(_ZN13llama_context14process_ubatchERK12llama_ubatch14llm_graph_typeP22llama_memory_context_iR11ggml_status+0x114)[0x7edca2cd9884]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libllama.so.0(_ZN13llama_context6decodeERK11llama_batch+0x386)[0x7edca2ce0d76]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libllama.so.0(llama_decode+0xf)[0x7edca2ce280f]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/llama-server(+0x152118)[0x61809b240118]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/llama-server(+0x199b0e)[0x61809b287b0e]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/llama-server(+0xb2920)[0x61809b1a0920]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca)[0x7edca25e41ca]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b)[0x7edca25e428b]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/llama-server(+0xb7b25)[0x61809b1a5b25]`\n\nAlready tried reducing context significantly, but the problem seems to be somewhere else :/\n\n**startup params**: -hf unsloth/Qwen3-Coder-Next-GGUF:Q6\\_K -c 32000 -ngl 99 -np 1 -t 16 -cb --port 8080 --host [0.0.0.0](http://0.0.0.0) \\-b 8192 -ub 4096 -fa auto --no-mmap --no-warmup --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --repeat-penalty 1.05 --jinja --seed 3407\n\n**hardware:** RTX PRO 6000\n\n**llama-server release** 8040  (latest)\n\n**base image:** [ghcr.io/ggml-org/llama.cpp:server-cuda13](http://ghcr.io/ggml-org/llama.cpp:server-cuda13)\n\nhelp?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r4jqpn/updates_to_qwen3codernext_broke_my_setup/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5c19iv",
          "author": "nunodonato",
          "text": "Tried Q8\\_0 and it worked... ",
          "score": 2,
          "created_utc": "2026-02-14 13:08:00",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5cdhbw",
              "author": "some_user_2021",
              "text": "Probably because the fix was applicable to lower quantizations.",
              "score": 1,
              "created_utc": "2026-02-14 14:25:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dej35",
              "author": "TokenRingAI",
              "text": "FWIW, VLLM is a better choice right now for this particular model on an RTX 6000",
              "score": 1,
              "created_utc": "2026-02-14 17:38:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dpsw2",
                  "author": "yoracale",
                  "text": "On a RTX 6000 though. Most people run models via unified memory Macs or vram + extra ram",
                  "score": 3,
                  "created_utc": "2026-02-14 18:34:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5byih7",
          "author": "Altruistic_Call_3023",
          "text": "I know there were updates to llama recently to make this work.  Do you know when the container was last updated?",
          "score": 1,
          "created_utc": "2026-02-14 12:48:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c0m52",
              "author": "yoracale",
              "text": "Do you know which updates? Do you have a link to the PR thanks!",
              "score": 1,
              "created_utc": "2026-02-14 13:03:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5c1847",
                  "author": "Altruistic_Call_3023",
                  "text": "I should wait for coffee before posting. I mean the changes that were made for performance.  The changes to make things work were for stepfun.  Although a couple weeks ago there was a PR for coder-next that did require new GGUF iirc?  I just was wondering how old the build of container was.  If more than a week, might we be that.  Sorry for my early confusing posts.",
                  "score": 1,
                  "created_utc": "2026-02-14 13:07:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5cnnjl",
          "author": "Responsible-Stock462",
          "text": "Try the llama-fit-params -m model [special params that affect context and or size]\n\nIf it outputs ot use them for your llama-server",
          "score": 1,
          "created_utc": "2026-02-14 15:22:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r46cl8",
      "title": "GLM-4.7-Flash-GGUF missing first  <think>",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r46cl8/glm47flashgguf_missing_first_think/",
      "author": "techmago",
      "created_utc": "2026-02-14 00:43:02",
      "score": 6,
      "num_comments": 11,
      "upvote_ratio": 0.75,
      "text": "Hello.  \nI'm using:\n\n[hf.co/unsloth/GLM-4.7-Flash-GGUF:Q8\\_0](http://hf.co/unsloth/GLM-4.7-Flash-GGUF:Q8_0)\n\nwith ollama 1.16.1 + openwebui.\n\nWhen GLM does the thinking, it's not oppening the thinking block\n\nThis make a mess... a bunch o redundant text, a random </thinking> closing nothing.\n\n    \\``docker run -d --name ollama `\n    --restart=unless-stopped \\\n    --gpus=all \\\n    -v /mnt/nvme/ollama/.ollama:/root/.ollama \\\n    --network=host \\\n    -e OLLAMA_VULKAN=0 \\\n    -e OLLAMA_FLASH_ATTENTION=0 \\\n    -e OLLAMA_KV_CACHE_TYPE=q8_0 \\\n    -e OLLAMA_NEW_ENGINE=1 \\\n    -e OLLAMA_NUM_PARALLEL=1 \\\n    -e OLLAMA_DEBUG=0 \\\n    -e GIN_MODE=release \\\n    -e OLLAMA_NEW_ESTIMATES=1 \\\n    -e OLLAMA_MAX_LOADED_MODELS=2 \\\n    -e OLLAMA_KEEP_ALIVE=320 \\\n    -e OLLAMA_CONTEXT_LENGTH=48128 \\\n    -e OLLAMA_NUM_PREDICT=600 \\\n    $IMAGE:$IMAGE_TAG\n    \\```\n\nAm i doing something wrong, or is the model that is broke?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r46cl8/glm47flashgguf_missing_first_think/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o59hbki",
          "author": "yoracale",
          "text": "We wrote many times in our guide not to use Ollama with Glm-4.7-flash gguf it doesn't work: https://unsloth.ai/docs/models/glm-4.7-flash",
          "score": 16,
          "created_utc": "2026-02-14 00:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cros0",
              "author": "_twrecks_",
              "text": "You need to put glm4 7 render and parser parameters in the ollama modelfile. Look at the modelfile from a glm4.7 family model from ollama.com.",
              "score": 1,
              "created_utc": "2026-02-14 15:43:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59ynau",
              "author": "techmago",
              "text": "I thought it was already fixed on the last version. Sorry.",
              "score": -1,
              "created_utc": "2026-02-14 02:36:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5atp72",
          "author": "eleqtriq",
          "text": "Dude.  Don‚Äôt use Ollama in a container.  At least use llama.cpp.",
          "score": 7,
          "created_utc": "2026-02-14 06:28:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a7tim",
          "author": "admajic",
          "text": "Research how to use it with llama.cpp\nIt dosen't work with vllm with a gguf atm either.\n\nThen go for a walk as it does it's thing. It's very slow on my 3090",
          "score": 5,
          "created_utc": "2026-02-14 03:37:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ab20w",
              "author": "techmago",
              "text": "The reason i don't use llama is ease of use. I did pilot it with llama-swap, but it was still... a chore.  \nI have around 10\\~15 models, i swap then A LOT and i use different context sizes depending on the caller. lamma cpp is to stiff for the way i use.\n\nI know the ollama crew is not exactly... friendly. But in my defense, there is people who like apple, and that company is the most anti-consumer thing i know.",
              "score": -2,
              "created_utc": "2026-02-14 04:00:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5b0sdd",
                  "author": "Zestyclose-Shift710",
                  "text": "llama.cpp has a router mode now, and supports .ini file models configuration¬†\n\n\nThey have official docker images too¬†\n\n\nAnd a new builtin webui\n\n\nIt's gotten a lot easier to use, is what I'm saying¬†",
                  "score": 8,
                  "created_utc": "2026-02-14 07:33:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5b8wc0",
                  "author": "admajic",
                  "text": "You can define every tiny detail ctx size temperature etc using llama-swap or llama.cpp and an ini",
                  "score": 4,
                  "created_utc": "2026-02-14 08:51:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r116zw",
      "title": "Finetuning query for gpt-oss 20b model",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r116zw/finetuning_query_for_gptoss_20b_model/",
      "author": "Double_Tourist3600",
      "created_utc": "2026-02-10 13:47:55",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "We are facing a **thinking-loop issue** after fine-tuning a reasoning-enabled model and would appreciate guidance.\n\n**Setup**\n\n* Created a custom medical dataset and prepared it using the OpenAI Harmony format\n* Fine-tuned using Unsloth (analysis samples included)\n* Converted to GGUF via `llama.cpp`, quantized to **Q4\\_K\\_M**, and deployed with Ollama\n* For short/simple prompts, outputs are correct; however, as conversation context grows, the model remains in continuous reasoning (‚Äúthinking‚Äù) and does not produce the final response\n\n**Questions**\n\n1. What are the common causes of this behavior (chat template mismatch, stop-token issues, reasoning token configuration, RLHF override during SFT, etc.)?\n2. What checks or precautions should be taken during fine-tuning, GGUF conversion, quantization, and Ollama model file setup to prevent reasoning loops?\n3. Are there recommended template or stop-sequence configurations specifically for reasoning-enabled models to ensure the model exits the thinking phase properly?\n\nAny debugging checklist or best practices would be very helpful.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r116zw/finetuning_query_for_gptoss_20b_model/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o4mspu4",
          "author": "arman-d0e",
          "text": "Train on assistant only loss, include stop tokens (training with harmony has been weird for me, but this helped a lot) but most of all just try lowering temperature with inference you‚Äôll see better results",
          "score": 2,
          "created_utc": "2026-02-10 15:55:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r0dr3",
              "author": "Double_Tourist3600",
              "text": "    trainer = train_on_responses_only(\n            trainer,\n            instruction_part=\"<|start|>user<|message|>\",\n            response_part=\"<|start|>assistant\",\n            force_match=True,\n        )\n    I used this to apply loss on assistant messages/channels, Included stop tokens in each examples as <|end|> for channel and EOS = <|return|>\n    Lowering the temperature works for short queries but as context grows it start repeating the thinking..\n    This are the above measures that I have taken please let me know if any inputs are there..",
              "score": 1,
              "created_utc": "2026-02-11 05:15:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r48dzu",
      "title": "First time fine tuning and need advice for tuning unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r48dzu/first_time_fine_tuning_and_need_advice_for_tuning/",
      "author": "ClientPrize9151",
      "created_utc": "2026-02-14 02:17:40",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "Hi, guys any advice would be nice. I will provide my current settings that I will be using and would appropriate any feedback to ensure as much accuracy  from the input and output from my dataset without over fitting. Any advice on the settings and if I can improved them to get better results would be really appropriated. Thanks.\n\nfrom unsloth import FastLanguageModel\n\nimport torch\n\nmodel\\_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n\nmax\\_seq\\_length = 2048  # Choose sequence length\n\ndtype = None  # Auto detection\n\n\\# Load model and tokenizer\n\nmodel, tokenizer = FastLanguageModel.from\\_pretrained(\n\nmodel\\_name=model\\_name,\n\nmax\\_seq\\_length=max\\_seq\\_length,\n\ndtype=dtype,\n\nload\\_in\\_4bit=True,\n\n)\n\n\\# Add LoRA adapters\n\nmodel = FastLanguageModel.get\\_peft\\_model(\n\nmodel,\n\nr=64,  # LoRA rank - higher = more capacity, more memory\n\ntarget\\_modules=\\[\n\n\"q\\_proj\", \"k\\_proj\", \"v\\_proj\", \"o\\_proj\",\n\n\"gate\\_proj\", \"up\\_proj\", \"down\\_proj\",\n\n\\],\n\nlora\\_alpha=128,  # LoRA scaling factor (usually 2x rank)\n\nlora\\_dropout=0,  # Supports any, but = 0 is optimized\n\nbias=\"none\",     # Supports any, but = \"none\" is optimized\n\nuse\\_gradient\\_checkpointing=\"unsloth\",  # Unsloth's optimized version\n\nrandom\\_state=3407,\n\nuse\\_rslora=False,  # Rank stabilized LoRA\n\nloftq\\_config=None, # LoftQ\n\n)\n\nfrom trl import SFTTrainer\n\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n\nmodel=model,\n\ntokenizer=tokenizer,\n\ntrain\\_dataset=dataset,\n\ndataset\\_text\\_field=\"text\",\n\nmax\\_seq\\_length=max\\_seq\\_length,\n\ndataset\\_num\\_proc=2,\n\nargs=TrainingArguments(\n\nper\\_device\\_train\\_batch\\_size=1,\n\ngradient\\_accumulation\\_steps=8,\n\ngradient\\_checkpointing=True,\n\nwarmup\\_steps=10,\n\nnum\\_train\\_epochs=3,\n\nlearning\\_rate=2e-4,\n\nfp16=not torch.cuda.is\\_bf16\\_supported(),\n\nbf16=torch.cuda.is\\_bf16\\_supported(),\n\nlogging\\_steps=25,\n\noptim=\"adamw\\_8bit\",\n\nweight\\_decay=0.01,\n\nlr\\_scheduler\\_type=\"linear\",\n\nseed=3407,\n\noutput\\_dir=\"outputs\",\n\nsave\\_strategy=\"epoch\",\n\nsave\\_total\\_limit=2,\n\ndataloader\\_pin\\_memory=False,\n\n),\n\n)\n\nExample of my dataset shown below- input receipt data and output is insight data.\n\n    [\n      {\n        \"id\": 1,\n        \"period_days\": 3,\n        \"receipts\": [\n          {\n            \"merchant_name\": \"WH Smith\",\n            \"date\": \"Jan 29, 2026\",\n            \"currency\": \"¬£\",\n            \"total\": 5.31,\n            \"category\": \"Other\"\n          },\n          {\n            \"merchant_name\": \"WH Smith\",\n            \"date\": \"Jan 29, 2026\",\n            \"currency\": \"¬£\",\n            \"total\": 15.07,\n            \"category\": \"Other\"\n          },\n          {\n            \"merchant_name\": \"Card Factory\",\n            \"date\": \"Jan 29, 2026\",\n            \"currency\": \"¬£\",\n            \"total\": 5.82,\n            \"category\": \"Other\"\n          },\n          {\n            \"merchant_name\": \"Tesco\",\n            \"date\": \"Jan 30, 2026\",\n            \"currency\": \"¬£\",\n            \"total\": 72.92,\n            \"category\": \"Groceries\"\n          }\n        ],\n        \"insights\": [\n          {\n            \"title\": \"You spent ¬£26.\",\n            \"category_tag\": \"Spending Insight\",\n            \"last_days\": \"Last 3 Days\",\n            \"date_generated\": \"Jan 30, 2026\",\n            \"description\": \"You spent ¬£26.20 on other 3 times. Small reductions here could add up significantly.\",\n            \"tag\": \"Other\"\n          },\n          {\n            \"title\": \"Groceries totaled ¬£72.\",\n            \"category_tag\": \"Spending Insight\",\n            \"last_days\": \"Last 3 Days\",\n            \"date_generated\": \"Jan 30, 2026\",\n            \"description\": \"Groceries totaled ¬£72.92 this period. Compare prices across stores for better deals.\",\n            \"tag\": \"Groceries\"\n          }\n        ]\n\nStep | Training Loss so far\n\nhttps://preview.redd.it/oiezdk0mkdjg1.png?width=804&format=png&auto=webp&s=628a824d36f704627c79b0e90ba1b6d5ed7cceb8\n\nNote: I have an i9, 4070 8gb vram and 32gb ram- Lenovo Legion 5 Pro.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r48dzu/first_time_fine_tuning_and_need_advice_for_tuning/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5c0qdv",
          "author": "yoracale",
          "text": "Everything you need for overfitting etc is in our docs: [https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)",
          "score": 1,
          "created_utc": "2026-02-14 13:04:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c3q2a",
              "author": "ClientPrize9151",
              "text": "Thanks got what i needed.",
              "score": 1,
              "created_utc": "2026-02-14 13:24:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}