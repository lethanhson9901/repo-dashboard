{
  "metadata": {
    "last_updated": "2026-01-26 08:59:58",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 11,
    "total_comments": 66,
    "file_size_bytes": 102337
  },
  "items": [
    {
      "id": "1qhscts",
      "title": "Run GLM-4.7-Flash locally Guide! (24GB RAM)",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/hgng1lc5ufeg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-20 05:22:23",
      "score": 222,
      "num_comments": 46,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qhscts/run_glm47flash_locally_guide_24gb_ram/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mccxs",
          "author": "TaroOk7112",
          "text": "Thanks!!!  \nDoes llama.cpp need to add some code to improve support or with this PR it's all supported?  \n[https://github.com/ggml-org/llama.cpp/pull/18936](https://github.com/ggml-org/llama.cpp/pull/18936)",
          "score": 9,
          "created_utc": "2026-01-20 05:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mdiub",
              "author": "danielhanchen",
              "text": "It should work in llama.cpp main now!",
              "score": 5,
              "created_utc": "2026-01-20 06:00:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0n9xr7",
                  "author": "goniz",
                  "text": "Does the repetitions happen on Q8 and BF16 GGUFs as well? Or just lower quants?",
                  "score": 3,
                  "created_utc": "2026-01-20 10:51:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0q5ocg",
                  "author": "TaroOk7112",
                  "text": "Be aware that flash attention degrades speed a lot, there are several PR in the works to improve support for  GLM-4.7-Flash in llama.cpp. It works, but is slower than even gpt-oss 120, so for now not too interesting.   \n[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/5#696f93fcbc8028d97e6fdf2b](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/5#696f93fcbc8028d97e6fdf2b)",
                  "score": 1,
                  "created_utc": "2026-01-20 19:59:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzq84",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:35:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0o83m1",
          "author": "maxpayne07",
          "text": "So far, yes",
          "score": 2,
          "created_utc": "2026-01-20 14:35:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzs2f",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qiwhk",
          "author": "Dramatic-Rub-7654",
          "text": "Is this model actually dumber than Qwen 3 Coder Flash, or is it just overly sensitive? To the point that with the --n-cpu-moe flag it gets stuck in an infinite loop repeating a single word, and without that flag it keeps creating endless files, all with errors, until the window runs out?",
          "score": 2,
          "created_utc": "2026-01-20 21:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rc1ba",
              "author": "yoracale",
              "text": "It's very sensitive. Did you try using our recommended parameters? It seems to be a must for the this model",
              "score": 2,
              "created_utc": "2026-01-20 23:23:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rm20t",
                  "author": "Dramatic-Rub-7654",
                  "text": "I tried using the parameters recommended on the Hugging Face page for llama-b7782/llama-server:\n\n-m GLM-4.7-Flash-Q4_K_M.gguf --host 0.0.0.0 --n-gpu-layers 999 -fa on -t 14 -n -1 -c 16384 --jinja --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1 \n\nThe only changes I experimented with were adding the --n-cpu-moe flag, which caused the model to bug out with severe repetition issues, and increasing the temperature to 1.0.\n\nAt temperature 1.0, the model‚Äôs reasoning and responses appear coherent, but when I try to use it with tools like Cline, it clearly doesn‚Äôt know what it‚Äôs doing. It can create and edit files and interact with the terminal, but it consistently outputs broken code and introduces errors when editing files.\n\nIn contrast, Qwen, even in version Q4, is capable of providing a fully functional implementation of Flappy Bird from start to finish. based on the tests I ran, the GGUF versions still need further refinement. I tested the model using the version available on OpenRouter, where it performs significantly better than in my GGUF-based tests. However, Coder Flash still demonstrates superior intelligence compared to this model.",
                  "score": 1,
                  "created_utc": "2026-01-21 00:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzv12",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 0,
              "created_utc": "2026-01-21 10:37:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mldpf",
          "author": "RMK137",
          "text": "Great turnaround! I just got my 5090 so this is perfect timing.",
          "score": 2,
          "created_utc": "2026-01-20 07:05:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzs82",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:36:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mjz8z",
          "author": "Psyko38",
          "text": "On a GPU with 16GB of VRAM, we're good at Q3?",
          "score": 1,
          "created_utc": "2026-01-20 06:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mma9p",
              "author": "Conscious_Chef_3233",
              "text": "you should go higher, i run qwen3 30b q4 on my 4070 12g",
              "score": 1,
              "created_utc": "2026-01-20 07:13:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mmg8z",
                  "author": "Psyko38",
                  "text": "But the weights, where do you put them in the RAM and you put the MoE in the VRAM?",
                  "score": 1,
                  "created_utc": "2026-01-20 07:14:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0tzsmj",
                  "author": "yoracale",
                  "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
                  "score": 1,
                  "created_utc": "2026-01-21 10:36:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzsdw",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u46lj",
                  "author": "Psyko38",
                  "text": "I already downloaded the model yesterday with your recommended settings and the result was not too bad. I'll try the new ones again, but in Q4 with the MoE weights on the GPU and the unused ones on the CPU.",
                  "score": 1,
                  "created_utc": "2026-01-21 11:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mmhfv",
          "author": "Unlikely_Database_87",
          "text": "In lm studio the model with those parameters you provided, cannot even give any clear response and it infinitely generates nonsense. Simple prompt I use - write java method  to merge two sorted arrays, no tests no explanation just code. My config 5080 and 64GB RAM",
          "score": 1,
          "created_utc": "2026-01-20 07:14:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mohp7",
              "author": "yoracale",
              "text": "You need to use dry multiplier which has the biggest impact, because LM Studio does not have it, you need to disable repeat penalty entirely.",
              "score": 4,
              "created_utc": "2026-01-20 07:32:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0mq2e5",
                  "author": "Unlikely_Database_87",
                  "text": "Thanks that helps",
                  "score": 1,
                  "created_utc": "2026-01-20 07:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tztmu",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:36:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mnfbw",
          "author": "Prudent-Ad4509",
          "text": "I'm not sure what they mean by full precision. The original seems to require 64Gb even with near zero context.",
          "score": 1,
          "created_utc": "2026-01-20 07:23:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n1ywr",
          "author": "Kirito_5",
          "text": "Thank you as always! \nWill try to see how well it compares to others on my 3090.",
          "score": 1,
          "created_utc": "2026-01-20 09:38:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nfyub",
              "author": "siggystabs",
              "text": "I really like GLM 4.7 Flash, but i can only fit a small context (like 5000 chars) on a single 3090 ‚Äî Not using System RAM. I end up having to split it across two 3090s to use it with decent context, and then it isn‚Äôt as fast.\n\nBut GLM 4.7 Flash is really exciting. The quality is excellent. If I didn‚Äôt have specific needs for my self-hosted apps, I‚Äôd probably use that exclusively for chatting. It‚Äôs definitely smarter than Qwen3 30B A3B in my usage.\n\nFor my workhorse applications I went back to Qwen3 30B A3B. Reluctantly. It‚Äôs just faster and better on VRAM, and ‚Äúgood enough‚Äù.",
              "score": 2,
              "created_utc": "2026-01-20 11:42:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tzu6e",
                  "author": "yoracale",
                  "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
                  "score": 1,
                  "created_utc": "2026-01-21 10:36:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzu0f",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nh57o",
          "author": "xanduonc",
          "text": "FP8 is half precision, original weights from glm are BF16",
          "score": 1,
          "created_utc": "2026-01-20 11:51:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nvq0g",
              "author": "yoracale",
              "text": "Yes thanks I edited my post",
              "score": 1,
              "created_utc": "2026-01-20 13:28:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nl0u3",
          "author": "maxpayne07",
          "text": "On LM studio, on the last step of reasoning, its starts a loop of repetition.  \n\n||\n|:-|",
          "score": 1,
          "created_utc": "2026-01-20 12:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0no5zm",
              "author": "yoracale",
              "text": "Did you disable repeat penalty?",
              "score": 1,
              "created_utc": "2026-01-20 12:41:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0np93r",
                  "author": "maxpayne07",
                  "text": "yes, just put it to value 1, solved!!! Thanks",
                  "score": 1,
                  "created_utc": "2026-01-20 12:49:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzuge",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pkbck",
          "author": "DuckyBlender",
          "text": "Will dynamic nvfp4 quants come out?",
          "score": 1,
          "created_utc": "2026-01-20 18:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0q1qym",
              "author": "Opposite-Station-337",
              "text": "https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4/tree/main\n\nAlready are. \n\n:edit: sorry, missed dynamic.",
              "score": 1,
              "created_utc": "2026-01-20 19:41:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tzur2",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": -1,
              "created_utc": "2026-01-21 10:37:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qiu5w8",
      "title": "GLM-4.7-Flash GGUFs updated - now produces much better outputs!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qiu5w8/glm47flash_ggufs_updated_now_produces_much_better/",
      "author": "yoracale",
      "created_utc": "2026-01-21 10:13:23",
      "score": 153,
      "num_comments": 30,
      "upvote_ratio": 0.99,
      "text": "Hey guys after the issues in the past day or so, llama.cpp has fixed a bug which caused the model to loop and produce poor outputs. Huge thanks to the `llama.cpp` team and all contributors for the fix: [https://github.com/ggml-org/llama.cpp/pull/18980](https://github.com/ggml-org/llama.cpp/pull/18980)\n\nWe‚Äôve reconverted and reuploaded the model, so **you‚Äôll need to re-download it** for the fix to take effect:  \n[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nThe issue was GLM 4.7 Flash did not set `\"scoring_func\": \"sigmoid\"`in the config.json file. We added the metadata in, so no need to reinstall llama.cpp, just re-download the quants.\n\nAfter our testing, outputs are **significantly improved**, and you should be able to use Z.ai‚Äôs recommended sampling settings with great results:\n\n* **General use:** `--temp 1.0 --top-p 0.95`\n* **Tool-calling:** `--temp 0.7 --top-p 1.0`\n* If using llama.cpp, don't forget to set `--min-p 0.01` as the default is 0.1\n\nNo need to update llama.cpp, just redownload the quants.\n\nGuide: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)\n\nLet us know if you notice the improvement!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qiu5w8/glm47flash_ggufs_updated_now_produces_much_better/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0tzhea",
          "author": "PixelatedCaffeine",
          "text": "Thank you!",
          "score": 6,
          "created_utc": "2026-01-21 10:33:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0u095h",
              "author": "danielhanchen",
              "text": ":) Let me know how it goes!",
              "score": 2,
              "created_utc": "2026-01-21 10:40:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0wpnwy",
                  "author": "PixelatedCaffeine",
                  "text": "It's working without any loops now, thanks! I couldn't compare the generation speed, but looking good so far!",
                  "score": 1,
                  "created_utc": "2026-01-21 19:14:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uiuwb",
          "author": "[deleted]",
          "text": "Confirmed fixed in llama.cpp and the new GLM-4.7-Flash-Q4\\_K\\_M.gguf is sweet. \n\nThe generation speed slowed a lot I don't know which part is responsible.",
          "score": 4,
          "created_utc": "2026-01-21 13:00:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ulii6",
              "author": "yoracale",
              "text": "Awesome, mmm might be a FA issue, i remember theyre still trying to optimize it.",
              "score": 2,
              "created_utc": "2026-01-21 13:16:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0u6kgj",
          "author": "Appropriate_Car_5599",
          "text": "is GLM good model for general reasoning not just for coding tasks? or will qwen3 be better for that case?",
          "score": 2,
          "created_utc": "2026-01-21 11:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ubmfc",
              "author": "danielhanchen",
              "text": "Yes GLM is good for all! I would actually try using it as a speculator for the larger ones as well maybe!",
              "score": 3,
              "created_utc": "2026-01-21 12:12:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0wbhyp",
              "author": "zoyer2",
              "text": "so far code-wise imo its better than **Qwen3 30B A3B instruct**. I would say it might be a bit more stable than **Qwen3-Next-80B-A3B** but **Next** seems to be able to complete more complex tasks but fails a bit more often.",
              "score": 1,
              "created_utc": "2026-01-21 18:12:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0uax2s",
          "author": "alhinai_03",
          "text": "Has the inference speed issue been fixed as well? And can we use ```-fa on``` now?",
          "score": 2,
          "created_utc": "2026-01-21 12:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ubkes",
              "author": "danielhanchen",
              "text": "I think you're referring to https://github.com/ggml-org/llama.cpp/pull/18953 right - there seems to be some issues on the PR :(",
              "score": 1,
              "created_utc": "2026-01-21 12:11:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ud1lc",
                  "author": "alhinai_03",
                  "text": "Yes, I will keep waiting until the PR gets merged. My system always offloads things to system ram for some reason with -fa off which massively degrades performance for me. I really can't wait to try this model, seeing how well it performs while people are running it broken is crazy.",
                  "score": 3,
                  "created_utc": "2026-01-21 12:22:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0x185t",
                  "author": "TokenRingAI",
                  "text": "The PR is working fine for me",
                  "score": 1,
                  "created_utc": "2026-01-21 20:06:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uepwf",
          "author": "zoyer2",
          "text": "Thanks! Works as intended now I believe (not sure if the notable speed drop after some context load is fixable), managed to pass my coding tests! Looks like a solid model",
          "score": 2,
          "created_utc": "2026-01-21 12:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11p9eo",
              "author": "yoracale",
              "text": "Awesome, thanks for trying. have you tried disabling flashattention? might help speed",
              "score": 1,
              "created_utc": "2026-01-22 13:57:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14zk11",
                  "author": "zoyer2",
                  "text": "yep sure helps! but only for a short while :,D waiting for llama.cpp fix i guess",
                  "score": 1,
                  "created_utc": "2026-01-22 23:15:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uzby4",
          "author": "Sad-Masterpiece-4730",
          "text": "Can you explain please what is the right way to understand how much q8_k_xl is better than q4_k_xl? Are there any benchmarks or is there a scientific way to see this info? Regarding glm4.7 flash or in general usecase. Thanks.",
          "score": 2,
          "created_utc": "2026-01-21 14:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v29at",
              "author": "yoracale",
              "text": "Different models will always have difference quantization sensitivity but for now, you can view some analysis and graphs here: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)",
              "score": 2,
              "created_utc": "2026-01-21 14:46:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0x45xp",
                  "author": "NoahFect",
                  "text": "Is there *any* document that explains what the different parts of the filenames mean?  q4/q8 are obvious enough, but what are _k_ and _xl_ and _iq_ and the rest?",
                  "score": 1,
                  "created_utc": "2026-01-21 20:20:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11egz4",
          "author": "marko_mavecki",
          "text": "Works way better now. I am getting 45 t/s on my dual RTX3060 with the following complete command line. Remember that this is for CUDA only and you have to modify \"\\~/models/\" path - this is the place where you have to have the model downloaded to.\n\ndocker run --gpus all -p 11434:11434 -v \\~/models/:/models ghcr.io/ggml-org/llama.cpp:full-cuda --server -m /models/GLM-4.7-Flash-UD-Q4\\_K\\_XL.gguf --jinja --threads -8 --ctx-size 20000 --temp 0.7 --top-p 0.95  --port 11434 --host 0.0.0.0\n\nThreads param is only needed in case CPU needs to take over some calculations. But if the whole thing fits your GPU then it is almost useless.  \nThe model has been downloaded from [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/blob/main/GLM-4.7-Flash-UD-Q4\\_K\\_XL.gguf](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/blob/main/GLM-4.7-Flash-UD-Q4_K_XL.gguf)\n\nKudos to u/danielhanchen for quick reaction to feedback!",
          "score": 2,
          "created_utc": "2026-01-22 12:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11p1xw",
              "author": "yoracale",
              "text": "Amazing thanks so much for trying again! And awesome to hear it works for you",
              "score": 1,
              "created_utc": "2026-01-22 13:56:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0u06sk",
          "author": "danielhanchen",
          "text": "As an example after fixing the `\"scoring_func\": \"sigmoid\"` issue, we tried a long convo:\n\n    Hi\n    What is 2+2\n    Create a Python Flappy Bird game\n    Create a totally different game in Rust\n    Find bugs in both\n    Make the 1st game I mentioned but in a standalone HTML file\n    Find bugs and show the fixed game\n\nAnd we get:\n\nhttps://preview.redd.it/h8ptha1floeg1.png?width=1422&format=png&auto=webp&s=4539e6bc931b10d31df9de9336f2416a244cd2ff\n\n**For LM Studio, disable** `repeat_penalty` (this causes issues rather) or set it to 1.0! And use `--temp 1.0 --min-p 0.01 --top-p 0.95`",
          "score": 2,
          "created_utc": "2026-01-21 10:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0u1xb4",
              "author": "Medium_Chemist_4032",
              "text": "Which quants?",
              "score": 2,
              "created_utc": "2026-01-21 10:55:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0u1yn2",
                  "author": "yoracale",
                  "text": "Q4\\_K\\_XL and Q2\\_K\\_XL",
                  "score": 3,
                  "created_utc": "2026-01-21 10:55:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0ujmwa",
              "author": "__Maximum__",
              "text": "What about other game in Rust? And 2+2",
              "score": 1,
              "created_utc": "2026-01-21 13:05:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0wc5yq",
              "author": "zoyer2",
              "text": "https://preview.redd.it/4ar6zdf8uqeg1.png?width=1503&format=png&auto=webp&s=fa87585b583bf8112337d2d6b9d19f8fd8bac10a\n\ncreate in one html file using canvas a 2d platformer game, features: camera following the player. Procedural generated world with trees, rocks. Collision system, weather system. Make it complete and complex, fully experience.\n\n\\- worked great, better output than Qwen Next 80B and other 30B models.",
              "score": 1,
              "created_utc": "2026-01-21 18:15:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ql8nnq",
      "title": "For GLM-4.7-Flash TURN OFF REPEAT PENALTY!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1ql8nnq/for_glm47flash_turn_off_repeat_penalty/",
      "author": "yoracale",
      "created_utc": "2026-01-24 00:56:19",
      "score": 116,
      "num_comments": 37,
      "upvote_ratio": 1.0,
      "text": "I've seen and spoken to over 40 people and it seems a lot of people are still experiencing issues with GLM-4.7-Flash but after they disable repeat penalty or set it to 1.0, it all got solved.\n\nSo please, turn it off as it screws up the model badly and maybe set by default for you! [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nRemember\n\n* For general use-case:  `--temp 1.0 --top-p 0.95`\n* For tool-calling:  `--temp 0.7 --top-p 1.0`\n* If using llama.cpp, set `--min-p 0.01` as llama.cpp's default is 0.1\n* Repeat penalty: Disable it, or set `--repeat-penalty 1.0`\n\nLet us know if you're still receiving bad outputs after this (keep in mind sometimes you may get bad outputs or looping - as such with any other model like GPT5 or Gemini, this is normal, but if it happens a lot this isn't normal).\n\nHave a good friday and weekend!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1ql8nnq/for_glm47flash_turn_off_repeat_penalty/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o1codiv",
          "author": "ScoreUnique",
          "text": "Seems like that's what I needed to get my open coder going, thanks a lot. I'm on Unsloth Q5, yesterday I found them radically bad bur today they've made up for it after I use your recommended settings. Finally something that is very independent and agentic that runs locally.",
          "score": 7,
          "created_utc": "2026-01-24 01:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cqg7z",
              "author": "yoracale",
              "text": "Amazing to hear and glad it worked for you :)",
              "score": 1,
              "created_utc": "2026-01-24 02:04:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1d4pm3",
                  "author": "ClimateBoss",
                  "text": "how do you \"disable\" it ? 1.0 = disable ?",
                  "score": 1,
                  "created_utc": "2026-01-24 03:28:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dfw0l",
          "author": "Thrumpwart",
          "text": "Thank you. I set the temp, top-p, and min-p yesterday on the unsloth UD model. I ran into errors. Sat down just now, loaded up reddit, saw this post, and the model is humming nicely now in LM Studio.\n\nEdit: For anyone curious, I'm running the Unsloth Q6_K_XL UD quant in the latest LM Studio. I run ROCM 7.2 on an AMD W7900 on Ubuntu 24.04.3. I gave the model a 6200 token document, added a 20 word prompt, and I got 4.13s tts and 25.16 tok/sec. The quality is very, very good for its size - the output is clear, concise, and very relevant to my word prompt.",
          "score": 3,
          "created_utc": "2026-01-24 04:41:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e6nbf",
              "author": "danielhanchen",
              "text": "Nice to hear that!",
              "score": 2,
              "created_utc": "2026-01-24 08:17:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eg5qr",
          "author": "gigascake",
          "text": "I hope this model where Blackwell pro 6000 based vllm.\nBut very slower then glm-4.5-air-fp8.\nWhat problem this model architecture? 80 token/s vs 10 token/s\nüò≠",
          "score": 3,
          "created_utc": "2026-01-24 09:44:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1en8ia",
              "author": "TaroOk7112",
              "text": "With llama.cpp we needed several days of improvements until GLM-4.7-Flash worked more or less fast and reliably.  \nI'm amazed at how fast and well still runs gpt-oss 120B. But this one is getting closer at half the size, very nice!",
              "score": 2,
              "created_utc": "2026-01-24 10:49:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1epay6",
                  "author": "Mr_Back",
                  "text": "GPT OSS 120b works perfectly for me, even with a large context. I'm getting around 10 tokens per second. GLM 4.7 air is performing terribly. The speed is awful. The most reasonable speed is with q4, followed by f16, and surprisingly, the slowest speed is with q2. I set the context to 128k, and the requests were between 33k and 40k tokens, depending on the model. The most interesting thing is that q4 had a good processing speed, better than nemotron 3 nano, but a terrible output speed. With smaller requests, q4 glm produced acceptable results.",
                  "score": 1,
                  "created_utc": "2026-01-24 11:08:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ejhhe",
              "author": "yoracale",
              "text": "This was before llama.cpp introduced FA optimizations, it should be faster now\n\nCould you test again and see?",
              "score": 1,
              "created_utc": "2026-01-24 10:15:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fjea7",
          "author": "LightBrightLeftRight",
          "text": "I was having all kinds of problems with loops on LM Studio, but it turns out it their built-in tools (wikipedia) just werent returning data to GLM properly. As soon as I turned their tools off with your general use settings it worked brilliantly.\n\nAlso, thank you unsloth, we love you",
          "score": 3,
          "created_utc": "2026-01-24 14:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jj5e8",
              "author": "danielhanchen",
              "text": "Oh great! Nice to hear that and thanks!",
              "score": 1,
              "created_utc": "2026-01-25 02:06:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1d1x6s",
          "author": "some_user_2021",
          "text": "Newbie here. Can't these default settings be part of the model?",
          "score": 2,
          "created_utc": "2026-01-24 03:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d6uhs",
              "author": "yoracale",
              "text": "You can only set it in some palces like Ollama, you cant set it auto for GGUFs in LM Studio.\n\nBut the issue is GLM Flash GGUFs don't work in Ollama, so unfortunately you have to manually set it.",
              "score": 3,
              "created_utc": "2026-01-24 03:41:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ha9xk",
                  "author": "iadanos",
                  "text": "So, these params are not taken from gguf by llama.cpp by default?\n\n\nIf no, I have a cheap room for improvement! :)",
                  "score": 1,
                  "created_utc": "2026-01-24 19:23:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1hf7fx",
                  "author": "debackerl",
                  "text": "Wow I must have missed something. I saw that Ollama has GLM 4.7 Flash in their library, so do you mean that Llama.cpp and Ollama start to be incompatible ? Like different GGUF format?",
                  "score": 1,
                  "created_utc": "2026-01-24 19:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1edt4e",
          "author": "cangaroo_hamam",
          "text": "(newbie) Is this something that can be set in LM Studio? I checked 'Edit model default parameters' but I don't see it.",
          "score": 1,
          "created_utc": "2026-01-24 09:23:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f2l4i",
              "author": "yoracale",
              "text": "repeat penalty should be here \n\nbelow are the default setttings. you have to turn it off or set it = 1.0\n\nhttps://preview.redd.it/s37xzradoafg1.png?width=644&format=png&auto=webp&s=9bfd8b62d742329478780d523957702f4dc89809",
              "score": 5,
              "created_utc": "2026-01-24 12:55:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ei1gb",
          "author": "Bluethefurry",
          "text": "still have issues with it where it will just repeat a token ad infinitum, even with repeat penalty off and the tool calling settings set, seems to be worse with thinking enabled but still sometimes happens with thinking off as well.",
          "score": 1,
          "created_utc": "2026-01-24 10:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f2n55",
              "author": "yoracale",
              "text": "mmm interesting, what platform are you currently using? llama.cpp?",
              "score": 1,
              "created_utc": "2026-01-24 12:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1g6ama",
                  "author": "Bluethefurry",
                  "text": "Yes.\n\nCUDA\\_VISIBLE\\_DEVICES=0,1 LLAMA\\_CACHE=/models/cache GGML\\_CUDA\\_GRAPH\\_OPT=1 /app/llama-server --port ${PORT} -hf unsloth/GLM-4.7-Flash-GGUF:Q4\\_K\\_XL --ctx-size 131072 --temp 1.0 --top-p 0.95 --min-p 0.01 --no-webui --no-warmup --jinja --main-gpu 0 --flash-attn auto --cache-reuse 128 --no-mmap --slot-save-path /models/cache/slots/ --parallel 1 --threads 1 --batch-size 4096\n\nRTX 3090 + RTX 3060 running b7819 (557515be1)\n\ni noticed i used the default temp/top-p here so i will try switching to the tool calling recommendation to see if i still get looping or not.",
                  "score": 1,
                  "created_utc": "2026-01-24 16:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g5onw",
          "author": "JMowery",
          "text": "Have these settings set and I get infinite loops all the time with Pydantic AI with GLM 4.7 Flash via llama.cpp. Can't even do the simplest test response. No other model I have installed (around 2 dozen) has any issue.",
          "score": 1,
          "created_utc": "2026-01-24 16:26:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j0fys",
              "author": "yoracale",
              "text": "Have you tried it without pydantic? Which quant are you using? When did you download the quants?",
              "score": 1,
              "created_utc": "2026-01-25 00:24:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1j0sk6",
                  "author": "JMowery",
                  "text": "Works fine without Pydantic AI. Using Q5KXL UD quant from Unsloth. Downloaded it yesterday.",
                  "score": 1,
                  "created_utc": "2026-01-25 00:26:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1h1ny9",
          "author": "TokenRingAI",
          "text": "I would be happy to use it, but tool calling is broken, so using VLLM until that is sorted\n\n[https://github.com/ggml-org/llama.cpp/issues/19009](https://github.com/ggml-org/llama.cpp/issues/19009)\n\nFWIW, the official model from Z doesn't include do\\_sample in the generation config, so it runs with a temperature 0, even though their docs say 0.7-1.0",
          "score": 1,
          "created_utc": "2026-01-24 18:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jjuev",
              "author": "danielhanchen",
              "text": "I added do_sample just into our upload!",
              "score": 1,
              "created_utc": "2026-01-25 02:10:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hqhbi",
          "author": "lundrog",
          "text": "Ill give it a shot. Anyone added tool support?",
          "score": 1,
          "created_utc": "2026-01-24 20:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hyvid",
              "author": "10F1",
              "text": "Worked fine for me with home assistant, used unsloth quants with llama-server.",
              "score": 1,
              "created_utc": "2026-01-24 21:17:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1i07zu",
                  "author": "lundrog",
                  "text": "Yeah was trying not to play with it and opencode",
                  "score": 1,
                  "created_utc": "2026-01-24 21:23:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kix2f",
          "author": "ga239577",
          "text": "u/yoracale I am trying to use it with Kilo Code and it asks follow up questions ... but after a few ... this happens:\n\nGreat, so you're targeting both individual and business customers. Let me ask a few more questions to better understand your business model:  \n\\[Tool Use: ask\\_followup\\_question\\]<arg\\_key>question</arg\\_key><arg\\_value>yaddayaddayadda</arg\\_value><arg\\_key>follow\\_up</arg\\_key><arg\\_value>\\[{\"text\": \"response1\", \"mode\": null}, {\"text\": \"response2\", \"mode\": null}, {\"text\": \"response3\", \"mode\": null}\\]</arg\\_value>\n\nNote: yaddayadda and the values response1, response2, response3 are just me replacing the actual response.\n\nAny way to fix this? I have seen similar things on other models when trying to use Kilo Code, Cline, etc. (not saying this is a common issue for me - most of my models are working just fine this way)\n\nEdit: For anyone who see this ... I switched back to Cline and the problem went away. Kind of amazing, because I switched from Cline to Kilo Code because Cline kept getting API errors. I guess this might just vary from model to model ...",
          "score": 1,
          "created_utc": "2026-01-25 05:40:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1g3jq7",
          "author": "NoahFect",
          "text": "Something I've been seeing with llama-server that I haven't noticed before is that the tokens/sec output rate steadily decreases from one prompt to the next.  Typically I'll hit 'New chat' each time I prompt it, and it will start out near 100 tokens/sec and eventually end up near 10 after a few chat sessions have been run.\n\nAnyone else seeing this behavior?  Hardware is 96GB Blackwell, command line is:\n\n    llama-server ^\n     --model GLM-4.7-Flash-BF16-00001-of-00002.gguf ^\n     --jinja              ^\n     --threads -1         ^\n     --ctx-size 131072    ^\n     --repeat-penalty 1.0 ^\n     --temp 1.0           ^\n     --top-p 0.95         ^\n     --min-p 0.01         ^\n     --port 2080          ^\n     --log-file glm47g.log\n\nAm I missing any useful/necessary parameters?\n\nHave to say, so far at least, GLM 4.7 Flash is not holding up very well next to Qwen (specifically Qwen30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf.)  That one has really been punching above its weight class.",
          "score": 1,
          "created_utc": "2026-01-24 16:16:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jjd2g",
              "author": "danielhanchen",
              "text": "Yes as context grows, the token processing speed does decrease somewhat - did you install the latest llama.cpp as well and use `--flash-attn on`",
              "score": 1,
              "created_utc": "2026-01-25 02:07:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1k0z4t",
                  "author": "NoahFect",
                  "text": "I'm a few weeks behind the llama.cpp repo, so will try updating next.\n\nDidn't realize the context from non-active chats still hung around in VRAM, but that does sound like the best explanation.",
                  "score": 1,
                  "created_utc": "2026-01-25 03:46:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1oyc1x",
          "author": "mtbMo",
          "text": "How do I achieve this with ollama? Rn run all my models with ollama wrapper",
          "score": 0,
          "created_utc": "2026-01-25 21:10:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qfsna",
              "author": "yoracale",
              "text": "Use Ollama default upload. Atm external GLM flash models don't work properly due to potential chat template incompatibility issues",
              "score": 1,
              "created_utc": "2026-01-26 01:18:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjy258",
      "title": "Fine-tuning Embedding models in Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/i0ojvxvg9xeg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-22 15:49:36",
      "score": 111,
      "num_comments": 5,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qjy258/finetuning_embedding_models_in_unsloth/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o12jo43",
          "author": "danielhanchen",
          "text": "Some benchmarks for 4bit QLoRA - more in our docs [https://unsloth.ai/docs/new/embedding-finetuning](https://unsloth.ai/docs/new/embedding-finetuning)\n\nhttps://preview.redd.it/jhnixy1pfxeg1.png?width=1109&format=png&auto=webp&s=142af4191cfc72fd509c349b719958fef63c31e4",
          "score": 3,
          "created_utc": "2026-01-22 16:23:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12ovqu",
          "author": "larrytheevilbunnie",
          "text": "Does this work for clip/siglip?",
          "score": 2,
          "created_utc": "2026-01-22 16:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12srjc",
              "author": "danielhanchen",
              "text": "Oh we do support VLMs like Gemma which has a siglip part - so I guess yes? Maybe try loading it and see if it works (any model name works)",
              "score": 4,
              "created_utc": "2026-01-22 17:04:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18dn02",
          "author": "AgileEfficiency2775",
          "text": "Awesome. Are full finetuning support for embeding model?",
          "score": 2,
          "created_utc": "2026-01-23 13:13:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18n4wb",
              "author": "yoracale",
              "text": "Yes definitely!",
              "score": 1,
              "created_utc": "2026-01-23 14:05:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qi3m95",
      "title": "Is GLM-4.7-Flash still looping / repeating for you?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qi3m95/is_glm47flash_still_looping_repeating_for_you/",
      "author": "yoracale",
      "created_utc": "2026-01-20 15:11:00",
      "score": 19,
      "num_comments": 37,
      "upvote_ratio": 0.96,
      "text": "Hey guys many of you are still experiencing looping/repetition issues.\n\n**Jan 21 UPDATE: llama.cpp has fixed a bug which caused the model to loop and produce poor outputs. We have reconverted and reuploaded the model so outputs should be much much better now.**\n\nYou can now use Z.ai's recommended parameters and get great results:  \n\\- For general use-case:  `--temp 1.0 --top-p 0.95`  \n\\- For tool-calling:  `--temp 0.7 --top-p 1.0`\n\nIf you still experience looping issues even after following all these steps, please let us know!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qi3m95/is_glm47flash_still_looping_repeating_for_you/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0ppvlj",
          "author": "Scared_Mycologist_92",
          "text": "i use lm-studio and try repeat penality 1,2 ..this fixed it for me and i think temp 0,6 or 0,9 or something. anything else beside repeat penality couldnt fix overthinking and those psychotic loops at any answer i tried. the model itself is pretty amazing",
          "score": 1,
          "created_utc": "2026-01-20 18:47:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0raa4l",
              "author": "yoracale",
              "text": "Interesting, for many people, adding repeat penalty didn't do anything for them unfortunately.",
              "score": 1,
              "created_utc": "2026-01-20 23:14:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0tvekv",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0q2llt",
          "author": "zoyer2",
          "text": "No loop or repeating but just in general suck at coding, making small mistakes here and there (Have only tested the quant versions). Something needs to be fixed",
          "score": 1,
          "created_utc": "2026-01-20 19:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tveuj",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u3wbc",
                  "author": "zoyer2",
                  "text": "damn, much better, great work! :)",
                  "score": 2,
                  "created_utc": "2026-01-21 11:12:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0q4czx",
          "author": "epigen01",
          "text": "Yup unsloths are not usable for me gonna wait it out",
          "score": 1,
          "created_utc": "2026-01-20 19:53:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvf1q",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qepz7",
          "author": "Final-Rush759",
          "text": "No problem,  I use mlx version.",
          "score": 1,
          "created_utc": "2026-01-20 20:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvf6d",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 2,
              "created_utc": "2026-01-21 09:56:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s6fur",
          "author": "Calm_Management_5090",
          "text": "Hi, I am using Q5\\_K\\_M on lm studio and get frequent looping with both unsloth and [z.ai](http://z.ai) suggested options above once the context gets a few thousand tokens long. Thank you for all the unsloth work on this and previous models, I will keep my eyes open for updates. Best regards.",
          "score": 1,
          "created_utc": "2026-01-21 02:13:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tba8e",
              "author": "kripper-de",
              "text": "Did you also try Q4?",
              "score": 1,
              "created_utc": "2026-01-21 06:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tvfka",
                  "author": "yoracale",
                  "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
                  "score": 1,
                  "created_utc": "2026-01-21 09:56:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tvff3",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11khx9",
          "author": "imqqmi",
          "text": "I've downloaded the updated version of the model after you've fixed it: GLM-4.7-Flash-UD-Q6\\_K\\_XL.gguf in LM-Studio and tried with a lot of combinations of settings ie temp=0 0.5 0.8 1.0, top\\_k = 0, 20, 40, top\\_p = 0.5 0.8 0.95 1, min\\_p 0 0.01 0.05, repeat penalty 1.1 1.2 1.8. Results remain the same. Also tried turning off flash attention. I even tried to compile the latest llama.cpp release in wsl ubuntu. I got similar speeds (90-100t/s) on my 5090 32GB and 4070ti 12GB combo, same results. The model doesn't appear stable to me. I also downloaded the LM-Studio version Q4, same issue. I've also increased the number of experts (ie 8 16 or 20) used to hopefully get a better response but alas.\n\nHere's the llama-server command:\n\nllama-server -m /mnt/e/LMStudioModels/unsloth/GLM-4.7-Flash-GGUF/GLM-4.7-Flash-UD-Q6\\_K\\_XL.gguf --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --n-gpu-layers 99 --flash-attn on --metrics --ubatch-size 512 --batch-size 512 --presence-penalty 1.5 --ctx-size 16000 --temp 1.0 --top-k 20 --top-p 0.95 --min-p 0.01 --repeat-penalty 1.1 --threads 12 --threads-http 5 --no-mma\n\np -kvo -dev CUDA0,CUDA1 --override-kv llama.expert\\_used\\_count=int:8 --batch-size 512 --jinja\n\nBut the output remains the same: garbage and repetition while still thinking, no final output. It also often lists numbers with one number on a line as if to count the characters but no characters appear. And it counts to 16 then starts repeating the pattern.",
          "score": 1,
          "created_utc": "2026-01-22 13:31:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o176rry",
              "author": "yoracale",
              "text": "I think the biggest issue is you turned on repeat penalty. You must turn it off! Over 10 people said after turning repeat penalty off, there's no more bad outputs",
              "score": 1,
              "created_utc": "2026-01-23 07:21:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17jbo1",
                  "author": "imqqmi",
                  "text": "Ah I must have missed that comment. It works, thanks!",
                  "score": 2,
                  "created_utc": "2026-01-23 09:15:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0p3js8",
          "author": "Sensitive_Song4219",
          "text": "Happy to test, have just grabbed *unsloth/GLM-4.7-Flash-GGUF* in LM Studio: what are the recommended settings  for it? (We don't seem to have a dry-multiplier?)",
          "score": 1,
          "created_utc": "2026-01-20 17:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0r9z4b",
              "author": "yoracale",
              "text": "Edit: llamacpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)\nI wrote it in my post. Use the parameters above and disable repeat penalty.",
              "score": 1,
              "created_utc": "2026-01-20 23:12:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rd2hu",
                  "author": "Sensitive_Song4219",
                  "text": "I see that now - thank you! I also see there's *zai-org/glm-4.7-flash* available now in LM Studio as-of an hour or two ago (I assume that's z-ai's release).\n\nWill test them both properly! In my early testing thinking seems to run fine (no loops); performance at small contexts (<4k) on my machine seems similar to Qwen3 30B A3B 2507. Looking forward to seeing how the output quality is (and how performance is at larger contexts!)",
                  "score": 2,
                  "created_utc": "2026-01-20 23:29:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pvp4m",
          "author": "ZeWishmastR",
          "text": "Same issue here",
          "score": 1,
          "created_utc": "2026-01-20 19:13:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvdn5",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0q6gfx",
          "author": "Rektile142",
          "text": "The model is responding well after using the recommended parameters, but throughput is utterly scuffed as context grows.\n\nMaybe the llama.cpp team still needs time to cook.",
          "score": 0,
          "created_utc": "2026-01-20 20:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qbd9j",
              "author": "Mr_Back",
              "text": "I really hope so. Nemotron 3 nano and GPT OSS 120b run ten times faster on my hardware when the context is increased.",
              "score": 1,
              "created_utc": "2026-01-20 20:26:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tvdw1",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pz5ow",
          "author": "Mr_Back",
          "text": "I'm more concerned about performance. I have a weak system, but even with it, similarly sized models perform much better (GPT OSS 120b, Qwen Next q6, Qwen3 Coder 30B, Nemotron 3 Nano). It's not about your specific quantization, but about the model as a whole.  \nI'm using a relatively large context (128k). The speed is still acceptable for small requests, but when I try a request with 35-40k tokens, I don't even want to wait for a response, it's so slow.\n\nMy PC: i5 12400, 96gb ram. 4070 - 12gb vram.",
          "score": -1,
          "created_utc": "2026-01-20 19:29:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tve65",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0xl09d",
                  "author": "Mr_Back",
                  "text": "https://preview.redd.it/x3mu0jnvtreg1.png?width=1280&format=png&auto=webp&s=df7e4507e88931948f20addf4ab66090e654c41c\n\nNemotron 3 nano F16 vs GLM 4.7 air Q4 UD. And yet GLM's response is shit(  \nI will wait for further improvements üí™.",
                  "score": 1,
                  "created_utc": "2026-01-21 21:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qhbr0p",
      "title": "glm 4.7 flash is out gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-19 18:08:55",
      "score": 18,
      "num_comments": 7,
      "upvote_ratio": 0.79,
      "text": "Guys do you plan to release quantisation variants of GLM-4.7 flash ? Its 30b a3b, unsloth chat template fixes are da best.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0mfece",
          "author": "yoracale",
          "text": "It's out now! GGUF: [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nTweet: [https://x.com/UnslothAI/status/2013482180564132092](https://x.com/UnslothAI/status/2013482180564132092)\n\nGuide: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)",
          "score": 1,
          "created_utc": "2026-01-20 06:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iq727",
          "author": "loadsamuny",
          "text": "architecture looks like a renamed deepseekv3, a pull request is in for it in llama.cpp so maybe tomorrow‚Ä¶.",
          "score": 4,
          "created_utc": "2026-01-19 18:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jnj3f",
          "author": "bobeeeeeeeee8964",
          "text": "i test it by the f16 gguf, and it seems the runing in a good speed, BUT it output garbage instead of proper text. We need waiting and see what going in this PR https://github.com/ggml-org/llama.cpp/pull/18936",
          "score": 3,
          "created_utc": "2026-01-19 20:55:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0k8is8",
              "author": "remghoost7",
              "text": "Pull request was merged and closed about an hour ago.  \nIt seems like they figured it out.",
              "score": 2,
              "created_utc": "2026-01-19 22:38:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0j9v1l",
          "author": "neph1010",
          "text": "[https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF](https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF)",
          "score": 1,
          "created_utc": "2026-01-19 19:51:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jk10j",
              "author": "noctrex",
              "text": "Still needs more work, its looping indefinitely",
              "score": 3,
              "created_utc": "2026-01-19 20:39:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0jzshk",
              "author": "Clqgg",
              "text": "this one is omega broken",
              "score": 2,
              "created_utc": "2026-01-19 21:55:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qk109a",
      "title": "The best (tiny) model I can run on my phone",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qk109a/the_best_tiny_model_i_can_run_on_my_phone/",
      "author": "gized00",
      "created_utc": "2026-01-22 17:36:05",
      "score": 16,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "I work in ML and I am quite familiar with Llama, fine tuning, etc. but I always work on 10s of billions parameters. \n\nI would like to train a tiny model that I can run on my phone (Pixel 8) and unsloth seems the right place to start with this (but feel free to suggest other solutions). I have some difficulties to identify what can realistically run (with a decent num tokens/s). Is a 1B model a reasonable choice if I am quantizing it?\n\nAny other suggestions?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qk109a/the_best_tiny_model_i_can_run_on_my_phone/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o134ezq",
          "author": "BenniB99",
          "text": "I believe Gemma3n was made specifically with phone and edge usage in mind.  \nIt is also a pretty decent model for its size imo (and multimodal).  \nAfaik it is also possible to switch between 2B and 4B effective parameter on the fly with it.",
          "score": 13,
          "created_utc": "2026-01-22 17:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13ow28",
              "author": "gized00",
              "text": "Nice! I will take a look.\nThank you",
              "score": 2,
              "created_utc": "2026-01-22 19:27:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17icir",
          "author": "schlammsuhler",
          "text": "I like trinity nano a 6b moe with 1b active. Nice persona\n\nhttps://huggingface.co/arcee-ai/Trinity-Nano-Preview-GGUF",
          "score": 3,
          "created_utc": "2026-01-23 09:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14w8qp",
          "author": "Azuriteh",
          "text": "The best model for on-device usage right now is probably [https://huggingface.co/LiquidAI/LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B), with 4 bit quantization it'd probably be alright!",
          "score": 2,
          "created_utc": "2026-01-22 22:58:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1684aw",
          "author": "Sure_Explorer_6698",
          "text": "Ive used Llama-3 1-3B, and Qwen-2&3 1-3B with llama.cpp on:\n\nSamusng A16 4Gb, and Samsung S20FE 6Gb. Both work great, but Llama seems more conversational.",
          "score": 2,
          "created_utc": "2026-01-23 03:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13flhy",
          "author": "Late_Huckleberry850",
          "text": "If you get the Apollo app that has some. Idk if they have an android version or not though",
          "score": 1,
          "created_utc": "2026-01-22 18:46:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13ub5t",
          "author": "[deleted]",
          "text": "Function Gemma or maybe liquid foundation models will be best fit for it",
          "score": 1,
          "created_utc": "2026-01-22 19:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13woxx",
          "author": "maxtheman",
          "text": "I have a pixel 9 and am working on fine-tuning functionalgemma, which is working great, but it really depends on your task. 1B or less can work great on a distilled task, but don't expect 90%+ perf unless you overfit the shit out of it and consider doing multiple types of fine-tuning.\n\n  \nOn pixel the hardest part, for me at least, will be getting it on an api that can actually access your gpu. I am targeting huggingfacejs for now due to the ease of use, but I don't know a better way to deploy than that or get on the google npu.",
          "score": 1,
          "created_utc": "2026-01-22 20:03:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o141j0z",
              "author": "gized00",
              "text": "It's a fairly simple task but it will require a bit of RL",
              "score": 1,
              "created_utc": "2026-01-22 20:26:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1irrfq",
          "author": "PruneRound704",
          "text": "Try lfm 2 from liquid ai, launched recently, probably best at benchmarks and available on llama cpp",
          "score": 1,
          "created_utc": "2026-01-24 23:39:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lh8ef",
          "author": "East-Muffin-6472",
          "text": "Liquid Foundation Models for sure as the have good engines to let model run on phones",
          "score": 1,
          "created_utc": "2026-01-25 10:28:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1litfc",
              "author": "gized00",
              "text": "What do you mean with good engines? Do they have a specialized/optimized layer to serve requests to the model?",
              "score": 1,
              "created_utc": "2026-01-25 10:42:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lj6oh",
                  "author": "East-Muffin-6472",
                  "text": "So I mean there has to be good optimised engines for models to reliably run on phones right so as to not hog all of its resources and to roved the models just the required stuff to run it efficiently and reliably on the said phone.\n\nPreviously it was just onnx this and that which really didn‚Äôt care about the devices stats but the L peeps did care about and this released a compatible engine for both ios ans android ig? To help with all the requirements as stated above",
                  "score": 1,
                  "created_utc": "2026-01-25 10:45:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o138iyq",
          "author": "Large-Example-1275",
          "text": "You can use the ¬´¬†Locally¬†¬ª app to check supported models on your device, but I don't know if it's available on Android.",
          "score": 0,
          "created_utc": "2026-01-22 18:15:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkntk1",
      "title": "RL for learning math",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qkntk1/rl_for_learning_math/",
      "author": "goldlord44",
      "created_utc": "2026-01-23 10:57:30",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "Hi there,\n\nI was wondering if anyone here has some advice for using unsloth to train models to be better at math?\n\nI am looking at using math text books and research papers to be able to post-train my models, specifically maths, physics and statistics. (And maybe some HF datasets).\n\nI am not sure which is the ideal post training technique for this and am looking for some direction advice before I dive head first into this.\n\nI am happy both with training on the raw text, but also understand that some post-processing is always required.\n\nI have a single Rtx Pro 6000 96GB so was hoping to train something like OSS-120B or some of the mid sized models like qwen3 30B.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qkntk1/rl_for_learning_math/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o17xu2v",
          "author": "yoracale",
          "text": "We have many RL notebooks for math, that might be a good starting point: [https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl](https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl)\n\nE.g. our Qwen3-Advanced GRPO notebook has a concrete example for math: [https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3\\_(4B)-GRPO.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)\n\nhttps://preview.redd.it/bh5w1be733fg1.png?width=2590&format=png&auto=webp&s=5f4cd2800213de88afd18c2b5d8d7dfec5959a1a",
          "score": 6,
          "created_utc": "2026-01-23 11:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a989h",
              "author": "samplebitch",
              "text": "FYI I think reddit messed up your link - here's the working URL for anyone else who might want to follow it:\n\nhttps://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb",
              "score": 2,
              "created_utc": "2026-01-23 18:35:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1b5kdr",
                  "author": "yoracale",
                  "text": "Oh thank you you're right, idk why reddit always does that üòÖ",
                  "score": 1,
                  "created_utc": "2026-01-23 21:06:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qj3ti9",
      "title": "Train Llama-3.2-11b-Vision-Instruct with GRPO",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qj3ti9/train_llama3211bvisioninstruct_with_grpo/",
      "author": "darkwigga",
      "created_utc": "2026-01-21 17:13:39",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "Hi,\n\nI was working on training Llama-3.2-11b-Vision-Instruct with GRPO using unsloth and trl grpotrainer.\n\nAfter starting the training, I am getting the following error\n\n`raise ValueError(\"\\`aspect\\_ratio\\_ids\\` must be provided if \\`pixel\\_values\\` is provided\")\\`\n\nMy trainer code worked for Qwen and Gemma.\n\n  \nCode for trainer and config\n\n    from\n     trl \n    import\n     GRPOConfig, GRPOTrainer\n    \n    \n    training_args = GRPOConfig(\n    ¬† ¬† learning_rate=learning_rate,\n    ¬† ¬† adam_beta1=adam_beta1,\n    ¬† ¬† adam_beta2=adam_beta2,\n    ¬† ¬† weight_decay=weight_decay,\n    ¬† ¬† warmup_ratio=warmup_ratio,\n    ¬† ¬† lr_scheduler_type=lr_scheduler_type,\n    ¬† ¬† optim=optim,\n    ¬† ¬† logging_steps=logging_steps,\n    ¬† ¬† log_completions=log_completions,\n    ¬† ¬† per_device_train_batch_size=per_device_train_batch_size,\n    ¬† ¬† gradient_accumulation_steps=gradient_accumulation_steps, ¬†\n    # Increase to 4 for smoother training\n    ¬† ¬† num_generations=num_generations, ¬†\n    # Decrease if out of memory\n    ¬† ¬† max_prompt_length=max_prompt_length,\n    ¬† ¬† max_completion_length=max_completion_length,\n    ¬† ¬† num_train_epochs=num_train_epochs, ¬†\n    # Set to 1 for a full training run\n    ¬† ¬† \n    # max_steps = 60,\n    ¬† ¬† save_steps=save_steps,\n    ¬† ¬† max_grad_norm=max_grad_norm,\n    ¬† ¬† report_to=report_to, ¬†\n    # Can use Weights & Biases\n    ¬† ¬† output_dir=output_dir,\n    ¬† ¬† \n    # # Below enables GSPO:\n    ¬† ¬† importance_sampling_level=importance_sampling_level,\n    ¬† ¬† mask_truncated_completions=mask_truncated_completions,\n    ¬† ¬† loss_type=loss_type,\n    )\n    \n    \n    from\n     unsloth.trainer \n    import\n     UnslothVisionDataCollator\n    \n    \n    trainer = GRPOTrainer(\n    ¬† ¬† model=model,\n    ¬† ¬† args=training_args,\n    ¬† ¬† \n    # Pass the processor to handle multimodal inputs\n    ¬† ¬† data_collator=UnslothVisionDataCollator(model, processor),\n    ¬† ¬† processing_class=processor,\n    ¬† ¬† reward_funcs=[\n    ¬† ¬† ¬† ¬† get_reward,\n    ¬† ¬† ],\n    ¬† ¬† train_dataset=train_ds,\n    ¬† ¬† eval_dataset=test_ds,\n    )\n    \n    \n    trainer.train()\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qj3ti9/train_llama3211bvisioninstruct_with_grpo/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o10dbtw",
          "author": "im_datta0",
          "text": "Hey u/darkwigga it would be helpful if you can tell us what dataset are you using and if you're doing any pre proessing...\n\nI think you might want to look at this notebook [https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2\\_5\\_7B\\_VL\\_GRPO.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb) for reference.\n\nThis is very possibly some issue with preprocessing the dataset. Would be happy to help if you can provide further information\n\nEdit: If you need any further assistance, feel free to open a github issue or hop on to our discord server",
          "score": 1,
          "created_utc": "2026-01-22 07:44:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10g9ll",
              "author": "im_datta0",
              "text": "For example, if you do something like\n\n\n\n    instruction = \"You are an expert radiographer. Describe accurately what you see in this image.\"\n    # image is a PIL object\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction}\n        ]}\n    ]\n    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n    inputs = tokenizer(\n        image,\n        input_text,\n        add_special_tokens = False,\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n\n\n\nThe inputs would contain input\\_ids, attention\\_mask, pixel\\_values, aspect\\_ratio\\_ids, aspect\\_ratio\\_mask....",
              "score": 1,
              "created_utc": "2026-01-22 08:10:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18w7ug",
                  "author": "darkwigga",
                  "text": "Hi, my dataset is a custom one formatted like similar to the notebook you shared. My training works perfectly with Qwen, but I was just facing the issue with Llama",
                  "score": 1,
                  "created_utc": "2026-01-23 14:51:21",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qk1qy4",
      "title": "Guide to use unsloth on windows",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qk1qy4/guide_to_use_unsloth_on_windows/",
      "author": "LahmeriMohamed",
      "created_utc": "2026-01-22 18:02:54",
      "score": 6,
      "num_comments": 15,
      "upvote_ratio": 0.88,
      "text": "hello guys hope i recieve help , i have recently installed unsloth to try it fine-tuning process , but due to dependencies  conflicts i had to remove it , if anyone can help me to fix this issue , my current env \npython 3.11.2\ntorch 2.5.1+cu121 \nif i ran install unsloth , it remove the cuda installation , so i used --no-deps instruction , but when running it , it require vllm , accelerate error .. .\ncan  you provide me with better/compatible versions ? \nthank you",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qk1qy4/guide_to_use_unsloth_on_windows/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o149lq3",
          "author": "Educational_Rent1059",
          "text": "Install WSL2 I highly recommend that. And hook up VSCode and you are good to go with Linux within windows without any overhead. The benchmarks in training vs native Ubuntu is between 1-4% diffs (native ahead) , if you do pure GPU work and ML, it's no diff at all.",
          "score": 3,
          "created_utc": "2026-01-22 21:04:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o149z5e",
              "author": "LahmeriMohamed",
              "text": "since i have 1ssd main system and hdd for env saving , can i save the wls env in the hdd and use it in env-variable to be accessible globaly ?",
              "score": 1,
              "created_utc": "2026-01-22 21:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14ag0l",
                  "author": "Educational_Rent1059",
                  "text": "Your WSL ext4 drive will be a single file that you can move anywhere into any HDD you like in windows. If you prefer to have it on a different disk etc that works as well. Everything you do will be stored within that single file (it's basically your entire OS and all files within it in one file on windows)",
                  "score": 2,
                  "created_utc": "2026-01-22 21:08:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o14aosb",
                  "author": "Educational_Rent1059",
                  "text": "You can also create another \"drive\" (single file) and attach it into linux, it's a bit messy as you need to attach it in windows command shell first, and then go into the linux WsL2 cli and attach it there too, but use GPT for guidance and do that. Be careful as so you don't delete any files by wrong instructions from GPT as they hallucinate alot",
                  "score": 1,
                  "created_utc": "2026-01-22 21:09:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15bmc0",
              "author": "fiery_prometheus",
              "text": "Except that wsl likes to crash under heavy workloads, I've had so many crashes when fine-tuning¬†",
              "score": 1,
              "created_utc": "2026-01-23 00:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15vh4u",
                  "author": "Educational_Rent1059",
                  "text": "I'm running WSL on 2  machines  with 4 gpus never had a single issue in years. I think the issue is either your bad/faulty hardware (probably memory or something else) or your environment/drivers. Yeah, just blaming WSL randomly for a crash without any input into what CAUSED the crash is not helpful at all - rather misleading.",
                  "score": 1,
                  "created_utc": "2026-01-23 02:08:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14t3zp",
          "author": "yoracale",
          "text": "We have this Windows guide which we revamped around a month ago, especially for WSL: [https://unsloth.ai/docs/get-started/install/windows-installation](https://unsloth.ai/docs/get-started/install/windows-installation)",
          "score": 2,
          "created_utc": "2026-01-22 22:41:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13lzmu",
          "author": "immediate_a982",
          "text": "Use virtual environments",
          "score": 1,
          "created_utc": "2026-01-22 19:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o145msj",
              "author": "LahmeriMohamed",
              "text": "did not work",
              "score": 0,
              "created_utc": "2026-01-22 20:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13mp3c",
          "author": "CMPUTX486",
          "text": "Use docker.. I think it save more time",
          "score": 1,
          "created_utc": "2026-01-22 19:18:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o145kpy",
              "author": "LahmeriMohamed",
              "text": "i am low on ram 1 * 8gb , so avoid it",
              "score": 0,
              "created_utc": "2026-01-22 20:45:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18rmc7",
                  "author": "StardockEngineer",
                  "text": "You don‚Äôt have enough system in any case.  Use colabs",
                  "score": 3,
                  "created_utc": "2026-01-23 14:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmhzg4",
      "title": "How to train vision model with IterableDataset?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qmhzg4/how_to_train_vision_model_with_iterabledataset/",
      "author": "willzocken",
      "created_utc": "2026-01-25 12:35:28",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hello I‚Äôm trying to create a IterableDataset with images to train a vision model (currently \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\").\n\nIf I use \\`Dataset.from\\_generator\\` it works, but it also loads all the training data into RAM before continuing, but my training data exceeds my 64 GB RAM I have on my disposal at the moment.\n\n    # dataset = Dataset.from_generator(Template.single_dataset)\n    dataset = IterableDataset.from_generator(Template.single_dataset)\n\nThis is my generator function:\n\n        u/staticmethod\n        def single_dataset() -> Iterator[ConversationDict]:\n            \"\"\"\n            Create template used to train 'kuzushiji-single' model\n            \"\"\"\n            conn = sql.connect(Path(\"output\") / \"single.db\")\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT * FROM prompts LIMIT 100\")\n            batch_size = 100\n    \n            while True:\n                rows: list[sql.Row] = cursor.fetchmany(batch_size)\n                if not rows:\n                    break\n                for row in rows:\n                    image = Image.open(io.BytesIO(row[1])).convert(\"RGB\")\n                    image_buffer = io.BytesIO()\n                    image.save(image_buffer, format=\"PNG\")\n                    image_bytes = image_buffer.getvalue()\n                    yield {\n                        \"messages\": [\n                            {\n                                \"role\": \"user\",\n                                \"content\": [\n                                    {\n                                        \"type\": \"text\",\n                                        \"text\": Template.single_instruction(),\n                                    },\n                                    {\n                                        \"type\": \"image\",\n                                        \"image\": image_bytes,\n                                    },\n                                ],\n                            },\n                            {\n                                \"role\": \"assistant\",\n                                \"content\": [\n                                    {\n                                        \"type\": \"text\",\n                                        \"text\": f\"{row[2]}\",\n                                    },\n                                ],\n                            },\n                        ],\n                    }\n    \n            conn.close()\n\nIf I use the value of the variable \\`image\\`, in other words just the PIL.Image or the \\`image\\_bytes\\` it works with \\`Dataset\\` but fails with \\`IterableDataset\\` even though they both create the same shape of data. For example here the first item of the dataset:\n\n    {'messages': [{'content': [{'image': None, 'text': \"You are an expert in reading old japanese handwritten kuzushiji characters. You will get an image of a kuzushiji character and you will give me only the correct modern japanese character. Nothing more. You'll always answer with just one single japanese character. May it be kanji or kana.\", 'type': 'text'}, {'image': b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x02\\x00\\x00\\x00\\xfdoH\\xc3\\x00\\x00\\x02VIDATx\\x9c\\xad\\x951h\\xf2@\\x18\\x86\\xef4\\xd8d\\xb2:\\x15\\x1b\\x82-\\xe2\\x81\\x83H[\\xd0*ZC\\x11\\x1c\\x1c\\x1c\\x1d\\xec\\xee\\xe2(\\x0eB'W\\x1d\\xdd:v)\\x142t\\xe9\\xd6\\x82\\x82\\x1ah\\x1d\\x12(RJT\\xec\\xe2\\xa6\\x92\\xf6\\x84\\xe4:\\xc8\\x1f~(\\x9a\\x1a}\\xb6\\xbb|\\xf7\\xe4\\xcd]\\xf2\\x05\\x80\\xb5\\xa4R)Y\\x96\\x11B^\\xaf\\xf7\\xf9\\xf9\\xf9\\xec\\xecl}\\xbd9\\xd1ht2\\x99\\xf0<\\xbf\\x1c&\\x93\\xc9\\x8f\\x8f\\x0f\\xa7\\xd3i\\xddH\\xd3\\xb4,\\xcb\\xb1X\\xcc\\x98a\\x18f<\\x1e_]]\\x99\\xae\\xa5V]@\\x08\\xbd\\xbe\\xbe\\xb6Z-\\x00\\x00\\x84\\xd0\\xe7\\xf3a\\x8c;\\x9d\\x8e\\xa6i\\xd6\\xa5\\x1c\\xc7)\\x8a\\xc2\\xb2l$\\x12\\xc9\\xe5r\\xd9lv6\\x9b\\xbd\\xbd\\xbd\\t\\x82`*]\\t\\xcf\\xf3\\x9a\\xa6}\\x7f\\x7f\\x13BDQ\\xacV\\xab\\x1e\\x8f\\xa7\\xddnW*\\x95H$\\x92H$\\xfc~\\xff\\xc6R\\x08a\\xa9Tj4\\x1a\\xe9t\\xda\\xe1p,'\\x05A \\xffh\\xb7\\xdb\\xd6#\\xffO\\xaf\\xd7#\\x84,\\x16\\x8b\\xfb\\xfb{\\x84\\xd0\\xb6:\\xa7\\xd3\\xd9h4TU\\xadV\\xab\\xa1P\\x08Bh\\xc5\\x02!dY\\x16\\x00@\\xd3t>\\x9f\\xff\\xfc\\xfc\\xd4u\\xfd\\xeb\\xebk\\xab\\x80\\xc1`P\\x96\\xe5z\\xbd>\\x1a\\x8dF\\xa3Q\\xb9\\\\\\xbe\\xbc\\xbc\\xd4u\\xfd\\xe4\\xe4\\xc4\\xba4\\x1e\\x8fK\\x924\\x9f\\xcf\\x05A8::\\x02\\x000\\x0c3\\x9f\\xcf/..\\xacK\\x01\\x00{{{.\\x97\\xcb\\xd8>\\x08\\xe1\\xfb\\xfb{\\xb1X4]\\xb8\\xf2\\xe5\\x07\\x00`\\x8c1\\xc6\\xc6\\x90\\x10\\xa2(\\x8a\\xdb\\xed6\\x95\\xdaL+\\x0cX\\x96\\xb5\\xd9l\\x7f9\\xf7?I\\xedv\\xfb\\xf5\\xf5\\xf5`0H&\\x93\\xaa\\xaa\\xfe=\\xc7J(\\x8az||4>$Q\\x14\\xf7\\xf7\\xf7\\xb7\\x95\\x06\\x02\\x81\\xe9t\\x8a16\\xbc\\xb7\\xb7\\xb76\\xdb\\xbaG\\\\wPK\\xce\\xcf\\xcf1\\xc6\\x14E\\x01\\x00\\x1e\\x1e\\x1e\\x08!\\xb9\\\\\\xee\\xe5\\xe5\\xa5V\\xabYOzzz:\\x1c\\x0e\\t!\\x92$\\x1d\\x1c\\x1cp\\x1c\\x871~zz\\xb2n\\\\\\xe2\\xf5zonn\\x8c^\\xd7j\\xb5\\xc6\\xe3\\xf1\\xfa\\x1d\\xd8\\x98\\xbb\\xbb\\xbb\\xe9tj\\xf4\\xc3\\xdfX\\xb9\\xdb\\xe1\\xe1a\\xb7\\xdb],\\x16;\\x93:\\x1c\\x8e\\xe3\\xe3\\xe3\\xe5\\xbfkg \\x84L{\\xd5\\xc6I3\\x99L\\xbf\\xdf\\x97$i\\x8b`\\xbfh6\\x9b\\x85Ba\\x97\\xc6p8\\xac\\xaa\\xaa\\xcf\\xe7[_\\xf6\\x03\\xd5W\\x08\\x12\\xaa'\\x16T\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\", 'text': None, 'type': 'image'}], 'role': 'user'}, {'content': [{'image': None, 'text': '„Åæ', 'type': 'text'}], 'role': 'assistant'}]}\n\nIf throughly checked it the is literally no difference between Dataset and IterableDataset when it comes to the shape of the data, but if I remove the image field then I can train with an IterableDataset!\n\nBut the moment I start training with an IterableDataset with an image field I get this cryptic error message:\n\n    ‚îÇ /home/kinski/Projects/kuzushiji/.venv/lib/python3.12/site-packages/torch/_tensor.py:1030 in split ‚îÇ\n    ‚îÇ ‚îÇ\n    ‚îÇ 1027 ‚îÇ ‚îÇ if isinstance(split_size, (int, torch.SymInt)): ‚îÇ\n    ‚îÇ 1028 ‚îÇ ‚îÇ ‚îÇ return torch._VF.split(self, split_size, dim) # type: ignore[attr-defined] ‚îÇ\n    ‚îÇ 1029 ‚îÇ ‚îÇ else: ‚îÇ\n    ‚îÇ ‚ù± 1030 ‚îÇ ‚îÇ ‚îÇ return torch._VF.split_with_sizes(self, split_size, dim) ‚îÇ\n    ‚îÇ 1031 ‚îÇ ‚îÇ\n    ‚îÇ 1032 ‚îÇ def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None): ‚îÇ\n    ‚îÇ 1033 ‚îÇ ‚îÇ r\"\"\"Returns the unique elements of the input tensor. ‚îÇ\n    ‚îÇ ‚îÇ\n    ‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ locals ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ\n    ‚îÇ ‚îÇ dim = 2 ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ self = tensor([[[[-4.7302e-03, -1.0620e-02, 5.5176e-02, ..., -1.6113e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -3.7994e-03, -4.0527e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 3.3936e-02, -9.5215e-03, -2.7466e-04, ..., -4.1260e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -2.6611e-02, -4.4434e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 1.6937e-03, 2.5513e-02, 2.7588e-02, ..., -1.2109e-01, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -7.6294e-03, -2.2583e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ..., ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[-1.6846e-02, -1.7212e-02, -1.0620e-02, ..., 8.4229e-03, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 5.0049e-02, -2.3828e-01]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 1.0559e-02, 9.8267e-03, 9.1553e-03, ..., -3.0884e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 3.9795e-02, -6.4697e-03]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[-2.5879e-02, 2.8442e-02, -8.4961e-02, ..., 3.3203e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 4.9072e-02, -2.8711e-01]]]], device='cuda:0', dtype=torch.bfloat16) ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ split_size = [16] ‚îÇ ‚îÇ\n    ‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ\n    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n    RuntimeError: split_with_sizes expects split_sizes to sum exactly to 1 (input tensor's size at dimension 2), but got\n    split_sizes=[16]\n\nDoes someone maybe know what I‚Äôm missing or what I‚Äôm doing wrong? Thanks in advance for your help!!!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qmhzg4/how_to_train_vision_model_with_iterabledataset/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o1ndo6w",
          "author": "mmathew23",
          "text": "Are you using UnslothVisionDataCollator? Trl handles iterabledatasets differently than regular Datasets. When it sees iterabledatasets it switches to a shared data loader that assumes the first index is the batch index. This assumption breaks for the qwen series vl models. You have to disable dispatch_batches and split_batches in the accelerator_config.",
          "score": 1,
          "created_utc": "2026-01-25 17:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1oavuc",
              "author": "willzocken",
              "text": "Thats it!!!! Thank you!  \nYes I was using UnslothVisionDataCollator.\n\n  \nAnother question how do you know this? :D\n\nI guess its somewhere written inside the docs of trl? Because I couldn't find it inside huggingface's datasets as well as in the unsloth docs",
              "score": 1,
              "created_utc": "2026-01-25 19:27:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1qkalo",
                  "author": "mmathew23",
                  "text": "accelerator_config is in the transformers documentation under TrainingArguments and the actual keywords should be listed in the documentation for accelerate.",
                  "score": 1,
                  "created_utc": "2026-01-26 01:41:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}