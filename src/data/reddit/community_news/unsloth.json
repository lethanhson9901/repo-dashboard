{
  "metadata": {
    "last_updated": "2026-01-29 17:09:57",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 12,
    "total_comments": 73,
    "file_size_bytes": 96145
  },
  "items": [
    {
      "id": "1qo5y54",
      "title": "DeepSeek releases DeepSeek-OCR 2. üêã",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/rjkucnsh3ufg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-27 06:15:19",
      "score": 300,
      "num_comments": 31,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qo5y54/deepseek_releases_deepseekocr_2/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1zg0su",
          "author": "GosuGian",
          "text": "Downloading now..",
          "score": 14,
          "created_utc": "2026-01-27 08:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20copo",
              "author": "danielhanchen",
              "text": "Let us know how it does!",
              "score": 2,
              "created_utc": "2026-01-27 13:10:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z7ocu",
          "author": "larrytheevilbunnie",
          "text": "Outperforming Gemma 3 pro is crazy",
          "score": 5,
          "created_utc": "2026-01-27 07:31:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z8wv5",
              "author": "Inflation_Artistic",
              "text": "Gemma 3 pro?",
              "score": 5,
              "created_utc": "2026-01-27 07:42:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z8ywk",
                  "author": "larrytheevilbunnie",
                  "text": "Gemini* oops\n\nI want Gemma 4 tho",
                  "score": 9,
                  "created_utc": "2026-01-27 07:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20cs28",
              "author": "danielhanchen",
              "text": "Ye it's pretty good for the reduction in edit distance!\n\nI'm sure Gemini 3 Pro does better on other image benchmarks though!",
              "score": 1,
              "created_utc": "2026-01-27 13:10:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zixpy",
          "author": "PaceZealousideal6091",
          "text": "Lcpp support?",
          "score": 3,
          "created_utc": "2026-01-27 09:15:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zjrin",
              "author": "yoracale",
              "text": "Not yet unfortunately. there was a PR back in nov 2025 but it never got merged",
              "score": 5,
              "created_utc": "2026-01-27 09:22:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o20f6ku",
          "author": "Cautious-Raccoon-364",
          "text": "This is completely insane, wow. Gonna try it now.",
          "score": 3,
          "created_utc": "2026-01-27 13:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22m80d",
              "author": "Cautious-Raccoon-364",
              "text": "Ok, tried it, worked well. I just hate the pdf to image conversion needed? Why not natively support pdf? I‚Äôm sure u could fine tune it further for your datasets, but still finding Claude a little better for bank document due diligence.",
              "score": 1,
              "created_utc": "2026-01-27 19:22:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zqgsg",
          "author": "Intelligent-Form6624",
          "text": "ROCm or vulkan?",
          "score": 2,
          "created_utc": "2026-01-27 10:24:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o207bbc",
              "author": "yoracale",
              "text": "Should work",
              "score": 2,
              "created_utc": "2026-01-27 12:36:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21lc80",
          "author": "arman-d0e",
          "text": "This feels like fate. I just started using deepseek ocr yesterday üò≠",
          "score": 2,
          "created_utc": "2026-01-27 16:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21si3w",
          "author": "vertigo235",
          "text": "Does it handle checkboxes on forms?",
          "score": 1,
          "created_utc": "2026-01-27 17:15:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27zxqm",
              "author": "yoracale",
              "text": "Yes it should be able to, due to its new reading ability",
              "score": 1,
              "created_utc": "2026-01-28 14:44:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2aeyaa",
                  "author": "vertigo235",
                  "text": "Yeah seems promising",
                  "score": 1,
                  "created_utc": "2026-01-28 21:07:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21z6dl",
          "author": "AntiqueAndroid0",
          "text": "How is this compared to mistral ocr3?",
          "score": 1,
          "created_utc": "2026-01-27 17:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2800gi",
              "author": "yoracale",
              "text": "Unsure but it 'should' be better.",
              "score": 1,
              "created_utc": "2026-01-28 14:45:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o221xb6",
          "author": "samplebitch",
          "text": "Anyone know if this model knows how to generate bounding boxes?",
          "score": 1,
          "created_utc": "2026-01-27 17:55:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28586w",
              "author": "yoracale",
              "text": "I'm not sure if it'll generate bounding boxes, but it will detect them yes",
              "score": 1,
              "created_utc": "2026-01-28 15:10:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o224v6m",
          "author": "eagledoto",
          "text": "Is it a vlm? Can we use it instead of qwen 3 vl in comfy to generate prompts from images?",
          "score": 1,
          "created_utc": "2026-01-27 18:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2851bx",
              "author": "yoracale",
              "text": "Yes it is a vision model but cannot replace Qwen 3 vl",
              "score": 1,
              "created_utc": "2026-01-28 15:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o285cgk",
                  "author": "eagledoto",
                  "text": "But can we use it in ComfyUI?",
                  "score": 1,
                  "created_utc": "2026-01-28 15:10:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2a7tx8",
          "author": "Outrageous-Phase-786",
          "text": "I wonder if it is also well performing with handwriting... any result on that?",
          "score": 1,
          "created_utc": "2026-01-28 20:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c2ypz",
          "author": "6969its_a_great_time",
          "text": "Vllm example doesn‚Äôt work",
          "score": 1,
          "created_utc": "2026-01-29 02:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ccvan",
              "author": "yoracale",
              "text": "Oh whoops it doesn't work because the PR is still open. You'll need to use transformers v5 or the Unsloth guide",
              "score": 1,
              "created_utc": "2026-01-29 03:00:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dui2c",
          "author": "someone383726",
          "text": "Anyone get this working in vLLM?  I tried nightly and a PR version that had a fix and all I got were garbage outputs.",
          "score": 1,
          "created_utc": "2026-01-29 09:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eop15",
          "author": "Late_Special_6705",
          "text": "How to launch this model? I don't want to write a python code. Maybe he works in ollama or vlm?",
          "score": 1,
          "created_utc": "2026-01-29 13:33:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zo5rt",
          "author": "loutishgamer",
          "text": "Does it have new coding and new texting generation coding and knowledge? Like when you're giving it a prompt does it give more new knowledge or coding or truth?",
          "score": 0,
          "created_utc": "2026-01-27 10:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o205cb0",
              "author": "Smilysis",
              "text": "That's a ocr model, not llm",
              "score": 2,
              "created_utc": "2026-01-27 12:23:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpbmrt",
      "title": "You can now run Kimi K2.5 locally!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/nwp8ammpf3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 13:40:33",
      "score": 136,
      "num_comments": 14,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qpbmrt/you_can_now_run_kimi_k25_locally/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o281v90",
          "author": "m98789",
          "text": "On a single H100 (80GB) VM with 256 GB of ram and 1TB of ssd and plenty of cpu cores, how fast in tokens / sec can we expect?",
          "score": 10,
          "created_utc": "2026-01-28 14:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284n8h",
              "author": "hudimudi",
              "text": "That would be very interesting to know",
              "score": 5,
              "created_utc": "2026-01-28 15:07:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o284ovk",
              "author": "yoracale",
              "text": "You can run the 2bit one possible with that. Maybe like 20 tokens?",
              "score": 4,
              "created_utc": "2026-01-28 15:07:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o28c5jm",
                  "author": "m98789",
                  "text": "So if this set up works, it‚Äôs about $3/hour on demand as an instance in a major cloud. So say we get Claude Opus level for $3/hour. And at 20 tokens a second, that‚Äôs 72,000 output tokens per hour. using the api for opus for that amount of tokens costs about $2. If using a reserved instance of the cloud vm, the cost goes to about $2/hour. So it‚Äôs a wash on cost effectiveness. But on privacy and control, it‚Äôs a win. Assuming quality is effectively the same.",
                  "score": 6,
                  "created_utc": "2026-01-28 15:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28ew1j",
          "author": "illkeepthatinmind",
          "text": "I mean.... \\*I\\* can't...",
          "score": 2,
          "created_utc": "2026-01-28 15:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28q2mv",
          "author": "Nooddlleee",
          "text": "Does anyone test the code quality? I am curious how it performs on the complex tasks and big projects",
          "score": 2,
          "created_utc": "2026-01-28 16:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28sxci",
          "author": "davidl002",
          "text": "Cannot imagine running it locally anytime soon but thanks for the work!\n\nHope in 5 years hardware can be affordable enough to run this locally cheaply.",
          "score": 2,
          "created_utc": "2026-01-28 16:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29lpma",
          "author": "emperorofrome13",
          "text": "That is the flux 2 dev of coding ai. Way too big to be useful.",
          "score": 2,
          "created_utc": "2026-01-28 18:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28h6dn",
          "author": "joninco",
          "text": "Did you apply any unsloth fixes? I ran the K2.5 from moonshot's hf page and was unimpressed with the coding results for creating a single page html tetris app. I used 1.0 temp, 0.95 topp.",
          "score": 1,
          "created_utc": "2026-01-28 16:03:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d0d8h",
              "author": "yoracale",
              "text": "There's no unsloth fixes for this model :(. Have you tried their API?",
              "score": 1,
              "created_utc": "2026-01-29 05:30:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28lhoq",
          "author": "nomorebuttsplz",
          "text": "Hi -- why is the K.2.5 Q 3 XL UD quite a bit larger than the same quant for K2 thinking?",
          "score": 1,
          "created_utc": "2026-01-28 16:22:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28p0fc",
          "author": "maxtheman",
          "text": "Would be VERY interested in the vision support, but already awesome work.",
          "score": 1,
          "created_utc": "2026-01-28 16:37:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql8nnq",
      "title": "For GLM-4.7-Flash TURN OFF REPEAT PENALTY!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1ql8nnq/for_glm47flash_turn_off_repeat_penalty/",
      "author": "yoracale",
      "created_utc": "2026-01-24 00:56:19",
      "score": 120,
      "num_comments": 39,
      "upvote_ratio": 1.0,
      "text": "I've seen and spoken to over 40 people and it seems a lot of people are still experiencing issues with GLM-4.7-Flash but after they disable repeat penalty or set it to 1.0, it all got solved.\n\nSo please, turn it off as it screws up the model badly and maybe set by default for you! [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nRemember\n\n* For general use-case:  `--temp 1.0 --top-p 0.95`\n* For tool-calling:  `--temp 0.7 --top-p 1.0`\n* If using llama.cpp, set `--min-p 0.01` as llama.cpp's default is 0.1\n* Repeat penalty: Disable it, or set `--repeat-penalty 1.0`\n\nLet us know if you're still receiving bad outputs after this (keep in mind sometimes you may get bad outputs or looping - as such with any other model like GPT5 or Gemini, this is normal, but if it happens a lot this isn't normal).\n\nHave a good friday and weekend!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1ql8nnq/for_glm47flash_turn_off_repeat_penalty/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o1codiv",
          "author": "ScoreUnique",
          "text": "Seems like that's what I needed to get my open coder going, thanks a lot. I'm on Unsloth Q5, yesterday I found them radically bad bur today they've made up for it after I use your recommended settings. Finally something that is very independent and agentic that runs locally.",
          "score": 7,
          "created_utc": "2026-01-24 01:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cqg7z",
              "author": "yoracale",
              "text": "Amazing to hear and glad it worked for you :)",
              "score": 1,
              "created_utc": "2026-01-24 02:04:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1d4pm3",
                  "author": "ClimateBoss",
                  "text": "how do you \"disable\" it ? 1.0 = disable ?",
                  "score": 1,
                  "created_utc": "2026-01-24 03:28:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dfw0l",
          "author": "Thrumpwart",
          "text": "Thank you. I set the temp, top-p, and min-p yesterday on the unsloth UD model. I ran into errors. Sat down just now, loaded up reddit, saw this post, and the model is humming nicely now in LM Studio.\n\nEdit: For anyone curious, I'm running the Unsloth Q6_K_XL UD quant in the latest LM Studio. I run ROCM 7.2 on an AMD W7900 on Ubuntu 24.04.3. I gave the model a 6200 token document, added a 20 word prompt, and I got 4.13s tts and 25.16 tok/sec. The quality is very, very good for its size - the output is clear, concise, and very relevant to my word prompt.",
          "score": 3,
          "created_utc": "2026-01-24 04:41:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e6nbf",
              "author": "danielhanchen",
              "text": "Nice to hear that!",
              "score": 2,
              "created_utc": "2026-01-24 08:17:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eg5qr",
          "author": "gigascake",
          "text": "I hope this model where Blackwell pro 6000 based vllm.\nBut very slower then glm-4.5-air-fp8.\nWhat problem this model architecture? 80 token/s vs 10 token/s\nüò≠",
          "score": 3,
          "created_utc": "2026-01-24 09:44:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1en8ia",
              "author": "TaroOk7112",
              "text": "With llama.cpp we needed several days of improvements until GLM-4.7-Flash worked more or less fast and reliably.  \nI'm amazed at how fast and well still runs gpt-oss 120B. But this one is getting closer at half the size, very nice!",
              "score": 2,
              "created_utc": "2026-01-24 10:49:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1epay6",
                  "author": "Mr_Back",
                  "text": "GPT OSS 120b works perfectly for me, even with a large context. I'm getting around 10 tokens per second. GLM 4.7 air is performing terribly. The speed is awful. The most reasonable speed is with q4, followed by f16, and surprisingly, the slowest speed is with q2. I set the context to 128k, and the requests were between 33k and 40k tokens, depending on the model. The most interesting thing is that q4 had a good processing speed, better than nemotron 3 nano, but a terrible output speed. With smaller requests, q4 glm produced acceptable results.",
                  "score": 1,
                  "created_utc": "2026-01-24 11:08:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ejhhe",
              "author": "yoracale",
              "text": "This was before llama.cpp introduced FA optimizations, it should be faster now\n\nCould you test again and see?",
              "score": 1,
              "created_utc": "2026-01-24 10:15:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fjea7",
          "author": "LightBrightLeftRight",
          "text": "I was having all kinds of problems with loops on LM Studio, but it turns out it their built-in tools (wikipedia) just werent returning data to GLM properly. As soon as I turned their tools off with your general use settings it worked brilliantly.\n\nAlso, thank you unsloth, we love you",
          "score": 3,
          "created_utc": "2026-01-24 14:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jj5e8",
              "author": "danielhanchen",
              "text": "Oh great! Nice to hear that and thanks!",
              "score": 1,
              "created_utc": "2026-01-25 02:06:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1d1x6s",
          "author": "some_user_2021",
          "text": "Newbie here. Can't these default settings be part of the model?",
          "score": 2,
          "created_utc": "2026-01-24 03:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d6uhs",
              "author": "yoracale",
              "text": "You can only set it in some palces like Ollama, you cant set it auto for GGUFs in LM Studio.\n\nBut the issue is GLM Flash GGUFs don't work in Ollama, so unfortunately you have to manually set it.",
              "score": 4,
              "created_utc": "2026-01-24 03:41:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ha9xk",
                  "author": "iadanos",
                  "text": "So, these params are not taken from gguf by llama.cpp by default?\n\n\nIf no, I have a cheap room for improvement! :)",
                  "score": 1,
                  "created_utc": "2026-01-24 19:23:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1hf7fx",
                  "author": "debackerl",
                  "text": "Wow I must have missed something. I saw that Ollama has GLM 4.7 Flash in their library, so do you mean that Llama.cpp and Ollama start to be incompatible ? Like different GGUF format?",
                  "score": 1,
                  "created_utc": "2026-01-24 19:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1edt4e",
          "author": "cangaroo_hamam",
          "text": "(newbie) Is this something that can be set in LM Studio? I checked 'Edit model default parameters' but I don't see it.",
          "score": 1,
          "created_utc": "2026-01-24 09:23:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f2l4i",
              "author": "yoracale",
              "text": "repeat penalty should be here \n\nbelow are the default setttings. you have to turn it off or set it = 1.0\n\nhttps://preview.redd.it/s37xzradoafg1.png?width=644&format=png&auto=webp&s=9bfd8b62d742329478780d523957702f4dc89809",
              "score": 4,
              "created_utc": "2026-01-24 12:55:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ei1gb",
          "author": "Bluethefurry",
          "text": "still have issues with it where it will just repeat a token ad infinitum, even with repeat penalty off and the tool calling settings set, seems to be worse with thinking enabled but still sometimes happens with thinking off as well.",
          "score": 1,
          "created_utc": "2026-01-24 10:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f2n55",
              "author": "yoracale",
              "text": "mmm interesting, what platform are you currently using? llama.cpp?",
              "score": 1,
              "created_utc": "2026-01-24 12:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1g6ama",
                  "author": "Bluethefurry",
                  "text": "Yes.\n\nCUDA\\_VISIBLE\\_DEVICES=0,1 LLAMA\\_CACHE=/models/cache GGML\\_CUDA\\_GRAPH\\_OPT=1 /app/llama-server --port ${PORT} -hf unsloth/GLM-4.7-Flash-GGUF:Q4\\_K\\_XL --ctx-size 131072 --temp 1.0 --top-p 0.95 --min-p 0.01 --no-webui --no-warmup --jinja --main-gpu 0 --flash-attn auto --cache-reuse 128 --no-mmap --slot-save-path /models/cache/slots/ --parallel 1 --threads 1 --batch-size 4096\n\nRTX 3090 + RTX 3060 running b7819 (557515be1)\n\ni noticed i used the default temp/top-p here so i will try switching to the tool calling recommendation to see if i still get looping or not.\n\nEDIT: I solved it, removing all parameters but the essentials (no-webui, no-warmup, jinja, temp, ctx, top-p..) and leaving everything else as is fixed the model going ham, i guess its an issue with slots or cache, not sure.",
                  "score": 1,
                  "created_utc": "2026-01-24 16:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g5onw",
          "author": "JMowery",
          "text": "Have these settings set and I get infinite loops all the time with Pydantic AI with GLM 4.7 Flash via llama.cpp. Can't even do the simplest test response. No other model I have installed (around 2 dozen) has any issue.",
          "score": 1,
          "created_utc": "2026-01-24 16:26:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j0fys",
              "author": "yoracale",
              "text": "Have you tried it without pydantic? Which quant are you using? When did you download the quants?",
              "score": 1,
              "created_utc": "2026-01-25 00:24:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1j0sk6",
                  "author": "JMowery",
                  "text": "Works fine without Pydantic AI. Using Q5KXL UD quant from Unsloth. Downloaded it yesterday.",
                  "score": 1,
                  "created_utc": "2026-01-25 00:26:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1h1ny9",
          "author": "TokenRingAI",
          "text": "I would be happy to use it, but tool calling is broken, so using VLLM until that is sorted\n\n[https://github.com/ggml-org/llama.cpp/issues/19009](https://github.com/ggml-org/llama.cpp/issues/19009)\n\nFWIW, the official model from Z doesn't include do\\_sample in the generation config, so it runs with a temperature 0, even though their docs say 0.7-1.0",
          "score": 1,
          "created_utc": "2026-01-24 18:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jjuev",
              "author": "danielhanchen",
              "text": "I added do_sample just into our upload!",
              "score": 1,
              "created_utc": "2026-01-25 02:10:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hqhbi",
          "author": "lundrog",
          "text": "Ill give it a shot. Anyone added tool support?",
          "score": 1,
          "created_utc": "2026-01-24 20:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hyvid",
              "author": "10F1",
              "text": "Worked fine for me with home assistant, used unsloth quants with llama-server.",
              "score": 1,
              "created_utc": "2026-01-24 21:17:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1i07zu",
                  "author": "lundrog",
                  "text": "Yeah was trying not to play with it and opencode",
                  "score": 1,
                  "created_utc": "2026-01-24 21:23:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kix2f",
          "author": "ga239577",
          "text": "u/yoracale I am trying to use it with Kilo Code and it asks follow up questions ... but after a few ... this happens:\n\nGreat, so you're targeting both individual and business customers. Let me ask a few more questions to better understand your business model:  \n\\[Tool Use: ask\\_followup\\_question\\]<arg\\_key>question</arg\\_key><arg\\_value>yaddayaddayadda</arg\\_value><arg\\_key>follow\\_up</arg\\_key><arg\\_value>\\[{\"text\": \"response1\", \"mode\": null}, {\"text\": \"response2\", \"mode\": null}, {\"text\": \"response3\", \"mode\": null}\\]</arg\\_value>\n\nNote: yaddayadda and the values response1, response2, response3 are just me replacing the actual response.\n\nAny way to fix this? I have seen similar things on other models when trying to use Kilo Code, Cline, etc. (not saying this is a common issue for me - most of my models are working just fine this way)\n\nEdit: For anyone who see this ... I switched back to Cline and the problem went away. Kind of amazing, because I switched from Cline to Kilo Code because Cline kept getting API errors. I guess this might just vary from model to model ...",
          "score": 1,
          "created_utc": "2026-01-25 05:40:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1g3jq7",
          "author": "NoahFect",
          "text": "Something I've been seeing with llama-server that I haven't noticed before is that the tokens/sec output rate steadily decreases from one prompt to the next.  Typically I'll hit 'New chat' each time I prompt it, and it will start out near 100 tokens/sec and eventually end up near 10 after a few chat sessions have been run.\n\nAnyone else seeing this behavior?  Hardware is 96GB Blackwell, command line is:\n\n    llama-server ^\n     --model GLM-4.7-Flash-BF16-00001-of-00002.gguf ^\n     --jinja              ^\n     --threads -1         ^\n     --ctx-size 131072    ^\n     --repeat-penalty 1.0 ^\n     --temp 1.0           ^\n     --top-p 0.95         ^\n     --min-p 0.01         ^\n     --port 2080          ^\n     --log-file glm47g.log\n\nAm I missing any useful/necessary parameters?\n\nHave to say, so far at least, GLM 4.7 Flash is not holding up very well next to Qwen (specifically Qwen30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf.)  That one has really been punching above its weight class.",
          "score": 1,
          "created_utc": "2026-01-24 16:16:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jjd2g",
              "author": "danielhanchen",
              "text": "Yes as context grows, the token processing speed does decrease somewhat - did you install the latest llama.cpp as well and use `--flash-attn on`",
              "score": 1,
              "created_utc": "2026-01-25 02:07:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1k0z4t",
                  "author": "NoahFect",
                  "text": "I'm a few weeks behind the llama.cpp repo, so will try updating next.\n\nDidn't realize the context from non-active chats still hung around in VRAM, but that does sound like the best explanation.",
                  "score": 1,
                  "created_utc": "2026-01-25 03:46:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1oyc1x",
          "author": "mtbMo",
          "text": "How do I achieve this with ollama? Rn run all my models with ollama wrapper",
          "score": 0,
          "created_utc": "2026-01-25 21:10:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qfsna",
              "author": "yoracale",
              "text": "Use Ollama default upload. Atm external GLM flash models don't work properly due to potential chat template incompatibility issues",
              "score": 1,
              "created_utc": "2026-01-26 01:18:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qndg98",
      "title": "New FP8 GLM-4.7-Flash Unsloth Dynamic Quants for vLLM, SGLang",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qndg98/new_fp8_glm47flash_unsloth_dynamic_quants_for/",
      "author": "danielhanchen",
      "created_utc": "2026-01-26 11:23:41",
      "score": 66,
      "num_comments": 58,
      "upvote_ratio": 1.0,
      "text": "Hey guys as we're already in the new year, throughout the rest of the year we hope to release FP4 and FP8 quants specifically designed for fast inference and deployment via vLLM! For performance, expect **13,000 tokens / s throughput on 1xB200 (130 token / s decoding per user)!** [FP8 quants here.](https://huggingface.co/unsloth/GLM-4.7-Flash-FP8-Dynamic)\n\nThese are **dynamically quantized** using our Dynamic methodology and in the near future, we aim to calibrate them using our dataset which is specifically designed for production, chat, coding, tool-calling and long context use-cases which shall improve the quality even further. And yes, we do intend to compare the difference via harder benchmarks like our previous Aider Polyglot benchmarks.\n\nWe also have a mini guide on how to exactly use the FP8 quants via vLLM here: [https://unsloth.ai/docs/models/glm-4.7-flash#glm-4.7-flash-in-vllm](https://unsloth.ai/docs/models/glm-4.7-flash#glm-4.7-flash-in-vllm) \\- vLLM is what we tested, and works well!\n\nThe code to serve for 4 GPUs is:\n\n    export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False\n    CUDA_VISIBLE_DEVICES='0,1,2,3' vllm serve unsloth/GLM-4.7-Flash-FP8-Dynamic \\\n        --served-model-name unsloth/GLM-4.7-Flash \\\n        --tensor-parallel-size 4 \\\n        --tool-call-parser glm47 \\\n        --reasoning-parser glm45 \\\n        --enable-auto-tool-choice \\\n        --dtype bfloat16 \\\n        --seed 3407 \\\n        --max-model-len 200000 \\\n        --gpu-memory-utilization 0.95 \\\n        --max_num_batched_tokens 16384 \\\n        --port 8000 \\\n        --kv-cache-dtype fp8\n\nNow my question to you guys is what would you like to see us do next for quantization? AWQ, INT4, or what? Thanks guys!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qndg98/new_fp8_glm47flash_unsloth_dynamic_quants_for/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o1szd0p",
          "author": "debackerl",
          "text": "Awesome! I'm using FP8 a lot. Maybe mxfp4?",
          "score": 6,
          "created_utc": "2026-01-26 12:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1szv21",
              "author": "danielhanchen",
              "text": "Yes NXFP4 is next!! Or would MXFP4 be better?",
              "score": 5,
              "created_utc": "2026-01-26 12:10:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tf6pr",
                  "author": "Acceptable-State-271",
                  "text": "mxfp4 is better, and you are best",
                  "score": 5,
                  "created_utc": "2026-01-26 13:46:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1u9wir",
                  "author": "debackerl",
                  "text": "MXFP4 please. As far as I know, NVFP4 is for RTX 50xx only. At least, MXFP4 still runs on 40xx.",
                  "score": 3,
                  "created_utc": "2026-01-26 16:12:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1wexvt",
                  "author": "eleqtriq",
                  "text": "No!  NVFP4!",
                  "score": 2,
                  "created_utc": "2026-01-26 21:44:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1t6w8t",
          "author": "thepetek",
          "text": "AWQ with an example guide of how you did it would be awesome!",
          "score": 6,
          "created_utc": "2026-01-26 12:58:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tdmyg",
              "author": "danielhanchen",
              "text": "Oh ok! Thanks for the suggestion!",
              "score": 3,
              "created_utc": "2026-01-26 13:38:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1y5q49",
              "author": "Phaelon74",
              "text": "There's a boat load of documentation in llm_compressor for how to do this and I just pr'd glm4.7 modeling, so it's easy for you to do now.",
              "score": 1,
              "created_utc": "2026-01-27 03:06:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vagus",
          "author": "mister2d",
          "text": "Here's a grenade: why do vLLM AND SGLang exist? They appear very similar when you use them.",
          "score": 3,
          "created_utc": "2026-01-26 18:48:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xlr2s",
              "author": "danielhanchen",
              "text": "They're both great! :) I think both can co-exist - SGLang is probably slightly faster in some cases, whilst vLLM has more support for archs",
              "score": 4,
              "created_utc": "2026-01-27 01:17:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xpweo",
                  "author": "mister2d",
                  "text": "I wish I were smart enough to create a meaningful PR to merge the codebases. üòÑ",
                  "score": 2,
                  "created_utc": "2026-01-27 01:40:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1y4y9a",
              "author": "Phaelon74",
              "text": "Ummm. Because they are for different user bases.  Vllm is for batching and sglang is the need for speed.  Different use cases and user bases.",
              "score": 2,
              "created_utc": "2026-01-27 03:02:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24my9c",
              "author": "kripper-de",
              "text": "vLLM is designed for massive user chat and SGLang is designed for agentic multi-turn with big context use cases. But they are converging in the same direction, adopting each other's advantages.",
              "score": 2,
              "created_utc": "2026-01-28 01:06:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24ueen",
                  "author": "mister2d",
                  "text": "Thanks! I do see the overlap. Perhaps one day the two shall become one for the sake of simplicity.",
                  "score": 1,
                  "created_utc": "2026-01-28 01:46:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o20vtvy",
          "author": "Phaelon74",
          "text": "AWQ is very different to GGUF, as the later is more mathmatical, and the former has important aspects as it relates to Datasets, etc.\n\nIf you do decide to make AWQs, there are many differences that you need to be super plugged into versus GGUFs, in order of importance:  \n1). Your Calibration Dataset is the MOST important thing when it comes to AWQs.  There's several papers out now talking through this.  Make sure you choose 10-20 datasets, melded based on what you want to capture.  You want to be really good at Coding?  Build a software engineering Dataset.  Creative Writing?  Build a dataset with lots of Creative writing examples.  \n2). If doing an MOE, make sure ALL experts are activated on every calibration sample.  Generally, without a model file, only the parts of experts that relate to a sample, will be activated.  You need to force every layer to be activated.  \n3). Everything you read about Calibration Samples and Sequence length are wrong.  Your starting should be 512 Calib Samples, and 2048 Seq length.  Anything shorter on either of these WILL see accuracy degrade  \n4). Make sure to understand Group Size and how it affects Accuracy  \n5). IGNORE LM\\_HEAD.  In GGUF land ya'll quant it at 6/Q6, etc.  Quanting LM\\_HEAD will reduce/degrae accuracy.  \n6). GLM4.7\\_FLASH does not natively work with AWQ due to layer mappings.  You'll have a lot of coding to do, to make it work.\n\nSeveral of us are already working through this with LLM\\_Compressor, so if you lock in, rock sauce.  Welcome to the bizarre land of AWQ.",
          "score": 3,
          "created_utc": "2026-01-27 14:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pbes",
              "author": "danielhanchen",
              "text": "Oh thank you so much for these pointers! This is extremely helpful!\n1. Agreed calib data is extremely important!\n2. Yes actually llama.cpp also has this issue - if too few experts are activated, it's always best to get more calib data\n3. Agreed! Interestingly our calib data we use for imatrix is in the order of 18K to 30K context length!\n4. Oh interesting will check\n5. Oh ok thanks!\n6. Ooo hmm I'll check that out!",
              "score": 1,
              "created_utc": "2026-01-28 09:28:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wma5i",
          "author": "meganoob1337",
          "text": "Would love awq  int4  ,\nThank you for your work!\n\nDid the performance issues in vllm get fixed yet btw? Last time I tried the model it was super slow üò≠ the outputs were good though",
          "score": 2,
          "created_utc": "2026-01-26 22:18:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1x1v7f",
              "author": "meganoob1337",
              "text": "Slow proportionate to models of the same size esp with higher context",
              "score": 2,
              "created_utc": "2026-01-26 23:34:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xlwzo",
                  "author": "danielhanchen",
                  "text": "Oh see https://unsloth.ai/docs/models/glm-4.7-flash#vllm-glm-4.7-flash-speculative-decoding which might be relative - we found spec decoding via the MTP module to slow things by 10x - disabling it on Blackwell can get 130 tokens / s per user decoding speed now",
                  "score": 1,
                  "created_utc": "2026-01-27 01:18:27",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1y5jvn",
              "author": "Phaelon74",
              "text": "We've got them for llm_compressor out there already.",
              "score": 1,
              "created_utc": "2026-01-27 03:05:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sveb2",
          "author": "Pentium95",
          "text": "Is '--dtype bfloat16' to be used with fp8 / fp4? \n\nAre there any PPL bench with those quants?",
          "score": 3,
          "created_utc": "2026-01-26 11:36:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sztxr",
              "author": "danielhanchen",
              "text": "Yes `--dtype bfloat16` is correct! This specifies the intermediate data-types.\n\nAnd yes - we're working on benchmarks! It'll hopefully come with our NVFP4 one - it should be similar to https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs",
              "score": 3,
              "created_utc": "2026-01-26 12:10:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tqkoa",
          "author": "Large-Example-1275",
          "text": "I'm looking to run the new **Unsloth FP8 GLM-4.7-Flash Dynamic Quants** on my **DGX Spark** (GB10). Given the strict requirements for FP8 support on Blackwell, I want to make sure I pick the right Docker approach to avoid compatibility headaches with vLLM versions and CUDA kernels.\n\nWhich route would you recommend?\n\n1. **Use the official NVIDIA vLLM container?** Is the `v25.12.post1-py3` image recent enough to handle this model/architecture combo out of the box? *Ref:*[*nvcr.io/nvidia/vllm:v25.12.post1-py3*](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/vllm?version=25.12.post1-py3)\n2. **Or build a custom container from the PyTorch base?** Is it safer to start with the PyTorch container and manually install `vllm-nightly` (CUDA 13) to ensure I have the latest kernels? *Ref:*[*nvcr.io/nvidia/pytorch:25.12-py3*](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch?version=25.12-py3)\n\nAny insights on the most stable workflow for this hardware would be appreciated",
          "score": 1,
          "created_utc": "2026-01-26 14:44:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxu9e",
              "author": "danielhanchen",
              "text": "I don't think stable vLLM works - you need the nightly one I think",
              "score": 1,
              "created_utc": "2026-01-26 17:55:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27ppsl",
                  "author": "1-a-n",
                  "text": "I hope they fix the nightly soon for sm-120, looks like a regression. In the mean time, is this only supported in vllm or might sglang also work perhaps?",
                  "score": 1,
                  "created_utc": "2026-01-28 13:52:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wmekr",
              "author": "Lyuseefur",
              "text": "vllm-studio helps",
              "score": 1,
              "created_utc": "2026-01-26 22:18:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xm37n",
                  "author": "danielhanchen",
                  "text": "vllm-studio??!! This is the first time I'm hearing about this haha",
                  "score": 1,
                  "created_utc": "2026-01-27 01:19:23",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vil8g",
          "author": "flobernd",
          "text": "Perfect timing! I‚Äôm just about to play with vLLM. So far I‚Äôve only used llama.cpp and a few forks. Hope GGUF support in vLLM will also improve since a lot of good quants (mostly unsloth) are GGUF. Especially Q3_K_XL is super cool for large models like DeepSeek. Don‚Äòt think I can fit the 4-bit vLLM compatible variants in VRAM.",
          "score": 1,
          "created_utc": "2026-01-26 19:22:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xmgg1",
              "author": "danielhanchen",
              "text": "Oh GGUF support might be complex in vLLM :( SGLang was actually working with us on it, but it'll be hard - we're planning to do dynamic 4bit quants so hopefully those will work!",
              "score": 1,
              "created_utc": "2026-01-27 01:21:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1y576z",
              "author": "Phaelon74",
              "text": "GGuf is unoptimized.  Do not use them with vllm.",
              "score": 1,
              "created_utc": "2026-01-27 03:03:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wmx86",
          "author": "Lyuseefur",
          "text": "I‚Äôm so confused. Is it 1,300 or 13,000 tokens a second - I see both numbers.\n\nIs it 1xB200 or 4xB200 required to run this ?",
          "score": 1,
          "created_utc": "2026-01-26 22:21:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xoa4w",
              "author": "danielhanchen",
              "text": "Oh I rechecked just to be 100% sure:\n\n1 x B200:\n\n1. Decoding speed per user is 130 tokens / s\n2. Total throughput on 1000 requests is in fact **13,000 tokens / s!!!**",
              "score": 2,
              "created_utc": "2026-01-27 01:31:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1y0db4",
                  "author": "Lyuseefur",
                  "text": "BRB, buying a B200 fleet of 100. 1,300,000 tokens a second on 10,000 Clawdbots here I come.",
                  "score": 1,
                  "created_utc": "2026-01-27 02:37:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o216zb4",
          "author": "Phaelon74",
          "text": "I'm confused, as your \"Dynamic\" quant approach, really is not relevant to FP8.  What specifically did you do, in FP8 quantization, where it mirrors your dynamic approach to GGUF?  Your FP8 dynamic would be identical to any other FP8, when approaching a normalized path, no?",
          "score": 1,
          "created_utc": "2026-01-27 15:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pdwk",
              "author": "danielhanchen",
              "text": "Oh some layers are actually upcasted to BF16 (the important ones :)\n\nI'm also thinking of calibrating the FP8 KV cache as well",
              "score": 1,
              "created_utc": "2026-01-28 09:29:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o224w46",
          "author": "quantier",
          "text": "If this is KV Cache fixed I would love to see AWQ quants! Also the REAP edition at 23B would be great to have functional solutions for",
          "score": 1,
          "created_utc": "2026-01-27 18:08:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pf7f",
              "author": "danielhanchen",
              "text": "Yes! I'll see what we can do for AWQ quants!",
              "score": 1,
              "created_utc": "2026-01-28 09:29:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o228qkx",
          "author": "StardockEngineer",
          "text": "I get errors that basically none of the backends suppport `--kv-cache-dtype fp8`.\n\n```\nValueError: No valid attention backend found for cuda with AttentionSelectorConfig(head_size=576, dtype=torch.bfloat16, kv_cache_dtype=fp8, block_size=None, use_mla=True, has_sink=False, use_sparse=False, use_mm_prefix=False, attn_type=AttentionType.DECODER). Reasons: {FLASH_ATTN_MLA: [kv_cache_dtype not supported, compute capability not supported, FlashAttention MLA not supported on this device], FLASHMLA: [compute capability not supported, FlashMLA Dense is only supported on Hopper devices.], FLASHINFER_MLA: [compute capability not supported, FlashInfer MLA kernel requires qk_nope_head_dim == 128, but got 192], TRITON_MLA: [kv_cache_dtype not supported], FLASHMLA_SPARSE: [kv_cache_dtype not supported, non-sparse not supported, compute capability not supported]}.\n[rank0]:[W127 18:15:45.494603369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n```\n\nAny ideas?  I've turned it off for now.  GPU is an RTX Pro 6000.",
          "score": 1,
          "created_utc": "2026-01-27 18:24:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22ahmy",
              "author": "StardockEngineer",
              "text": "lol container failed to start.  Too much compute capability I guess  \n\\`\\`\\`  \n(EngineCore\\_DP0 pid=153) NotImplementedError: No compiled cutlass\\_scaled\\_mm for CUDA device capability: 120. Required capability: 90 or 100  \n\\`\\`\\`",
              "score": 1,
              "created_utc": "2026-01-27 18:32:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26pnlj",
                  "author": "danielhanchen",
                  "text": "Wait that's weird hmmm so 10.X is B200 and 9.X is H100, H200. I actually have not tested on 12.X hmm maybe that's the issue? (RTX 50X, NVIDIA RTX PRO 6000, DGX Spark)",
                  "score": 1,
                  "created_utc": "2026-01-28 09:31:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24pqfg",
          "author": "kripper-de",
          "text": "Hi Daniel, have you tried doing task-aware pruning?\nI would love to see big models like Kiki K2.5 running on a Strix Halo 128GB by pruning all neurons that are not related to coding and agentic stuff.\n\nIt could even improve the quality.",
          "score": 1,
          "created_utc": "2026-01-28 01:21:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pokp",
              "author": "danielhanchen",
              "text": "Oh maybe REAP is maybe similar to this?",
              "score": 1,
              "created_utc": "2026-01-28 09:31:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26qf7s",
                  "author": "kripper-de",
                  "text": "But it prunes whole experts, while you are quantizing specific parameters.\nCan you also quantize to 0-bits (prune)? :-)",
                  "score": 1,
                  "created_utc": "2026-01-28 09:38:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v9hqg",
          "author": "TokenRingAI",
          "text": "MXFP8 or NVFP8 for the Blackwell crowd\n\nAlso, why in the world is this model so slow? 130 tokens/second decode is not that impressive for a model this size",
          "score": 0,
          "created_utc": "2026-01-26 18:44:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xm0td",
              "author": "danielhanchen",
              "text": "It's not fully opimized at this time, hence 130 tokens / s doesn't sound that crazy - I'm sure it'll be better though over time!",
              "score": 1,
              "created_utc": "2026-01-27 01:19:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qohf9o",
      "title": "Kimi-K2.5 Prelim Dynamic 2bit 4bit GGUFs out!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qohf9o/kimik25_prelim_dynamic_2bit_4bit_ggufs_out/",
      "author": "danielhanchen",
      "created_utc": "2026-01-27 15:46:41",
      "score": 41,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey everyone! we made some dynamic imatrix 1bit to 4bit ~~preliminary GGUFs~~ (now final release) for Kimi-K2.5! Currently they're text only (no vision yet) and the Dynamic 2bit, 4bit and normal 8bit quants are out at [https://huggingface.co/unsloth/Kimi-K2.5-GGUF](https://huggingface.co/unsloth/Kimi-K2.5-GGUF)\n\nHow to run dynamic 1bit:\n\n    LLAMA_SET_ROWS=1 ./llama.cpp/llama-cli \\\n        --model unsloth/Kimi-K2.5-GGUF/UD-TQ1_0/Kimi-K2.5-UD-TQ1_0-00001-of-00005.gguf \\\n        --temp 1.0 \\\n        --min_p 0.01 \\\n        --top-p 0.95 \\\n        --ctx-size 16384 \\\n        --seed 3407 \\\n        --fit on \\\n        --jinja\n\nGuide to run quants at [https://unsloth.ai/docs/models/kimi-k2.5](https://unsloth.ai/docs/models/kimi-k2.5)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qohf9o/kimik25_prelim_dynamic_2bit_4bit_ggufs_out/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o21lnfq",
          "author": "m98789",
          "text": "Is this possible to run on a single H100 server with plenty of RAM, CPU and disk?",
          "score": 2,
          "created_utc": "2026-01-27 16:45:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22dfeg",
              "author": "sjoerdmaessen",
              "text": "Yes, its literally in the link ‚ÄúYou need 247GB of disk space to run the 1bit quant!\n\nThe only requirement is disk space + RAM + VRAM ‚â• 247GB. That means you do not need to have that much RAM or VRAM (GPU) to run the model, but it will be much slower.‚Äù",
              "score": 4,
              "created_utc": "2026-01-27 18:44:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24gd29",
                  "author": "danielhanchen",
                  "text": "Yes the only requirement is (RAM + VRAM >= disk_space(GGUF)) then using `--fit on` in llama.cpp will optimally allocate space",
                  "score": 2,
                  "created_utc": "2026-01-28 00:33:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24ugxz",
          "author": "danielhanchen",
          "text": "Update: The imatrix dynamic quants are out! 1bit, 2bit are out",
          "score": 2,
          "created_utc": "2026-01-28 01:46:57",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqc06x",
      "title": "How to Run Local LLMs with Claude Code & OpenAI Codex!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/6mhzmpzd6bgg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-29 15:42:38",
      "score": 22,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qqc06x/how_to_run_local_llms_with_claude_code_openai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2fnep4",
          "author": "__Maximum__",
          "text": "Fine-tune?",
          "score": 2,
          "created_utc": "2026-01-29 16:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fr3lz",
              "author": "moonflowerseed",
              "text": "On Mac/Apple Silicon?",
              "score": 1,
              "created_utc": "2026-01-29 16:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2fre76",
                  "author": "yoracale",
                  "text": "We're working on Mac support for real. Optimizations are done, only thing next is checking, benchmarking and Integra tion",
                  "score": 1,
                  "created_utc": "2026-01-29 16:38:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2frhwe",
              "author": "yoracale",
              "text": "Yep fine-tune! We use glm flash to autonomously fine-tune an LLM with unsloth",
              "score": 1,
              "created_utc": "2026-01-29 16:38:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fnjti",
          "author": "toreobsidian",
          "text": "This is awesome. I'll Test this with Just a dataset I'm currently preparing that Features content of a famous german political figure. Too bad I have so little time for this nonsens Project but this should be a nice boost üòÖ",
          "score": 2,
          "created_utc": "2026-01-29 16:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fuma5",
          "author": "stuckinmotion",
          "text": "Oh what, so we don't need a proxy like Claude code router anymore?",
          "score": 1,
          "created_utc": "2026-01-29 16:52:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk109a",
      "title": "The best (tiny) model I can run on my phone",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qk109a/the_best_tiny_model_i_can_run_on_my_phone/",
      "author": "gized00",
      "created_utc": "2026-01-22 17:36:05",
      "score": 17,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "I work in ML and I am quite familiar with Llama, fine tuning, etc. but I always work on 10s of billions parameters. \n\nI would like to train a tiny model that I can run on my phone (Pixel 8) and unsloth seems the right place to start with this (but feel free to suggest other solutions). I have some difficulties to identify what can realistically run (with a decent num tokens/s). Is a 1B model a reasonable choice if I am quantizing it?\n\nAny other suggestions?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qk109a/the_best_tiny_model_i_can_run_on_my_phone/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o134ezq",
          "author": "BenniB99",
          "text": "I believe Gemma3n was made specifically with phone and edge usage in mind.  \nIt is also a pretty decent model for its size imo (and multimodal).  \nAfaik it is also possible to switch between 2B and 4B effective parameter on the fly with it.",
          "score": 14,
          "created_utc": "2026-01-22 17:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13ow28",
              "author": "gized00",
              "text": "Nice! I will take a look.\nThank you",
              "score": 2,
              "created_utc": "2026-01-22 19:27:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17icir",
          "author": "schlammsuhler",
          "text": "I like trinity nano a 6b moe with 1b active. Nice persona\n\nhttps://huggingface.co/arcee-ai/Trinity-Nano-Preview-GGUF",
          "score": 3,
          "created_utc": "2026-01-23 09:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14w8qp",
          "author": "Azuriteh",
          "text": "The best model for on-device usage right now is probably [https://huggingface.co/LiquidAI/LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B), with 4 bit quantization it'd probably be alright!",
          "score": 2,
          "created_utc": "2026-01-22 22:58:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1684aw",
          "author": "Sure_Explorer_6698",
          "text": "Ive used Llama-3 1-3B, and Qwen-2&3 1-3B with llama.cpp on:\n\nSamusng A16 4Gb, and Samsung S20FE 6Gb. Both work great, but Llama seems more conversational.",
          "score": 2,
          "created_utc": "2026-01-23 03:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13flhy",
          "author": "Late_Huckleberry850",
          "text": "If you get the Apollo app that has some. Idk if they have an android version or not though",
          "score": 1,
          "created_utc": "2026-01-22 18:46:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13ub5t",
          "author": "[deleted]",
          "text": "Function Gemma or maybe liquid foundation models will be best fit for it",
          "score": 1,
          "created_utc": "2026-01-22 19:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13woxx",
          "author": "maxtheman",
          "text": "I have a pixel 9 and am working on fine-tuning functionalgemma, which is working great, but it really depends on your task. 1B or less can work great on a distilled task, but don't expect 90%+ perf unless you overfit the shit out of it and consider doing multiple types of fine-tuning.\n\n  \nOn pixel the hardest part, for me at least, will be getting it on an api that can actually access your gpu. I am targeting huggingfacejs for now due to the ease of use, but I don't know a better way to deploy than that or get on the google npu.",
          "score": 1,
          "created_utc": "2026-01-22 20:03:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o141j0z",
              "author": "gized00",
              "text": "It's a fairly simple task but it will require a bit of RL",
              "score": 1,
              "created_utc": "2026-01-22 20:26:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1irrfq",
          "author": "PruneRound704",
          "text": "Try lfm 2 from liquid ai, launched recently, probably best at benchmarks and available on llama cpp",
          "score": 1,
          "created_utc": "2026-01-24 23:39:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lh8ef",
          "author": "East-Muffin-6472",
          "text": "Liquid Foundation Models for sure as the have good engines to let model run on phones",
          "score": 1,
          "created_utc": "2026-01-25 10:28:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1litfc",
              "author": "gized00",
              "text": "What do you mean with good engines? Do they have a specialized/optimized layer to serve requests to the model?",
              "score": 1,
              "created_utc": "2026-01-25 10:42:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1lj6oh",
                  "author": "East-Muffin-6472",
                  "text": "So I mean there has to be good optimised engines for models to reliably run on phones right so as to not hog all of its resources and to roved the models just the required stuff to run it efficiently and reliably on the said phone.\n\nPreviously it was just onnx this and that which really didn‚Äôt care about the devices stats but the L peeps did care about and this released a compatible engine for both ios ans android ig? To help with all the requirements as stated above",
                  "score": 1,
                  "created_utc": "2026-01-25 10:45:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o138iyq",
          "author": "Large-Example-1275",
          "text": "You can use the ¬´¬†Locally¬†¬ª app to check supported models on your device, but I don't know if it's available on Android.",
          "score": 0,
          "created_utc": "2026-01-22 18:15:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qocvl6",
      "title": "How to develop using Apple Sillicon?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qocvl6/how_to_develop_using_apple_sillicon/",
      "author": "growndemon",
      "created_utc": "2026-01-27 12:46:14",
      "score": 14,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hi,  \nI'm developing my codebase on my macbook and afterwards submit trainingjobs to a gpu cluster. However I can't create a virtual env with unsloth and thus don't have any ide support and also can't have a dry run with a small model to test my code.\n\nIs there any workflow / workaround that is recommended or widely used by apple users working with unsloth?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qocvl6/how_to_develop_using_apple_sillicon/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o20cj6g",
          "author": "danielhanchen",
          "text": "We're working on it as we speak! :) Unsloth on Mac will come!",
          "score": 10,
          "created_utc": "2026-01-27 13:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20f4pn",
              "author": "pokemonplayer2001",
              "text": "‚ù§Ô∏è",
              "score": 2,
              "created_utc": "2026-01-27 13:23:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20qde1",
                  "author": "A-Rahim",
                  "text": "u/pokemonplayer2001   \nMeanwhile, you may try this: [https://github.com/ARahim3/unsloth-mlx](https://github.com/ARahim3/unsloth-mlx)",
                  "score": 0,
                  "created_utc": "2026-01-27 14:22:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkntk1",
      "title": "RL for learning math",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qkntk1/rl_for_learning_math/",
      "author": "goldlord44",
      "created_utc": "2026-01-23 10:57:30",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "Hi there,\n\nI was wondering if anyone here has some advice for using unsloth to train models to be better at math?\n\nI am looking at using math text books and research papers to be able to post-train my models, specifically maths, physics and statistics. (And maybe some HF datasets).\n\nI am not sure which is the ideal post training technique for this and am looking for some direction advice before I dive head first into this.\n\nI am happy both with training on the raw text, but also understand that some post-processing is always required.\n\nI have a single Rtx Pro 6000 96GB so was hoping to train something like OSS-120B or some of the mid sized models like qwen3 30B.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qkntk1/rl_for_learning_math/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o17xu2v",
          "author": "yoracale",
          "text": "We have many RL notebooks for math, that might be a good starting point: [https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl](https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl)\n\nE.g. our Qwen3-Advanced GRPO notebook has a concrete example for math: [https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3\\_(4B)-GRPO.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)\n\nhttps://preview.redd.it/bh5w1be733fg1.png?width=2590&format=png&auto=webp&s=5f4cd2800213de88afd18c2b5d8d7dfec5959a1a",
          "score": 5,
          "created_utc": "2026-01-23 11:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a989h",
              "author": "samplebitch",
              "text": "FYI I think reddit messed up your link - here's the working URL for anyone else who might want to follow it:\n\nhttps://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb",
              "score": 2,
              "created_utc": "2026-01-23 18:35:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1b5kdr",
                  "author": "yoracale",
                  "text": "Oh thank you you're right, idk why reddit always does that üòÖ",
                  "score": 1,
                  "created_utc": "2026-01-23 21:06:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq0m1x",
      "title": "Guidance Needed: GPT-OSS 20B Fine-Tuning with Unsloth ‚Üí GGUF ‚Üí Ollama ‚Üí Triton (vLLM / TensorRT-LLM)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qq0m1x/guidance_needed_gptoss_20b_finetuning_with/",
      "author": "Double_Tourist3600",
      "created_utc": "2026-01-29 06:07:11",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "I am currently fine-tuning the **GPT-OSS 20B** model using **Unsloth** with **HuggingFace TRL (SFTTrainer)**.\n\n**Long-term goal**\n\n* Serve the model in production using **Triton** with either **vLLM** or **TensorRT-LLM** as the backend\n* **Short-term / initial deployment** using **Ollama (GGUF)**\n\n**Current challenge**  \nGPT-OSS uses a **Harmony-style chat template**, which includes:\n\n* `developer` role\n* Explicit EOS handling\n* `thinking` / `analysis` channels\n* Tool / function calling structure\n\nWhen converting the fine-tuned model to **GGUF** and deploying it in **Ollama** using the **default GPT-OSS Modelfile**, I am running into ambiguity around:\n\n1. Whether the **default Jinja chat template** provided by GPT-OSS should be **modified** for Ollama compatibility\n2. How to correctly handle:\n   * EOS token behavior\n   * Internal reasoning / analysis channels\n   * Developer role alignment\n3. How to do this **without degrading the model‚Äôs default performance or alignment**\n\n**Constraints / Intent**\n\n* I already have training data prepared strictly in **system / user / assistant** format\n* I want to:\n   * Preserve GPT-OSS‚Äôs native behavior as much as possible\n   * Perform **accurate, non-destructive fine-tuning**\n   * Avoid hacks that work short-term but break compatibility with **vLLM / TensorRT-LLM** later\n\n**What I‚Äôm looking for**\n\n* Has anyone successfully:\n   * Fine-tuned GPT-OSS\n   * Converted it to GGUF\n   * Deployed it with **Ollama**\n   * While preserving the Harmony template behavior?\n* If yes:\n   * Did you modify the **chat template / Modelfile**?\n   * How did you handle EOS + reasoning channels?\n   * Any pitfalls to avoid to keep it production-ready for Triton later?\n\nAny concrete guidance, references, or proven setups would be extremely helpful.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qq0m1x/guidance_needed_gptoss_20b_finetuning_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2d9qb5",
          "author": "max6296",
          "text": "https://github.com/openai/harmony",
          "score": 1,
          "created_utc": "2026-01-29 06:43:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk1qy4",
      "title": "Guide to use unsloth on windows",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qk1qy4/guide_to_use_unsloth_on_windows/",
      "author": "LahmeriMohamed",
      "created_utc": "2026-01-22 18:02:54",
      "score": 5,
      "num_comments": 15,
      "upvote_ratio": 0.78,
      "text": "hello guys hope i recieve help , i have recently installed unsloth to try it fine-tuning process , but due to dependencies  conflicts i had to remove it , if anyone can help me to fix this issue , my current env \npython 3.11.2\ntorch 2.5.1+cu121 \nif i ran install unsloth , it remove the cuda installation , so i used --no-deps instruction , but when running it , it require vllm , accelerate error .. .\ncan  you provide me with better/compatible versions ? \nthank you",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qk1qy4/guide_to_use_unsloth_on_windows/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o149lq3",
          "author": "Educational_Rent1059",
          "text": "Install WSL2 I highly recommend that. And hook up VSCode and you are good to go with Linux within windows without any overhead. The benchmarks in training vs native Ubuntu is between 1-4% diffs (native ahead) , if you do pure GPU work and ML, it's no diff at all.",
          "score": 3,
          "created_utc": "2026-01-22 21:04:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o149z5e",
              "author": "LahmeriMohamed",
              "text": "since i have 1ssd main system and hdd for env saving , can i save the wls env in the hdd and use it in env-variable to be accessible globaly ?",
              "score": 1,
              "created_utc": "2026-01-22 21:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14ag0l",
                  "author": "Educational_Rent1059",
                  "text": "Your WSL ext4 drive will be a single file that you can move anywhere into any HDD you like in windows. If you prefer to have it on a different disk etc that works as well. Everything you do will be stored within that single file (it's basically your entire OS and all files within it in one file on windows)",
                  "score": 2,
                  "created_utc": "2026-01-22 21:08:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o14aosb",
                  "author": "Educational_Rent1059",
                  "text": "You can also create another \"drive\" (single file) and attach it into linux, it's a bit messy as you need to attach it in windows command shell first, and then go into the linux WsL2 cli and attach it there too, but use GPT for guidance and do that. Be careful as so you don't delete any files by wrong instructions from GPT as they hallucinate alot",
                  "score": 1,
                  "created_utc": "2026-01-22 21:09:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15bmc0",
              "author": "fiery_prometheus",
              "text": "Except that wsl likes to crash under heavy workloads, I've had so many crashes when fine-tuning¬†",
              "score": 1,
              "created_utc": "2026-01-23 00:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15vh4u",
                  "author": "Educational_Rent1059",
                  "text": "I'm running WSL on 2  machines  with 4 gpus never had a single issue in years. I think the issue is either your bad/faulty hardware (probably memory or something else) or your environment/drivers. Yeah, just blaming WSL randomly for a crash without any input into what CAUSED the crash is not helpful at all - rather misleading.",
                  "score": 1,
                  "created_utc": "2026-01-23 02:08:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14t3zp",
          "author": "yoracale",
          "text": "We have this Windows guide which we revamped around a month ago, especially for WSL: [https://unsloth.ai/docs/get-started/install/windows-installation](https://unsloth.ai/docs/get-started/install/windows-installation)",
          "score": 2,
          "created_utc": "2026-01-22 22:41:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13lzmu",
          "author": "immediate_a982",
          "text": "Use virtual environments",
          "score": 1,
          "created_utc": "2026-01-22 19:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o145msj",
              "author": "LahmeriMohamed",
              "text": "did not work",
              "score": 0,
              "created_utc": "2026-01-22 20:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13mp3c",
          "author": "CMPUTX486",
          "text": "Use docker.. I think it save more time",
          "score": 1,
          "created_utc": "2026-01-22 19:18:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o145kpy",
              "author": "LahmeriMohamed",
              "text": "i am low on ram 1 * 8gb , so avoid it",
              "score": 0,
              "created_utc": "2026-01-22 20:45:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18rmc7",
                  "author": "StardockEngineer",
                  "text": "You don‚Äôt have enough system in any case.  Use colabs",
                  "score": 3,
                  "created_utc": "2026-01-23 14:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmhzg4",
      "title": "How to train vision model with IterableDataset?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qmhzg4/how_to_train_vision_model_with_iterabledataset/",
      "author": "willzocken",
      "created_utc": "2026-01-25 12:35:28",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "Hello I‚Äôm trying to create a IterableDataset with images to train a vision model (currently \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\").\n\nIf I use \\`Dataset.from\\_generator\\` it works, but it also loads all the training data into RAM before continuing, but my training data exceeds my 64 GB RAM I have on my disposal at the moment.\n\n    # dataset = Dataset.from_generator(Template.single_dataset)\n    dataset = IterableDataset.from_generator(Template.single_dataset)\n\nThis is my generator function:\n\n        u/staticmethod\n        def single_dataset() -> Iterator[ConversationDict]:\n            \"\"\"\n            Create template used to train 'kuzushiji-single' model\n            \"\"\"\n            conn = sql.connect(Path(\"output\") / \"single.db\")\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT * FROM prompts LIMIT 100\")\n            batch_size = 100\n    \n            while True:\n                rows: list[sql.Row] = cursor.fetchmany(batch_size)\n                if not rows:\n                    break\n                for row in rows:\n                    image = Image.open(io.BytesIO(row[1])).convert(\"RGB\")\n                    image_buffer = io.BytesIO()\n                    image.save(image_buffer, format=\"PNG\")\n                    image_bytes = image_buffer.getvalue()\n                    yield {\n                        \"messages\": [\n                            {\n                                \"role\": \"user\",\n                                \"content\": [\n                                    {\n                                        \"type\": \"text\",\n                                        \"text\": Template.single_instruction(),\n                                    },\n                                    {\n                                        \"type\": \"image\",\n                                        \"image\": image_bytes,\n                                    },\n                                ],\n                            },\n                            {\n                                \"role\": \"assistant\",\n                                \"content\": [\n                                    {\n                                        \"type\": \"text\",\n                                        \"text\": f\"{row[2]}\",\n                                    },\n                                ],\n                            },\n                        ],\n                    }\n    \n            conn.close()\n\nIf I use the value of the variable \\`image\\`, in other words just the PIL.Image or the \\`image\\_bytes\\` it works with \\`Dataset\\` but fails with \\`IterableDataset\\` even though they both create the same shape of data. For example here the first item of the dataset:\n\n    {'messages': [{'content': [{'image': None, 'text': \"You are an expert in reading old japanese handwritten kuzushiji characters. You will get an image of a kuzushiji character and you will give me only the correct modern japanese character. Nothing more. You'll always answer with just one single japanese character. May it be kanji or kana.\", 'type': 'text'}, {'image': b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x02\\x00\\x00\\x00\\xfdoH\\xc3\\x00\\x00\\x02VIDATx\\x9c\\xad\\x951h\\xf2@\\x18\\x86\\xef4\\xd8d\\xb2:\\x15\\x1b\\x82-\\xe2\\x81\\x83H[\\xd0*ZC\\x11\\x1c\\x1c\\x1c\\x1d\\xec\\xee\\xe2(\\x0eB'W\\x1d\\xdd:v)\\x142t\\xe9\\xd6\\x82\\x82\\x1ah\\x1d\\x12(RJT\\xec\\xe2\\xa6\\x92\\xf6\\x84\\xe4:\\xc8\\x1f~(\\x9a\\x1a}\\xb6\\xbb|\\xf7\\xe4\\xcd]\\xf2\\x05\\x80\\xb5\\xa4R)Y\\x96\\x11B^\\xaf\\xf7\\xf9\\xf9\\xf9\\xec\\xecl}\\xbd9\\xd1ht2\\x99\\xf0<\\xbf\\x1c&\\x93\\xc9\\x8f\\x8f\\x0f\\xa7\\xd3i\\xddH\\xd3\\xb4,\\xcb\\xb1X\\xcc\\x98a\\x18f<\\x1e_]]\\x99\\xae\\xa5V]@\\x08\\xbd\\xbe\\xbe\\xb6Z-\\x00\\x00\\x84\\xd0\\xe7\\xf3a\\x8c;\\x9d\\x8e\\xa6i\\xd6\\xa5\\x1c\\xc7)\\x8a\\xc2\\xb2l$\\x12\\xc9\\xe5r\\xd9lv6\\x9b\\xbd\\xbd\\xbd\\t\\x82`*]\\t\\xcf\\xf3\\x9a\\xa6}\\x7f\\x7f\\x13BDQ\\xacV\\xab\\x1e\\x8f\\xa7\\xddnW*\\x95H$\\x92H$\\xfc~\\xff\\xc6R\\x08a\\xa9Tj4\\x1a\\xe9t\\xda\\xe1p,'\\x05A \\xffh\\xb7\\xdb\\xd6#\\xffO\\xaf\\xd7#\\x84,\\x16\\x8b\\xfb\\xfb{\\x84\\xd0\\xb6:\\xa7\\xd3\\xd9h4TU\\xadV\\xab\\xa1P\\x08Bh\\xc5\\x02!dY\\x16\\x00@\\xd3t>\\x9f\\xff\\xfc\\xfc\\xd4u\\xfd\\xeb\\xebk\\xab\\x80\\xc1`P\\x96\\xe5z\\xbd>\\x1a\\x8dF\\xa3Q\\xb9\\\\\\xbe\\xbc\\xbc\\xd4u\\xfd\\xe4\\xe4\\xc4\\xba4\\x1e\\x8fK\\x924\\x9f\\xcf\\x05A8::\\x02\\x000\\x0c3\\x9f\\xcf/..\\xacK\\x01\\x00{{{.\\x97\\xcb\\xd8>\\x08\\xe1\\xfb\\xfb{\\xb1X4]\\xb8\\xf2\\xe5\\x07\\x00`\\x8c1\\xc6\\xc6\\x90\\x10\\xa2(\\x8a\\xdb\\xed6\\x95\\xdaL+\\x0cX\\x96\\xb5\\xd9l\\x7f9\\xf7?I\\xedv\\xfb\\xf5\\xf5\\xf5`0H&\\x93\\xaa\\xaa\\xfe=\\xc7J(\\x8az||4>$Q\\x14\\xf7\\xf7\\xf7\\xb7\\x95\\x06\\x02\\x81\\xe9t\\x8a16\\xbc\\xb7\\xb7\\xb76\\xdb\\xbaG\\\\wPK\\xce\\xcf\\xcf1\\xc6\\x14E\\x01\\x00\\x1e\\x1e\\x1e\\x08!\\xb9\\\\\\xee\\xe5\\xe5\\xa5V\\xabYOzzz:\\x1c\\x0e\\t!\\x92$\\x1d\\x1c\\x1cp\\x1c\\x871~zz\\xb2n\\\\\\xe2\\xf5zonn\\x8c^\\xd7j\\xb5\\xc6\\xe3\\xf1\\xfa\\x1d\\xd8\\x98\\xbb\\xbb\\xbb\\xe9tj\\xf4\\xc3\\xdfX\\xb9\\xdb\\xe1\\xe1a\\xb7\\xdb],\\x16;\\x93:\\x1c\\x8e\\xe3\\xe3\\xe3\\xe5\\xbfkg \\x84L{\\xd5\\xc6I3\\x99L\\xbf\\xdf\\x97$i\\x8b`\\xbfh6\\x9b\\x85Ba\\x97\\xc6p8\\xac\\xaa\\xaa\\xcf\\xe7[_\\xf6\\x03\\xd5W\\x08\\x12\\xaa'\\x16T\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\", 'text': None, 'type': 'image'}], 'role': 'user'}, {'content': [{'image': None, 'text': '„Åæ', 'type': 'text'}], 'role': 'assistant'}]}\n\nIf throughly checked it the is literally no difference between Dataset and IterableDataset when it comes to the shape of the data, but if I remove the image field then I can train with an IterableDataset!\n\nBut the moment I start training with an IterableDataset with an image field I get this cryptic error message:\n\n    ‚îÇ /home/kinski/Projects/kuzushiji/.venv/lib/python3.12/site-packages/torch/_tensor.py:1030 in split ‚îÇ\n    ‚îÇ ‚îÇ\n    ‚îÇ 1027 ‚îÇ ‚îÇ if isinstance(split_size, (int, torch.SymInt)): ‚îÇ\n    ‚îÇ 1028 ‚îÇ ‚îÇ ‚îÇ return torch._VF.split(self, split_size, dim) # type: ignore[attr-defined] ‚îÇ\n    ‚îÇ 1029 ‚îÇ ‚îÇ else: ‚îÇ\n    ‚îÇ ‚ù± 1030 ‚îÇ ‚îÇ ‚îÇ return torch._VF.split_with_sizes(self, split_size, dim) ‚îÇ\n    ‚îÇ 1031 ‚îÇ ‚îÇ\n    ‚îÇ 1032 ‚îÇ def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None): ‚îÇ\n    ‚îÇ 1033 ‚îÇ ‚îÇ r\"\"\"Returns the unique elements of the input tensor. ‚îÇ\n    ‚îÇ ‚îÇ\n    ‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ locals ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ\n    ‚îÇ ‚îÇ dim = 2 ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ self = tensor([[[[-4.7302e-03, -1.0620e-02, 5.5176e-02, ..., -1.6113e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -3.7994e-03, -4.0527e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 3.3936e-02, -9.5215e-03, -2.7466e-04, ..., -4.1260e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -2.6611e-02, -4.4434e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 1.6937e-03, 2.5513e-02, 2.7588e-02, ..., -1.2109e-01, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -7.6294e-03, -2.2583e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ..., ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[-1.6846e-02, -1.7212e-02, -1.0620e-02, ..., 8.4229e-03, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 5.0049e-02, -2.3828e-01]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 1.0559e-02, 9.8267e-03, 9.1553e-03, ..., -3.0884e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 3.9795e-02, -6.4697e-03]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[-2.5879e-02, 2.8442e-02, -8.4961e-02, ..., 3.3203e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 4.9072e-02, -2.8711e-01]]]], device='cuda:0', dtype=torch.bfloat16) ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ split_size = [16] ‚îÇ ‚îÇ\n    ‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ\n    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n    RuntimeError: split_with_sizes expects split_sizes to sum exactly to 1 (input tensor's size at dimension 2), but got\n    split_sizes=[16]\n\nDoes someone maybe know what I‚Äôm missing or what I‚Äôm doing wrong? Thanks in advance for your help!!!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qmhzg4/how_to_train_vision_model_with_iterabledataset/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o1ndo6w",
          "author": "mmathew23",
          "text": "Are you using UnslothVisionDataCollator? Trl handles iterabledatasets differently than regular Datasets. When it sees iterabledatasets it switches to a shared data loader that assumes the first index is the batch index. This assumption breaks for the qwen series vl models. You have to disable dispatch_batches and split_batches in the accelerator_config.",
          "score": 1,
          "created_utc": "2026-01-25 17:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1oavuc",
              "author": "willzocken",
              "text": "Thats it!!!! Thank you!  \nYes I was using UnslothVisionDataCollator.\n\n  \nAnother question how do you know this? :D\n\nI guess its somewhere written inside the docs of trl? Because I couldn't find it inside huggingface's datasets as well as in the unsloth docs",
              "score": 1,
              "created_utc": "2026-01-25 19:27:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1qkalo",
                  "author": "mmathew23",
                  "text": "accelerator_config is in the transformers documentation under TrainingArguments and the actual keywords should be listed in the documentation for accelerate.",
                  "score": 1,
                  "created_utc": "2026-01-26 01:41:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}