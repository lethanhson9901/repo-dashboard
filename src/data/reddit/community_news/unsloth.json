{
  "metadata": {
    "last_updated": "2026-01-21 08:44:59",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 7,
    "total_comments": 35,
    "file_size_bytes": 42040
  },
  "items": [
    {
      "id": "1qhscts",
      "title": "Run GLM-4.7-Flash locally Guide! (24GB RAM)",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/hgng1lc5ufeg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-20 05:22:23",
      "score": 171,
      "num_comments": 30,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qhscts/run_glm47flash_locally_guide_24gb_ram/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mccxs",
          "author": "TaroOk7112",
          "text": "Thanks!!!  \nDoes llama.cpp need to add some code to improve support or with this PR it's all supported?  \n[https://github.com/ggml-org/llama.cpp/pull/18936](https://github.com/ggml-org/llama.cpp/pull/18936)",
          "score": 8,
          "created_utc": "2026-01-20 05:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mdiub",
              "author": "danielhanchen",
              "text": "It should work in llama.cpp main now!",
              "score": 5,
              "created_utc": "2026-01-20 06:00:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0n9xr7",
                  "author": "goniz",
                  "text": "Does the repetitions happen on Q8 and BF16 GGUFs as well? Or just lower quants?",
                  "score": 3,
                  "created_utc": "2026-01-20 10:51:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0q5ocg",
                  "author": "TaroOk7112",
                  "text": "Be aware that flash attention degrades speed a lot, there are several PR in the works to improve support for  GLM-4.7-Flash in llama.cpp. It works, but is slower than even gpt-oss 120, so for now not too interesting.   \n[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/5#696f93fcbc8028d97e6fdf2b](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/5#696f93fcbc8028d97e6fdf2b)",
                  "score": 1,
                  "created_utc": "2026-01-20 19:59:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0o83m1",
          "author": "maxpayne07",
          "text": "So far, yes",
          "score": 2,
          "created_utc": "2026-01-20 14:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mldpf",
          "author": "RMK137",
          "text": "Great turnaround! I just got my 5090 so this is perfect timing.",
          "score": 2,
          "created_utc": "2026-01-20 07:05:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mjz8z",
          "author": "Psyko38",
          "text": "On a GPU with 16GB of VRAM, we're good at Q3?",
          "score": 1,
          "created_utc": "2026-01-20 06:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mma9p",
              "author": "Conscious_Chef_3233",
              "text": "you should go higher, i run qwen3 30b q4 on my 4070 12g",
              "score": 1,
              "created_utc": "2026-01-20 07:13:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mmg8z",
                  "author": "Psyko38",
                  "text": "But the weights, where do you put them in the RAM and you put the MoE in the VRAM?",
                  "score": 1,
                  "created_utc": "2026-01-20 07:14:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mmhfv",
          "author": "Unlikely_Database_87",
          "text": "In lm studio the model with those parameters you provided, cannot even give any clear response and it infinitely generates nonsense. Simple prompt I use - write java method  to merge two sorted arrays, no tests no explanation just code. My config 5080 and 64GB RAM",
          "score": 1,
          "created_utc": "2026-01-20 07:14:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mohp7",
              "author": "yoracale",
              "text": "You need to use dry multiplier which has the biggest impact, because LM Studio does not have it, you need to disable repeat penalty entirely.",
              "score": 5,
              "created_utc": "2026-01-20 07:32:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0mq2e5",
                  "author": "Unlikely_Database_87",
                  "text": "Thanks that helps",
                  "score": 1,
                  "created_utc": "2026-01-20 07:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mnfbw",
          "author": "Prudent-Ad4509",
          "text": "I'm not sure what they mean by full precision. The original seems to require 64Gb even with near zero context.",
          "score": 1,
          "created_utc": "2026-01-20 07:23:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n1ywr",
          "author": "Kirito_5",
          "text": "Thank you as always! \nWill try to see how well it compares to others on my 3090.",
          "score": 1,
          "created_utc": "2026-01-20 09:38:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nfyub",
              "author": "siggystabs",
              "text": "I really like GLM 4.7 Flash, but i can only fit a small context (like 5000 chars) on a single 3090 ‚Äî Not using System RAM. I end up having to split it across two 3090s to use it with decent context, and then it isn‚Äôt as fast.\n\nBut GLM 4.7 Flash is really exciting. The quality is excellent. If I didn‚Äôt have specific needs for my self-hosted apps, I‚Äôd probably use that exclusively for chatting. It‚Äôs definitely smarter than Qwen3 30B A3B in my usage.\n\nFor my workhorse applications I went back to Qwen3 30B A3B. Reluctantly. It‚Äôs just faster and better on VRAM, and ‚Äúgood enough‚Äù.",
              "score": 2,
              "created_utc": "2026-01-20 11:42:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nh57o",
          "author": "xanduonc",
          "text": "FP8 is half precision, original weights from glm are BF16",
          "score": 1,
          "created_utc": "2026-01-20 11:51:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nvq0g",
              "author": "yoracale",
              "text": "Yes thanks I edited my post",
              "score": 1,
              "created_utc": "2026-01-20 13:28:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nl0u3",
          "author": "maxpayne07",
          "text": "On LM studio, on the last step of reasoning, its starts a loop of repetition.  \n\n||\n|:-|",
          "score": 1,
          "created_utc": "2026-01-20 12:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0no5zm",
              "author": "yoracale",
              "text": "Did you disable repeat penalty?",
              "score": 1,
              "created_utc": "2026-01-20 12:41:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0np93r",
                  "author": "maxpayne07",
                  "text": "yes, just put it to value 1, solved!!! Thanks",
                  "score": 1,
                  "created_utc": "2026-01-20 12:49:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pkbck",
          "author": "DuckyBlender",
          "text": "Will dynamic nvfp4 quants come out?",
          "score": 1,
          "created_utc": "2026-01-20 18:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0q1qym",
              "author": "Opposite-Station-337",
              "text": "https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4/tree/main\n\nAlready are. \n\n:edit: sorry, missed dynamic.",
              "score": 1,
              "created_utc": "2026-01-20 19:41:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qiwhk",
          "author": "Dramatic-Rub-7654",
          "text": "Is this model actually dumber than Qwen 3 Coder Flash, or is it just overly sensitive? To the point that with the --n-cpu-moe flag it gets stuck in an infinite loop repeating a single word, and without that flag it keeps creating endless files, all with errors, until the window runs out?",
          "score": 1,
          "created_utc": "2026-01-20 21:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rc1ba",
              "author": "yoracale",
              "text": "It's very sensitive. Did you try using our recommended parameters? It seems to be a must for the this model",
              "score": 2,
              "created_utc": "2026-01-20 23:23:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rm20t",
                  "author": "Dramatic-Rub-7654",
                  "text": "I tried using the parameters recommended on the Hugging Face page for llama-b7782/llama-server:\n\n-m GLM-4.7-Flash-Q4_K_M.gguf --host 0.0.0.0 --n-gpu-layers 999 -fa on -t 14 -n -1 -c 16384 --jinja --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1 \n\nThe only changes I experimented with were adding the --n-cpu-moe flag, which caused the model to bug out with severe repetition issues, and increasing the temperature to 1.0.\n\nAt temperature 1.0, the model‚Äôs reasoning and responses appear coherent, but when I try to use it with tools like Cline, it clearly doesn‚Äôt know what it‚Äôs doing. It can create and edit files and interact with the terminal, but it consistently outputs broken code and introduces errors when editing files.\n\nIn contrast, Qwen, even in version Q4, is capable of providing a fully functional implementation of Flappy Bird from start to finish. based on the tests I ran, the GGUF versions still need further refinement. I tested the model using the version available on OpenRouter, where it performs significantly better than in my GGUF-based tests. However, Coder Flash still demonstrates superior intelligence compared to this model.",
                  "score": 1,
                  "created_utc": "2026-01-21 00:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qdmqcu",
      "title": "Reinforcement Learning with ultra long context is here!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/1btxn3558jdg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-15 15:36:04",
      "score": 74,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qdmqcu/reinforcement_learning_with_ultra_long_context_is/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzqvsga",
          "author": "____vladrad",
          "text": "Woahhhh nice! This is perfect for what I was trying to do",
          "score": 2,
          "created_utc": "2026-01-15 15:52:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqw4j5",
              "author": "____vladrad",
              "text": "Seriously nice job!!! Do you think there will be a multi gpu version for rl?",
              "score": 3,
              "created_utc": "2026-01-15 15:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzqwr0j",
                  "author": "yoracale",
                  "text": "Yes, early this year for sure!",
                  "score": 2,
                  "created_utc": "2026-01-15 15:56:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzr1ojh",
          "author": "WolfeheartGames",
          "text": "Yuge!",
          "score": 2,
          "created_utc": "2026-01-15 16:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0go7p6",
          "author": "igvarh",
          "text": "Sorry, could you do a dynamic guff for Google/translategemma with RL? This would be very useful for working with subtitles, which are just a document with a lot of context. I would also ask you or someone else to train models in the SRT format, which is a big challenge for any LLM. Not only do they lose context, but they confuse numbering and timings, completely breaking the format.",
          "score": 1,
          "created_utc": "2026-01-19 12:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mf9l1",
              "author": "yoracale",
              "text": "Currently translategemma seems to have chat template issues which is why we decided to not release quants for it yet.\n\nFor an RL notebook, it's a little complicated since it's a translation model but we'll see what we can do.",
              "score": 2,
              "created_utc": "2026-01-20 06:14:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0hf71j",
              "author": "anyandsomeone",
              "text": "i think creating a small script for this would be the better solution.\nextract all strings. give an llm the string in question +10 strings before that one (so it knows the context) and ask it to translate.\n\nclaide code, gemini, chatgpt, ... all of them will be able to give you a script that can do this within a few seconds for free and then you can hook that up with a small llm you could run locally for the actual translation.",
              "score": 1,
              "created_utc": "2026-01-19 14:48:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qf6qvv",
      "title": "Translategemma-27b",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qf6qvv/translategemma27b/",
      "author": "StormrageBG",
      "created_utc": "2026-01-17 07:17:53",
      "score": 21,
      "num_comments": 7,
      "upvote_ratio": 0.93,
      "text": "Guys do you plan to release quantisation variants of Translategemma-27b ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qf6qvv/translategemma27b/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o02thsb",
          "author": "danielhanchen",
          "text": "We did investigate it but the chat template sadly is quite specialized for it - I can check again later today, but currently it looks complex to support :(",
          "score": 3,
          "created_utc": "2026-01-17 09:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02u4gf",
              "author": "Purple-Programmer-7",
              "text": "Hugely supportive of everything you guys do.\n\nCan you educate me though? The whole ‚Äúwe fixed the chat template‚Äù didn‚Äôt really pan out IMO under scrutiny. And the copyright statement seems a bit of the anthesis  of everything you guys do to support the community.\n\nIf the chat template is functional and embedded, it‚Äôs not going to prevent a gguf from being made.\n\nAgain, not trying to be adversarial here, just want to understand better your position on all of this.\n\nThank you again for everything you‚Äôve provided (and continue to provide) this community, I genuinely am rooting for your success!\n\nEdit: spelling",
              "score": 2,
              "created_utc": "2026-01-17 09:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o07g10j",
                  "author": "danielhanchen",
                  "text": "Hey! Thanks!\n\n1. Translate Gemma has a specialized chat template that requires you to specify the language you want to translate to - this means it's not a chat model - you will need to specify the chat template kwargs separately, so it can't be loaded well in frontend UIs and make the process just harder for folks\n2. The chat template fixes are wide ranging, and we also do direct model implementation fixes - see the following:\n    * Gemma 1, Gemma 3 bug fixes: https://x.com/karpathy/status/1765473722985771335\n    * Phi 4 fixes: https://simonwillison.net/2025/Jan/11/phi-4-bug-fixes/\n    * Llama 4 fixes: https://www.reddit.com/r/LocalLLaMA/comments/1jxo7lb/llamacpp_got_2_fixes_for_llama_4_rope_wrong_norms/\n    * GPT OSS fixes: https://x.com/danielhanchen/status/1953901104150065544\n    * Kimi K2 bug fixes: https://x.com/danielhanchen/status/1946163064665260486\n    * And many more - we do a lot of our work behind the scenes now, so whatever model that was released with us generally is already fixed.\n3. The copyright header is Apache 2 (fully open source) - we place it because folks would simply copy and paste and rebrand it as their own fixes - we just want attribution that's all. Likewise all fixes before a model releases is licensed as whatever the model provider wants.",
                  "score": 2,
                  "created_utc": "2026-01-18 00:48:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03a9z5",
          "author": "DocWolle",
          "text": "Here are ggufs\n\n[https://huggingface.co/mradermacher/translategemma-27b-it-GGUF](https://huggingface.co/mradermacher/translategemma-27b-it-GGUF)",
          "score": 2,
          "created_utc": "2026-01-17 11:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03hq44",
              "author": "StormrageBG",
              "text": "Unsloth quants are always way better than mradermacher... I don't know what  the magic they did but on my test especially for translating unsloth Gemma 3 quants rulz... So I really hope ü§û that can do it again with this model too...",
              "score": 3,
              "created_utc": "2026-01-17 12:47:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03kj2v",
                  "author": "DocWolle",
                  "text": "this may be true, but the mradermacher gguf for translategemma works fine for me.",
                  "score": 1,
                  "created_utc": "2026-01-17 13:06:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0gp3rl",
          "author": "igvarh",
          "text": "\\+RL please. For subtitles, you need to work with a long context.",
          "score": 1,
          "created_utc": "2026-01-19 12:09:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhbr0p",
      "title": "glm 4.7 flash is out gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-19 18:08:55",
      "score": 19,
      "num_comments": 7,
      "upvote_ratio": 0.81,
      "text": "Guys do you plan to release quantisation variants of GLM-4.7 flash ? Its 30b a3b, unsloth chat template fixes are da best.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0mfece",
          "author": "yoracale",
          "text": "It's out now! GGUF: [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nTweet: [https://x.com/UnslothAI/status/2013482180564132092](https://x.com/UnslothAI/status/2013482180564132092)\n\nGuide: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)",
          "score": 1,
          "created_utc": "2026-01-20 06:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iq727",
          "author": "loadsamuny",
          "text": "architecture looks like a renamed deepseekv3, a pull request is in for it in llama.cpp so maybe tomorrow‚Ä¶.",
          "score": 4,
          "created_utc": "2026-01-19 18:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jnj3f",
          "author": "bobeeeeeeeee8964",
          "text": "i test it by the f16 gguf, and it seems the runing in a good speed, BUT it output garbage instead of proper text. We need waiting and see what going in this PR https://github.com/ggml-org/llama.cpp/pull/18936",
          "score": 3,
          "created_utc": "2026-01-19 20:55:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0k8is8",
              "author": "remghoost7",
              "text": "Pull request was merged and closed about an hour ago.  \nIt seems like they figured it out.",
              "score": 2,
              "created_utc": "2026-01-19 22:38:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0j9v1l",
          "author": "neph1010",
          "text": "[https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF](https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF)",
          "score": 1,
          "created_utc": "2026-01-19 19:51:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jk10j",
              "author": "noctrex",
              "text": "Still needs more work, its looping indefinitely",
              "score": 3,
              "created_utc": "2026-01-19 20:39:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0jzshk",
              "author": "Clqgg",
              "text": "this one is omega broken",
              "score": 2,
              "created_utc": "2026-01-19 21:55:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qi3m95",
      "title": "Is GLM-4.7-Flash still looping / repeating for you?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qi3m95/is_glm47flash_still_looping_repeating_for_you/",
      "author": "yoracale",
      "created_utc": "2026-01-20 15:11:00",
      "score": 16,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "Hey guys many of you are still experiencing looping/repetition issues however, after helping some people out, we found that some of you might still be running the incorrect settings. Correcting them with the correct settings seems to have mostly solved their issues.\n\nWe tried GLM-4.7's original sampling parameters with our quants and other non-Unsloth uploader quants and found the looping issues still persist, and after using the new parameters, especially `--dry-multiplier 1.1` seemed to have solved the issue.\n\nTo make things easier, we made an easy to understand table:\n\n|Goal / situation|What to set|\n|:-|:-|\n|Reduce looping / improve outputs|`--dry-multiplier 1.1` *(Not the same as repeat penalty.)*|\n|Recommended sampling parameters|`--temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1`|\n|Dry multiplier not available (e.g. LM Studio)|Disable Repeat Penalty or set it = 1|\n|Tool-calling|Lower `--dry-multiplier` (from `1.1` toward `0.0`) or disable it. Can improve tool-calling in some cases.|\n\nImportant: If `dry-multiplier` isn‚Äôt available (e.g. LM Studio), **disable repeat penalty** instead.\n\nRecommended parameters:\n\n    --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1\n\nIf the above parameters do not work, you can also try Z.ai's recommended parameters:\n\n* For general use-case: `--temp 1.0 --top-p 0.95`\n* For tool-calling: `--temp 0.7 --top-p 1.0`\n\nIf you still experience looping issues even after following all these steps, please let us know!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qi3m95/is_glm47flash_still_looping_repeating_for_you/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0ppvlj",
          "author": "Scared_Mycologist_92",
          "text": "i use lm-studio and try repeat penality 1,2 ..this fixed it for me and i think temp 0,6 or 0,9 or something. anything else beside repeat penality couldnt fix overthinking and those psychotic loops at any answer i tried. the model itself is pretty amazing",
          "score": 1,
          "created_utc": "2026-01-20 18:47:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0raa4l",
              "author": "yoracale",
              "text": "Interesting, for many people, adding repeat penalty didn't do anything for them unfortunately.",
              "score": 1,
              "created_utc": "2026-01-20 23:14:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0q2llt",
          "author": "zoyer2",
          "text": "No loop or repeating but just in general suck at coding, making small mistakes here and there (Have only tested the quant versions). Something needs to be fixed",
          "score": 1,
          "created_utc": "2026-01-20 19:45:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0q4czx",
          "author": "epigen01",
          "text": "Yup unsloths are not usable for me gonna wait it out",
          "score": 1,
          "created_utc": "2026-01-20 19:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qepz7",
          "author": "Final-Rush759",
          "text": "No problem,  I use mlx version.",
          "score": 1,
          "created_utc": "2026-01-20 20:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0s6fur",
          "author": "Calm_Management_5090",
          "text": "Hi, I am using Q5\\_K\\_M on lm studio and get frequent looping with both unsloth and [z.ai](http://z.ai) suggested options above once the context gets a few thousand tokens long. Thank you for all the unsloth work on this and previous models, I will keep my eyes open for updates. Best regards.",
          "score": 1,
          "created_utc": "2026-01-21 02:13:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tba8e",
              "author": "kripper-de",
              "text": "Did you also try Q4?",
              "score": 1,
              "created_utc": "2026-01-21 06:47:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0p3js8",
          "author": "Sensitive_Song4219",
          "text": "Happy to test, have just grabbed *unsloth/GLM-4.7-Flash-GGUF* in LM Studio: what are the recommended settings  for it? (We don't seem to have a dry-multiplier?)",
          "score": 1,
          "created_utc": "2026-01-20 17:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0r9z4b",
              "author": "yoracale",
              "text": "I wrote it in my post. Use the parameters above and disable repeat penalty.",
              "score": 1,
              "created_utc": "2026-01-20 23:12:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rd2hu",
                  "author": "Sensitive_Song4219",
                  "text": "I see that now - thank you! I also see there's *zai-org/glm-4.7-flash* available now in LM Studio as-of an hour or two ago (I assume that's z-ai's release).\n\nWill test them both properly! In my early testing thinking seems to run fine (no loops); performance at small contexts (<4k) on my machine seems similar to Qwen3 30B A3B 2507. Looking forward to seeing how the output quality is (and how performance is at larger contexts!)",
                  "score": 2,
                  "created_utc": "2026-01-20 23:29:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pvp4m",
          "author": "ZeWishmastR",
          "text": "Same issue here",
          "score": 0,
          "created_utc": "2026-01-20 19:13:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0q6gfx",
          "author": "Rektile142",
          "text": "The model is responding well after using the recommended parameters, but throughput is utterly scuffed as context grows.\n\nMaybe the llama.cpp team still needs time to cook.",
          "score": -1,
          "created_utc": "2026-01-20 20:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qbd9j",
              "author": "Mr_Back",
              "text": "I really hope so. Nemotron 3 nano and GPT OSS 120b run ten times faster on my hardware when the context is increased.",
              "score": 1,
              "created_utc": "2026-01-20 20:26:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pz5ow",
          "author": "Mr_Back",
          "text": "I'm more concerned about performance. I have a weak system, but even with it, similarly sized models perform much better (GPT OSS 120b, Qwen Next q6, Qwen3 Coder 30B, Nemotron 3 Nano). It's not about your specific quantization, but about the model as a whole.  \nI'm using a relatively large context (128k). The speed is still acceptable for small requests, but when I try a request with 35-40k tokens, I don't even want to wait for a response, it's so slow.\n\nMy PC: i5 12400, 96gb ram. 4070 - 12gb vram.",
          "score": -2,
          "created_utc": "2026-01-20 19:29:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgvg1u",
      "title": "Fine tuning Gpt oss on thinking dataset , which tokens to mask ?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qgvg1u/fine_tuning_gpt_oss_on_thinking_dataset_which/",
      "author": "Hulksulk666",
      "created_utc": "2026-01-19 05:23:52",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 0.89,
      "text": "from the official unsloth [notebook ](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb#) for fine tuning Gpt oss 20b ,they used\n\n    from unsloth.chat_templates import train_on_responses_only\n    gpt_oss_kwargs = dict( instruction_part = \"<|start|>user<|message|>\", response_part = \"<|start|>assistant<|channel|>final<|message|>\" )\n     trainer = train_on_responses_only( trainer, **gpt_oss_kwargs, )\n\nBut doesn't this effectively mean the thinking tokens are also being masked ? if so , how is the model actually learning from the thinking tokens of the dataset ? or am i missing something .",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qgvg1u/fine_tuning_gpt_oss_on_thinking_dataset_which/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0g9qyp",
          "author": "im_datta0",
          "text": "Hey u/Hulksulk666  \nThanks for noticing this. Yeah if I read it right, this might be masking \"thinking/analysis\" tokens from the training loss.  \nEdit: Talked to my friends and he mentioned that masking the analysis/thinking seems to produce better down stream results and hence the decision",
          "score": 2,
          "created_utc": "2026-01-19 09:54:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hbime",
              "author": "Hulksulk666",
              "text": "Thanks , i was confused whether or not this masking meant the thinking tokens would not be part of training loss and how not being part of the training loss makes the model learn from the data . From my intuitive understanding it seemed the model should take loss if the data has some ground truth outcome like math if the solution step by step is passed onto thinking , ig i lack some deeper understanding and should resort to some papers",
              "score": 1,
              "created_utc": "2026-01-19 14:29:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0on6kq",
          "author": "WolfeheartGames",
          "text": "The reason thinking works is because it causes a dependency. What comes after thinking depends on the thinking.\n\nIf you train a model's thinking to be too far out of its own distribution, it will degrade.",
          "score": 1,
          "created_utc": "2026-01-20 15:50:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcuyiz",
      "title": "Gemma 1b-it finetune worked great for multi-turn chat, but failed for `dialect text ‚Üí standard text` conversion",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qcuyiz/gemma_1bit_finetune_worked_great_for_multiturn/",
      "author": "_hasin",
      "created_utc": "2026-01-14 18:21:47",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm fine-tuning **Gemma 1B instruction-tuned** locally and ran into a failure I can‚Äôt explain.\n\n### Target task\n\nNormalize **regional dialect text** into **Standard Text** so downstream LLMs / rule-based extractors don‚Äôt hallucinate, while extracting data from the regional text (since now it'll be extracting from the standard text)\n\n### What worked\n\nI previously fine-tuned Gemma 1B it for a **multi-turn phone survey agent** using the standard chat template:\n\n* `<start_of_turn>user / model`\n* Instruction-heavy system prompt\n* Multi-turn conversational data\n\n**Result:** Tuned model followed instructions extremely well and performed reliably.\n\n### What I changed\n\nI reused **the same fine-tuning script, base model, trainer, and hyperparams**, but switched to a **single-turn parallel text task**:\n\n```\nUser:\nConvert the following dialect text to standard text.\nRespond ONLY with the converted text.\n[dialect text]\nModel:\n[standard text]\n```\n\nDataset = `dialect_text ‚Üí standard_text`.\nAnd, still using Gemma 1B **instruction-tuned** as the base.\n\n### Result\n\nThe fine-tuned model performed **very poorly**:\n\n* Inconsistent outputs\n* Often ignored the instruction\n* Much worse than the multi-turn chat model\n\n### What I‚Äôm trying to understand\n\nWhere is the conceptual mistake?\n\n* Is dialect text ‚Üí standard text fundamentally a **translation / seq2seq task**, not an instruction-following task?\n* Does instruction-tuned Gemma fight against clean text-to-text mapping?\n* Is this translation task different on an architecture level from the basic LLM architecture?\n* Should this be trained without LLM fine-tuning & rather moved to a different type of ML model?\n* Why does the *harder* multi-turn task succeed, but the ‚Äúsimpler‚Äù rewrite task fail?\n\nAnd I **_apologize_** in advance if I come of as rude, But I‚Äôm not looking for `use a bigger model` answers‚Äî I want to understand **why this failed at a training-objective level**. Maybe I am conceptually doing some mistake? Maybe I don't know something that I need to understand on an architecture level about this task of mine?\n\nI‚Äôd really appreciate your insights on this matter",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qcuyiz/gemma_1bit_finetune_worked_great_for_multiturn/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nznai78",
          "author": "nborwankar",
          "text": "1B just may not have enough linguistic power to see the patterns for changing dialects to standard language. You may need a 7B model or even more depending on the base language. If you are looking for a dialect of a language that is itself rare then you will need to look for a multilingual model that has enough training data on your language. \n1B models can be trained for focused tasks - they don‚Äôt have enough training to do nuanced language translation.",
          "score": 2,
          "created_utc": "2026-01-15 00:59:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp9f1j",
              "author": "_hasin",
              "text": "I plan to be able to use & serve these models via llama.cpp server in GGUF formats, so using 7b models wasn't my first thought tbh. And, i don't think i could use 7b",
              "score": 1,
              "created_utc": "2026-01-15 09:36:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nznbhya",
          "author": "LA_rent_Aficionado",
          "text": "My understanding with translation tasks is that the pre-training needs to understand next token prediction association between two languages/dialects, etc. so one strategy is to train interleaved or per line text, for example:\n\n[english] hello how are you \n[spanish] hola como estas\n\nEtc..\n\nAnd then you can better bridge the two with SFT saying translate X to Y. \n\nIf the model isn‚Äôt initially trained multi lingual associations or the language period, then SFT to bridge the two won‚Äôt really make the connection.  I‚Äôd suspect the 1B model barely has the degree of multilingual training you need to make the connections you desire.\n\nI‚Äôm not sure what dialect you are using but try a CPT epoch making sure it learns the associations and then your SFT and see if it improves.",
          "score": 2,
          "created_utc": "2026-01-15 01:05:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpaf27",
              "author": "_hasin",
              "text": "I am targeting Regional bangla texts to be Normalized into standard & formal texts. As for my knowledge, Gemma3 pre-trained wasn't trained on those regional texts at all.\n\nBut, I'm not sure that is the sole reason for the wrong inference. Currently, I'm trying to do the Base pt 4b model with the alpaca Instruction based tuning & trying to see how it turns out.\n\nBut another main issue is the fact that, I am unable to train 4b models on the free tier of kaggle :\")",
              "score": 1,
              "created_utc": "2026-01-15 09:46:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzp5f8s",
          "author": "danielhanchen",
          "text": "Did you use training on completions only as well? https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#training-on-completions-only-masking-out-inputs",
          "score": 1,
          "created_utc": "2026-01-15 08:57:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp9ts9",
              "author": "_hasin",
              "text": "I tried, but it gives me the `All labels in your dataset are -100. Training losses will be all 0`.\n\nThis is probably because I'm setting up the dataset to be in `alpaca` format, for the single task instruction-based tuning. But, I'd like to use the `train_on_responses_only` - maybe it might give better results? But how can i do that with my alpaca format data?",
              "score": 1,
              "created_utc": "2026-01-15 09:40:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}