{
  "metadata": {
    "last_updated": "2025-12-31 06:39:02",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 10,
    "total_comments": 46,
    "file_size_bytes": 66498
  },
  "items": [
    {
      "id": "1pup2bm",
      "title": "Merry Christmas from Unsloth! üéÑüéÅ",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/vyy68ceg069g1.png",
      "author": "yoracale",
      "created_utc": "2025-12-24 14:49:38",
      "score": 129,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pup2bm/merry_christmas_from_unsloth/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvqeqz8",
          "author": "Educational_Rent1059",
          "text": "Thanks for an awesome year of amazing work! Merry christmas Unsloth",
          "score": 6,
          "created_utc": "2025-12-24 15:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtbkno",
              "author": "danielhanchen",
              "text": "Merry christmas thank you! üéÑüéÅ",
              "score": 3,
              "created_utc": "2025-12-25 02:39:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvq5g7q",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 4,
          "created_utc": "2025-12-24 15:03:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvq9bnn",
              "author": "yoracale",
              "text": "That's amazing to hear thanks so much for the support! We're rolling our super efficient multiGPU support and our UI early next year which will hopefully be even more impactful and exciting! üôèü¶•",
              "score": 8,
              "created_utc": "2025-12-24 15:24:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvr5ars",
                  "author": "Amazing_Athlete_2265",
                  "text": "UI will be awesome! Looking forward to that",
                  "score": 2,
                  "created_utc": "2025-12-24 18:16:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqy98s",
          "author": "joninco",
          "text": "Merry Christmas guys, keep up the great work and don‚Äôt sell :)",
          "score": 3,
          "created_utc": "2025-12-24 17:38:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvshkl4",
              "author": "yoracale",
              "text": "Thank you! Merry Christmas!! üôè‚ô•Ô∏è‚ô•Ô∏è",
              "score": 2,
              "created_utc": "2025-12-24 23:01:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvralue",
          "author": "LegacyRemaster",
          "text": "Legend! Merry Christmas to youuu!",
          "score": 2,
          "created_utc": "2025-12-24 18:45:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtblwq",
              "author": "danielhanchen",
              "text": "Thank you! Merry Christmas to you as well! ü•∞üéÑ",
              "score": 1,
              "created_utc": "2025-12-25 02:39:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvrjye5",
          "author": "Eyesuk",
          "text": "You guys are great! Merry Christmas still very jealous of those who‚Äôs chipset is supported ü´∂üèΩ",
          "score": 2,
          "created_utc": "2025-12-24 19:37:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtbn70",
              "author": "danielhanchen",
              "text": "Thank you ahaha!!! Merry christmas! ‚ô•Ô∏è‚ô•Ô∏è",
              "score": 1,
              "created_utc": "2025-12-25 02:40:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtkaej",
          "author": "TBisonbeda",
          "text": "Merry Christmas! You guys are awesome! üéÑ",
          "score": 2,
          "created_utc": "2025-12-25 03:46:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvumofu",
          "author": "Pxlkind",
          "text": "Thanks for all your work - greatly appreciated! :)",
          "score": 2,
          "created_utc": "2025-12-25 09:53:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxqp4q",
      "title": "All GLM 4.7, GLM 4.6 and GLM 4.6V-Flash GGUFs are now updated!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pxqp4q/all_glm_47_glm_46_and_glm_46vflash_ggufs_are_now/",
      "author": "yoracale",
      "created_utc": "2025-12-28 12:54:48",
      "score": 117,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "Hey guys, we did a refresh of quants (quality of life updates) for GLM 4.5, 4.6, 4.6V-Flash and 4.7\n\nllama.cpp and other inference engines like LM Studio now support more features including but not limited to:\n\n1. Non ascii decoding for tools (affects non English languages) For eg before the default (ensure\\_ascii=True) would cause \"caf√©\" ‚Üí \"caf\\\\u00e9\", whilst now ensure\\_ascii=False would tokenize \"caf√©\" ‚Üí \"caf√©\". I would re-download our quants if you use languages other than English.\n2. Converts reasoning content parsing to original \\[0\\], \\[-1\\] from our changes of |first and |last. We used to change \\[0\\] to |first and \\[-1\\] to |last so we be compatible with LM Studio and llama-cli. With the upgrade of llama-cli to use llama-server, we can revert this. llama-server also didn't like |first, so we fixed it as well.\n3. Many of you reported Chinese thinking with the GLM-4.6V-Flash GGUFs. After investigating, we confirmed the same behavior appears in all uploads regardless of uploader (e.g., LM Studio and bartowski). LM Studio‚Äôs Q8\\_0, bartowski‚Äôs BF16, and our BF16 all produce Chinese ‚Äúthinking,‚Äù so this is just the way Z . ai intended for the model and is not unique to our uploads. [See our investigation here.](https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/discussions/4#694bdbac91a48021d8b210a1)\n\nAlso other changes:\n\n1. **Added lot of tool calls in our calibration dataset - makes tool calling better, especially for smaller quants.**\n2. **A bit more calibration data for GLM models., adding a teeny tiny bit more accuracy overall.**\n\n**This does mean you need to re-download them to use the latest changes**\n\nGGUFs which received Quality of Life updates:\n\n* [https://huggingface.co/unsloth/GLM-4.6V-GGUF](https://huggingface.co/unsloth/GLM-4.6V-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.7-GGUF](https://huggingface.co/unsloth/GLM-4.7-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.6-GGUF](https://huggingface.co/unsloth/GLM-4.6-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.5-GGUF](https://huggingface.co/unsloth/GLM-4.5-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.5-Air-GGUF](https://huggingface.co/unsloth/GLM-4.5-Air-GGUF)\n\nOur guides are all in our docs or model cards: [https://unsloth.ai/docs/models/glm-4.7](https://unsloth.ai/docs/models/glm-4.7)\n\nThanks so much guys! :)",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1pxqp4q/all_glm_47_glm_46_and_glm_46vflash_ggufs_are_now/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwe2k2y",
          "author": "kiwibonga",
          "text": "\"Sorry guys, it just wants to think in Chinese, but it understands everything you say.\"\n\nI have that exact problem with my bilingual toddler",
          "score": 6,
          "created_utc": "2025-12-28 17:08:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxs6q",
          "author": "keypa_",
          "text": "Thanks a lot, being a french it's been a nightmare trying to understand why all accents are not being properly displayed !",
          "score": 5,
          "created_utc": "2025-12-28 16:44:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfmsm1",
          "author": "elsung",
          "text": "awesome! now i just need to figure out /wait for conversion of these into MLX =)",
          "score": 3,
          "created_utc": "2025-12-28 21:38:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdyahx",
          "author": "Magnus114",
          "text": "Should we still specify the jinja from this pr?\n\nhttps://github.com/ggml-org/llama.cpp/pull/16932",
          "score": 2,
          "created_utc": "2025-12-28 16:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjjpy0",
              "author": "yoracale",
              "text": "Our current jinja chat template should work fine as is",
              "score": 1,
              "created_utc": "2025-12-29 13:38:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwegwto",
          "author": "Sensitive_Sweet_1850",
          "text": "nice",
          "score": 2,
          "created_utc": "2025-12-28 18:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nweqpdi",
          "author": "LegacyRemaster",
          "text": "thx sir!",
          "score": 2,
          "created_utc": "2025-12-28 19:03:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgq8gj",
          "author": "RedditUsr2",
          "text": "Is unsloth/GLM-4.6V-Flash-GGUF usable with cline?",
          "score": 2,
          "created_utc": "2025-12-29 01:05:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhhpy4",
              "author": "yoracale",
              "text": "It should be yes",
              "score": 2,
              "created_utc": "2025-12-29 03:44:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwd6kjn",
          "author": "Brave-Hold-9389",
          "text": "Why not glm 4.6v?",
          "score": 3,
          "created_utc": "2025-12-28 14:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe8kv6",
              "author": "molbal",
              "text": "https://huggingface.co/unsloth/GLM-4.6V-GGUF",
              "score": 7,
              "created_utc": "2025-12-28 17:38:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwgqgky",
              "author": "yoracale",
              "text": "Glm4.6v is also updated",
              "score": 3,
              "created_utc": "2025-12-29 01:06:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfobor",
          "author": "gofiend",
          "text": "Thank you for your amazing work! \n\nOne request - I know that when you do your quantization you run some sort of quick validation test to ensure you got a functional quant. Could you please share those scores (whatever it is KL divergence, perplexity - anything) consistantly when you release a set of quants?\n\nI'm constantly trying to get a feel for what tradeoff to make around the Q3-Q6 range (64GB VRAM) and it would be amazing to have some sort of signal. I fully understand that \"real usability\" will be different and non-linearly related to the scores you report. \n\n(I also suspect, long term, across multiple model families, we'll be able to model \"real usibility\" differences starting from the scores + the model architecture).",
          "score": 1,
          "created_utc": "2025-12-28 21:46:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhi41k",
              "author": "yoracale",
              "text": "To be honest it is a lot of work to do it for every quant. In general we recommend using at least 2-bit for large quants and 3-bit for medium sized and 4-bit for small sized.\n\nWe did do Aider benchmarks for example DeepSeek-V3.1: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)\n\nAider is one of the top 3 benchmarks for real use-case benchmarks",
              "score": 2,
              "created_utc": "2025-12-29 03:47:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwhp74a",
                  "author": "gofiend",
                  "text": "Is there some limited version (say only K\\_M) that is possible?\n\nAlso I‚Äôm definitely not asking for accurate benchmarking (like the terrific Aider effort). I‚Äôve found that fairly small bits of text converge on perplexity etc. pretty quickly. I‚Äôm thinking of whatever 60-120 seconds of compute per under 96GB quant can provide in terms of signal. Basically automate a quick perplexity / KL estimate and auto publish. Even better if it is whatever sense check you use to make sure the quant process didn‚Äôt fail.",
                  "score": 1,
                  "created_utc": "2025-12-29 04:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwiusop",
                  "author": "thedarkbobo",
                  "text": "Awesome, imagine we had a table for filter model scores i.e. coding then sort by size with different quants.",
                  "score": 1,
                  "created_utc": "2025-12-29 10:22:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhxz0h",
          "author": "Aggressive-Dingo-993",
          "text": "Wait, is this why I get dramatically improved performance when I re-tested GLM 4.5 air yesterday????",
          "score": 1,
          "created_utc": "2025-12-29 05:31:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwicypz",
              "author": "yoracale",
              "text": "Yes could definitely be possible",
              "score": 1,
              "created_utc": "2025-12-29 07:36:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwe7ma",
      "title": "Run MiniMax-M2.1 with Unsloth Dynamic GGUFs!",
      "subreddit": "unsloth",
      "url": "https://huggingface.co/unsloth/MiniMax-M2.1-GGUF",
      "author": "yoracale",
      "created_utc": "2025-12-26 20:31:50",
      "score": 75,
      "num_comments": 16,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pwe7ma/run_minimaxm21_with_unsloth_dynamic_ggufs/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nw3b571",
          "author": "MarketsandMayhem",
          "text": "You all are absolutely awesome. Thank you for all that you do!",
          "score": 6,
          "created_utc": "2025-12-26 21:58:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4ai0h",
              "author": "danielhanchen",
              "text": "Appreciate it!",
              "score": 4,
              "created_utc": "2025-12-27 01:27:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2wfgr",
          "author": "Particular-Way7271",
          "text": "Thanks again!",
          "score": 2,
          "created_utc": "2025-12-26 20:38:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4ahh1",
              "author": "danielhanchen",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2025-12-27 01:27:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh9q3p",
          "author": "RedditUsr2",
          "text": "Man I'm going to have to buy a mac.",
          "score": 2,
          "created_utc": "2025-12-29 02:57:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnw75i",
              "author": "texasdude11",
              "text": "Don't... Get cuda compatible device, gains are huge.",
              "score": 1,
              "created_utc": "2025-12-30 02:53:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwr6ab8",
                  "author": "RedditUsr2",
                  "text": "Do I bite the bullet and get a RTX PRO 6000 Blackwell 96GB workstation instead of a mac with double the vram?",
                  "score": 1,
                  "created_utc": "2025-12-30 16:34:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2vz3r",
          "author": "KvAk_AKPlaysYT",
          "text": "Hey, I also created some GGUFs, did you guys encounter issues for the BPE pre-tokenizer not being recognized? I had to hack on a new hash in convert_ht_to_gguf.py",
          "score": 2,
          "created_utc": "2025-12-26 20:35:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2wg52",
              "author": "yoracale",
              "text": "Hello nice work! I'm not sure I have to ask Daniel, I'll get back to you",
              "score": 2,
              "created_utc": "2025-12-26 20:38:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw2yn6m",
                  "author": "KvAk_AKPlaysYT",
                  "text": "Just tried with a fresh env. Yep, it's consistent. I'll make an issue + PR.\n\nEdit: PR: [https://github.com/ggml-org/llama.cpp/pull/18399](https://github.com/ggml-org/llama.cpp/pull/18399)",
                  "score": 4,
                  "created_utc": "2025-12-26 20:50:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4vn8a",
          "author": "Tema_Art_7777",
          "text": "Until I can run these on a 5090 all hope is lost üòÄ",
          "score": 1,
          "created_utc": "2025-12-27 03:45:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5jvl4",
              "author": "yoracale",
              "text": "You can technically via offloading but it'll be slow.",
              "score": 1,
              "created_utc": "2025-12-27 06:51:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7z9u1",
          "author": "e0xTalk",
          "text": "How much ram is needed to run this?",
          "score": 1,
          "created_utc": "2025-12-27 17:32:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pya97t",
      "title": "Progressive LoRA Merging - complete model identity replacement on consumer hardware",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pya97t/progressive_lora_merging_complete_model_identity/",
      "author": "TastyWriting8360",
      "created_utc": "2025-12-29 02:38:39",
      "score": 43,
      "num_comments": 43,
      "upvote_ratio": 0.79,
      "text": "I'm here to democratize model creation. After 3+ months of development, I've figured out how to **completely replace a model's weights while preserving the architecture**.\n\nThis means you can take Qwen3, Llama, or any open model - reuse the millions of dollars they spent on pretraining - and replace the identity for a few bucks on consumer hardware.\n\n**How it works:**\n\n1. Train a LoRA adapter on your data\n2. Merge the LoRA into the base model permanently (in BF16, not quantized)\n3. The merged model becomes your new base\n4. Apply a **fresh** LoRA and train again\n5. Repeat\n\nEach merge **dissolves** the adapter into the weights. The next cycle starts with fresh random LoRA weights on the new base. This is not stacking - it's sequential replacement.\n\n**Why this works:**\n\nWe deliberately use catastrophic forgetting to erase the base model's identity while preserving your injected patterns through dataset mixing (50% new data / 50% historical).\n\nAfter enough cycles, the model stops saying \"I am Qwen\" and fully adopts your identity, reasoning style, and knowledge.\n\n---\n\n**Resources:**\n\n- Paper & Code: [https://huggingface.co/hitonet/progressive-lora-merging](https://huggingface.co/hitonet/progressive-lora-merging)\n- GitHub: [https://github.com/antibitcoin/progressive-lora-merging](https://github.com/antibitcoin/progressive-lora-merging)\n- Working demo: [https://chat.hitonet.com](https://chat.hitonet.com) (try Hito-small - it was Qwen 8B)\n- Example model: [https://huggingface.co/hitonet/hito-1.7b](https://huggingface.co/hitonet/hito-1.7b)\n\n---\n\n**FAQ:**\n\n**Q: Isn't this just LoRA stacking? Won't errors compound like (a+b)¬≤ √ó (a+b)¬≤?**\n\nNo. After each merge, the LoRA adapter is **dissolved** into the base weights via `merge_and_unload()` and ceases to exist. The next cycle initializes a **fresh LoRA with random weights**. There is no stacking. After 100 cycles, you have ONE model with 100 sequential weight modifications, not 100 stacked adapters.\n\n**Q: Won't quantization errors accumulate?**\n\nNot if you merge correctly. We train in 4-bit/8-bit (memory efficient), but merge in **BF16 full precision** (error-free). This asymmetric precision prevents error accumulation.\n\n**Q: Won't this cause catastrophic forgetting?**\n\nYes - that's the goal. We selectively forget the base model's identity while preserving yours through dataset mixing.\n\n**Q: How is this different from full fine-tuning?**\n\nSame result, 10-100x cheaper. Full fine-tuning needs 4-8x A100s. This runs on a single 24GB GPU.\n\n**Q: How many cycles until identity replacement?**\n\n- 25 cycles: Noticeable shift (~40%)\n- 50 cycles: Fundamentally different (~70%)\n- 100 cycles: Near-complete replacement (~93%)\n\n---\n\n**Citation:**\n\n    @article{drissi2024bodysnatching,\n      title={Body Snatching: Complete Model Identity Replacement via Progressive LoRA Merging},\n      author={Drissi, Ouissam Said},\n      year={2024},\n      url={https://github.com/antibitcoin/progressive-lora-merging}\n    }\n\n---\n\nThe math, code, and working models are all public. Try it before theorizing why it can't work.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pya97t/progressive_lora_merging_complete_model_identity/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwhkp1g",
          "author": "zmarty",
          "text": "I am confused how this does not destroy the model and its capabilities. Have you run the standard benchmarks?",
          "score": 7,
          "created_utc": "2025-12-29 04:03:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhqols",
              "author": "TastyWriting8360",
              "text": "**Why it doesn't destroy capabilities:**\n\nThe architecture stays intact - attention patterns, layer structure, everything that makes the model \"work\" is preserved. What changes is the *\\*content\\** of the weights, not the *\\*structure\\**.\n\nThink of it like this: the model learned \"how to reason\" during pretraining. That's baked into the architecture and weight relationships. We're replacing *\\*what\\** it reasons about and *\\*how it identifies itself\\**, not its ability to reason. The key is the dataset.\n\nIf you train on garbage, you get garbage. If you train on high-quality synthetic data with proper reasoning chains, the model retains (and can even improve) its capabilities while adopting a new identity.\n\n**Benchmarks:** Honestly? No, I haven't run standard benchmarks. I built this for production use, not for leaderboard chasing. \n\nWhat I can tell you: - It works in my production environment daily\n\n \\- The models reason coherently, follow instructions, and don't hallucinate more than base models\n\n \\- You can try it yourself: [https://chat.hitonet.com](https://chat.hitonet.com) (Hito-small was Qwen 8B) If someone wants to run benchmarks, the models are public: - [https://huggingface.co/hitonet/hito-1.7b](https://huggingface.co/hitonet/hito-1.7b) I'd actually be curious to see the results. But \"works in production\" matters more to me than MMLU scores.",
              "score": 1,
              "created_utc": "2025-12-29 04:40:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhhbzs",
          "author": "vichustephen",
          "text": "Sorry I'm noob but can't we just use DPO to change the identity? Why do all this ?",
          "score": 6,
          "created_utc": "2025-12-29 03:42:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhjcom",
              "author": "TastyWriting8360",
              "text": "\nDPO changes *preferences* (what the model prefers to say), not *identity* (what the model fundamentally is).\n\n**DPO:**\n- Trains the model to prefer response A over response B\n- The base model's knowledge, reasoning patterns, and self-concept remain intact\n- Qwen + DPO = Qwen that prefers certain outputs\n\n**Progressive LoRA Merging:**\n- Rewrites the actual weights over many cycles\n- The base model's identity is progressively erased and replaced\n- Qwen + PLM = Not Qwen anymore\n\nThink of it this way:\n- DPO is like teaching someone to give different answers\n- PLM is like replacing the person entirely\n\nDPO is great for alignment and steering. But if you want the model to genuinely *be* something else - different reasoning style, different knowledge, different self-identity - you need to replace the weights, not just the preferences.\n\nAlso, DPO still requires significant compute for large models. PLM runs on a single 24GB GPU.",
              "score": 2,
              "created_utc": "2025-12-29 03:54:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwhpjrk",
                  "author": "vichustephen",
                  "text": "Though it sounds interesting do you know what you are doing? I smell chatgpt ahhh reply. Also I've done lots of lora fine-tuning in 6gb vram using unsloth",
                  "score": 10,
                  "created_utc": "2025-12-29 04:33:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwijs0t",
                  "author": "__Maximum__",
                  "text": "I feel like you contradict your other comment here. Which is it, the models attention patterns stay the same and only the identity changes, which is equivalent to DPO or the model completely changes (different reasoning style, different knowledge, different self-identity), which means you should run benchmarks (your own or public) to see if it degraded or not.\n\nI appreciate the open source nature of your work, don't get me wrong, and whatever the outcome is, you found a way to fine-tune a model with way less memory, trading of time.",
                  "score": 4,
                  "created_utc": "2025-12-29 08:39:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwievg1",
          "author": "Ok_Appearance3584",
          "text": "Duh? This is called finetuning.",
          "score": 5,
          "created_utc": "2025-12-29 07:53:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwifmdc",
              "author": "TastyWriting8360",
              "text": "Fine-tuning adds a layer on top. The base model is still there underneath - that's why fine-tuned models still say \"I am Qwen\" or revert to base behavior on edge cases.  \n  \nThis erases and replaces the base model. After 100 merge cycles, there is no \"Qwen underneath\" anymore. The weights that made it Qwen are gone, overwritten by sequential modifications.  \n  \nIt's the difference between putting a new coat of paint on a car vs replacing the engine, transmission, and interior piece by piece until nothing original remains.  \n  \nSame architecture, completely different model. That's not fine-tuning, that's replacement.  \n  \nBut you need a good quality dataset. Check the repo for details. Garbage data = garbage model, same as any training method.",
              "score": -1,
              "created_utc": "2025-12-29 08:00:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwim9st",
                  "author": "Ok_Appearance3584",
                  "text": "That's incorrect, the end result does not change if you merge lora to base model. Lora is just a diff layer to the base. During inference it's behaving exactly as if the lora was merged to the base model.¬†\n\n\nYou can merge your lora back to base if you wish to release a standalone finetuned model. If you want to swap different loras during runtime, it's better to have one big base model and multiple adapters as opposed to multiple big models.\n\n\n\nYour method could be streamlined to simply finetune one lora for a hundred epochs.\n\n\nBut as a concept, yes, if you finetune a model (as opposed to designing your own + training from scratch) you get to keep the benefits of large scale pretraining and engineering and make it your own via finetuning.",
                  "score": 5,
                  "created_utc": "2025-12-29 09:02:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhq9m1",
          "author": "SpaceNinjaDino",
          "text": "Do you think that the planned progression should end on the most important as it will be the most influential?",
          "score": 2,
          "created_utc": "2025-12-29 04:38:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhr27o",
              "author": "TastyWriting8360",
              "text": "Good intuition, but I'd actually argue the opposite based on how the method works.\n\nEarly cycles are most influential for identity. The first 25-50 cycles cause the biggest drift from the base model. This is when the model is most malleable because you're breaking its original identity. Whatever you train early gets reinforced across all subsequent cycles.\n\nLater cycles are for refinement. By cycle 50+, the model has largely adopted your identity. Later training is fine-tuning your own model at that point, not fighting against Qwen/Llama anymore.\n\nMy approach:\n\nEarly cycles (1-25): Core identity, personality, self-concept. This is where you establish \"I am X, not Qwen\"\n\nMid cycles (25-50): Reasoning style, knowledge patterns, how it thinks\n\nLate cycles (50-100): Edge cases, specific behaviors, polish\n\nThink of it like painting. You lay down the base colors first, then add details. If you save your most important stuff for last, the earlier paint might show through.\n\nThat said, the 50/50 dataset mixing means important early data keeps getting reinforced throughout. So it's not like early stuff gets forgotten. It's more about what gets the most total exposure across all cycles.\n\nWhat's your use case? That might change the recommendation.",
              "score": 1,
              "created_utc": "2025-12-29 04:43:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkuf8i",
          "author": "sirfitzwilliamdarcy",
          "text": "You can usually achieve the same effect with more steps during fine-tuning. Would be interested in seeing Evals before and after this process though. It may have unique advantages.",
          "score": 2,
          "created_utc": "2025-12-29 17:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsun2y",
              "author": "TastyWriting8360",
              "text": "Yep one of the key advantages is new ways of reasoning, you are not stuck with the model default, you are not finetuning you are basically rewriting the model, SFT on a budget but more accurate, remember when we merge and dissolve the lora into the new base we actually do it on the full wieghts of the model not the quantized.\n\nthe following is nested reasoning my own invention have you seen sideways reasoning before?\n\nhttps://preview.redd.it/x21tgfqwqeag1.png?width=950&format=png&auto=webp&s=55de2f0ea77eebc32d6e5a3ea42541eb5c30d16a\n\nyou can test it out by downloading the hito 1.7b which was created from qwen3 1.7b I have trained full embeddings with my own custom tokens and applied styling to make it clear you can use it without downloading the model at [chat.hitonet.com](http://chat.hitonet.com) just make sure to signup cuz the non signed up version is bullshit demo. just remember to make good stuff, you have to forget about following tutorials, generate your own high quality dataset, real dataset not what everyone is using. I spent a good amount of time on mine,",
              "score": 1,
              "created_utc": "2025-12-30 21:18:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp2k8r",
          "author": "schlammsuhler",
          "text": "How to overfit",
          "score": 2,
          "created_utc": "2025-12-30 07:52:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwssndt",
              "author": "TastyWriting8360",
              "text": "It does not overfit read the paper. already solved that problem long ago, download the model, test it your self, try it on qwen3-0.6b.",
              "score": 1,
              "created_utc": "2025-12-30 21:09:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj3ts0",
          "author": "Agreeable-Market-692",
          "text": "Congratulations on \"discovering\" iterative fine-tuning with extra steps... \n\nYou fell prey to one of the classic blunders, you believed ChatGPT when it glazed you. You have achieved dimensional shifting Star Seed! If you thought ChatGPT was going to make you smarter or make you sound smarter you played yourself. It in fact made several people here dumber and made the people who actually read papers annoyed. Your post is pure spam.\n\nThe data mixing you described is just replay based continual learning, not very novel at all and is well-researched. \n\nThe percentages (40%, 70%, 93%) are almost certainly made up. There's no citation, no metric definition, no evaluation methodology. What does \"93% identity replacement\" even mean?\n\nThese are not the only problems with this post but these are the ones I cared to type out before I got too annoyed to keep going.\n\nIf you have some kind of ADHD issue that means you can't focus long enough to get through papers or get motivated enough to start get it handled now and you'll live a much better life, medication is effective. You are responsible for your own actions though, you can't lean on excuses in the real world.   \n  \n\"Independent researcher\" is a fun way to say unemployed. And if you keep posting stuff like this you're gonna stay \"independent\". Interviewers can see through the pretending and would be much harsher than I have been here. If you did this kind of thing in an academic setting you'd rightfully be trotted down to the dean's office to explain yourself.",
          "score": 5,
          "created_utc": "2025-12-29 11:43:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj9mgx",
              "author": "vichustephen",
              "text": "All his replies are from chatgpt ü§£ü§£",
              "score": 5,
              "created_utc": "2025-12-29 12:29:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjag9q",
                  "author": "Agreeable-Market-692",
                  "text": "he also followed me to one of my posts in another sub, classic narcissistic injury",
                  "score": 2,
                  "created_utc": "2025-12-29 12:35:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj45n3",
              "author": "TastyWriting8360",
              "text": "How is that related to my research or work? are you speaking about your own experiance, did that happen to you? I am sorry but I spent 3 months to come up with this even if I summerized it with AI, that does not matter. your post is very toxic, unethical, unprofessional and not related to the topic, you are welcome to run any benchmarks everything is public.",
              "score": 0,
              "created_utc": "2025-12-29 11:45:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwj9qov",
                  "author": "vichustephen",
                  "text": "Bro this is your only reply others are chatgpt . I don't understand how people fell for your trap ü§£",
                  "score": 3,
                  "created_utc": "2025-12-29 12:30:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwj4ham",
                  "author": "Agreeable-Market-692",
                  "text": "I actually work in the industry and your post smacks of slop. Take it personally if you want, but I'm just telling you what other experienced people are going to think but may or may not say to your face. Address my critiques or you're just conceding what you did here.",
                  "score": 2,
                  "created_utc": "2025-12-29 11:48:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwjjpep",
                  "author": "Agreeable-Market-692",
                  "text": "Really pathetic behavior. You are simply outing yourself at this point. I feel secondhand embarassment for you.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:38:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuo2vg",
              "author": "TheRiddler79",
              "text": "On a side note, your entire premise hinges on your own failures and inability to carefully read more than just the post.\n\nIt's an equally stupid \"burn\" to pretend to know about LLM tuning, but also clown on someone using AI to make a clearly stated post.\n\nFinally, you claim the percentages are made up, but also can't prove they can't be true.\n\nIn summary, your entire post was dripping with amateur attacks and flawed logic based on your own personal shortcomings.",
              "score": 0,
              "created_utc": "2025-12-31 03:12:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvbzqe",
                  "author": "Agreeable-Market-692",
                  "text": "\"but also can't prove they can't be true\"\n\nthis is the lowest grade bait",
                  "score": 1,
                  "created_utc": "2025-12-31 05:53:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwha6cd",
          "author": "SlavaSobov",
          "text": "Funny you mention this, I've been doing exactly this for my companion AI. \n\nFirst was a LoRA adapter on the base personality, to become the ground truth. \n\nThen further fine tuning on other tokens and behavior I wanted to introduce.",
          "score": 2,
          "created_utc": "2025-12-29 03:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhao9e",
              "author": "TastyWriting8360",
              "text": "Finally I am happy to see a smart fellow adventurer can you please clarify that it works here for this people if you don't mind of course. [https://www.reddit.com/r/LocalLLaMA/comments/1py8yyw/r\\_progressive\\_lora\\_merging\\_complete\\_model/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1py8yyw/r_progressive_lora_merging_complete_model/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
              "score": 1,
              "created_utc": "2025-12-29 03:03:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhzqel",
          "author": "ZealousidealShoe7998",
          "text": "wait until people realize that are literally models out there that super small and efficient and could literally work great for very niche scenarios and it would take is just to train it a bit.   \nbeing a generalist takes lots of parameters but niche knowledge not so much.",
          "score": 1,
          "created_utc": "2025-12-29 05:44:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwidzyb",
              "author": "TastyWriting8360",
              "text": "Well said, Exactly! meanwhile people at r/LocalLLaMA  is down voting me saying it wont work, even tho I provided a working model, the math, the paper and the code lol, it was a mistake sharing there, this is the correct place to share this, people who actually technical and do finetuning and machine learning, not just users.",
              "score": 1,
              "created_utc": "2025-12-29 07:45:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi67an",
          "author": "ramendik",
          "text": "What do you mean by \"model identity\"?",
          "score": 1,
          "created_utc": "2025-12-29 06:37:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwidgfh",
              "author": "TastyWriting8360",
              "text": "The patterns that make the model \"itself\":\n\n\\- How it responds to \"who are you\" / \"what are you\"\n\n\\- Its default reasoning style and thought patterns\n\n\\- Built-in safety responses and refusals\n\n\\- Knowledge it considers core vs peripheral\n\n\\- Personality traits baked in during RLHF\n\nA fresh Qwen will say \"I am Qwen, developed by Alibaba\" and reason in a specific way. After PLM, it says \"I am \\[your model name\\]\" and reasons the way YOUR training data taught it.\n\nIt's not just changing a system prompt. The weights themselves no longer encode \"I am Qwen.\" That information is gone, replaced by your training data.\n\nIts still finetuning but the end result is a fully trained model not related to the original anymore.",
              "score": 1,
              "created_utc": "2025-12-29 07:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiggat",
          "author": "Thick-Protection-458",
          "text": "How is that different from ReLoRA?",
          "score": 1,
          "created_utc": "2025-12-29 08:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwih2ve",
              "author": "TastyWriting8360",
              "text": "ReLoRA is for pretraining from scratch. You start with random weights and want to train a new model efficiently. The problem it solves: full-rank pretraining is expensive, so use iterative low-rank updates to approximate full-rank training with less memory.  \n  \nReLoRA requirements:  \n\\- Warm start: 25% of training must be full-rank before switching to LoRA  \n\\- Jagged LR scheduler: learning rate resets after each merge to prevent divergence  \n\\- Partial optimizer reset: prune 99% of optimizer state by magnitude after each merge  \n\\- Multi-GPU setup: paper uses multiple A100s  \n\\- Tested up to 1.3B, attempts to scale beyond that were unsuccessful  \n  \nPLM is for identity replacement in pretrained models. You start with Qwen/Llama/etc and want to make it into YOUR model. The problem it solves: you want to leverage billions of dollars of pretraining but completely change what the model is.  \n  \nPLM requirements:  \n\\- No warm start: you inherit the pretrained weights as-is  \n\\- Standard training: no special schedulers or optimizer resets  \n\\- Single 24GB GPU: train in 4-bit, merge in BF16 on CPU  \n\\- Dataset mixing: 50% new / 50% historical to preserve your identity while erasing base  \n\\- Tested on 14B, scales higher with CPU offload for merge  \n  \nReLoRA: random weights ‚Üí efficient pretraining ‚Üí new model (up to 1.3B)  \nPLM: pretrained model ‚Üí identity replacement ‚Üí your model (14B+)  \n  \nSame mechanic, opposite directions, different scale.",
              "score": 2,
              "created_utc": "2025-12-29 08:13:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhdxzd",
          "author": "Ok-Adhesiveness-4141",
          "text": "Am most interested in this, is there anything on GitHub?",
          "score": 1,
          "created_utc": "2025-12-29 03:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhemjd",
              "author": "TastyWriting8360",
              "text": "Yes there is in the link",
              "score": 1,
              "created_utc": "2025-12-29 03:26:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhj2dl",
          "author": "ANTIVNTIANTI",
          "text": "omfgoddamn this sounds freaking amazing! MUCH LOVES!!!",
          "score": -1,
          "created_utc": "2025-12-29 03:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhji5r",
              "author": "TastyWriting8360",
              "text": "Thank you I am glad you like enjoy!",
              "score": 1,
              "created_utc": "2025-12-29 03:55:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwx4s5",
      "title": "Minimax M2.1 LoRa",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pwx4s5/minimax_m21_lora/",
      "author": "Mother_Context_2446",
      "created_utc": "2025-12-27 12:52:17",
      "score": 17,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hey guys,\n\nWill Unsloth plan to support fine tuning of this model in the near future?\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pwx4s5/minimax_m21_lora/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nw6sy7a",
          "author": "yoracale",
          "text": "It should already be supported. Use trust\\_remote\\_code = true\n\nHere is more info in docs: [https://unsloth.ai/docs/basics/troubleshooting-and-faqs#fine-tuning-a-new-model-not-supported-by-unsloth](https://unsloth.ai/docs/basics/troubleshooting-and-faqs#fine-tuning-a-new-model-not-supported-by-unsloth)",
          "score": 4,
          "created_utc": "2025-12-27 13:40:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw76q5v",
              "author": "Mother_Context_2446",
              "text": "Very cool thank you",
              "score": 1,
              "created_utc": "2025-12-27 15:05:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvdrzw",
      "title": "Should I switch to using DoRA instead of LoRA?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pvdrzw/should_i_switch_to_using_dora_instead_of_lora/",
      "author": "CartographerFun4221",
      "created_utc": "2025-12-25 13:50:11",
      "score": 16,
      "num_comments": 17,
      "upvote_ratio": 0.91,
      "text": "I've been training a small LLM on the medical field and have been doing CPT using full parameters. Due to this I've been limited to models around 3B in size (GPU poor, AWS creds almost ran out). I know LoRA won't be ideal for me, I have about 200M high quality tokens to do CPT with and I feel like LoRA will just not instill as much as I want. If I used DoRA, will I get as much benefit as full parameter fine-tuning? I'm okay with eating the slower processing costs because at least they'll be instances I can afford. \n\nAdditionally, should I be using DoRA for SFT too? Does each model need bespoke support upon release or is it more of a case of it being so new that the unsloth implementation could be improved? If the only downside right now is slower processing + maybe slightly more VRAM usage compared to LoRA, but gives similar performance to full parameter tuning then that's a win IMO. thoughts? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pvdrzw/should_i_switch_to_using_dora_instead_of_lora/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nvvh263",
          "author": "KvAk_AKPlaysYT",
          "text": "![gif](giphy|l0IsIC9ZNOELYmuqc|downsized)\n\nD-d-d-d-dora",
          "score": 6,
          "created_utc": "2025-12-25 14:31:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvthr9",
          "author": "LA_rent_Aficionado",
          "text": "What‚Äôs your vram situation like, what ranks are you using, what layers are you targeting ? \n\nEven in ideal scenarios and high quality data I suspect a 3B model just isn‚Äôt capable enough for anything meaningful especially considering a LORA is only updating target parameters.\n\nBest bet would be to switch to collab - some of the 40GB options may even enable full fine tune with gradient checkpoint/offloading",
          "score": 2,
          "created_utc": "2025-12-25 15:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw6ckc",
              "author": "CartographerFun4221",
              "text": "24GB - 48GB instances from AWS Sagemaker, quickly running out of credit hence why i want to use the 24GB cards as much as possible. I'm using r 64, alpha 128 and targetting all layers",
              "score": 1,
              "created_utc": "2025-12-25 17:10:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwetza",
                  "author": "LA_rent_Aficionado",
                  "text": "Without knowing much about your quality of data, model, epochs, training loss etc it‚Äôs really hard to gauge.\n\nThat said, when you say medical I think pretty extensive domain expertise.  A 3B model is not smart by any standard, I‚Äôd trust it for very specialized tasks (to a degree) but instilling domain knowledge on perhaps 10% of parameters (with those ranks) and likely at Q4 with unsloth will have it‚Äôs limitations.",
                  "score": 2,
                  "created_utc": "2025-12-25 18:00:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy9h7t",
          "author": "Mabuse046",
          "text": "I prefer DoRA to Lora just as a matter of progression - first we had LoRA, then we had QLoRA, now we have DoRA/QDORA. I'm a little curious when other trainers are faced with the choice why they don't just use DoRA because it's the newest hotness.\n\nIf you have the system for training and you want to train full weights, you might look into LISA training which helps against catastrophic forgetting by randomly selecting a few layers to train while keeping the rest of the weights frozen, then shuffling which layers are frozen/unfrozen every so many steps. It's not built into any trainers as far as I know but I usually just write in a simple callback to rotate layers every x number of steps.",
          "score": 1,
          "created_utc": "2025-12-26 00:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw44to",
          "author": "Ok_Appearance3584",
          "text": "You \"feel\" LoRA won't instill enough. It will.",
          "score": 0,
          "created_utc": "2025-12-25 16:56:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw6o9p",
              "author": "CartographerFun4221",
              "text": "To what extent, compared to full parameter finetuning and DoRA? I'm embedding new knowledge into the model consisting of case vignettes, atomic facts, QAs etc, a LoRA will objectively instill less than the other techniques",
              "score": 1,
              "created_utc": "2025-12-25 17:12:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvx3fsy",
                  "author": "Ok_Appearance3584",
                  "text": "Do whatever you want. But there's nothing special about your use case, LoRA will be just fine. You won't be able to tell the difference from full finetuning.¬†\n\n\nYou can merge the adapter to the main model and pretend you did full finetuning if it makes you feel better:)",
                  "score": 2,
                  "created_utc": "2025-12-25 20:25:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pv75f9",
      "title": "How to do continuous pre-training for GPT-OSS 20B",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pv75f9/how_to_do_continuous_pretraining_for_gptoss_20b/",
      "author": "Similar_Pick2914",
      "created_utc": "2025-12-25 06:30:35",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.82,
      "text": "This model itself is already a reasoning model after instruction tuning, how can we perform CPT on it? I'd like to inject private knowledge into this",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pv75f9/how_to_do_continuous_pretraining_for_gptoss_20b/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nvu4x82",
          "author": "yoracale",
          "text": "Read our blog for continued pretraining here: [https://unsloth.ai/blog/contpretraining](https://unsloth.ai/blog/contpretraining)\n\nYou can use our continued pretraining notebook for Korean or text completion and apply similar concepts for gpt-oss-20b",
          "score": 3,
          "created_utc": "2025-12-25 06:44:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvu5lq8",
              "author": "Similar_Pick2914",
              "text": "Hi u/yoracale thanks for your reply, but CPT is typically performed on a Base Model, right? And the GPT OSS model is already a chat model with reasoning capabilities, so how do I perform CPT on this kind of model",
              "score": 2,
              "created_utc": "2025-12-25 06:50:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvunwli",
                  "author": "shing3232",
                  "text": "you can still do it but CPT might degrade instructions following somewhat.",
                  "score": 1,
                  "created_utc": "2025-12-25 10:06:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvva9g1",
                  "author": "CartographerFun4221",
                  "text": "You can still do CPT on the reasoning model but it focuses the model more towards the distribution of your CPT data but any degradation of general abilities can be kinda fixed in SFT. Also make sure you consider full fine-tuning (or at least DoRA) instead of LoRA depending on how much knowledge you want to instill into the model. LoRA will only help so much as it freezes layers, full fine-tuning is best but requires lots of VRAM and DoRA is supposed to be a middle ground between both. After CPT consider how you want to build and mix your SFT dataset to make sure your use cases are catered to (e.g if you want it to use tool calling then make sure you have those examples in SFT, or if you want to keep reasoning abilities then ensure the SFT examples have reasoning in them and not just input/output). GLHF",
                  "score": 1,
                  "created_utc": "2025-12-25 13:41:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1py5j8p",
      "title": "Can't load Ministral-3 models for finetuning. Config file issue ?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1py5j8p/cant_load_ministral3_models_for_finetuning_config/",
      "author": "LostBejamin",
      "created_utc": "2025-12-28 23:10:53",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "**EDIT : I corrected the problem by installing transformers library via github with this command:**\n\n    pip install git+https://github.com/huggingface/transformers.git@bf3f0ae70d0e902efab4b8517fce88f6697636ce\n\n\\---\n\nI tried loading Ministral-3 models (bnb-4bit and basic versions of all size) locally, but I was unable to do so as It get me this error:\n\n`RuntimeError: Unsloth: No config file found - are you sure the \\`model\\_name\\` is correct?\\`\n\nI also tried with other models like [unsloth/functiongemma-270m-it-unsloth-bnb-4bit](https://huggingface.co/unsloth/functiongemma-270m-it-unsloth-bnb-4bit) and [unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit](https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit), and they seem to work just fine.\n\nDoes anyone has this problem or know how to deal with it ? Here the code I used:\n\n    from unsloth import FastLanguageModel\n    \n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Ministral-3-14B-Instruct-2512\",\n        load_in_4bit=True,\n    )\n\n(PS: I also wrote an issue ticket on [Github](https://github.com/unslothai/unsloth/issues/3788).)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1py5j8p/cant_load_ministral3_models_for_finetuning_config/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwjm7co",
          "author": "7h3_50urc3",
          "text": "You need the newest unsloth Version with transformers 5.0 lib. There is no official docker image from unsloth yet which fulfills these requirements, so you have to install it yourself or install all required libs into the the official docker image.",
          "score": 2,
          "created_utc": "2025-12-29 13:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpc8cb",
              "author": "LostBejamin",
              "text": "I installed transformers library via github with this command:\n\n    pip install git+https://github.com/huggingface/transformers.git@bf3f0ae70d0e902efab4b8517fce88f6697636ce\n\nAnd now the model load without any issues !",
              "score": 1,
              "created_utc": "2025-12-30 09:22:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwizdx0",
          "author": "im_datta0",
          "text": "Hey u/LostBejamin Thanks for creating a github issue for this. I am looking into it as we speak and would prefer to communicate there itself.",
          "score": 1,
          "created_utc": "2025-12-29 11:04:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv3b92",
      "title": "Best open source vision models for hockey tracking (and maybe analytics)?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pv3b92/best_open_source_vision_models_for_hockey/",
      "author": "NoClueDrew2",
      "created_utc": "2025-12-25 02:32:19",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "I have an RTX 5090 with 7970 Threadripper and an M3 Ultra Mac Studio with 80 GPU‚Äôs and 256GB of unified RAM. Unsloth team, 1) Thank you for what you guys do, you are fantastic. 2) would love your opinion on the best vision models to date for detecting and clipping shifts out of full youth/college/pro games. I have all the raw files but am struggling to find something capable. Would be appreciative of any insight/guidance considering your expertise. Thank you in advance and happy holidays! ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pv3b92/best_open_source_vision_models_for_hockey/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nvtc7x1",
          "author": "Hour_Association545",
          "text": "Some ideas to get the neurons firing https://blog.roboflow.com/identify-basketball-players/",
          "score": 1,
          "created_utc": "2025-12-25 02:44:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtfpmu",
              "author": "NoClueDrew2",
              "text": "Yeah the problem is basketball barely ever has any occlusions and hockey has them all the time. I‚Äôve been at this for months now. It‚Äôs such a fast moving sport, it‚Äôs been hard to figure out as far as accuracy in the 90-95% range.",
              "score": 1,
              "created_utc": "2025-12-25 03:11:07",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw8kf34",
              "author": "Hour_Association545",
              "text": "Yeah thats a good point, hockey has the puck occluded for a significant percentage of the game (along the boards) and the puck is really small so it's tougher for the object models to pick up.  Its a challenging problem given the tv based camera views.  If you could get an overhead view it might be a different story",
              "score": 1,
              "created_utc": "2025-12-27 19:19:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxf30i",
          "author": "LA_rent_Aficionado",
          "text": "RF-DETR seems very promising for accuracy. It has outperformed YOLO on complex scenes for me after a few days of testing\n\nAlso if you don‚Äôt have latency concerns it may be worth exploring a multi step workflow across various vision and VLM models, to more complex the task the less likely you are to have accurate one shot workflows in my experiences",
          "score": 1,
          "created_utc": "2025-12-25 21:37:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1px2ig2",
      "title": "Dreaming persistent Ai architecture > model size",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/2wmeztt69q9g1.jpeg",
      "author": "Empty-Poetry8197",
      "created_utc": "2025-12-27 16:54:47",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1px2ig2/dreaming_persistent_ai_architecture_model_size/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    }
  ]
}