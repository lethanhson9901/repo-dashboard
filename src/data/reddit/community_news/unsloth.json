{
  "metadata": {
    "last_updated": "2026-02-05 09:15:01",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 11,
    "total_comments": 85,
    "file_size_bytes": 94177
  },
  "items": [
    {
      "id": "1quvrmn",
      "title": "Qwen3-Coder-Next is released! üíú",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/7kswd313pahg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-03 15:59:43",
      "score": 500,
      "num_comments": 104,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1quvrmn/qwen3codernext_is_released/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3d2cke",
          "author": "danielhanchen",
          "text": "MXFP4 MoE and FP8-Dynamic quants are still converting!",
          "score": 26,
          "created_utc": "2026-02-03 16:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dabs3",
              "author": "GlobalLadder9461",
              "text": "How do you rate MXFP4 vs UD Q4 K XL in terms of quality and speed ?\n\nAny chance of getting KL divergence graph. Between them also adding q4_1. These are new quants added.\n\nHopefully we get a reply",
              "score": 7,
              "created_utc": "2026-02-03 16:48:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dr16r",
                  "author": "sourceholder",
                  "text": "Related question: is \"UD Q4 K XL\" able to leverage fast Blackwell 4-bit registers or does it fallback to 8-bits?   The primary appeal of MXFP4¬†is 4-bit native acceleration.",
                  "score": 7,
                  "created_utc": "2026-02-03 18:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dtk12",
              "author": "yoracale",
              "text": "They're all up now!",
              "score": 4,
              "created_utc": "2026-02-03 18:15:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3de1qg",
              "author": "StardockEngineer",
              "text": "Let‚Äôs gooooooooooooo üëè üéâ",
              "score": 1,
              "created_utc": "2026-02-03 17:05:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3hf121",
              "author": "debackerl",
              "text": "Awesome guys? Would it be possible to get MXFP4 on vLLM? vLLM is never getting as many quantization options as llama.cpp I find, but as shown by GPT OSS, it actually helps a lot to use MXFP4, even on an engine such as vLLM.",
              "score": 1,
              "created_utc": "2026-02-04 05:49:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3l7zwz",
              "author": "SomeAcanthocephala17",
              "text": "Has anyone tried benchmarking the MXFP4 model to see how much i scores compared tot he full FP16 model?",
              "score": 1,
              "created_utc": "2026-02-04 19:55:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d2100",
          "author": "qwen_next_gguf_when",
          "text": "This is perfection ü§©",
          "score": 7,
          "created_utc": "2026-02-03 16:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dekui",
          "author": "Effective_Head_5020",
          "text": "Thank you so much!\n\n\nI always wondered about the VRAM requirement. If I have 64gb of RAM only, will it work or will I have performance degradation?",
          "score": 4,
          "created_utc": "2026-02-03 17:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhj1f",
              "author": "yoracale",
              "text": "Absolutely you can. More VRAM will just make it faster.",
              "score": 2,
              "created_utc": "2026-02-03 17:21:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3hxr9z",
                  "author": "CatEatsDogs",
                  "text": "How I can do that? Was trying to run qwen 80b on 16+16vram and 64ram. It was failing to load models under ollama and lm studio.",
                  "score": 1,
                  "created_utc": "2026-02-04 08:32:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d2jpi",
          "author": "msrdatha",
          "text": "Thanks for the quick release...! \n\nWill there be an IQ4\\_NL Quant also?",
          "score": 3,
          "created_utc": "2026-02-03 16:11:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d5cew",
              "author": "yoracale",
              "text": "Yes it's converting right now!",
              "score": 2,
              "created_utc": "2026-02-03 16:24:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d3hyp",
          "author": "oldassveteran",
          "text": "Let‚Äôs gooooo!",
          "score": 4,
          "created_utc": "2026-02-03 16:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dthbw",
          "author": "brhkim",
          "text": "Okay I've never attempted to run an agentic coding LLM locally before -- seemed totally out of reach and not worth it v. paying for Claude. But this is WILD.\n\nHow do the hardware requirements scale when you're running subagents? If you have 3 separate subagents running with their own context (in addition to an orchestrator agent you're interacting with directly), how much more RAM/VRAM do you need to make things continue to run smoothly? Does that make sense? I assume tok/sec gen gets spread across parallel running subagents, and the added context per session means there's a lot more RAM usage just to context. But the model can be loaded \"centrally\", right? Or can it not run parallel sessions at all, they'd end up being sequential by query?",
          "score": 4,
          "created_utc": "2026-02-03 18:15:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jndl4",
              "author": "txgsync",
              "text": "At least on Mac, ‚Äúbatch inference‚Äù ‚Äî re-using the same model with multiple KV caches ‚Äî can allow a model that runs at dozens of tokens per second to thousands. Each slows down a tad but aggregate performance is wild. \n\nI‚Äôve been experimenting with handing the same model slightly different prompts and then having the model evaluate the best answer to a baseline prompt. This kind of ‚Äúswarm programming‚Äù seems to lead to better outcomes than rolling the dice with a single context. \n\nBut my harness is quite primitive.",
              "score": 2,
              "created_utc": "2026-02-04 15:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3k3pl3",
                  "author": "brhkim",
                  "text": "Huh, that's super interesting and extremely unintuitive. How could it be that they calculate totally independently??? I don't doubt what you're saying, it just is really hard to make sense of from an underlying technical perspective",
                  "score": 1,
                  "created_utc": "2026-02-04 16:50:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3gsiuh",
              "author": "not-really-adam",
              "text": "This is a really solid question. I haven‚Äôt thought about sub agents in the LLM context. My experience has been so poor compared to Opus 4.5 that I haven‚Äôt been bothered to push past just getting it setup.\n\nIt‚Äôs just so slow. And I have an M3 Ultra with 256GB. \n\nI hope this model works well and I can push it with some subagents. Might even consider running different versions of this model for primary vs subagents.",
              "score": 1,
              "created_utc": "2026-02-04 03:18:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hxu7o",
                  "author": "ImOutOfIceCream",
                  "text": "Fwiw i am running the same hardware as you and have had good results with opencode using qwen3-coder-30b for code gen/small model and qwen3 next as the reasoning model. There are definitely some quirks in behavior that differ from opus 4.5 but with a well configured set of instructions, skills, hooks, etc you can accomplish a lot. I‚Äôm excited to try this one.",
                  "score": 1,
                  "created_utc": "2026-02-04 08:33:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3i8xyx",
                  "author": "Street-Buyer-2428",
                  "text": "use vllm-mlx\n\n",
                  "score": 1,
                  "created_utc": "2026-02-04 10:18:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dzqq3",
          "author": "flavio_geo",
          "text": "Great performance on single XTX 7900 + Ryzen 7 9700X CPU 2x48gb 6000 MT/s with Q4\\_K\\_XL\n\n29.5 tokens/s (21.5 GB VRAM used)\n\nllama.cpp config:\n\n\"-ot\", \"blk.(2\\[0-9\\]|\\[3-4\\]\\[0-9\\]).ffn\\_.\\*\\_exps.=CPU\",\n\nUsing K and V type q8\\_0 and 64k token context setup\n\nNow lets go test the model in the daily works",
          "score": 5,
          "created_utc": "2026-02-03 18:43:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fsezt",
              "author": "BigYoSpeck",
              "text": "You should try the MXFP4 and if you aren't already ROCm for the backend. I'm on a Ryzen 9 5900X but only use 8 threads as performance caps out, 64gb DDR4 and a 16gb RX 6800 XT\n\nhttps://preview.redd.it/wlrgal8nadhg1.png?width=1368&format=png&auto=webp&s=e16215d54da118c756b7dea63c5c038f405f3179",
              "score": 3,
              "created_utc": "2026-02-03 23:56:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hxiif",
                  "author": "flavio_geo",
                  "text": "Thank you for the tip.\n\nJust tried the MXFP4 quant and got 32.7 tokens/s on same config. Using only \\~20.1GB VRAM.\n\nTried different option using Unsloth guide:\n\n\"-ot\", \"\\\\\\\\.(2\\[1-9\\]|\\[3-4\\]\\[0-9\\])\\\\\\\\.ffn\\_(gate|up|down)\\_exps.=CPU\",\n\nGot 34.9 tokens/s using 21.0GB VRAM\n\n\\*Feels like there is still room for improvement\n\n\\---\n\nAlso, for update: yesterday I tested the model inside my personal assistant platform (which is not for coding) and decided to try his coding skills just to check, and he just decide by himself (no instructions) to use my obsidian (which he has access too since he create tasks and notes for me), to create a note for tracking his coding task, and write the code with versions inside obsidian. That seems, at first, to indicate a very strong alignment towards agentic behavior. The code was very simple dinossaur pygame, so i cant say anything yet about his coding skills.",
                  "score": 2,
                  "created_utc": "2026-02-04 08:30:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mdtnk",
              "author": "usofrob",
              "text": "FYI, I tried keeping the kv cache unset through lm studio and saw a slight improvement in throughput with minimal impact on vram use.\n\nI've been using this model all day, and it's better than my ~70 GB versions of M2.1 and GLM 4.7 and any other models in this size that I've tested. I'm using it for python, json, html stuff today. I would run into jinja prompt template issues after 30k to 80k tokens until I removed \"| safe\" from the default template. I've gotten over 130k token usage without explicit errors, but it is having trouble with my current task. So, I may reset it soon to get a clean start again.\n\nBtw, I'm using opencode through lmstudio to 88GB of Amd VRAM.",
              "score": 2,
              "created_utc": "2026-02-04 23:21:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d5pit",
          "author": "Zeranor",
          "text": "Nice, let's see how this does compared to GLM 4.7 flash and Devstral 2 Small. But quick question: WHERE can I find the MXFP4 quants? :D I only find the \"regular\" quants.",
          "score": 3,
          "created_utc": "2026-02-03 16:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d6edn",
              "author": "yoracale",
              "text": "Sorry they're still converting ahaha will let u know once theyre up\n\nEdit: they're out now: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF?show_file_info=Qwen3-Coder-Next-MXFP4_MOE.gguf",
              "score": 6,
              "created_utc": "2026-02-03 16:29:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3d6vem",
                  "author": "Zeranor",
                  "text": "ahh yes, nice, I see, sorry for being too excited ;)",
                  "score": 2,
                  "created_utc": "2026-02-03 16:31:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3imz7c",
                  "author": "1-a-n",
                  "text": "Thanks! What sort of tps is expected on Blackwell 6000, 1st simple test with LMStudio and the MXFP4 guff only managed \\~44tps? Utilisation \\~60% power max 200W.",
                  "score": 1,
                  "created_utc": "2026-02-04 12:14:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d6xxq",
          "author": "FullstackSensei",
          "text": "Guess it's still uploading? Q8 isn't there yet üòÇ",
          "score": 2,
          "created_utc": "2026-02-03 16:32:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dlpuy",
              "author": "yoracale",
              "text": "Should be all up now!",
              "score": 2,
              "created_utc": "2026-02-03 17:40:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3dnovw",
                  "author": "FullstackSensei",
                  "text": "Thanks! Already halfway through the download\n\nWas checking the page every couple of mins üòÇ",
                  "score": 2,
                  "created_utc": "2026-02-03 17:49:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d7nn2",
              "author": "yoracale",
              "text": "You're right lol, I just realised. Will need to wait a few more mins xD",
              "score": 1,
              "created_utc": "2026-02-03 16:35:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dmeep",
          "author": "ChopSticksPlease",
          "text": "Any comparision to Devstral small 2, Qwen3 coder and GLM-4.7-Flash ?",
          "score": 2,
          "created_utc": "2026-02-03 17:43:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hj41y",
              "author": "gtrak",
              "text": "It's much better",
              "score": 1,
              "created_utc": "2026-02-04 06:22:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l6ttc",
                  "author": "SomeAcanthocephala17",
                  "text": "SPEED or Quality?",
                  "score": 1,
                  "created_utc": "2026-02-04 19:49:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ewzku",
          "author": "Suitable-Program-181",
          "text": "Damn this is so gooooood!",
          "score": 2,
          "created_utc": "2026-02-03 21:18:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fz8z3",
              "author": "Motor_Ocelot_1547",
              "text": "more story pz",
              "score": 1,
              "created_utc": "2026-02-04 00:33:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3if40k",
          "author": "timbo2m",
          "text": "Oh wow, on my i9 rig with a 4090 with only 32GB of RAM I can get 32 tokens per second.\n\nAMAZING\n\nhttps://preview.redd.it/evieatpynghg1.png?width=977&format=png&auto=webp&s=316dd91249530bb544c780283491d3eeeab1d129",
          "score": 2,
          "created_utc": "2026-02-04 11:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d3eu2",
          "author": "PrefersAwkward",
          "text": "Can anyone recommend a good tool that can use a local LLM like this for development?¬†",
          "score": 2,
          "created_utc": "2026-02-03 16:15:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d4u53",
              "author": "yoracale",
              "text": "We made a guide for Codex and Claude Code which you can view here: [https://unsloth.ai/docs/models/qwen3-coder-next#improving-generation-speed](https://unsloth.ai/docs/models/qwen3-coder-next#improving-generation-speed)",
              "score": 8,
              "created_utc": "2026-02-03 16:22:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3dt2m4",
              "author": "synth_mania",
              "text": "Aider's community fork, \"cecli\" is a good bet.\n\n[https://github.com/dwash96/cecli](https://github.com/dwash96/cecli)",
              "score": 3,
              "created_utc": "2026-02-03 18:13:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3i22ab",
              "author": "timbo2m",
              "text": "For llama.cpp:  \nUI: [https://github.com/ggml-org/llama.cpp/discussions/16938](https://github.com/ggml-org/llama.cpp/discussions/16938)  \nAPI: [https://github.com/ggml-org/llama.cpp/tree/master/tools/server](https://github.com/ggml-org/llama.cpp/tree/master/tools/server)",
              "score": 2,
              "created_utc": "2026-02-04 09:13:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d6q62",
              "author": "dsartori",
              "text": "Cline recommends qwen3-coder and they work really well together. This should be good too.",
              "score": 1,
              "created_utc": "2026-02-03 16:31:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gra2l",
              "author": "stuckinmotion",
              "text": "I've had good experience with roo code",
              "score": 1,
              "created_utc": "2026-02-04 03:11:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ddua6",
          "author": "Fox-Lopsided",
          "text": "I wonder how fast it would be with 16 VRAM and 32DRAM",
          "score": 1,
          "created_utc": "2026-02-03 17:04:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhs4v",
              "author": "yoracale",
              "text": "10+ tokens/s",
              "score": 1,
              "created_utc": "2026-02-03 17:22:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3de3wz",
          "author": "KillerX629",
          "text": "Is there any chance of getting a QAD version? Very interested in looking at how that performs",
          "score": 1,
          "created_utc": "2026-02-03 17:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dtnfl",
              "author": "yoracale",
              "text": "QAD or QAT?",
              "score": 1,
              "created_utc": "2026-02-03 18:16:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3frk96",
                  "author": "KillerX629",
                  "text": "QAD, the one recently proposed by nvidia",
                  "score": 1,
                  "created_utc": "2026-02-03 23:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dfd7k",
          "author": "Proper_Taste_6778",
          "text": "Could you make your version of Step 3.5 Flash?",
          "score": 1,
          "created_utc": "2026-02-03 17:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhl5b",
              "author": "yoracale",
              "text": "I'm not sure if llama.cpp supports it tbh",
              "score": 2,
              "created_utc": "2026-02-03 17:21:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3diwpy",
                  "author": "Proper_Taste_6778",
                  "text": "They're working on it probablyüòÖ thx for fast answer \n\nhttps://github.com/stepfun-ai/Step-3.5-Flash/blob/main/llama.cpp/docs/step3.5-flash.md",
                  "score": 2,
                  "created_utc": "2026-02-03 17:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3eke2o",
                  "author": "Mr_Back",
                  "text": "[https://github.com/ggml-org/llama.cpp/pull/19283](https://github.com/ggml-org/llama.cpp/pull/19283)",
                  "score": 1,
                  "created_utc": "2026-02-03 20:19:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dil5r",
          "author": "alfpacino2020",
          "text": "funvionara  con 16vram y 48 ram?  en llm studio?",
          "score": 1,
          "created_utc": "2026-02-03 17:26:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dkolh",
              "author": "yoracale",
              "text": "Yes it works, just follow our guide: [https://unsloth.ai/docs/models/qwen3-coder-next](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 1,
              "created_utc": "2026-02-03 17:35:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dv2tk",
          "author": "milkipedia",
          "text": "Nice that there is an MXFP4 quant in there! Going to give this a try soon",
          "score": 1,
          "created_utc": "2026-02-03 18:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dy30q",
          "author": "fernando782",
          "text": "Any benchmark comparison with Claude Sonnet 4.5, Claude Opus 4.5? those are the best coding models out there!\n\n",
          "score": 1,
          "created_utc": "2026-02-03 18:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3etnod",
          "author": "turbomedoqa",
          "text": "I tried the MXFP4 version and it flies at 50 t/s on 48GB VRAM. I am using it at Temperature 0.1. Is there any reason why would I run it at 1.0 for coding or instructions?",
          "score": 1,
          "created_utc": "2026-02-03 21:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3euuvh",
          "author": "TheSpicyBoi123",
          "text": "Do you have images of the spectrogram of the generated music? Would be very interresting what it actually makes. Additionally, on which data was it trained? Its not exactly a \\*garden variety\\* project to find \\~thousands of hours of genuine lossless music. Either way, solid job!",
          "score": 1,
          "created_utc": "2026-02-03 21:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f1kxl",
          "author": "Skt_97",
          "text": "Has anyone had a chance to compare it with a 3.5 step flash? It would be interesting to see which of the two is better.",
          "score": 1,
          "created_utc": "2026-02-03 21:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3grgcl",
              "author": "stuckinmotion",
              "text": "In my preliminary testing step flash was struggling and qwen was doing well",
              "score": 1,
              "created_utc": "2026-02-04 03:12:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hlf0h",
                  "author": "Skt_97",
                  "text": "It's crazy how the Step 3.5 Flash benchmarks are so much higher (probably \"maxed\"?)\nWhat did you test with? I'd like to see how it performs with Opencode.",
                  "score": 1,
                  "created_utc": "2026-02-04 06:42:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fa3do",
          "author": "Status_Contest39",
          "text": "The open-source model supports the first echelon of speed, and this operation is so great that it takes off directly!",
          "score": 1,
          "created_utc": "2026-02-03 22:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fxlfx",
          "author": "LittleBlueLaboratory",
          "text": "Anyone else getting this error when¬† trying to use Q6_K_XL in llama.cpp?\n\n\nLlama_model_load: error loading model: missing tensor 'blk.0.ssm_in.weight'\n\n\nI have downloaded the model twice already thinking I just got a corrupted download or something but it keeps happening.",
          "score": 1,
          "created_utc": "2026-02-04 00:24:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gyu9e",
              "author": "yoracale",
              "text": "Can you try another quant and see if it still happens?",
              "score": 1,
              "created_utc": "2026-02-04 03:57:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ju5ho",
                  "author": "LittleBlueLaboratory",
                  "text": "I just tried Q2_K_XL and confirmed the exact same error. Must be something with my environment? Any suggestions on what I should look at to fix it? I just did a git pull on my llama.cpp right before trying this.",
                  "score": 1,
                  "created_utc": "2026-02-04 16:06:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3g4ljy",
          "author": "DaringNinja",
          "text": "I am definitely doing something wrong seeing everyone else's token numbers. Using a 3090 and 128gb RAM only seeing 7 tokens/s with MXFP4 on LM Studio.",
          "score": 1,
          "created_utc": "2026-02-04 01:02:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gwnta",
              "author": "yoracale",
              "text": "Did you try using llama.cpp isntead and follow our guide? it's more optimized",
              "score": 1,
              "created_utc": "2026-02-04 03:43:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jzrv0",
                  "author": "DaringNinja",
                  "text": "I hadn't, but finally set it up last night based on the guide. Around 28 t/s now! Totally usable, especially for a model that doesn't fully fit on vram.\n\n12900K, RTX 3090, 128gb DDR4 3300.",
                  "score": 1,
                  "created_utc": "2026-02-04 16:32:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hjdij",
              "author": "gtrak",
              "text": "I can get 30 t/s on lm studio with a 4090.",
              "score": 1,
              "created_utc": "2026-02-04 06:24:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3i64nf",
              "author": "turbomedoqa",
              "text": "I have 48gb VRAM (5000 blackwell) and 192GB ram. It runs completely on VRAM with 50t/s.",
              "score": 1,
              "created_utc": "2026-02-04 09:52:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3grzzf",
          "author": "ab2377",
          "text": "i have 48gb, on mb, tell me in your opinion which gguf quant will be best, or is there not much hope!",
          "score": 1,
          "created_utc": "2026-02-04 03:15:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hho1f",
          "author": "UfuomaBabatunde",
          "text": "*cries in 12 GB VRAM",
          "score": 1,
          "created_utc": "2026-02-04 06:10:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hmyhk",
          "author": "Zeranor",
          "text": "So, talking configuration: Would this be a model with which I should chose to \"offload MoE experts to CPU\"? (16 GB VRAM / 128 GB RAM) :)",
          "score": 1,
          "created_utc": "2026-02-04 06:55:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i0b0h",
          "author": "Calm-Republic9370",
          "text": "how much context would be able to experience if i have 48gb vram\n\n?",
          "score": 1,
          "created_utc": "2026-02-04 08:56:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i60il",
              "author": "turbomedoqa",
              "text": "Around 140.000, at least in my case. And it's fast, 50t/s.",
              "score": 1,
              "created_utc": "2026-02-04 09:50:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3if3tv",
          "author": "Spiritual_Leg_7683",
          "text": "Can this shit run on my RTX 3090?",
          "score": 1,
          "created_utc": "2026-02-04 11:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iqmq2",
          "author": "Americanuu",
          "text": "I might not ask this in the right place but what agentic code works decent on 32gb of RAM  and 8GB VRAM ?",
          "score": 1,
          "created_utc": "2026-02-04 12:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j3gqa",
          "author": "dwrz",
          "text": "/u/danielhanchen -- sorry to ping you directly, but with `llama.cpp` the model seems to constantly hallucinate missing closing braces. Seems like this is happening to others as well: https://www.reddit.com/r/LocalLLaMA/comments/1quvqs9/comment/o3edjam/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button. Do you have any insights on this? I'm using the Q8_0 GGUF.",
          "score": 1,
          "created_utc": "2026-02-04 13:54:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3juvem",
          "author": "zp-87",
          "text": "I will try to run it on 2x 5060TI 16GB + 96GB RAM. I hope it will work",
          "score": 1,
          "created_utc": "2026-02-04 16:10:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lnske",
              "author": "zp-87",
              "text": "It does work in LM Studio with RooCode (I had to edit prompt template and remove |safe).  \nGPU offload: 22, Context length: 100 000.  \n \\- Prompt Evaluation (Input): \\~48 tokens/second.  \n \\- Generation (Output): \\~3.6 tokens/second.  \nQuite slow but it works.",
              "score": 1,
              "created_utc": "2026-02-04 21:10:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k3c2t",
          "author": "Shoddy_Bed3240",
          "text": "I‚Äôm using Unsloth Q8\\_K\\_XL (93.4 GB) across two GPUs with 56 GB total VRAM. Generation speed is about 35 tokens/sec, which is totally usable.",
          "score": 1,
          "created_utc": "2026-02-04 16:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nbsae",
          "author": "FartOnYourBoofMound",
          "text": "cool - is that why i was getting weird stuff like this?\n\nhttps://preview.redd.it/ov5s9ir38lhg1.png?width=1711&format=png&auto=webp&s=c9911275e8f92ac2542e5bc0a6c95d6daaf2b8a8",
          "score": 1,
          "created_utc": "2026-02-05 02:32:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o6b9h",
              "author": "yoracale",
              "text": "Yes you need to update llama.cpp and redownload our quants",
              "score": 1,
              "created_utc": "2026-02-05 05:51:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oehli",
          "author": "Proximity_afk",
          "text": "Hey, just a beginner here, what exact quantized model can I  run on 48gb VRAM (typically over an Agentic rag system)???",
          "score": 1,
          "created_utc": "2026-02-05 07:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dwej6",
          "author": "No_Afternoon_4260",
          "text": "How benchmaxxed is it?",
          "score": 1,
          "created_utc": "2026-02-03 18:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i1qtg",
              "author": "RealisticPrimary8",
              "text": "probably a ton, no way it outperforms kimi k2.5 in the real world.",
              "score": 1,
              "created_utc": "2026-02-04 09:10:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d0tei",
          "author": "Otherwise_Wave9374",
          "text": "That 256K context + \"agentic coding\" angle is really interesting, local agent setups get way more usable once you can keep a lot of repo + docs in context without constant chunking. Have you noticed any gotchas with tool calling or long horizon tasks (like refactors) vs quick one shot codegen?\n\nIm always looking for notes on building coding agents and evaling them, a few posts Ive bookmarked are here: https://www.agentixlabs.com/blog/",
          "score": -3,
          "created_utc": "2026-02-03 16:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dd177",
              "author": "pokemonplayer2001",
              "text": "You‚Äôre such a shill, FO.",
              "score": 5,
              "created_utc": "2026-02-03 17:00:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k475p",
          "author": "Oxffff0000",
          "text": "How do we build machines with that amount of VRAM? The cards I know are only 24Gb. Does that mean, you'll have to install multiple nvidia cards?",
          "score": 0,
          "created_utc": "2026-02-04 16:52:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kln4x",
              "author": "kkazakov",
              "text": "Not necessarily. I have ADA 6000 and I plan to try it.",
              "score": 1,
              "created_utc": "2026-02-04 18:13:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3daw4d",
          "author": "getmevodka",
          "text": "How big is that ? I have 96gb vram available üòäüòÖüëç",
          "score": -4,
          "created_utc": "2026-02-03 16:50:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dg7gg",
              "author": "some_user_2021",
              "text": "The answer is on the post",
              "score": 2,
              "created_utc": "2026-02-03 17:15:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dgb02",
                  "author": "getmevodka",
                  "text": "Yes it is indeed. Thanks",
                  "score": 3,
                  "created_utc": "2026-02-03 17:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvt6qy",
      "title": "Qwen3-Coder-Next GGUFs updated - now produces much better outputs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/yoeghkey7ihg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-04 16:28:56",
      "score": 147,
      "num_comments": 44,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qvt6qy/qwen3codernext_ggufs_updated_now_produces_much/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3kl6da",
          "author": "Nieles1337",
          "text": "Hey, just wanted to check the update and noticed the MXFP4\\_MOE is still from yesterday, is this still cooking? Or is this the \"This week we'll also release a new MoE update\" you mention? Thanks!",
          "score": 6,
          "created_utc": "2026-02-04 18:10:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ks81r",
              "author": "GlobalLadder9461",
              "text": "Also q8 are not updated",
              "score": 3,
              "created_utc": "2026-02-04 18:42:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3limxk",
                  "author": "NebulaBetter",
                  "text": "I am waiting for this one too",
                  "score": 1,
                  "created_utc": "2026-02-04 20:46:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m7tb3",
                  "author": "Zc5Gwu",
                  "text": "Welp, that was a waste of bandwidth, sorry huggingface.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:48:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3n65m3",
                  "author": "danielhanchen",
                  "text": "Q8_0 and Q8_K_XL should be fine - updating llama.cpp is all you need - sorry should have mentioned it earlier",
                  "score": 1,
                  "created_utc": "2026-02-05 02:00:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m3ic7",
              "author": "StardockEngineer",
              "text": "u/yorascale Is mxfp4 ok or?",
              "score": 2,
              "created_utc": "2026-02-04 22:26:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n69fs",
                  "author": "danielhanchen",
                  "text": "MXFP4 is fine - just a llama.cpp update!",
                  "score": 1,
                  "created_utc": "2026-02-05 02:00:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3n62px",
              "author": "danielhanchen",
              "text": "Oh hey so sorry - MXFP4-MoE is fine - just a llama.cpp update would work!",
              "score": 1,
              "created_utc": "2026-02-05 01:59:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ko1fk",
          "author": "DocWolle",
          "text": "was the update needed? For me it seems fixed after updating llama.cpp.",
          "score": 3,
          "created_utc": "2026-02-04 18:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6bnq",
              "author": "danielhanchen",
              "text": "Yes imatrix ones still need them - BF16, Q8_0, Q8_K_XL, MXFP4 are fine, but the rest need updating",
              "score": 1,
              "created_utc": "2026-02-05 02:01:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kjisa",
          "author": "CogahniMarGem",
          "text": "Sorry but how about the tool call, are there anyone test it.",
          "score": 2,
          "created_utc": "2026-02-04 18:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lsg8t",
              "author": "zoyer2",
              "text": "tested IQ4\\_XS now webgl and javascript repo. No looping, loaded 100k context. Actually performs really well",
              "score": 2,
              "created_utc": "2026-02-04 21:33:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n6cg2",
                  "author": "danielhanchen",
                  "text": "Oh nice!",
                  "score": 1,
                  "created_utc": "2026-02-05 02:01:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3kknyn",
          "author": "sausagefritters",
          "text": "Is it possible to make torrents or diffs to prevent having to redownload gigs of data again? Or are the models changed that much?",
          "score": 2,
          "created_utc": "2026-02-04 18:08:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6j2u",
              "author": "danielhanchen",
              "text": "Sadly imatrix changed a lot",
              "score": 1,
              "created_utc": "2026-02-05 02:02:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3nnn49",
              "author": "illkeepthatinmind",
              "text": "Boggles my mind torrents arent a thing with models.",
              "score": 1,
              "created_utc": "2026-02-05 03:41:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lpe5v",
          "author": "zoyer2",
          "text": "Many thanks! The IQ4\\_XS now performs really good compared to before! This is for sure a great replacement for GPT-OSS 120B for coding\n\nhttps://preview.redd.it/51hzzo02ojhg1.png?width=1578&format=png&auto=webp&s=701d6334c31ff311ca5504e4a1d3910ac9a0ed45\n\n",
          "score": 2,
          "created_utc": "2026-02-04 21:18:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6jjv",
              "author": "danielhanchen",
              "text": "Fantastic!",
              "score": 1,
              "created_utc": "2026-02-05 02:02:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lxk05",
          "author": "Zeranor",
          "text": "Oh boy.... this is an excellent model for (minor) local coding tasks. This is working WAY better than my recent attempts with GLM 4.7 flash, Devstral 2 Small or Qwen3 Next .... nice",
          "score": 2,
          "created_utc": "2026-02-04 21:57:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6p6p",
              "author": "danielhanchen",
              "text": "Yes the model is really good! Qwen really cooked this time!",
              "score": 1,
              "created_utc": "2026-02-05 02:03:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k87c2",
          "author": "FinalsMVPZachZarba",
          "text": "Did the bug affect qwen3-next too?",
          "score": 1,
          "created_utc": "2026-02-04 17:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3koujm",
              "author": "DocWolle",
              "text": "not sure if new ggufs were needed.¬†\nThe prompt from¬†https://github.com/ggml-org/llama.cpp/issues/19305\n\n\nworks by just updating llama.cpp",
              "score": 3,
              "created_utc": "2026-02-04 18:27:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n6hj1",
                  "author": "danielhanchen",
                  "text": "Yes! But imatrix needs updating - I'll need to redo the other Qwen3-Next ones as well",
                  "score": 1,
                  "created_utc": "2026-02-05 02:02:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kikud",
              "author": "DocWolle",
              "text": "I think so",
              "score": 2,
              "created_utc": "2026-02-04 17:59:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lrdxn",
          "author": "kryptkpr",
          "text": "In case anyone hasn't learned this lesson yet you should wait a week before downloading any new model the first implementation is broke 95% of the tome",
          "score": 1,
          "created_utc": "2026-02-04 21:27:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ly9yg",
              "author": "ServiceOver4447",
              "text": "probably too much AI to release it\n\nokay, i'll leave myself out now.",
              "score": 1,
              "created_utc": "2026-02-04 22:00:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3n6lf4",
              "author": "danielhanchen",
              "text": "Yes it's best probably to wait 3 days ish - we also do bug fixes on day 1 or 2",
              "score": 1,
              "created_utc": "2026-02-05 02:02:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mju9f",
          "author": "lukepacman",
          "text": "the version i downloaded about 12h ago generated at speed of 18 tok/s on apple silicon m1  \n  \nim on llama.cpp 7930 and IQ3\\_XXS\n\nwould the new version improve the speed?",
          "score": 1,
          "created_utc": "2026-02-04 23:54:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6n2e",
              "author": "danielhanchen",
              "text": "Speed not so much, but definitely accuracy",
              "score": 1,
              "created_utc": "2026-02-05 02:02:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nryk2",
                  "author": "lukepacman",
                  "text": "yeah im re-downloading the model and will retry to see how it's going then.\n\njust curious, since this is also a MoE 3B active params model, do you know why it's significantly slower than the others in the same class?",
                  "score": 1,
                  "created_utc": "2026-02-05 04:09:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mqqjl",
          "author": "iadanos",
          "text": "Does this change affect previous Qwen-Next models?",
          "score": 1,
          "created_utc": "2026-02-05 00:32:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6nll",
              "author": "danielhanchen",
              "text": "Yes sadly I have to reupload",
              "score": 3,
              "created_utc": "2026-02-05 02:03:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n99o2",
                  "author": "iadanos",
                  "text": "Would you kindly give a notice once it's done?",
                  "score": 1,
                  "created_utc": "2026-02-05 02:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k0ck9",
          "author": "boatbomber",
          "text": "Why does Qwen refuse to contribute to prevent these problems? It makes their models look worse on release when they underperform with community implementations of their architecture",
          "score": 2,
          "created_utc": "2026-02-04 16:35:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k1yb5",
              "author": "yoracale",
              "text": "This isn't related to them not contributing or not. There was no need for custom/new implementation because it's just using Qwen3 next arch which was already supported in llama.cpp\n\nIt was a normal human accident that caused this issue and it happens and was fixed. Even if Qwen helped for this release, this would've likely happened as it was a needle in a haystack situation.\n\nQwen team also previously contributed to llama.cpp for Qwen3 release. They need to allocate their time for smaller model releases like Qwen3-next otherwise there would a million repos to implement and contribute to\n\nI'm guessing they're working with the llama.cpp team for the next important Qwen model release.",
              "score": 20,
              "created_utc": "2026-02-04 16:42:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3k4oiw",
                  "author": "fancyrocket",
                  "text": "Is it worth downloading now or will i have to re-download it again in the near future for updates? Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-04 16:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qqc06x",
      "title": "How to Run Local LLMs with Claude Code & OpenAI Codex!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/6mhzmpzd6bgg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-29 15:42:38",
      "score": 123,
      "num_comments": 27,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qqc06x/how_to_run_local_llms_with_claude_code_openai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2fnep4",
          "author": "__Maximum__",
          "text": "Fine-tune?",
          "score": 4,
          "created_utc": "2026-01-29 16:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2frhwe",
              "author": "yoracale",
              "text": "Yep fine-tune! We use glm flash to autonomously fine-tune an LLM with unsloth",
              "score": 3,
              "created_utc": "2026-01-29 16:38:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2fr3lz",
              "author": "moonflowerseed",
              "text": "On Mac/Apple Silicon?",
              "score": 1,
              "created_utc": "2026-01-29 16:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2fre76",
                  "author": "yoracale",
                  "text": "We're working on Mac support for real. Optimizations are done, only thing next is checking, benchmarking and Integra tion",
                  "score": 8,
                  "created_utc": "2026-01-29 16:38:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2j0zdt",
          "author": "PixelatedCaffeine",
          "text": "Is there a way to change the Claude Code limit to match the model‚Äôs limit? It always seems to default to 200k, and I would love to use the auto compact feature based on that",
          "score": 2,
          "created_utc": "2026-01-30 02:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g6qtr",
          "author": "ethereal_intellect",
          "text": "They lose web search capability when linked to local models right?",
          "score": 1,
          "created_utc": "2026-01-29 17:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kd5p2",
              "author": "admajic",
              "text": "Not is you ask it to build you a mcp search tool.",
              "score": 1,
              "created_utc": "2026-01-30 07:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gahui",
          "author": "Glittering-Call8746",
          "text": "Prompt \"You can only work in the cwd project/. Do not search for CLAUDE.md - this is it. Install Unsloth via a virtual environment via uv. See https://unsloth.ai/docs/get-started/install/pip-install on how (get it and read). Then do a simple Unsloth finetuning run described in https://github.com/unslothai/unsloth. You have access to 1 GPU.\" What's the model it's finetuning..",
          "score": 1,
          "created_utc": "2026-01-29 18:04:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrpqc",
              "author": "yoracale",
              "text": "Llama most likely",
              "score": 1,
              "created_utc": "2026-01-29 22:14:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gu6cv",
          "author": "creminology",
          "text": "Has Anthropic ever given any indication that they view this as a breach of terms of service? Asking because they have come down on hard on using Claude Code subscriptions in other environments, although this is doing the reverse.",
          "score": 1,
          "created_utc": "2026-01-29 19:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrmg0",
              "author": "yoracale",
              "text": "Oh no, they allow this because Claude Code was meant to be used locally!",
              "score": 3,
              "created_utc": "2026-01-29 22:13:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2luumi",
              "author": "Otherwise-Way1316",
              "text": "They don‚Äôt like their models being used in other platforms, like OpenCode.\n\nAll indications are that they are ok with Claude Code being used with other models.",
              "score": 2,
              "created_utc": "2026-01-30 14:20:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kpq0d",
          "author": "No-Weird-7389",
          "text": "Still waiting for nvfp4",
          "score": 1,
          "created_utc": "2026-01-30 09:34:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l4ryc",
              "author": "yoracale",
              "text": "We're working on it! :) Might not be for this model but for future ones",
              "score": 1,
              "created_utc": "2026-01-30 11:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l3ars",
          "author": "SatoshiNotMe",
          "text": "Last I checked, running glm-4.7-flash with CC on my M1 Pro Max 64GB MacBook via llama-server got me an abysmal 3 tok/s, for less than the 20 tok/s I got with Qwen3-30B-A3B. I use this setup to hook up CC with local models:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nCurious what llama-server settings you recommend to get good performance with GLM-4.7-flash",
          "score": 1,
          "created_utc": "2026-01-30 11:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2x6voc",
              "author": "yoracale",
              "text": "When was the last time you tried it? A week ago llamacpp was updated to imrpove speed a lot for it",
              "score": 1,
              "created_utc": "2026-02-01 05:15:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oegdi",
          "author": "stuckinmotion",
          "text": "Does this work for anyone? I followed the steps, set ANTHROPIC\\_BASE\\_URL to my llama-server instance, but I'm getting \"Missing API key\"\n\nedit: Ok so exporting ANTHROPIC\\_API\\_KEY=sk-1234 got it working. Maybe the guide can be updated",
          "score": 1,
          "created_utc": "2026-01-30 21:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pq0sy",
              "author": "yoracale",
              "text": "Ooo ok interesting we'll update it in our guide then thanks for the feedback",
              "score": 1,
              "created_utc": "2026-01-31 01:30:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2x752j",
              "author": "yoracale",
              "text": "We just added it to our guide: [https://unsloth.ai/docs/basics/claude-codex](https://unsloth.ai/docs/basics/claude-codex)\n\nThanks so much for your feedback!",
              "score": 1,
              "created_utc": "2026-02-01 05:17:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xb6bx",
                  "author": "stuckinmotion",
                  "text": "Hey nice! Thanks for the work. It's been interesting playing with Claude code locally though it makes it obvious how much worse it is without their models",
                  "score": 1,
                  "created_utc": "2026-02-01 05:47:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2vuwel",
          "author": "JonatasLaw",
          "text": "Can I run it in a rtx 3090 + 64gb RAM?",
          "score": 1,
          "created_utc": "2026-02-01 00:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2x6zfd",
              "author": "yoracale",
              "text": "Yes ofc, it'll be fast for you. You can even run the 8-bit one",
              "score": 1,
              "created_utc": "2026-02-01 05:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fnjti",
          "author": "toreobsidian",
          "text": "This is awesome. I'll Test this with Just a dataset I'm currently preparing that Features content of a famous german political figure. Too bad I have so little time for this nonsens Project but this should be a nice boost üòÖ",
          "score": 1,
          "created_utc": "2026-01-29 16:21:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt7q4c",
      "title": "I bullied my dual 3060s into ruinning GLM-4.7-Flash  500+ T/s @ 70k Context on a Ryzen 2500 Potato. (Two Configs: \"Daily Driver\" vs. \"The Diesel Factory\")",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/gallery/1qt7q4c",
      "author": "MohammedGomaa",
      "created_utc": "2026-02-01 19:15:29",
      "score": 63,
      "num_comments": 44,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qt7q4c/i_bullied_my_dual_3060s_into_ruinning_glm47flash/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o310uy4",
          "author": "mukz_mckz",
          "text": "Please stop writing posts with AI.",
          "score": 19,
          "created_utc": "2026-02-01 19:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o312ok4",
              "author": "MohammedGomaa",
              "text": "i am not a native english speaker , i used ai to polish it",
              "score": 2,
              "created_utc": "2026-02-01 20:03:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31gwup",
                  "author": "--Spaci--",
                  "text": "Id rather struggle to read your paragraph, than have to read an AI's attempt at conveying information",
                  "score": 14,
                  "created_utc": "2026-02-01 21:13:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o34s89y",
              "author": "marko_mavecki",
              "text": "Huh? Maybe he can't? I am here for information. Not for formatting or the beauty of words. Besides, in few months you will no longer be able to differentiate between human and machine. Will it then be acceptable for you?",
              "score": 1,
              "created_utc": "2026-02-02 10:19:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3277zm",
          "author": "i_wayyy_over_think",
          "text": "Thanks for the detailed run configs. What do you use to actually use 64 concurrent quest at once besides running a benchmark? Can moltbot use that many concurrent requests effectively to get actual useful work done? I‚Äôve not tried it yet.",
          "score": 3,
          "created_utc": "2026-02-01 23:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o329d4z",
              "author": "MohammedGomaa",
              "text": "A swarm of AI agents plus multiple instance open code and the other coding agents",
              "score": 1,
              "created_utc": "2026-02-01 23:38:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o348hst",
          "author": "Previous_Nature_5319",
          "text": "good job, I think it will be useful for many people. thank you!",
          "score": 2,
          "created_utc": "2026-02-02 07:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37j6xu",
          "author": "Shivam_R_A",
          "text": "This is just awesome man!",
          "score": 2,
          "created_utc": "2026-02-02 19:25:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o316ecp",
          "author": "heaven00",
          "text": "Interesting stuff will have to try out",
          "score": 1,
          "created_utc": "2026-02-01 20:21:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31mixg",
          "author": "AbheekG",
          "text": "How do you handle the cache going stale? Doesn‚Äôt that happen fairly frequently with a codebase?",
          "score": 1,
          "created_utc": "2026-02-01 21:40:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31nnv4",
              "author": "MohammedGomaa",
              "text": "Sglang finds previously computed tokens and reuses it , very useful for ai agents  play books , system prompts and skills, i ran scheduled script    that removes chunks not accessed in the last 7 days",
              "score": 3,
              "created_utc": "2026-02-01 21:45:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31nsth",
                  "author": "AbheekG",
                  "text": "Very cool, thanks!",
                  "score": 2,
                  "created_utc": "2026-02-01 21:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33h8sb",
          "author": "Soft_Syllabub_3772",
          "text": "Can modify to use with rtx3090 x 2 and 192gb ram and 1tb nvme?",
          "score": 1,
          "created_utc": "2026-02-02 03:47:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33hp44",
              "author": "MohammedGomaa",
              "text": "Go fo it , the sky is the limit",
              "score": 1,
              "created_utc": "2026-02-02 03:50:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34ef1f",
          "author": "Aggravating_Bee3757",
          "text": "what model did you use? can i use it in my single 5060 ti 16/16?",
          "score": 1,
          "created_utc": "2026-02-02 08:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34lc83",
              "author": "MohammedGomaa",
              "text": "**GLM-4.7-Flash**¬†(the¬†`QuantTrio-AWQ`¬†flavor) , i dont think so , not without c\\[u offloading tanking performance",
              "score": 2,
              "created_utc": "2026-02-02 09:12:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34u3r2",
          "author": "jannemansonh",
          "text": "impressive setup for agent swarms... curious though - for production agent workflows, ended up using needle app since you just describe what you want vs configuring cuda graphs and cache layers. kept the self-hosted setup for experimenting but way easier when agents need to understand code/docs",
          "score": 1,
          "created_utc": "2026-02-02 10:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o355g99",
              "author": "MohammedGomaa",
              "text": "sorry , i dont realy get what you are asking about , BTW setting \n\n    --cuda-graph-bs 4 16 32  # makes sure that single requist get 20-70 t/s , depending on context cach , ie single  agent get 20-70 t/s depending on caching context and concurancy , with 450 + t/s on max concurancy",
              "score": 1,
              "created_utc": "2026-02-02 12:12:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38kj0z",
          "author": "SectionCrazy5107",
          "text": "i have not been able to get vllm to work with glm 4.7 flash on my v100+3080 so far, does sglang work out of the box? or any specific cuda + python + .. required?",
          "score": 1,
          "created_utc": "2026-02-02 22:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b3u9r",
              "author": "MohammedGomaa",
              "text": "uv pip install sglang==0.3.2.dev9039+pr-17247.g90c446848 --extra-index-url [https://sgl-project.github.io/whl/pr/](https://sgl-project.github.io/whl/pr/)\n\nuv pip install git+https://github.com/huggingface/transformers.git@76732b4e7120808ff989edbd16401f61fa6a0afa",
              "score": 1,
              "created_utc": "2026-02-03 08:07:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3icgpi",
          "author": "Flkhuo",
          "text": "ŸäÿßÿπŸÖ ŸÇŸÑÿ™ŸÑŸÉ ÿπŸÑŸä ÿßŸÑŸÅŸäÿ≥ ÿ®ŸàŸÉ ŸÖÿ™ŸÉÿ™ÿ®ÿ¥ ÿ®ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿßŸà ÿπŸÑŸä ÿßŸÑÿßŸÇŸÑ ŸÖÿ™ÿÆŸÑŸäŸáŸàÿ¥ ŸäŸÉÿ™ÿ® ŸÉÿ™Ÿäÿ± ŸÉŸÑÿßŸÖ ŸÉŸÑŸàÿ¥ ŸÑÿßÿ≤ŸÖŸá ÿπŸÑÿ¥ÿßŸÜ ÿ∞ÿßŸÉÿ±ÿ™ŸÜÿß ÿßÿµÿ®ÿ≠ÿ™ ŸÖÿ´ŸÑ ÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑÿ≥ŸÖŸÉŸá ŸÖŸÜ ŸÉÿ´ÿ± ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ÿå ÿßŸÑŸÜÿßÿ≥ ŸÉŸÑŸáÿß ÿπÿ±ÿ® Ÿàÿßÿ¨ÿßŸÜÿ® ÿßÿπÿ∑ŸàŸÉ ŸÅŸäÿØ ÿ®ÿßŸÉ ÿßŸÜŸÉ ŸÖÿ™ŸÉÿ™ÿ®ÿ¥ ŸÉÿ™Ÿäÿ± ÿ®ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸäÿå Ÿäÿßÿ±ÿ® ÿ™ÿßÿÆÿ∞ Ÿáÿ∞ÿß ÿßŸÑŸÉŸàŸÖŸÜÿ™ ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿßŸäÿ¨ÿßŸäŸá Ÿàÿ¥ÿ∫ŸÑŸÉ ÿ¨ŸÖŸäŸÑŸÉ ŸàÿßŸÜ ÿ¥ÿßÿ° ÿßŸÑŸÑŸá ŸÖŸÖŸÉŸÜ ÿ™ÿ∑Ÿàÿ± ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿßÿ≠ÿ≥ŸÜ ŸÖŸÜ ŸÉÿØÿß ÿ®ÿßŸÑÿ™ŸàŸÅŸäŸÇ",
          "score": 1,
          "created_utc": "2026-02-04 10:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j5qoh",
              "author": "MohammedGomaa",
              "text": "ÿ®ŸÉÿ±Ÿá ŸàŸÑÿß ÿ®ÿπÿØŸá ŸÖÿ¥ ŸáŸäŸÅÿ±ŸÇ ŸÉÿ™Ÿäÿ± ŸÑŸÖÿß ÿßŸÉÿ™ÿ® ÿ®ÿßŸäÿØŸä 5 % Ÿà Ÿäÿ∑ŸÑ ÿ®ÿ¨ŸàÿØŸá 90%  ÿßÿ≠ÿ≥ŸÜ ŸÖÿßŸÉÿ™ÿ® 100% ÿ®ÿ¨ŸàÿØŸá 100% .... ÿßŸÜÿß ÿßŸàŸÑŸä ÿ®ŸàŸÇÿ™Ÿä ÿßŸÜÿ™ ÿ≠ÿßÿ® ÿ™ÿ≥ÿ™ŸÅŸäÿØ ŸÖŸÜ ÿßŸÑŸÉŸÑÿßŸÖ ÿßÿ¥ÿ∑ÿß ŸÖÿ¥ ÿ≠ÿßÿ®ÿ® ÿ®ÿ±ÿ∂Ÿàÿß ÿßÿ¥ÿ∑ÿß .... ÿßŸÜÿß ÿßŸÜÿ¥ÿ± ŸÅŸä ŸÉÿ∞ÿß ŸÖŸÉÿßŸÜ .. ÿßŸÑŸä ÿ±ŸÉÿ≤Ÿàÿß ŸÅŸä ÿßŸÑŸÅÿ±ÿπ Ÿàÿ≥ÿßÿ®Ÿàÿß ÿßŸÑÿßÿµŸÑ 2-3 ŸÖŸÜ  Ÿà ÿßŸÑŸä ŸÜÿßŸÇÿ¥Ÿàÿß ÿ≥ÿßŸÑŸä ÿπŸÑŸä ÿ™ŸÅÿßÿµŸÑ 200+ ÿ∂ÿπŸÅ",
              "score": 1,
              "created_utc": "2026-02-04 14:07:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3j67vy",
              "author": "MohammedGomaa",
              "text": "Ÿà ÿßŸÑŸä ŸÖÿ¥ ÿπÿßÿ¨ÿ® ÿπÿßÿ¨ÿ® ÿ∫Ÿäÿ± : [https://www.reddit.com/r/LocalLLM/comments/1qt5l53/comment/o3gpvom/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLM/comments/1qt5l53/comment/o3gpvom/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)Ÿäÿßÿ±Ÿäÿ®ÿ™ ŸÜÿ±ŸÉÿ≤ ŸÅŸä ÿßŸÑŸÖŸáŸÖ",
              "score": 1,
              "created_utc": "2026-02-04 14:09:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31gezn",
          "author": "alhinai_03",
          "text": "no one will bother reading AI produced text.",
          "score": 0,
          "created_utc": "2026-02-01 21:10:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32d7hy",
              "author": "MohammedGomaa",
              "text": "You know you are on an AI focused subreddit almost 99% of AI mentioned here argued towards Text generation",
              "score": 7,
              "created_utc": "2026-02-01 23:59:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32gt6c",
                  "author": "alhinai_03",
                  "text": "Brother, this doesn't mean I'm okay with reading AI generated text. I get it if it's not your first language but I'd rather read human written content no matter how bad the grammar is. I'm sure you have good intentions but people are generally getting tired of it.",
                  "score": 2,
                  "created_utc": "2026-02-02 00:19:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qr4i8a",
      "title": "Experimental DeepSeek-V3.2 Dynamic GGUFs",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/",
      "author": "danielhanchen",
      "created_utc": "2026-01-30 12:46:49",
      "score": 31,
      "num_comments": 1,
      "upvote_ratio": 0.97,
      "text": "We made some experimental DeepSeek-V3.2 GGUFs for those interested! [https://huggingface.co/unsloth/DeepSeek-V3.2-GGUF](https://huggingface.co/unsloth/DeepSeek-V3.2-GGUF)\n\nThey **don't need any llama.cpp updates or special forks** \\- these should work in llama.cpp, LM Studio, Ollama (UD-TQ1\\_0).\n\nDeepSeek Sparse Attention (DSA) is disabled for now, and this mostly acts like a normal DeepSeek V3.1 model. However, we had to cook up the chat\\_template.jinja from scratch.\n\nUse [https://unsloth.ai/docs/models/tutorials/deepseek-v3.1-how-to-run-locally](https://unsloth.ai/docs/models/tutorials/deepseek-v3.1-how-to-run-locally) and replace \"DeepSeek-V3.1\" with \"DeepSeek-V3.2\"\n\nAn example Flappy Bird game in HTML with the UD-Q2\\_K\\_XL quant:\n\nhttps://preview.redd.it/a5d7sugrfhgg1.png?width=1547&format=png&auto=webp&s=26f2c96289c84fe8cace79c30a633f7a8e3b5a62\n\nLet us know how it goes!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o3iv5sy",
          "author": "gjsmo",
          "text": "I downloaded the Q4_K_XL quant and tried running it, seems to work! However I'm having trouble with I believe the chat template. While DeepSeek-V3.1-Terminus worked well using the `--jinja --reasoning-format deepseek` arguments, DeepSeek-V3.2 doesn't seem to output a <think> starting tag. Open WebUI unfortunately can't quite deal with this and just outputs the reasoning content as regular text.\n\nI've applied the chat template from the DeepSeek-V3.1-Terminus GGUFs and it does seem to work, but given that the new one is significantly different, I wonder if it enables new functionality. Is it possible to patch the new one?",
          "score": 1,
          "created_utc": "2026-02-04 13:08:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvvks9",
      "title": "TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF ¬∑ Hugging Face",
      "subreddit": "unsloth",
      "url": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF",
      "author": "No-Intention-5521",
      "created_utc": "2026-02-04 17:54:22",
      "score": 27,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qvvks9/teichaiglm47flashclaudeopus45highreasoningdistillg/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o3kkfeq",
          "author": "KvAk_AKPlaysYT",
          "text": "Just 250 examples enough for a valuable distillation? Where are the benches?",
          "score": 7,
          "created_utc": "2026-02-04 18:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lh2i4",
              "author": "Zyguard7777777",
              "text": "Thinking the same, would be nice to have even a small bench to compare",
              "score": 4,
              "created_utc": "2026-02-04 20:38:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mlias",
                  "author": "arman-d0e",
                  "text": "I was thinking HumanEval and maybe LiveCodeBench? Or would you suggest others",
                  "score": 1,
                  "created_utc": "2026-02-05 00:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3lnl5z",
              "author": "ethereal_intellect",
              "text": "Afaik the whole idea was to lessen the abysmally long thinking, opus thinks 50x less. But yeah benches would be nice to see for a lot of these :/",
              "score": 2,
              "created_utc": "2026-02-04 21:09:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3md9kz",
                  "author": "ClimateBoss",
                  "text": "or try it can it even code bruh ?",
                  "score": 1,
                  "created_utc": "2026-02-04 23:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ml4hv",
              "author": "arman-d0e",
              "text": "Our goal with this model (as with all our others) has been to distill CoT not knowledge. 250 is still small don‚Äôt get me wrong, however it‚Äôs surprisingly proven to still successfully teach the student model to ‚Äúthink like‚Äù the teacher (given the diverse types of prompts). I‚Äôm currently looking at rentable gpus to do some benchmarks. Feel free to suggest what benchmarks you‚Äôd like to see for this model.\n\nI wouldn‚Äôt have much expectation though as the model could use some form of RL to better adapt to it‚Äôs new CoT",
              "score": 2,
              "created_utc": "2026-02-05 00:01:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3mw0be",
              "author": "CoruNethronX",
              "text": "Check this: [Less Is More for Reasoning](https://arxiv.org/abs/2502.03387)",
              "score": 2,
              "created_utc": "2026-02-05 01:01:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o4z9d",
                  "author": "Thrumpwart",
                  "text": "Then check this: [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)",
                  "score": 2,
                  "created_utc": "2026-02-05 05:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mypb8",
          "author": "CoruNethronX",
          "text": "Checked the actual dataset and looks, like it's 90% filled with the same question \"what is requerments\", rephrased in different ways. But also includes more practical questions, like \"how to subtract 47 from 89\". Not sure that it's really useful for programming and especially for agentic skills, that are strongest sides of glm 4.7 flash in comparison to other open models of the similar size.",
          "score": 3,
          "created_utc": "2026-02-05 01:17:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nsvrg",
          "author": "Successful_Bit7710",
          "text": "Anyone use and perform benchmarks?",
          "score": 1,
          "created_utc": "2026-02-05 04:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oalfu",
              "author": "TokenRingAI",
              "text": "Not needed, falls apart under basic testing.",
              "score": 1,
              "created_utc": "2026-02-05 06:26:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ofk9b",
              "author": "kironlau",
              "text": "worst than original one, tested MXFP4 quant, a few days ago.",
              "score": 1,
              "created_utc": "2026-02-05 07:09:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qruejs",
      "title": "cerebras MiniMax M2.1 REAP gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qruejs/cerebras_minimax_m21_reap_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-31 06:03:05",
      "score": 13,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "[https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B](https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B)\n\nmradermacher GGUF dont work, only Unsloth has best chat template fixes ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qruejs/cerebras_minimax_m21_reap_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2qzkik",
          "author": "StardockEngineer",
          "text": "I use the mrader reap daily.  Didn‚Äôt do anything special to make it work.",
          "score": 2,
          "created_utc": "2026-01-31 06:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ric8p",
              "author": "LegacyRemaster",
              "text": "me too.",
              "score": 1,
              "created_utc": "2026-01-31 09:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2u41ym",
                  "author": "ClimateBoss",
                  "text": "M2 not M2.1 dat came out like yday bruh",
                  "score": 1,
                  "created_utc": "2026-01-31 18:58:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qs8gcp",
      "title": "Should I use UnslothTrainer or SFTTrainer for Continued Pre-training (Raw Text) to create a LoRA for later merging?",
      "subreddit": "unsloth",
      "url": "https://arxiv.org/abs/2507.18294",
      "author": "choco132134",
      "created_utc": "2026-01-31 17:28:47",
      "score": 13,
      "num_comments": 4,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qs8gcp/should_i_use_unslothtrainer_or_sfttrainer_for/",
      "domain": "arxiv.org",
      "is_self": false,
      "comments": [
        {
          "id": "o2ujqvz",
          "author": "Educational_Rent1059",
          "text": "UnslothTrainer is the same thing with additional support for these internal params which are needed for continued pre training  args = UnslothTrainingArguments(\n        ....\n        learning_rate = 5e-5,\n        embedding_learning_rate = 5e-6, # 2-10x smaller than learning_rate",
          "score": 1,
          "created_utc": "2026-01-31 20:13:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xti8b",
          "author": "yoracale",
          "text": "Use unsloth trainer if it works as it has continued pretraining benefits. If it doesn't work then use SFTTrainer",
          "score": 1,
          "created_utc": "2026-02-01 08:27:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o323ker",
          "author": "de4dee",
          "text": "I do CPT on Instruct model, using UnslothTrainer and it is looking ok..",
          "score": 1,
          "created_utc": "2026-02-01 23:06:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kge1l",
          "author": "SafetyGloomy2637",
          "text": "Check out LlamaFactory",
          "score": 1,
          "created_utc": "2026-02-04 17:49:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs6esv",
      "title": "Are Usnloth planning to provide a notebook for the Ministral 3 text?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qs6esv/are_usnloth_planning_to_provide_a_notebook_for/",
      "author": "kompania",
      "created_utc": "2026-01-31 16:13:01",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "I tried tuning the Ministral 3 3B model by swapping the training sets provided by Unsloth notebook with my own. I tried tuning the VL and Sudoku versions using the Alpaca dataset.\n\nUnfortunately, I was unsuccessful. Both Gemini and ChatGPT claim that this is currently impossible due to the lack of MistralAI support.\n\nDoes Unsloth plan to provide notebooks for Colab for tuning Ministral 3 using text?\n\nI also want to thank the people behind this system/library. I'm 63, and thanks to their extensive guides, I've made some very satisfying tweaks for Gemma 3. Thank you, Unsloth, for your work!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qs6esv/are_usnloth_planning_to_provide_a_notebook_for/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2w5qy7",
          "author": "yoracale",
          "text": "You need to disable the vision component, it is quite simple. Use the same notebook and just follow the vision finetuning guide: https://unsloth.ai/docs/basics/vision-fine-tuning\n\nE.g. turn fine-tune vision layers = off in the guide",
          "score": 1,
          "created_utc": "2026-02-01 01:19:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y15vy",
              "author": "kompania",
              "text": "Unfortunately, this solution doesn't work.\n\nI tried it myself, but unfortunately, despite entering the correct name, the model doesn't download.\n\nI get the error:\n\nRuntimeError: Unsloth: No config file found \"unsloth/Ministral-3-3B-Instruct-2512-bnb-4bit\"\n\nAnalysis of the configuration files (config.json) reveals that the model architecture is defined as Mistral3ForConditionalGeneration. This is a significant departure from the standard MistralForCausalLM or LlamaForCausalLM architectures, which are natively and seamlessly supported by the FastLanguageModel class in the Unsloth library. The Mistral3ForConditionalGeneration class implies the presence of visual projection layers and mechanisms for combining text and visual embeddings, which complicates the process of initializing the model as a pure text generator.\n\nI tried to bypass this, but encountered another problem. The main challenge is that the FastLanguageModel expects an architecture that follows the Causal LM paradigm. Forcing this class to load Ministral-3-3B requires precise dependency management, including installing the unsloth\\_zoo module, which contains definitions for non-standard architectures. The absence of this component leads to import errors or an inability to find the configuration file, which is one of the most frequently reported issues in GitHub repositories related to this model.\n\nI have unsloth\\_zoo, but it doesn't help.\n\nI tried solving the problem with Gemini Pro and ChatGPT, but they also failed.\n\nI'm at a loss here. I'm not an engineer and I can't progress any further.",
              "score": 2,
              "created_utc": "2026-02-01 09:38:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o397yxo",
                  "author": "arman-d0e",
                  "text": "This is a transformers version issue with the dependencies. Add a line after the provided dependencies cell to install the ministral transformers fork mentioned in their model README. Had the same issue and solved it like this",
                  "score": 1,
                  "created_utc": "2026-02-03 00:27:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qurswa",
      "title": "Vllm is not supported in Asus GX10 machine",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qurswa/vllm_is_not_supported_in_asus_gx10_machine/",
      "author": "maayon",
      "created_utc": "2026-02-03 13:23:20",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "The SFT notebooks run properly in ASUS gx10 but when i try to run GRPO the vLLM installation corrupts the venv installations.\n\nIs there anyway to run GRPO notebooks without vllm ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qurswa/vllm_is_not_supported_in_asus_gx10_machine/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o3c9b5p",
          "author": "Final-Rush759",
          "text": "Pip install accelerator, I think.",
          "score": 2,
          "created_utc": "2026-02-03 13:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ccir8",
              "author": "eleqtriq",
              "text": "This sounds right",
              "score": 1,
              "created_utc": "2026-02-03 14:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3c7knx",
          "author": "eleqtriq",
          "text": "Can you be more specific?  What notebook?  What is the error?",
          "score": 1,
          "created_utc": "2026-02-03 13:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3c831y",
              "author": "maayon",
              "text": "[https://github.com/unslothai/unsloth/issues/3976](https://github.com/unslothai/unsloth/issues/3976)",
              "score": 1,
              "created_utc": "2026-02-03 13:37:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3c8c1j",
                  "author": "eleqtriq",
                  "text": "You‚Äôre in a container?  What container did you use?",
                  "score": 1,
                  "created_utc": "2026-02-03 13:38:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qw0ch3",
      "title": "FSDP with Unsloth",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qw0ch3/fsdp_with_unsloth/",
      "author": "Potential_Nerve_4381",
      "created_utc": "2026-02-04 20:43:51",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm trying to load Qwen3-30B-A3B model on my g5.12xlarge GPU. I need to shard the model as it doesn't fit in one GPU. Does anyone have an example working script that runs FSDP with Unsloth and Hugging face Trainer? I can't seem to find one anywhere ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qw0ch3/fsdp_with_unsloth/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    }
  ]
}