{
  "metadata": {
    "last_updated": "2026-01-19 08:49:34",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 12,
    "total_comments": 41,
    "file_size_bytes": 66832
  },
  "items": [
    {
      "id": "1qcc34f",
      "title": "Google releases their first reasoning model: MedGemma-1.5",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/gallery/1qcc34f",
      "author": "yoracale",
      "created_utc": "2026-01-14 03:08:11",
      "score": 147,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qcc34f/google_releases_their_first_reasoning_model/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzh92ik",
          "author": "danielhanchen",
          "text": "So essentially MedGemma uses these special tokens:\n\n`<unused94>thought` same as DeepSeek's `<think> `\n\n`<unused95>` same as DeepSeek's `</think> `\n\nJust like DeepSeek it gives a response after `</think>`",
          "score": 7,
          "created_utc": "2026-01-14 03:42:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkt8pm",
              "author": "ObjectiveOctopus2",
              "text": "Looks like you found the Easter egg",
              "score": 3,
              "created_utc": "2026-01-14 17:54:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzl0hba",
                  "author": "danielhanchen",
                  "text": "Haha :)",
                  "score": 1,
                  "created_utc": "2026-01-14 18:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzhcqkl",
          "author": "m98789",
          "text": "Is fine tuning any different than with original medgemma considering now we have reasoning? Ie do we have to provide reasoning traces in our SFT dataset?",
          "score": 3,
          "created_utc": "2026-01-14 04:05:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhg1e2",
              "author": "yoracale",
              "text": "Yes, i'm pretty sure you'll now need to have reasoning traces in your dataset to maintain its reasoning capabilities. Otherwise, you can use your old dataset but the reasoning will be baked away",
              "score": 1,
              "created_utc": "2026-01-14 04:27:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzj2ohd",
                  "author": "m98789",
                  "text": "How can one do that though? Normal datasets have (source, target), how can one get (source, target, reasoning).",
                  "score": 1,
                  "created_utc": "2026-01-14 12:42:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzizmax",
          "author": "igvarh",
          "text": "Will a model of this size be able to analyze MRI images?",
          "score": 1,
          "created_utc": "2026-01-14 12:21:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzj5rvn",
              "author": "Equal-Document4213",
              "text": "It already can",
              "score": 2,
              "created_utc": "2026-01-14 13:02:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjik65",
                  "author": "simracerman",
                  "text": "Reliably?",
                  "score": 1,
                  "created_utc": "2026-01-14 14:14:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl689w",
          "author": "itsstroom",
          "text": "I guess inferior to 27B. The latter one literally saved my life so props to google. Lets see what happens.",
          "score": 1,
          "created_utc": "2026-01-14 18:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03co20",
              "author": "ObjectiveOctopus2",
              "text": "How did it save your life?",
              "score": 2,
              "created_utc": "2026-01-17 12:07:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzms2ps",
              "author": "simracerman",
              "text": "The 27B is smarter than GPT and Claude in my experience. It really shines with text only prompts and a good system prompt that keeps it focused.",
              "score": 1,
              "created_utc": "2026-01-14 23:19:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrau7y",
          "author": "Turbulent_Jump_2000",
          "text": "I can‚Äôt find a use case for this. 4B just way too many hallucinations. ¬†I like the concept though. ¬†Med ASR on the other hand seems to be really strong.¬†",
          "score": 1,
          "created_utc": "2026-01-15 16:59:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03cye8",
              "author": "ObjectiveOctopus2",
              "text": "I think you need to focus on one medical domain and fine tune for a 4b model.",
              "score": 1,
              "created_utc": "2026-01-17 12:10:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0awjad",
              "author": "waterBoy__",
              "text": "I think Google is doing this in preparation of the Apple deal where google‚Äôs Gemini will power the new Siri, and Apple is revamping Heath to run a local model to analyze your health information. This seems like small local model which can run on iPhone.",
              "score": 1,
              "created_utc": "2026-01-18 15:21:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qar1wo",
      "title": "Qwen3-Next-80B Instruct, Thinking Updated - 20% faster",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qar1wo/qwen3next80b_instruct_thinking_updated_20_faster/",
      "author": "danielhanchen",
      "created_utc": "2026-01-12 10:04:28",
      "score": 113,
      "num_comments": 63,
      "upvote_ratio": 0.98,
      "text": "Hey all! Qwen3 Next had to be updated due to [https://github.com/ggml-org/llama.cpp/pull/18683](https://github.com/ggml-org/llama.cpp/pull/18683)\n\nSon from Hugging Face and the llama.cpp team managed to make Qwen3 Next run **5 to 20% faster** on the latest llama.cpp branch! Re-download the quants also for:\n\n* **Improved imatrix calibration** with a lot more **tool calling data**\n* Returned chat template to original since llama.cpp's llama-cli now is just llama-server under the hood, so the original template works\n\nUse snapshot\\_download so old shards don't all need to be refreshed:\n\n    # !pip install huggingface_hub hf_transfer\n    import os\n    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        repo_id = \"unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF\",\n        local_dir = \"Qwen3-Next-80B-A3B-Instruct-GGUF\",\n        allow_patterns = [\"*UD-Q4_K_XL*\"],\n    )\n\nAlso re-get llama.cpp from source - see [https://unsloth.ai/docs/models/tutorials/qwen3-next](https://unsloth.ai/docs/models/tutorials/qwen3-next) on how to use Qwen3-Next.\n\nThe GGUFs are updated to\n\n* [https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF)\n* [https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF](https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF)",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qar1wo/qwen3next80b_instruct_thinking_updated_20_faster/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nz5rsfs",
          "author": "TaroOk7112",
          "text": "Thanks for all those quants.",
          "score": 5,
          "created_utc": "2026-01-12 13:41:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz916pi",
              "author": "danielhanchen",
              "text": "Thanks!",
              "score": 2,
              "created_utc": "2026-01-12 22:57:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6rnjn",
          "author": "maxpayne07",
          "text": "having this error on lmstudio:     \\`\\`\\`\n\nü•≤ Failed to load the model\n\n\n\nFailed to load model\n\n\n\nerror loading model: missing tensor 'blk.0.ssm\\_in.weight'\n\n\\`\\`\\`",
          "score": 5,
          "created_utc": "2026-01-12 16:39:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz92oa3",
              "author": "danielhanchen",
              "text": "Oh apologies I'll notify the LM Studio team to see if they can update LM Studio's backend!",
              "score": 5,
              "created_utc": "2026-01-12 23:05:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz6y091",
              "author": "IbetitsBen",
              "text": "Same issue",
              "score": 3,
              "created_utc": "2026-01-12 17:08:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz7ghfp",
              "author": "Toastti",
              "text": "From this post it seems you must get the latest main branch of llama.cpp from GitHub and build from source to run this.",
              "score": 2,
              "created_utc": "2026-01-12 18:32:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz8bs9n",
              "author": "somethingdangerzone",
              "text": "a full rebuild will fix this (but for me it made it slower...still debugging now)",
              "score": 1,
              "created_utc": "2026-01-12 20:56:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz57uhp",
          "author": "igvarh",
          "text": "Qwen3-Omni-30B-A3B-Instruct - Could you make quants for that please?",
          "score": 6,
          "created_utc": "2026-01-12 11:23:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5g5wi",
              "author": "danielhanchen",
              "text": "I would love to, but I don't think it's supported :(",
              "score": 4,
              "created_utc": "2026-01-12 12:27:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzcfhcd",
              "author": "beneath_steel_sky",
              "text": "There's a feature request for llama at https://github.com/ggml-org/llama.cpp/issues/16186",
              "score": 1,
              "created_utc": "2026-01-13 12:59:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5z8i7",
          "author": "zoyer2",
          "text": "Thanks a lot! Btw does anyone have any trick on how to reduce the thinking of the thinking version? Love it but just thinks waaay too long\n\nedit: using 2x3090 i'm finding these new quants and improvements the best local model for agent use",
          "score": 3,
          "created_utc": "2026-01-12 14:21:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7gpy0",
              "author": "Toastti",
              "text": "Probably could include system prompt instructions that your thinking level is now set at low. Do not spend as much time during the thinking process but still focus on accuracy",
              "score": 2,
              "created_utc": "2026-01-12 18:33:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz91ag2",
                  "author": "danielhanchen",
                  "text": "Ye probably a system prompt - like how Claude has \"ultrathink\" to think longer",
                  "score": 1,
                  "created_utc": "2026-01-12 22:58:02",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzmzn03",
              "author": "Snoo_28140",
              "text": "If the system prompt doesn't help, you can try to increase the probability of the token to end thought.",
              "score": 1,
              "created_utc": "2026-01-15 00:00:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz8uufu",
          "author": "PlasticTourist6527",
          "text": "You got to love the pen and paper image in the original pull request",
          "score": 3,
          "created_utc": "2026-01-12 22:25:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz92sr1",
              "author": "danielhanchen",
              "text": "Ye love the picture Son made!",
              "score": 2,
              "created_utc": "2026-01-12 23:05:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbxpat",
          "author": "iadanos",
          "text": "By the way, anyone tried to re-make gguf for other Qwen models to check if new update influenced it's speedup? Just in case...",
          "score": 3,
          "created_utc": "2026-01-13 10:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbyrw9",
              "author": "danielhanchen",
              "text": "Oh interesting question I'm actually not sure!",
              "score": 1,
              "created_utc": "2026-01-13 10:51:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7ntpm",
          "author": "hashms0a",
          "text": "Thank you; this is what they call an update.",
          "score": 2,
          "created_utc": "2026-01-12 19:05:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz92ows",
              "author": "danielhanchen",
              "text": "Thanks!",
              "score": 2,
              "created_utc": "2026-01-12 23:05:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz8bbfn",
          "author": "somethingdangerzone",
          "text": "I just pulled the master branch, rebuilt my llama.cpp build, and now my Qwen3 next thinking is 50% slower. RIP\n\nEdit: built from source on Ubuntu 24.04 LTS with CUDA",
          "score": 2,
          "created_utc": "2026-01-12 20:54:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz92qyw",
              "author": "danielhanchen",
              "text": "Oh my is it due to more thinking tokens or is the actual tokens / s slower?",
              "score": 2,
              "created_utc": "2026-01-12 23:05:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz966gc",
                  "author": "somethingdangerzone",
                  "text": "Token generation on it's own went from ~12 TPS to 3 TPS. I'm rocking a 4090 with 64 GB DDR5 RAM. llama-server with 49 GPU layers and 49 layers of MoE on CPU. I noticed that the newest build seems to be using much more RAM, so much so that it's overflowing into my swapfile.",
                  "score": 1,
                  "created_utc": "2026-01-12 23:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz8bjg6",
          "author": "Altruistic_Call_3023",
          "text": "I had to do a clean rebuild of llama.cpp - but WOW!!  I have one 3090 and the rest on cpu.  I was getting 16t/s before.  I‚Äôm getting 30-36 now.  That‚Äôs insane.  This is truly amazing.",
          "score": 2,
          "created_utc": "2026-01-12 20:55:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nza2fcg",
              "author": "Altruistic_Call_3023",
              "text": "In case someone reads this - I'm using Q4\\_K\\_M - and I see this massive speed on Thinking and Instruct",
              "score": 3,
              "created_utc": "2026-01-13 02:18:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nza9xc5",
                  "author": "somethingdangerzone",
                  "text": "oh just saw this now. thanks!",
                  "score": 1,
                  "created_utc": "2026-01-13 02:58:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz92rso",
              "author": "danielhanchen",
              "text": "Yes it's really good!",
              "score": 2,
              "created_utc": "2026-01-12 23:05:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzuuuiu",
                  "author": "shrug_hellifino",
                  "text": "It is broken for me now ;(\n\n# Issue: llama.cpp (ROCm/HIP) loads model, but inference fails with hipErrorInvalidDeviceFunction (SOLVE_TRI)\n\nAfter updating GGUF models and rebuilding `llama.cpp` with ROCm/HIP, the model **loads successfully** and the HTTP server comes up normally, but **the first inference request crashes** with a ROCm/rocBLAS failure:\n\n* Server endpoints respond (`/health`, `/props`, `/v1/models`)\n* Prompt is accepted and tokenization completes\n* Failure happens during the first decode/compute step on GPU\n\n# Sanitized error excerpt\n\n    main: model loaded\n    main: server is listening on http://127.0.0.1:<PORT>\n    ...\n    slot update_slots: ... prompt done ...\n    rocBLAS error from hip error code: hipErrorInvalidDeviceFunction (98)\n    .../ggml/src/ggml-cuda/ggml-cuda.cu:<line>: ROCm error\n    ggml_cuda_compute_forward: SOLVE_TRI failed\n    ROCm error: invalid device function\n      current device: 0, in ggml_cuda_compute_forward ...\n    (backtrace into libggml-hip.so / libllama.so)\n    \n\n# Repro (high-level)\n\n1. Pull newer GGUF model(s) (previously working model still loads fine).\n2. Rebuild `llama.cpp` with ROCm/HIP enabled (Docker-based workflow).\n3. Start `llama-server`.\n4. Send any completion/chat request (first inference call) ‚Üí crash with `hipErrorInvalidDeviceFunction`.\n\nEnvironment\n\n\\- OS: Ubuntu 24.04\n\n\\- GPUs: 5x AMD Radeon Pro VII (16GB)\n\n\\- ROCm target: gfx906",
                  "score": 1,
                  "created_utc": "2026-01-16 03:34:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nza9vwv",
              "author": "somethingdangerzone",
              "text": "What are you gpu layers/layers of experts on CPU set up like? I'm on a 4090 and nowhere near 30 tk/s with the new build",
              "score": 1,
              "created_utc": "2026-01-13 02:57:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzajrg9",
                  "author": "hashms0a",
                  "text": "I Just do --cpu-moe or --n-cpu-moe # and reduce the number until fill all the VRAM.",
                  "score": 2,
                  "created_utc": "2026-01-13 03:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzgi6m9",
          "author": "0xNullsector",
          "text": "https://preview.redd.it/e959wzbes7dg1.png?width=2694&format=png&auto=webp&s=8d7e85c412b636422df2a11d10172b9194e1e87c\n\n36.50 tok/sec in RTX PRO 6000 Blackwell Workstation Edition Driver Version: 582.16 ,  CUDA Version: 13.0 using Qwen3-Next-80B-A3B-Thinking-Q4\\_K\\_S.gguf context: 262144",
          "score": 2,
          "created_utc": "2026-01-14 01:08:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzh3glh",
              "author": "danielhanchen",
              "text": "Oh nice!",
              "score": 1,
              "created_utc": "2026-01-14 03:09:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzxdzcz",
          "author": "jul1to",
          "text": "On laptop,  4090 16Gb vram, 64Gb ddr5. With 128k context 50% used, IQ4_XS : i have now 37t/s !! Thanks",
          "score": 1,
          "created_utc": "2026-01-16 14:40:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02dzgz",
              "author": "danielhanchen",
              "text": "Nice!",
              "score": 1,
              "created_utc": "2026-01-17 06:49:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdmqcu",
      "title": "Reinforcement Learning with ultra long context is here!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/1btxn3558jdg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-15 15:36:04",
      "score": 72,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qdmqcu/reinforcement_learning_with_ultra_long_context_is/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzqvsga",
          "author": "____vladrad",
          "text": "Woahhhh nice! This is perfect for what I was trying to do",
          "score": 2,
          "created_utc": "2026-01-15 15:52:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqw4j5",
              "author": "____vladrad",
              "text": "Seriously nice job!!! Do you think there will be a multi gpu version for rl?",
              "score": 3,
              "created_utc": "2026-01-15 15:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzqwr0j",
                  "author": "yoracale",
                  "text": "Yes, early this year for sure!",
                  "score": 2,
                  "created_utc": "2026-01-15 15:56:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzr1ojh",
          "author": "WolfeheartGames",
          "text": "Yuge!",
          "score": 2,
          "created_utc": "2026-01-15 16:18:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc4ngj",
      "title": "Reinforcement Learning, Agents & RL Environments Mini Conference",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/7dydgf1gt6dg1.png",
      "author": "danielhanchen",
      "created_utc": "2026-01-13 21:54:57",
      "score": 35,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "GRPO (Reasoning):sloth_magnify_square_fin:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qc4ngj/reinforcement_learning_agents_rl_environments/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzfrx63",
          "author": "larrytheevilbunnie",
          "text": "We can access the video afterwards right? I can‚Äôt make it live",
          "score": 3,
          "created_utc": "2026-01-13 22:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzguvm7",
              "author": "danielhanchen",
              "text": "Yes it'll be recorded! The same link as the live YouTube",
              "score": 3,
              "created_utc": "2026-01-14 02:20:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qb9vdj",
      "title": "Deploying Unsloth SLMs on Mobile Devices",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qb9vdj/deploying_unsloth_slms_on_mobile_devices/",
      "author": "Henrie_the_dreamer",
      "created_utc": "2026-01-12 22:44:33",
      "score": 30,
      "num_comments": 14,
      "upvote_ratio": 0.96,
      "text": "https://preview.redd.it/fius93ayxzcg1.jpg?width=2722&format=pjpg&auto=webp&s=e10f8c8a6eca7da900f5b873df75df409a68cfd6\n\nSmall models lag big counterparts in performance but run blazing fast on small devices.\n\nHowever, fine-tuning on specific tasks have been shown to enable small models match even frontier models on the specific tasks: [https://arxiv.org/html/2406.08660v2](https://arxiv.org/html/2406.08660v2)\n\nYou can now fine-tune SLMs with Unsloth and deploy to Cactus, an inference engine for mobile devices, macs and ARM chips like Raspberry Pi.\n\nAt INT8, Cactus runs¬†`Qwen3-0.6B`¬†and¬†`LFM2-1.2B`¬†at¬†`60-70 toks/sec`¬†on iPhone 17 Pro,¬†`13-18 toks/sec`¬†on budget Pixel 6a.\n\nINT4 quantization provides \\~50% memory reduction with minimal quality loss.\n\nTask-Specific INT8 tunes of¬†`Gemma3-270m`¬†hit¬†`150 toks/sec`¬†on iPhone 17 Pro and¬†`23 toks/sec`¬†on Raspberry Pi.\n\nMore details on porting Unsloth fine-tunes:  [https://github.com/cactus-compute/cactus/blob/main/docs/finetuning.md](https://github.com/cactus-compute/cactus/blob/main/docs/finetuning.md)\n\nPlease feel free to share your thoughts!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qb9vdj/deploying_unsloth_slms_on_mobile_devices/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nz8zvja",
          "author": "danielhanchen",
          "text": "Thanks to the Cactus team! We added the Cactus docs also to our main docs section under Phone Deployment as well! https://unsloth.ai/docs/new/deploy-llms-phone#deploying-to-cactus-for-phones",
          "score": 5,
          "created_utc": "2026-01-12 22:50:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz90vw3",
          "author": "wektor420",
          "text": "Is it capable of using NPUs? Like those in samsung S24?",
          "score": 1,
          "created_utc": "2026-01-12 22:56:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz951x5",
              "author": "Henrie_the_dreamer",
              "text": "Yes for Apple devices, Qualcomm coming this month!",
              "score": 2,
              "created_utc": "2026-01-12 23:17:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz96ng3",
                  "author": "wektor420",
                  "text": "Cries in exynos",
                  "score": 1,
                  "created_utc": "2026-01-12 23:26:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz91p8v",
          "author": "FullstackSensei",
          "text": "It's always nice to have more options. Thanks!\nDoes it run on CPU or GPU? What are the differences/advantages of Cactus versus something like Executorch or LiteRT, from Meta and Google, respectively?",
          "score": 1,
          "created_utc": "2026-01-12 23:00:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz921gi",
              "author": "wektor420",
              "text": "They have a demo on play store and there is gpu option in settings",
              "score": 2,
              "created_utc": "2026-01-12 23:01:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz95lyw",
                  "author": "Henrie_the_dreamer",
                  "text": "Correct, we are switching to NPU support due to GPU heating & battery drain issues in production.¬†\n\nExecutorch & LiteRT are solid projects, it‚Äôs easy to port Torch models on Executorch and LiteRT has coverage.¬†\n\nCactus is designed to be blazing fast and compact, with functionalities like Cloud Handoff for complex tasks.¬†\n\nSee the project readme:¬†https://github.com/cactus-compute/cactus",
                  "score": 2,
                  "created_utc": "2026-01-12 23:20:49",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz92fsa",
          "author": "wektor420",
          "text": "Nice to have markdown support in the demo, lateX would be good to have",
          "score": 1,
          "created_utc": "2026-01-12 23:03:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz95qrc",
              "author": "Henrie_the_dreamer",
              "text": "Thanks, latex & PDFs will be added in the future!",
              "score": 2,
              "created_utc": "2026-01-12 23:21:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzb1t3w",
          "author": "Raise_Fickle",
          "text": "how does it compare with mediapipe's llm infernece?",
          "score": 1,
          "created_utc": "2026-01-13 05:51:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbkfly",
              "author": "Henrie_the_dreamer",
              "text": "While Mediapipe can run LLMs, it‚Äôs primary strength is computer vision, while Cactus is focused on generative AI and more feature rich & faster there. Mediapipe is the most powerful for computer vision.¬†",
              "score": 1,
              "created_utc": "2026-01-13 08:35:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzbpja5",
                  "author": "Raise_Fickle",
                  "text": "any comparison exsists with this vs mediapipe vs [executorch](https://github.com/pytorch/executorch)",
                  "score": 1,
                  "created_utc": "2026-01-13 09:25:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qf6qvv",
      "title": "Translategemma-27b",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qf6qvv/translategemma27b/",
      "author": "StormrageBG",
      "created_utc": "2026-01-17 07:17:53",
      "score": 18,
      "num_comments": 6,
      "upvote_ratio": 0.95,
      "text": "Guys do you plan to release quantisation variants of Translategemma-27b ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qf6qvv/translategemma27b/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o02thsb",
          "author": "danielhanchen",
          "text": "We did investigate it but the chat template sadly is quite specialized for it - I can check again later today, but currently it looks complex to support :(",
          "score": 4,
          "created_utc": "2026-01-17 09:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02u4gf",
              "author": "Purple-Programmer-7",
              "text": "Hugely supportive of everything you guys do.\n\nCan you educate me though? The whole ‚Äúwe fixed the chat template‚Äù didn‚Äôt really pan out IMO under scrutiny. And the copyright statement seems a bit of the anthesis  of everything you guys do to support the community.\n\nIf the chat template is functional and embedded, it‚Äôs not going to prevent a gguf from being made.\n\nAgain, not trying to be adversarial here, just want to understand better your position on all of this.\n\nThank you again for everything you‚Äôve provided (and continue to provide) this community, I genuinely am rooting for your success!\n\nEdit: spelling",
              "score": 2,
              "created_utc": "2026-01-17 09:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o07g10j",
                  "author": "danielhanchen",
                  "text": "Hey! Thanks!\n\n1. Translate Gemma has a specialized chat template that requires you to specify the language you want to translate to - this means it's not a chat model - you will need to specify the chat template kwargs separately, so it can't be loaded well in frontend UIs and make the process just harder for folks\n2. The chat template fixes are wide ranging, and we also do direct model implementation fixes - see the following:\n    * Gemma 1, Gemma 3 bug fixes: https://x.com/karpathy/status/1765473722985771335\n    * Phi 4 fixes: https://simonwillison.net/2025/Jan/11/phi-4-bug-fixes/\n    * Llama 4 fixes: https://www.reddit.com/r/LocalLLaMA/comments/1jxo7lb/llamacpp_got_2_fixes_for_llama_4_rope_wrong_norms/\n    * GPT OSS fixes: https://x.com/danielhanchen/status/1953901104150065544\n    * Kimi K2 bug fixes: https://x.com/danielhanchen/status/1946163064665260486\n    * And many more - we do a lot of our work behind the scenes now, so whatever model that was released with us generally is already fixed.\n3. The copyright header is Apache 2 (fully open source) - we place it because folks would simply copy and paste and rebrand it as their own fixes - we just want attribution that's all. Likewise all fixes before a model releases is licensed as whatever the model provider wants.",
                  "score": 2,
                  "created_utc": "2026-01-18 00:48:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03a9z5",
          "author": "DocWolle",
          "text": "Here are ggufs\n\n[https://huggingface.co/mradermacher/translategemma-27b-it-GGUF](https://huggingface.co/mradermacher/translategemma-27b-it-GGUF)",
          "score": 2,
          "created_utc": "2026-01-17 11:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03hq44",
              "author": "StormrageBG",
              "text": "Unsloth quants are always way better than mradermacher... I don't know what  the magic they did but on my test especially for translating unsloth Gemma 3 quants rulz... So I really hope ü§û that can do it again with this model too...",
              "score": 5,
              "created_utc": "2026-01-17 12:47:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03kj2v",
                  "author": "DocWolle",
                  "text": "this may be true, but the mradermacher gguf for translategemma works fine for me.",
                  "score": 1,
                  "created_utc": "2026-01-17 13:06:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qbjiqr",
      "title": "How to test maximum VRAM Usage while GRPO training?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qbjiqr/how_to_test_maximum_vram_usage_while_grpo_training/",
      "author": "Free-Letterhead5008",
      "created_utc": "2026-01-13 05:56:32",
      "score": 11,
      "num_comments": 10,
      "upvote_ratio": 0.87,
      "text": "Hey everyone,\n\nI'm currently running GRPO training and hitting a snag when trying to determine the maximum VRAM requirement. The training itself runs smoothly, initially using around 25GB of VRAM. However, after approximately 140 steps, the VRAM usage spikes and exceeds my GPU's 48GB capacity.\n\nI've already sorted my dataset by length, ensuring the longest inputs are processed first.  \nMy suspicion is that at step 140 all generations utilize the maximum context size of 5120. This results in a significantly larger average context size in this step compared to others.\n\nIs there a way to force the trainer to utilize the full context size or ignore the EOS token, so I can test if the peak VRAM usage is too high right from the first step? I‚Äôm looking for a method to proactively identify this issue before it crashes the training process.\n\nAny insights or suggestions would be greatly appreciated!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qbjiqr/how_to_test_maximum_vram_usage_while_grpo_training/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nzbcoy0",
          "author": "im_datta0",
          "text": "Hey u/Free-Letterhead5008 if you want to stress test the training, you can set \\`min\\_tokens\\` in vllm to 5120-max\\_input\\_tokens and that should force every generation to be of the max length.\n\nBut memory usage going up from 25GB to 48GB is very odd and should not happen. If you can describe what setup you are using and what model/trainer config and perhaps share wandb run link for me to look at, I can probably better help you :)",
          "score": 1,
          "created_utc": "2026-01-13 07:22:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbnb70",
              "author": "aaronr_90",
              "text": "Happens to me all the time and is slightly annoying. I use the provided notebook templates. \n\nI have various setups ranging from a single Nvidia RTX a5000 to H200. Last night I was fine-tuning a Qwen3-30B-A3B loaded in 4bit (~18gb) on an H200: rank 64, max_tokens 8192, train_batch_size 64, eval_steps 64, eval_batch_size 64.\n\nH200 had 140 gb of VRAM. VRAM usage during Initial training was 75gb then as it was beginning to do the first eval (I suppose, the tqdm progress bar lags behind the actual progress) VRAM usage spiked and I got an OOM error.\n\nDuring a normal eval run I‚Äôll typically see VRAM usage drop in the beginning and then go back up. I assume it‚Äôs unloading gradients then loads the batches.\n\n‚Äî-\n\nThis was my first attempt with this model and GPU setup and I don‚Äôt have anything dialed in yet but I have had similar experiences training 0.6B to 22B models on the a5000. Also I have noticed Qwen3 0.6B requires more VRAM than Qwen3 4B, both loaded in 4bit with BnB and everything thing else remaining identical.",
              "score": 1,
              "created_utc": "2026-01-13 09:03:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbr2t9",
                  "author": "im_datta0",
                  "text": "If the memory usage is increasing steadily over time, then that suggests a possible memory leak somewhere.  \nIf the memory usage increased abruptly at a particular step, then it is potentially due to that particular input being much longer than the rest (which I guess from your description is possibly unlikely)\n\nAnd on that note, because you're using MoE, the experts that each tokens choose can also potentially introduce a point of variance across steps.\n\nAlso when you are using rank 64 on an MoE you're adding a lot of parameters already. That along with a batch size of 64 is perhaps flying close to the sun. But given that you say it worked for 140 steps, I'd like to know how the memory usage increased over time to better be able to understand and help here.\n\n\\> Qwen3 0.6B requires more VRAM than Qwen3 4B, both loaded in 4bit with BnB and everything   \nI'm quite suprised at this one though. This should never happen tbh. If you can provide a script/notebook to compare 1-1 that is identical to your setup that would be of great help",
                  "score": 2,
                  "created_utc": "2026-01-13 09:40:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzhxg8w",
                  "author": "Free-Letterhead5008",
                  "text": "I also tried the Qwen3-VL-2B-Instruct-unsloth-bnb-4bit model, but it used significantly more VRAM for some reason. With the same settings, I received an OOM error. I had to reduce the num\\_generations to 2, and even then, it used almost all of the VRAM. (Compared to the 4B Variant)",
                  "score": 1,
                  "created_utc": "2026-01-14 06:40:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzhwwte",
              "author": "Free-Letterhead5008",
              "text": "Thanks for the reply. I am using an RTX Quadro 8000 with 48GB. Unfortunately, I am stuck with Windows and cannot use VLLM. I am training Qwen3-VL-4B-Instruct-unsloth-bnb-4bit. My dataset consists of images, and I am using the same prompt for each. I have limited the image size to 1024x1024. I run 4 generations per step with a maximum context of 5120. I‚Äôve played with, gpu\\_memory\\_utilization but it made no perceivable difference.",
              "score": 1,
              "created_utc": "2026-01-14 06:35:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzhx942",
                  "author": "Free-Letterhead5008",
                  "text": "Here are my settings:\n\nhttps://preview.redd.it/nrlasq7xf9dg1.png?width=649&format=png&auto=webp&s=9882d32d873102db04f9c9ce45ec74ff4cc677f3",
                  "score": 1,
                  "created_utc": "2026-01-14 06:38:36",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzhx9ne",
                  "author": "Free-Letterhead5008",
                  "text": "https://preview.redd.it/ym3345i0g9dg1.png?width=342&format=png&auto=webp&s=e7aa318fe17b3e02517768db9f55054868067d60",
                  "score": 1,
                  "created_utc": "2026-01-14 06:38:43",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nziew9y",
                  "author": "im_datta0",
                  "text": "If you are trying to do GRPO, I would strongly recommend you consider using vLLM. Because it is much much faster and much more efficient. If you are stuck with Windows, I would say maybe try our docker images and see if that helps you.\n\nThe gpu\\_memory\\_utilization flag is only for vllm case (fast\\_inference=True) btw",
                  "score": 1,
                  "created_utc": "2026-01-14 09:23:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzsfvvs",
          "author": "WolfeheartGames",
          "text": "You're most likely not freeing up gradients. Or you're on windows.",
          "score": 1,
          "created_utc": "2026-01-15 20:04:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb7qo6",
      "title": "Looking for help testing a new Matrix Multiplication algorithm (Strassen variant)",
      "subreddit": "unsloth",
      "url": "/r/CUDA/comments/1qat13s/looking_for_help_testing_a_new_matrix/",
      "author": "Rich_Obligation1510",
      "created_utc": "2026-01-12 21:24:45",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qb7qo6/looking_for_help_testing_a_new_matrix/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nz90zlc",
          "author": "danielhanchen",
          "text": "Very cool! Did you manage to do a sweep of random numbers for A and B, and random matrix sizes to see how much error and or speedup is there? If there is small amounts of error, this could be very interesting!\n\nIe a heatmap of error and heatmap of speedup over different A and B matrix sizes",
          "score": 3,
          "created_utc": "2026-01-12 22:56:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz95r9w",
              "author": "Rich_Obligation1510",
              "text": "Good idea. I'll throw this together and add it to the repo soon. will let you know.",
              "score": 3,
              "created_utc": "2026-01-12 23:21:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz9b2vs",
                  "author": "danielhanchen",
                  "text": "Great! And if you post it here so I can keep track that would be awesome! Nice work!",
                  "score": 3,
                  "created_utc": "2026-01-12 23:50:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nziluvi",
          "author": "Rich_Obligation1510",
          "text": "Have just now added a suite of test results and some plots / heatmaps. u/danielhanchen \n\nThe qualitative results on mean error and error stddev should hopefully speak for itself.\n\nReadme includes links to high level plots. additional extensive test data is located in the test\\_results directory in markdown, json and png images. Now in the repo.\n\n[https://github.com/biodigitalfish/alpha\\_kernel/raw/main/test\\_results/sweep/scaling\\_analysis.png](https://github.com/biodigitalfish/alpha_kernel/raw/main/test_results/sweep/scaling_analysis.png)",
          "score": 2,
          "created_utc": "2026-01-14 10:29:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nzhb5wl",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-14 03:55:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbsp7c",
      "title": "Finetuning Qwen-3-VL for keypoint coordinates recognition",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qbsp7c/finetuning_qwen3vl_for_keypoint_coordinates/",
      "author": "Due_Veterinarian5820",
      "created_utc": "2026-01-13 14:26:21",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm trying to fine-tune Qwen-3-VL-8B-Instruct for object keypoint detection, and I‚Äôm running into serious issues.\nBack in August, I managed to do something similar with Qwen-2.5-VL, and while it took some effort, it did work. One reliable signal back then was the loss behavior:\nIf training started with a high loss (e.g., ~100+) and steadily decreased, things were working.\nIf the loss started low, it almost always meant something was wrong with the setup or data formatting.\nWith Qwen-3-VL, I can‚Äôt reproduce that behavior at all. The loss starts low and stays there, regardless of what I try.\nSo far I‚Äôve:\nTried Unsloth\nFollowed the official Qwen-3-VL docs\nExperimented with different prompts / data formats\nNothing seems to click, and it‚Äôs unclear whether fine-tuning is actually happening in a meaningful way because it's not improving its keypoint detection post fine-tuning as well.\nIf anyone has successfully fine-tuned Qwen-3-VL for keypoints (or similar structured vision outputs), I‚Äôd really appreciate it if you could share:\nTraining data format\nPrompt / supervision structure\nCode or repo\nAny gotchas specific to Qwen-3-VL\nAt this point I‚Äôm wondering if I‚Äôm missing something fundamental about how Qwen-3-VL expects supervision compared to 2.5-VL.\nThanks in advance üôè",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qbsp7c/finetuning_qwen3vl_for_keypoint_coordinates/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nzdbyyf",
          "author": "LA_rent_Aficionado",
          "text": "It doesn‚Äôt sound like you‚Äôre actually unlocking your vision layers for training but it‚Äôs unclear whether you actually want to train vision layers.\n\nCompare your model weights before and after training and see if anything is getting updated.  Aside from that, your post is so light on detail it could literally be dozens of things that are happening.",
          "score": 4,
          "created_utc": "2026-01-13 15:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzuq3ff",
              "author": "Due_Veterinarian5820",
              "text": "The model's getting trained as I could see significant differences in prediction before and after the training, but the problem is that it's not getting trained meaningfully. The intention to post was mainly to figure out whether there are alternative ways to finetune the Qwen 3 VL for keypoint detection apart from what's added in the cookbook.  \n\nI just wished there were different instructions/different cookbooks based on the underlying usecase. For 2D Grounding, I'm not sure if the same token loss is applicable as regular text-based fine-tuning as we can measure accurate pixel coordinate distances without having to rely on token similarity.",
              "score": 1,
              "created_utc": "2026-01-16 03:07:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzfewp9",
          "author": "danielhanchen",
          "text": "If it was possible you could share a screenshot of the training process (loss, grad norms etc), or even better a Colab if possible that would be phenomenal! You can also go to our Discord for async help",
          "score": 2,
          "created_utc": "2026-01-13 21:44:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzuohyl",
              "author": "Due_Veterinarian5820",
              "text": "Will definitely do this soon! There are some proprietary data constraints but I'll try to create a generic flow of what I tried and post that in the discord channel",
              "score": 1,
              "created_utc": "2026-01-16 02:58:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzh5lyi",
          "author": "mmathew23",
          "text": "Given what you describe, could you check that the key point labels are not being masked out after getting the batch from the dataloader? Maybe even try to run forward or inference with on of the dataset examples to see if anything jumps out.",
          "score": 1,
          "created_utc": "2026-01-14 03:21:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzuoa24",
              "author": "Due_Veterinarian5820",
              "text": "Yes, I thoroughly validated the training data and also ensured that it complies with the desired normalized coordinates format of Qwen 3 VL",
              "score": 1,
              "created_utc": "2026-01-16 02:57:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzutx9j",
                  "author": "mmathew23",
                  "text": "What happens when you run inference on the same example before and after training? When you say last time has a loss of 100, you‚Äôre doing cross entropy loss, right? A low cross entropy loss isn‚Äôt necessarily a bad thing, but again depends how low. 100 is really high and that seems more suspicious to me.",
                  "score": 1,
                  "created_utc": "2026-01-16 03:29:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzyrc97",
          "author": "LA_rent_Aficionado",
          "text": "Cookbooks are just starting points but not authoritative.  In my experiences the key thing is to work backwards from desired outputs to formulate a training strategy.  It could just be a matter of dataset quality, incorrect masking or special token/chat template disconnects. There could be layers you are not training that tie all the pieces together.\n\nI‚Äôd recommend looking at the qwen3vl documentation and working backwards from there.",
          "score": 1,
          "created_utc": "2026-01-16 18:21:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qawche",
      "title": "Fine-Tuning Qwen3-Coder-30B-A3B MoE: Expert Targeting vs Router Training in Unsloth",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qawche/finetuning_qwen3coder30ba3b_moe_expert_targeting/",
      "author": "aaronr_90",
      "created_utc": "2026-01-12 14:29:27",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "I am looking into finetuning Qwen3-Coder-30B-A3B for a domain specific programming language dataset.\n\nI read in the [Unsloth Docs](https://unsloth.ai/docs/models/qwen3-how-to-run-and-fine-tune) that fine-tuning of the router layers is disabled by default.\n\nThis leads me to believe if I use [a Qwen3 MoE expert activation analyzer](https://github.com/sionic-ai/qwen3-moe-analyzer) with a sample of my dataset before finetuning I would be able to have insight into the utilization of experts. I was hoping I could identify the expert layers that are underutilized and target those expert layers. But if the router layers are untouched this would essentially remain the same and I would need to fine tune the router layers to take advantage of fine-tuned expert layers. Could I first fine tune the expert layers, and the do a second pass and fine-tune the router layers?\n\nI have had success doing something similar to a 7B model with [arcee-ai/PruneMe](https://github.com/arcee-ai/PruneMe) to compute block similarity to identify redundant layers and instead of pruning them I used Axolotl to freeze all other layers and target those redundant layers.\n\nIs my understanding correct that, unless the router is also fine-tuned, any changes I make to the experts won‚Äôt materially affect which experts get selected (and therefore won‚Äôt change expert utilization in practice)?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qawche/finetuning_qwen3coder30ba3b_moe_expert_targeting/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nz64yjv",
          "author": "LA_rent_Aficionado",
          "text": "I don‚Äôt think this would be much different than multi-staged training in a non-MOE where you train head layers separately after doing a run without head, although it seems like a lot more could go wrong with training MOE routers vs training head.\n\nI‚Äôm curious why you would want to train underutilized layers though, wouldn‚Äôt you be better off focusing on the layers that are most often activating with your intended subject matter and then enhancing those? This would likely help eliminate breaking the router all together I suspect.",
          "score": 2,
          "created_utc": "2026-01-12 14:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbdt1j",
          "author": "im_datta0",
          "text": "Hey u/aaronr_90 If I understand your situation correctly, this is what I'm thinking right now. There are two ways to go about this:\n\n1. As you mentioned, you can do a multi-stage fine-tuning by fine-tuning only the export layers and then the router layers. I really wouldn't suggest that because whatever learning we are intending the model to happen, it should happen ideally.\n2. You can just enable fine-tuning on the router layers along with the fine-tuning of expert layers and train them in the single training run. That should be more coherent. \n\nAgain, if you know what you are doing, we don't stop you from fine-tuning the router layers if you want to. But it might hurt training and downstream performance so we generally recommend against doing that.\n\nTo clarify or correct your understanding that router utilization wouldn't change if we don't find you in the router, I think there are ways where even if you freeze the router but only train the experts, the expert utilisation/token distribution would change. This is because changing one layer's output would mean that the inputs to the second layer already change, and that can potentially end up causing the token to pick different experts over the course of the training.",
          "score": 1,
          "created_utc": "2026-01-13 07:33:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcuyiz",
      "title": "Gemma 1b-it finetune worked great for multi-turn chat, but failed for `dialect text ‚Üí standard text` conversion",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qcuyiz/gemma_1bit_finetune_worked_great_for_multiturn/",
      "author": "_hasin",
      "created_utc": "2026-01-14 18:21:47",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm fine-tuning **Gemma 1B instruction-tuned** locally and ran into a failure I can‚Äôt explain.\n\n### Target task\n\nNormalize **regional dialect text** into **Standard Text** so downstream LLMs / rule-based extractors don‚Äôt hallucinate, while extracting data from the regional text (since now it'll be extracting from the standard text)\n\n### What worked\n\nI previously fine-tuned Gemma 1B it for a **multi-turn phone survey agent** using the standard chat template:\n\n* `<start_of_turn>user / model`\n* Instruction-heavy system prompt\n* Multi-turn conversational data\n\n**Result:** Tuned model followed instructions extremely well and performed reliably.\n\n### What I changed\n\nI reused **the same fine-tuning script, base model, trainer, and hyperparams**, but switched to a **single-turn parallel text task**:\n\n```\nUser:\nConvert the following dialect text to standard text.\nRespond ONLY with the converted text.\n[dialect text]\nModel:\n[standard text]\n```\n\nDataset = `dialect_text ‚Üí standard_text`.\nAnd, still using Gemma 1B **instruction-tuned** as the base.\n\n### Result\n\nThe fine-tuned model performed **very poorly**:\n\n* Inconsistent outputs\n* Often ignored the instruction\n* Much worse than the multi-turn chat model\n\n### What I‚Äôm trying to understand\n\nWhere is the conceptual mistake?\n\n* Is dialect text ‚Üí standard text fundamentally a **translation / seq2seq task**, not an instruction-following task?\n* Does instruction-tuned Gemma fight against clean text-to-text mapping?\n* Is this translation task different on an architecture level from the basic LLM architecture?\n* Should this be trained without LLM fine-tuning & rather moved to a different type of ML model?\n* Why does the *harder* multi-turn task succeed, but the ‚Äúsimpler‚Äù rewrite task fail?\n\nAnd I **_apologize_** in advance if I come of as rude, But I‚Äôm not looking for `use a bigger model` answers‚Äî I want to understand **why this failed at a training-objective level**. Maybe I am conceptually doing some mistake? Maybe I don't know something that I need to understand on an architecture level about this task of mine?\n\nI‚Äôd really appreciate your insights on this matter",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qcuyiz/gemma_1bit_finetune_worked_great_for_multiturn/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nznai78",
          "author": "nborwankar",
          "text": "1B just may not have enough linguistic power to see the patterns for changing dialects to standard language. You may need a 7B model or even more depending on the base language. If you are looking for a dialect of a language that is itself rare then you will need to look for a multilingual model that has enough training data on your language. \n1B models can be trained for focused tasks - they don‚Äôt have enough training to do nuanced language translation.",
          "score": 2,
          "created_utc": "2026-01-15 00:59:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp9f1j",
              "author": "_hasin",
              "text": "I plan to be able to use & serve these models via llama.cpp server in GGUF formats, so using 7b models wasn't my first thought tbh. And, i don't think i could use 7b",
              "score": 1,
              "created_utc": "2026-01-15 09:36:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nznbhya",
          "author": "LA_rent_Aficionado",
          "text": "My understanding with translation tasks is that the pre-training needs to understand next token prediction association between two languages/dialects, etc. so one strategy is to train interleaved or per line text, for example:\n\n[english] hello how are you \n[spanish] hola como estas\n\nEtc..\n\nAnd then you can better bridge the two with SFT saying translate X to Y. \n\nIf the model isn‚Äôt initially trained multi lingual associations or the language period, then SFT to bridge the two won‚Äôt really make the connection.  I‚Äôd suspect the 1B model barely has the degree of multilingual training you need to make the connections you desire.\n\nI‚Äôm not sure what dialect you are using but try a CPT epoch making sure it learns the associations and then your SFT and see if it improves.",
          "score": 2,
          "created_utc": "2026-01-15 01:05:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpaf27",
              "author": "_hasin",
              "text": "I am targeting Regional bangla texts to be Normalized into standard & formal texts. As for my knowledge, Gemma3 pre-trained wasn't trained on those regional texts at all.\n\nBut, I'm not sure that is the sole reason for the wrong inference. Currently, I'm trying to do the Base pt 4b model with the alpaca Instruction based tuning & trying to see how it turns out.\n\nBut another main issue is the fact that, I am unable to train 4b models on the free tier of kaggle :\")",
              "score": 1,
              "created_utc": "2026-01-15 09:46:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzp5f8s",
          "author": "danielhanchen",
          "text": "Did you use training on completions only as well? https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#training-on-completions-only-masking-out-inputs",
          "score": 1,
          "created_utc": "2026-01-15 08:57:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp9ts9",
              "author": "_hasin",
              "text": "I tried, but it gives me the `All labels in your dataset are -100. Training losses will be all 0`.\n\nThis is probably because I'm setting up the dataset to be in `alpaca` format, for the single task instruction-based tuning. But, I'd like to use the `train_on_responses_only` - maybe it might give better results? But how can i do that with my alpaca format data?",
              "score": 1,
              "created_utc": "2026-01-15 09:40:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qatpys",
      "title": "GLM-4.7 218B REAP model by Cerebras",
      "subreddit": "unsloth",
      "url": "/r/LocalLLaMA/comments/1qatpgb/glm47_218b_reap_model_by_cerebras/",
      "author": "ResearchWheel5",
      "created_utc": "2026-01-12 12:34:34",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qatpys/glm47_218b_reap_model_by_cerebras/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nz8zznt",
          "author": "danielhanchen",
          "text": "I uploaded some quants here: https://huggingface.co/unsloth/GLM-4.7-REAP-218B-A32B-GGUF",
          "score": 6,
          "created_utc": "2026-01-12 22:51:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}