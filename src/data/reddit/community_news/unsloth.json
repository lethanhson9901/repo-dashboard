{
  "metadata": {
    "last_updated": "2026-01-20 08:59:58",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 9,
    "total_comments": 29,
    "file_size_bytes": 40485
  },
  "items": [
    {
      "id": "1qcc34f",
      "title": "Google releases their first reasoning model: MedGemma-1.5",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/gallery/1qcc34f",
      "author": "yoracale",
      "created_utc": "2026-01-14 03:08:11",
      "score": 148,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qcc34f/google_releases_their_first_reasoning_model/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzh92ik",
          "author": "danielhanchen",
          "text": "So essentially MedGemma uses these special tokens:\n\n`<unused94>thought` same as DeepSeek's `<think> `\n\n`<unused95>` same as DeepSeek's `</think> `\n\nJust like DeepSeek it gives a response after `</think>`",
          "score": 8,
          "created_utc": "2026-01-14 03:42:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkt8pm",
              "author": "ObjectiveOctopus2",
              "text": "Looks like you found the Easter egg",
              "score": 3,
              "created_utc": "2026-01-14 17:54:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzl0hba",
                  "author": "danielhanchen",
                  "text": "Haha :)",
                  "score": 1,
                  "created_utc": "2026-01-14 18:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzhcqkl",
          "author": "m98789",
          "text": "Is fine tuning any different than with original medgemma considering now we have reasoning? Ie do we have to provide reasoning traces in our SFT dataset?",
          "score": 3,
          "created_utc": "2026-01-14 04:05:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhg1e2",
              "author": "yoracale",
              "text": "Yes, i'm pretty sure you'll now need to have reasoning traces in your dataset to maintain its reasoning capabilities. Otherwise, you can use your old dataset but the reasoning will be baked away",
              "score": 1,
              "created_utc": "2026-01-14 04:27:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzj2ohd",
                  "author": "m98789",
                  "text": "How can one do that though? Normal datasets have (source, target), how can one get (source, target, reasoning).",
                  "score": 1,
                  "created_utc": "2026-01-14 12:42:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzizmax",
          "author": "igvarh",
          "text": "Will a model of this size be able to analyze MRI images?",
          "score": 1,
          "created_utc": "2026-01-14 12:21:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzj5rvn",
              "author": "Equal-Document4213",
              "text": "It already can",
              "score": 2,
              "created_utc": "2026-01-14 13:02:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjik65",
                  "author": "simracerman",
                  "text": "Reliably?",
                  "score": 1,
                  "created_utc": "2026-01-14 14:14:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl689w",
          "author": "itsstroom",
          "text": "I guess inferior to 27B. The latter one literally saved my life so props to google. Lets see what happens.",
          "score": 1,
          "created_utc": "2026-01-14 18:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03co20",
              "author": "ObjectiveOctopus2",
              "text": "How did it save your life?",
              "score": 2,
              "created_utc": "2026-01-17 12:07:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzms2ps",
              "author": "simracerman",
              "text": "The 27B is smarter than GPT and Claude in my experience. It really shines with text only prompts and a good system prompt that keeps it focused.",
              "score": 1,
              "created_utc": "2026-01-14 23:19:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrau7y",
          "author": "Turbulent_Jump_2000",
          "text": "I can‚Äôt find a use case for this. 4B just way too many hallucinations. ¬†I like the concept though. ¬†Med ASR on the other hand seems to be really strong.¬†",
          "score": 1,
          "created_utc": "2026-01-15 16:59:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03cye8",
              "author": "ObjectiveOctopus2",
              "text": "I think you need to focus on one medical domain and fine tune for a 4b model.",
              "score": 1,
              "created_utc": "2026-01-17 12:10:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0awjad",
              "author": "waterBoy__",
              "text": "I think Google is doing this in preparation of the Apple deal where google‚Äôs Gemini will power the new Siri, and Apple is revamping Heath to run a local model to analyze your health information. This seems like small local model which can run on iPhone.",
              "score": 1,
              "created_utc": "2026-01-18 15:21:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdmqcu",
      "title": "Reinforcement Learning with ultra long context is here!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/1btxn3558jdg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-15 15:36:04",
      "score": 73,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qdmqcu/reinforcement_learning_with_ultra_long_context_is/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzqvsga",
          "author": "____vladrad",
          "text": "Woahhhh nice! This is perfect for what I was trying to do",
          "score": 2,
          "created_utc": "2026-01-15 15:52:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqw4j5",
              "author": "____vladrad",
              "text": "Seriously nice job!!! Do you think there will be a multi gpu version for rl?",
              "score": 3,
              "created_utc": "2026-01-15 15:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzqwr0j",
                  "author": "yoracale",
                  "text": "Yes, early this year for sure!",
                  "score": 2,
                  "created_utc": "2026-01-15 15:56:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzr1ojh",
          "author": "WolfeheartGames",
          "text": "Yuge!",
          "score": 2,
          "created_utc": "2026-01-15 16:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0go7p6",
          "author": "igvarh",
          "text": "Sorry, could you do a dynamic guff for Google/translategemma with RL? This would be very useful for working with subtitles, which are just a document with a lot of context. I would also ask you or someone else to train models in the SRT format, which is a big challenge for any LLM. Not only do they lose context, but they confuse numbering and timings, completely breaking the format.",
          "score": 1,
          "created_utc": "2026-01-19 12:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hf71j",
              "author": "anyandsomeone",
              "text": "i think creating a small script for this would be the better solution.\nextract all strings. give an llm the string in question +10 strings before that one (so it knows the context) and ask it to translate.\n\nclaide code, gemini, chatgpt, ... all of them will be able to give you a script that can do this within a few seconds for free and then you can hook that up with a small llm you could run locally for the actual translation.",
              "score": 1,
              "created_utc": "2026-01-19 14:48:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0mf9l1",
              "author": "yoracale",
              "text": "Currently translategemma seems to have chat template issues which is why we decided to not release quants for it yet.\n\nFor an RL notebook, it's a little complicated since it's a translation model but we'll see what we can do.",
              "score": 1,
              "created_utc": "2026-01-20 06:14:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhscts",
      "title": "Run GLM-4.7-Flash locally Guide! (24GB RAM)",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/hgng1lc5ufeg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-20 05:22:23",
      "score": 50,
      "num_comments": 13,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qhscts/run_glm47flash_locally_guide_24gb_ram/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mccxs",
          "author": "TaroOk7112",
          "text": "Thanks!!!  \nDoes llama.cpp need to add some code to improve support or with this PR it's all supported?  \n[https://github.com/ggml-org/llama.cpp/pull/18936](https://github.com/ggml-org/llama.cpp/pull/18936)",
          "score": 4,
          "created_utc": "2026-01-20 05:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mdiub",
              "author": "danielhanchen",
              "text": "It should work in llama.cpp main now!",
              "score": 2,
              "created_utc": "2026-01-20 06:00:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mjz8z",
          "author": "Psyko38",
          "text": "On a GPU with 16GB of VRAM, we're good at Q3?",
          "score": 1,
          "created_utc": "2026-01-20 06:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mma9p",
              "author": "Conscious_Chef_3233",
              "text": "you should go higher, i run qwen3 30b q4 on my 4070 12g",
              "score": 1,
              "created_utc": "2026-01-20 07:13:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mmg8z",
                  "author": "Psyko38",
                  "text": "But the weights, where do you put them in the RAM and you put the MoE in the VRAM?",
                  "score": 1,
                  "created_utc": "2026-01-20 07:14:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mmhfv",
          "author": "Unlikely_Database_87",
          "text": "In lm studio the model with those parameters you provided, cannot even give any clear response and it infinitely generates nonsense. Simple prompt I use - write java method  to merge two sorted arrays, no tests no explanation just code. My config 5080 and 64GB RAM",
          "score": 1,
          "created_utc": "2026-01-20 07:14:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mohp7",
              "author": "yoracale",
              "text": "You need to use dry multiplier which has the biggest impact, because LM Studio does not have it, you need to disable repeat penalty entirely.",
              "score": 2,
              "created_utc": "2026-01-20 07:32:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0mq2e5",
                  "author": "Unlikely_Database_87",
                  "text": "Thanks that helps",
                  "score": 1,
                  "created_utc": "2026-01-20 07:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mnfbw",
          "author": "Prudent-Ad4509",
          "text": "I'm not sure what they mean by full precision. The original seems to require 64Gb even with near zero context.",
          "score": 1,
          "created_utc": "2026-01-20 07:23:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mldpf",
          "author": "RMK137",
          "text": "Great turnaround! I just got my 5090 so this is perfect timing.",
          "score": 1,
          "created_utc": "2026-01-20 07:05:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc4ngj",
      "title": "Reinforcement Learning, Agents & RL Environments Mini Conference",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/7dydgf1gt6dg1.png",
      "author": "danielhanchen",
      "created_utc": "2026-01-13 21:54:57",
      "score": 37,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "GRPO (Reasoning):sloth_magnify_square_fin:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qc4ngj/reinforcement_learning_agents_rl_environments/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzfrx63",
          "author": "larrytheevilbunnie",
          "text": "We can access the video afterwards right? I can‚Äôt make it live",
          "score": 3,
          "created_utc": "2026-01-13 22:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzguvm7",
              "author": "danielhanchen",
              "text": "Yes it'll be recorded! The same link as the live YouTube",
              "score": 3,
              "created_utc": "2026-01-14 02:20:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qf6qvv",
      "title": "Translategemma-27b",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qf6qvv/translategemma27b/",
      "author": "StormrageBG",
      "created_utc": "2026-01-17 07:17:53",
      "score": 20,
      "num_comments": 7,
      "upvote_ratio": 0.95,
      "text": "Guys do you plan to release quantisation variants of Translategemma-27b ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qf6qvv/translategemma27b/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o02thsb",
          "author": "danielhanchen",
          "text": "We did investigate it but the chat template sadly is quite specialized for it - I can check again later today, but currently it looks complex to support :(",
          "score": 4,
          "created_utc": "2026-01-17 09:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02u4gf",
              "author": "Purple-Programmer-7",
              "text": "Hugely supportive of everything you guys do.\n\nCan you educate me though? The whole ‚Äúwe fixed the chat template‚Äù didn‚Äôt really pan out IMO under scrutiny. And the copyright statement seems a bit of the anthesis  of everything you guys do to support the community.\n\nIf the chat template is functional and embedded, it‚Äôs not going to prevent a gguf from being made.\n\nAgain, not trying to be adversarial here, just want to understand better your position on all of this.\n\nThank you again for everything you‚Äôve provided (and continue to provide) this community, I genuinely am rooting for your success!\n\nEdit: spelling",
              "score": 2,
              "created_utc": "2026-01-17 09:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o07g10j",
                  "author": "danielhanchen",
                  "text": "Hey! Thanks!\n\n1. Translate Gemma has a specialized chat template that requires you to specify the language you want to translate to - this means it's not a chat model - you will need to specify the chat template kwargs separately, so it can't be loaded well in frontend UIs and make the process just harder for folks\n2. The chat template fixes are wide ranging, and we also do direct model implementation fixes - see the following:\n    * Gemma 1, Gemma 3 bug fixes: https://x.com/karpathy/status/1765473722985771335\n    * Phi 4 fixes: https://simonwillison.net/2025/Jan/11/phi-4-bug-fixes/\n    * Llama 4 fixes: https://www.reddit.com/r/LocalLLaMA/comments/1jxo7lb/llamacpp_got_2_fixes_for_llama_4_rope_wrong_norms/\n    * GPT OSS fixes: https://x.com/danielhanchen/status/1953901104150065544\n    * Kimi K2 bug fixes: https://x.com/danielhanchen/status/1946163064665260486\n    * And many more - we do a lot of our work behind the scenes now, so whatever model that was released with us generally is already fixed.\n3. The copyright header is Apache 2 (fully open source) - we place it because folks would simply copy and paste and rebrand it as their own fixes - we just want attribution that's all. Likewise all fixes before a model releases is licensed as whatever the model provider wants.",
                  "score": 2,
                  "created_utc": "2026-01-18 00:48:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03a9z5",
          "author": "DocWolle",
          "text": "Here are ggufs\n\n[https://huggingface.co/mradermacher/translategemma-27b-it-GGUF](https://huggingface.co/mradermacher/translategemma-27b-it-GGUF)",
          "score": 2,
          "created_utc": "2026-01-17 11:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03hq44",
              "author": "StormrageBG",
              "text": "Unsloth quants are always way better than mradermacher... I don't know what  the magic they did but on my test especially for translating unsloth Gemma 3 quants rulz... So I really hope ü§û that can do it again with this model too...",
              "score": 4,
              "created_utc": "2026-01-17 12:47:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03kj2v",
                  "author": "DocWolle",
                  "text": "this may be true, but the mradermacher gguf for translategemma works fine for me.",
                  "score": 1,
                  "created_utc": "2026-01-17 13:06:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0gp3rl",
          "author": "igvarh",
          "text": "\\+RL please. For subtitles, you need to work with a long context.",
          "score": 1,
          "created_utc": "2026-01-19 12:09:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhbr0p",
      "title": "glm 4.7 flash is out gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-19 18:08:55",
      "score": 18,
      "num_comments": 7,
      "upvote_ratio": 0.8,
      "text": "Guys do you plan to release quantisation variants of GLM-4.7 flash ? Its 30b a3b, unsloth chat template fixes are da best.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0mfece",
          "author": "yoracale",
          "text": "It's out now! GGUF: [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nTweet: [https://x.com/UnslothAI/status/2013482180564132092](https://x.com/UnslothAI/status/2013482180564132092)\n\nGuide: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)",
          "score": 1,
          "created_utc": "2026-01-20 06:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iq727",
          "author": "loadsamuny",
          "text": "architecture looks like a renamed deepseekv3, a pull request is in for it in llama.cpp so maybe tomorrow‚Ä¶.",
          "score": 5,
          "created_utc": "2026-01-19 18:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jnj3f",
          "author": "bobeeeeeeeee8964",
          "text": "i test it by the f16 gguf, and it seems the runing in a good speed, BUT it output garbage instead of proper text. We need waiting and see what going in this PR https://github.com/ggml-org/llama.cpp/pull/18936",
          "score": 3,
          "created_utc": "2026-01-19 20:55:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0k8is8",
              "author": "remghoost7",
              "text": "Pull request was merged and closed about an hour ago.  \nIt seems like they figured it out.",
              "score": 2,
              "created_utc": "2026-01-19 22:38:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0j9v1l",
          "author": "neph1010",
          "text": "[https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF](https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF)",
          "score": 1,
          "created_utc": "2026-01-19 19:51:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jk10j",
              "author": "noctrex",
              "text": "Still needs more work, its looping indefinitely",
              "score": 3,
              "created_utc": "2026-01-19 20:39:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0jzshk",
              "author": "Clqgg",
              "text": "this one is omega broken",
              "score": 2,
              "created_utc": "2026-01-19 21:55:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgvg1u",
      "title": "Fine tuning Gpt oss on thinking dataset , which tokens to mask ?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qgvg1u/fine_tuning_gpt_oss_on_thinking_dataset_which/",
      "author": "Hulksulk666",
      "created_utc": "2026-01-19 05:23:52",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "from the official unsloth [notebook ](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb#) for fine tuning Gpt oss 20b ,they used\n\n    from unsloth.chat_templates import train_on_responses_only\n    gpt_oss_kwargs = dict( instruction_part = \"<|start|>user<|message|>\", response_part = \"<|start|>assistant<|channel|>final<|message|>\" )\n     trainer = train_on_responses_only( trainer, **gpt_oss_kwargs, )\n\nBut doesn't this effectively mean the thinking tokens are also being masked ? if so , how is the model actually learning from the thinking tokens of the dataset ? or am i missing something .",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qgvg1u/fine_tuning_gpt_oss_on_thinking_dataset_which/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0g9qyp",
          "author": "im_datta0",
          "text": "Hey u/Hulksulk666  \nThanks for noticing this. Yeah if I read it right, this might be masking \"thinking/analysis\" tokens from the training loss.  \nEdit: Talked to my friends and he mentioned that masking the analysis/thinking seems to produce better down stream results and hence the decision",
          "score": 2,
          "created_utc": "2026-01-19 09:54:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hbime",
              "author": "Hulksulk666",
              "text": "Thanks , i was confused whether or not this masking meant the thinking tokens would not be part of training loss and how not being part of the training loss makes the model learn from the data . From my intuitive understanding it seemed the model should take loss if the data has some ground truth outcome like math if the solution step by step is passed onto thinking , ig i lack some deeper understanding and should resort to some papers",
              "score": 1,
              "created_utc": "2026-01-19 14:29:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qbsp7c",
      "title": "Finetuning Qwen-3-VL for keypoint coordinates recognition",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qbsp7c/finetuning_qwen3vl_for_keypoint_coordinates/",
      "author": "Due_Veterinarian5820",
      "created_utc": "2026-01-13 14:26:21",
      "score": 10,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm trying to fine-tune Qwen-3-VL-8B-Instruct for object keypoint detection, and I‚Äôm running into serious issues.\nBack in August, I managed to do something similar with Qwen-2.5-VL, and while it took some effort, it did work. One reliable signal back then was the loss behavior:\nIf training started with a high loss (e.g., ~100+) and steadily decreased, things were working.\nIf the loss started low, it almost always meant something was wrong with the setup or data formatting.\nWith Qwen-3-VL, I can‚Äôt reproduce that behavior at all. The loss starts low and stays there, regardless of what I try.\nSo far I‚Äôve:\nTried Unsloth\nFollowed the official Qwen-3-VL docs\nExperimented with different prompts / data formats\nNothing seems to click, and it‚Äôs unclear whether fine-tuning is actually happening in a meaningful way because it's not improving its keypoint detection post fine-tuning as well.\nIf anyone has successfully fine-tuned Qwen-3-VL for keypoints (or similar structured vision outputs), I‚Äôd really appreciate it if you could share:\nTraining data format\nPrompt / supervision structure\nCode or repo\nAny gotchas specific to Qwen-3-VL\nAt this point I‚Äôm wondering if I‚Äôm missing something fundamental about how Qwen-3-VL expects supervision compared to 2.5-VL.\nThanks in advance üôè",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qbsp7c/finetuning_qwen3vl_for_keypoint_coordinates/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nzdbyyf",
          "author": "LA_rent_Aficionado",
          "text": "It doesn‚Äôt sound like you‚Äôre actually unlocking your vision layers for training but it‚Äôs unclear whether you actually want to train vision layers.\n\nCompare your model weights before and after training and see if anything is getting updated.  Aside from that, your post is so light on detail it could literally be dozens of things that are happening.",
          "score": 4,
          "created_utc": "2026-01-13 15:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzuq3ff",
              "author": "Due_Veterinarian5820",
              "text": "The model's getting trained as I could see significant differences in prediction before and after the training, but the problem is that it's not getting trained meaningfully. The intention to post was mainly to figure out whether there are alternative ways to finetune the Qwen 3 VL for keypoint detection apart from what's added in the cookbook.  \n\nI just wished there were different instructions/different cookbooks based on the underlying usecase. For 2D Grounding, I'm not sure if the same token loss is applicable as regular text-based fine-tuning as we can measure accurate pixel coordinate distances without having to rely on token similarity.",
              "score": 1,
              "created_utc": "2026-01-16 03:07:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzfewp9",
          "author": "danielhanchen",
          "text": "If it was possible you could share a screenshot of the training process (loss, grad norms etc), or even better a Colab if possible that would be phenomenal! You can also go to our Discord for async help",
          "score": 2,
          "created_utc": "2026-01-13 21:44:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzuohyl",
              "author": "Due_Veterinarian5820",
              "text": "Will definitely do this soon! There are some proprietary data constraints but I'll try to create a generic flow of what I tried and post that in the discord channel",
              "score": 1,
              "created_utc": "2026-01-16 02:58:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzh5lyi",
          "author": "mmathew23",
          "text": "Given what you describe, could you check that the key point labels are not being masked out after getting the batch from the dataloader? Maybe even try to run forward or inference with on of the dataset examples to see if anything jumps out.",
          "score": 1,
          "created_utc": "2026-01-14 03:21:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzuoa24",
              "author": "Due_Veterinarian5820",
              "text": "Yes, I thoroughly validated the training data and also ensured that it complies with the desired normalized coordinates format of Qwen 3 VL",
              "score": 1,
              "created_utc": "2026-01-16 02:57:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzutx9j",
                  "author": "mmathew23",
                  "text": "What happens when you run inference on the same example before and after training? When you say last time has a loss of 100, you‚Äôre doing cross entropy loss, right? A low cross entropy loss isn‚Äôt necessarily a bad thing, but again depends how low. 100 is really high and that seems more suspicious to me.",
                  "score": 1,
                  "created_utc": "2026-01-16 03:29:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzyrc97",
          "author": "LA_rent_Aficionado",
          "text": "Cookbooks are just starting points but not authoritative.  In my experiences the key thing is to work backwards from desired outputs to formulate a training strategy.  It could just be a matter of dataset quality, incorrect masking or special token/chat template disconnects. There could be layers you are not training that tie all the pieces together.\n\nI‚Äôd recommend looking at the qwen3vl documentation and working backwards from there.",
          "score": 1,
          "created_utc": "2026-01-16 18:21:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcuyiz",
      "title": "Gemma 1b-it finetune worked great for multi-turn chat, but failed for `dialect text ‚Üí standard text` conversion",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qcuyiz/gemma_1bit_finetune_worked_great_for_multiturn/",
      "author": "_hasin",
      "created_utc": "2026-01-14 18:21:47",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm fine-tuning **Gemma 1B instruction-tuned** locally and ran into a failure I can‚Äôt explain.\n\n### Target task\n\nNormalize **regional dialect text** into **Standard Text** so downstream LLMs / rule-based extractors don‚Äôt hallucinate, while extracting data from the regional text (since now it'll be extracting from the standard text)\n\n### What worked\n\nI previously fine-tuned Gemma 1B it for a **multi-turn phone survey agent** using the standard chat template:\n\n* `<start_of_turn>user / model`\n* Instruction-heavy system prompt\n* Multi-turn conversational data\n\n**Result:** Tuned model followed instructions extremely well and performed reliably.\n\n### What I changed\n\nI reused **the same fine-tuning script, base model, trainer, and hyperparams**, but switched to a **single-turn parallel text task**:\n\n```\nUser:\nConvert the following dialect text to standard text.\nRespond ONLY with the converted text.\n[dialect text]\nModel:\n[standard text]\n```\n\nDataset = `dialect_text ‚Üí standard_text`.\nAnd, still using Gemma 1B **instruction-tuned** as the base.\n\n### Result\n\nThe fine-tuned model performed **very poorly**:\n\n* Inconsistent outputs\n* Often ignored the instruction\n* Much worse than the multi-turn chat model\n\n### What I‚Äôm trying to understand\n\nWhere is the conceptual mistake?\n\n* Is dialect text ‚Üí standard text fundamentally a **translation / seq2seq task**, not an instruction-following task?\n* Does instruction-tuned Gemma fight against clean text-to-text mapping?\n* Is this translation task different on an architecture level from the basic LLM architecture?\n* Should this be trained without LLM fine-tuning & rather moved to a different type of ML model?\n* Why does the *harder* multi-turn task succeed, but the ‚Äúsimpler‚Äù rewrite task fail?\n\nAnd I **_apologize_** in advance if I come of as rude, But I‚Äôm not looking for `use a bigger model` answers‚Äî I want to understand **why this failed at a training-objective level**. Maybe I am conceptually doing some mistake? Maybe I don't know something that I need to understand on an architecture level about this task of mine?\n\nI‚Äôd really appreciate your insights on this matter",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qcuyiz/gemma_1bit_finetune_worked_great_for_multiturn/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nznai78",
          "author": "nborwankar",
          "text": "1B just may not have enough linguistic power to see the patterns for changing dialects to standard language. You may need a 7B model or even more depending on the base language. If you are looking for a dialect of a language that is itself rare then you will need to look for a multilingual model that has enough training data on your language. \n1B models can be trained for focused tasks - they don‚Äôt have enough training to do nuanced language translation.",
          "score": 2,
          "created_utc": "2026-01-15 00:59:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp9f1j",
              "author": "_hasin",
              "text": "I plan to be able to use & serve these models via llama.cpp server in GGUF formats, so using 7b models wasn't my first thought tbh. And, i don't think i could use 7b",
              "score": 1,
              "created_utc": "2026-01-15 09:36:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nznbhya",
          "author": "LA_rent_Aficionado",
          "text": "My understanding with translation tasks is that the pre-training needs to understand next token prediction association between two languages/dialects, etc. so one strategy is to train interleaved or per line text, for example:\n\n[english] hello how are you \n[spanish] hola como estas\n\nEtc..\n\nAnd then you can better bridge the two with SFT saying translate X to Y. \n\nIf the model isn‚Äôt initially trained multi lingual associations or the language period, then SFT to bridge the two won‚Äôt really make the connection.  I‚Äôd suspect the 1B model barely has the degree of multilingual training you need to make the connections you desire.\n\nI‚Äôm not sure what dialect you are using but try a CPT epoch making sure it learns the associations and then your SFT and see if it improves.",
          "score": 2,
          "created_utc": "2026-01-15 01:05:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpaf27",
              "author": "_hasin",
              "text": "I am targeting Regional bangla texts to be Normalized into standard & formal texts. As for my knowledge, Gemma3 pre-trained wasn't trained on those regional texts at all.\n\nBut, I'm not sure that is the sole reason for the wrong inference. Currently, I'm trying to do the Base pt 4b model with the alpaca Instruction based tuning & trying to see how it turns out.\n\nBut another main issue is the fact that, I am unable to train 4b models on the free tier of kaggle :\")",
              "score": 1,
              "created_utc": "2026-01-15 09:46:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzp5f8s",
          "author": "danielhanchen",
          "text": "Did you use training on completions only as well? https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#training-on-completions-only-masking-out-inputs",
          "score": 1,
          "created_utc": "2026-01-15 08:57:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp9ts9",
              "author": "_hasin",
              "text": "I tried, but it gives me the `All labels in your dataset are -100. Training losses will be all 0`.\n\nThis is probably because I'm setting up the dataset to be in `alpaca` format, for the single task instruction-based tuning. But, I'd like to use the `train_on_responses_only` - maybe it might give better results? But how can i do that with my alpaca format data?",
              "score": 1,
              "created_utc": "2026-01-15 09:40:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}