{
  "metadata": {
    "last_updated": "2026-02-22 02:59:49",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 12,
    "total_comments": 79,
    "file_size_bytes": 80097
  },
  "items": [
    {
      "id": "1r6564b",
      "title": "Qwen3.5 is out now!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/xtgnyvb2stjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-16 09:28:34",
      "score": 357,
      "num_comments": 44,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r6564b/qwen35_is_out_now/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5nqycb",
          "author": "Zestyclose-Shift710",
          "text": "Cant wait to run this on my 8gb vram 32gb ram system",
          "score": 38,
          "created_utc": "2026-02-16 10:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nrau1",
              "author": "yoracale",
              "text": "They're apparently releasing smaller models soon, so you should wait for that.",
              "score": 17,
              "created_utc": "2026-02-16 10:09:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nrvf8",
                  "author": "Zestyclose-Shift710",
                  "text": "Oh that's exciting\n\nRunning sparse midsized MoEs like 30b a3b with experts on CPU and the rest on GPU is so nice on normal hardware",
                  "score": 7,
                  "created_utc": "2026-02-16 10:14:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q5fpo",
              "author": "Neither-Phone-7264",
              "text": "i mean, with enough storage you can. won't be fun, but it's doable.",
              "score": 1,
              "created_utc": "2026-02-16 18:24:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5tmb44",
                  "author": "Zestyclose-Shift710",
                  "text": "It'll be like that computer from the hitchhikers guide to the Galaxy¬†\n\n\nGonna take a billion years to output 42",
                  "score": 2,
                  "created_utc": "2026-02-17 06:14:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nu76s",
          "author": "joninco",
          "text": "Whelp.. I was gonna sleep in, but now gotta get up.",
          "score": 7,
          "created_utc": "2026-02-16 10:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oj9sa",
          "author": "jikilan_",
          "text": "Ok I am downloading more rams now. Will report back the token/s on the software ram",
          "score": 4,
          "created_utc": "2026-02-16 13:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q60fk",
          "author": "RandomUserRU123",
          "text": "Hoping for Qwen3.5-VL-32B-Instruct soon",
          "score": 4,
          "created_utc": "2026-02-16 18:27:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zw8vn",
              "author": "Resident_Suit_9916",
              "text": "They will not be releasing VL models",
              "score": 1,
              "created_utc": "2026-02-18 04:22:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5oav1p",
          "author": "getmevodka",
          "text": "4 bit mxpf it is then ü§©üòç",
          "score": 3,
          "created_utc": "2026-02-16 12:50:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oflri",
          "author": "EbbNorth7735",
          "text": "This is really close in size MOE to Llama 3 Mavrick if I recall correctly. Interesting how that is. META should have continued working on it.",
          "score": 3,
          "created_utc": "2026-02-16 13:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ouoea",
          "author": "xor_2",
          "text": "I am 70GB short to fit this model in my ram... and by this model I mean Q2 version.\n\nOtherwise Qwen3-Max was my favorite non-local model so its nice to get an upgrade.\n\nWaiting for 35B version to release. It will be slow but heck, maybe it will be smarter than GLM 4.7 Flash",
          "score": 2,
          "created_utc": "2026-02-16 14:44:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t2xj2",
              "author": "yoracale",
              "text": "You should try MiniMax-2.5 instead: [https://unsloth.ai/docs/models/minimax-2.5](https://unsloth.ai/docs/models/minimax-2.5)",
              "score": 2,
              "created_utc": "2026-02-17 03:51:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v4ws2",
                  "author": "xor_2",
                  "text": "Thanks for the suggestion but I only have only 64GB RAM + 24GB VRAM and on computer I actually use and have a lot of RAM used at all times so MinMax is too big to fit.",
                  "score": 1,
                  "created_utc": "2026-02-17 13:45:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q0acp",
          "author": "txgsync",
          "text": "YAY! Unsloth is on the MXFP4 train now! WooWoo!",
          "score": 2,
          "created_utc": "2026-02-16 18:01:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t2tl1",
              "author": "yoracale",
              "text": "Let us know if it's faster for you :)",
              "score": 1,
              "created_utc": "2026-02-17 03:51:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tu0yi",
                  "author": "txgsync",
                  "text": "Alas. I  slumming it on a M4 Max 128GB. Gotta REAP it before I can play.",
                  "score": 1,
                  "created_utc": "2026-02-17 07:22:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5xo6yi",
                  "author": "SquirrelEStuff",
                  "text": "I keep trying to install this in LM Studio on Mac Studio M3 Ultra with 256GB but keep getting loading errors. How do you recommend I install it?\n\nTIA!",
                  "score": 1,
                  "created_utc": "2026-02-17 21:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rieqs",
          "author": "ChadThunderDownUnder",
          "text": "Can‚Äôt wait to run this on my dual 5090 + 512GB DDR5 setup",
          "score": 2,
          "created_utc": "2026-02-16 22:21:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t2usj",
              "author": "yoracale",
              "text": "You can run full Q8 very well with that setup",
              "score": 1,
              "created_utc": "2026-02-17 03:51:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tcw3k",
                  "author": "ChadThunderDownUnder",
                  "text": "Yep. Looking forward to seeing what this model can do. It‚Äôs the most promising so far. GPT-OSS-120B was solid but too sanitized for my liking.",
                  "score": 2,
                  "created_utc": "2026-02-17 05:01:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60qkcy",
          "author": "pppreddit",
          "text": "If only we had prompt caching locally...",
          "score": 2,
          "created_utc": "2026-02-18 08:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sqzv5",
          "author": "ButCaptainThatsMYRum",
          "text": "Ok but will it think and talk like a moody teenager? The previous qwen were fairly smart but sometimes insufferable.",
          "score": 1,
          "created_utc": "2026-02-17 02:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5us90w",
          "author": "depressedclassical",
          "text": "How well does it perform on 16GB VRAM+256GB RAM in terms of t/s (or s/t)?",
          "score": 1,
          "created_utc": "2026-02-17 12:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yif9d",
              "author": "greatwilt",
              "text": "the guide says 25 tps with 24GB of VRAM and 256GB of RAM (It doesnt specify unified, but maybe it needs to be) but I cant seem to get more than 10 tps with 4x 48GB GPUS and 512GB of system RAM.\n\nedit: got up to 15 tps with the 4bit quant and offloading, and 60tps with the 2bit quant",
              "score": 2,
              "created_utc": "2026-02-17 23:43:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o61tj1u",
          "author": "PsychologicalDemand0",
          "text": "Will any smaller versions be also released at sometime ?",
          "score": 1,
          "created_utc": "2026-02-18 13:33:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61uyas",
              "author": "yoracale",
              "text": "Hopefully, apparently according to leaks they are going to",
              "score": 2,
              "created_utc": "2026-02-18 13:41:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o62nycz",
          "author": "gracchusjanus",
          "text": "17B means it hopefully, with thoughts and prayers, runs on my 16GB 9070xt, right?",
          "score": 1,
          "created_utc": "2026-02-18 16:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62tc3k",
              "author": "Ok_Bug1610",
              "text": "They state at the bottom that at 4-bit (average most use them with say Ollama, etc.) it uses 256GB RAM... and while you can use less, you will see a huge performance loss (offloading to system memory). So don't get your hopes up... but if you have 220GB+ space available (UD-Q4\\_K\\_XL 219 GB), give it a shot.",
              "score": 1,
              "created_utc": "2026-02-18 16:28:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62u7z5",
                  "author": "gracchusjanus",
                  "text": "I was joking because of the active parameter number ;D\n\n\nWith my setup, 9070xt + 32GB DDR4 , I only use Gemma, OSS and Mistral. GLM also works but not in my native language (which is the language my docs are in).",
                  "score": 1,
                  "created_utc": "2026-02-18 16:32:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nob3j",
          "author": "Zerokx",
          "text": "Seems very good. But won't run on my machine. Also I thought you need enough VRAM to hold the model, how would it get 20 token/s just running from System RAM?",
          "score": 1,
          "created_utc": "2026-02-16 09:41:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5npde4",
              "author": "yoracale",
              "text": "I get 20 tokens/s on my unified memory Mac laptop!  \nWe're uploading smaller ones as we speak :)",
              "score": 4,
              "created_utc": "2026-02-16 09:51:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nqh31",
                  "author": "Final-Rush759",
                  "text": "Mlx or GGUF?",
                  "score": 2,
                  "created_utc": "2026-02-16 10:01:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5r13ga",
                  "author": "captainhukk",
                  "text": "Wait you‚Äôre getting that on a 128GB MacBook?",
                  "score": 1,
                  "created_utc": "2026-02-16 20:56:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o1nq8",
          "author": "SectionCrazy5107",
          "text": "please advise best config for vibe coding with claude code: should be thinking or non-thinking and the temp etc details please. Thanks",
          "score": -1,
          "created_utc": "2026-02-16 11:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5onuir",
              "author": "yoracale",
              "text": "We have a guide for Claude Code: [https://unsloth.ai/docs/basics/claude-codex](https://unsloth.ai/docs/basics/claude-codex)\n\nJust apply our Qwen3.5 settings in there.",
              "score": 2,
              "created_utc": "2026-02-16 14:07:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5p3sqv",
                  "author": "ochonueve89",
                  "text": "Super useful, thanks!1",
                  "score": 2,
                  "created_utc": "2026-02-16 15:30:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5rv4wj",
                  "author": "SectionCrazy5107",
                  "text": "Many many thanks",
                  "score": 2,
                  "created_utc": "2026-02-16 23:29:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5oht1d",
              "author": "siggystabs",
              "text": "It just came out. You could be the first to try it.",
              "score": 1,
              "created_utc": "2026-02-16 13:34:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ollr0",
                  "author": "SectionCrazy5107",
                  "text": "understand thanks. I downloaded UD-Q3\\_K\\_XL, running on V100, 176GB DDR5, getting around 9-10 t/s. just followed the unsloth guide, responses are same as in their online chat site.",
                  "score": 3,
                  "created_utc": "2026-02-16 13:55:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9wkxd",
      "title": "100,000+ models trained with Unsloth have been open-sourced",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/6f7zem4yrnkg1.png",
      "author": "danielhanchen",
      "created_utc": "2026-02-20 14:19:29",
      "score": 208,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r9wkxd/100000_models_trained_with_unsloth_have_been/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6fiw48",
          "author": "SnooPuppers4132",
          "text": "wow",
          "score": 5,
          "created_utc": "2026-02-20 14:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fnlqw",
          "author": "immediate_a982",
          "text": "With open weights you get the model‚Äôs parameters ie the actual numbers so you can run and fine-tune the model, you don‚Äôt necessarily get the **training data, training code**, needed to reproduce the gguf from scratch.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
          "score": 6,
          "created_utc": "2026-02-20 15:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6isms7",
              "author": "zp-87",
              "text": "So basically in terms of software, you get compiled .exe file but not the source code. So it is not open source, it is free to use",
              "score": 6,
              "created_utc": "2026-02-21 00:32:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fkc51",
          "author": "Comrade-Porcupine",
          "text": "Basically none of this is \"open source\".  It's open weight. There is an important difference -- apart from AllenAI almost none of the openweight models on huggingface have their \"sources\" open.\n\nUsing the term \"open source\" for these models is an abuse of the term.",
          "score": 7,
          "created_utc": "2026-02-20 15:00:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fpk0j",
              "author": "Rhinoseri0us",
              "text": "I believe you are correct. What‚Äôs a good AllenAI model to start with?",
              "score": 1,
              "created_utc": "2026-02-20 15:25:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6fogx8",
              "author": "yoracale",
              "text": "I think we're getting stressed over semantics for no reason. The models are released under an opensource license aka APACHE 2.0 and this qualify for it to be open source.",
              "score": 0,
              "created_utc": "2026-02-20 15:20:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6grcsc",
                  "author": "Tema_Art_7777",
                  "text": "Does ‚Äòsource‚Äô definition include training data?",
                  "score": 1,
                  "created_utc": "2026-02-20 18:19:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6k23z7",
          "author": "No_Conversation9561",
          "text": "So let me get this straight.\n\n- These guys took weights of an existing model (for free)\n- They used Unsloth‚Äôs toolkit (for free)\n- They finetuned the existing model using Unsloth‚Äôs toolkit using their own dataset. (compute not free)\n- They released only the finetuned weights on huggingface but not the dataset.\n\nIf you don‚Äôt release the dataset then there‚Äôs no distinction between open source community and big corporations.",
          "score": 1,
          "created_utc": "2026-02-21 05:35:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5f4v8",
      "title": "Run MiniMax-2.5 locally Guide!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/61b97oryxnjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-15 13:51:53",
      "score": 203,
      "num_comments": 38,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r5f4v8/run_minimax25_locally_guide/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5iap76",
          "author": "Status_Contest39",
          "text": "This is super and you are always the best !",
          "score": 9,
          "created_utc": "2026-02-15 14:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5if6vn",
              "author": "yoracale",
              "text": "Thanks a lot! Let us know if you encounter any issues. üôè There were previously some identification issues with Qwen3 coder next where it was detected nor loading.",
              "score": 6,
              "created_utc": "2026-02-15 14:27:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5or9kc",
          "author": "randomh4cker",
          "text": "I set this up on a MBP M4 Max w/128GB using the 3-bit dynamic quant and it's fast and smart!  With an empty context it runs about 40t/sec.  I was able to fit about 64k (65535) tokens and keep the machine stable.  More than this number of tokens context anyway takes too long to process and my integrations timeout.\n\nSo far this works very well with OpenClaw for coding tasks and tool calling. It does get a little bogged down when using a browser with Playwright. It seems to be very good at Kubernetes as well so it could be useful as a DevOps agent.\n\nThanks Unsloth for making this possible!",
          "score": 3,
          "created_utc": "2026-02-16 14:25:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vso57",
              "author": "mr_chillout",
              "text": "Thanks for sharing the information about your observations. Would it make sense to go for 256GB RAM M3 Ultra to make use of the AI for the big iOS project (200+ swift files)? ",
              "score": 1,
              "created_utc": "2026-02-17 15:48:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iauwa",
          "author": "Sad-Bat6310",
          "text": "Many thanks folks, you are truly the best !",
          "score": 2,
          "created_utc": "2026-02-15 14:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jirfz",
          "author": "flavio_geo",
          "text": "Unsloth, first thank you very much for the work bringing the quality quants to us that fast.\n\nI run local models on a AMD XTX 7900 24GB + Ryzen 7 9700 96GB. I am using the Q3\\_K\\_XL and getting \\~12 tokens/s (separating VRAM for 48k context in KV q8). In the announcement it is stated that with 16GB VRAM + 96GB RAM one could run it 25+ tokens. Could you please share what would be the optimal offload configuration to get there?",
          "score": 2,
          "created_utc": "2026-02-15 17:45:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5majvh",
              "author": "yoracale",
              "text": "Did you use fiton? 15 tokens/s doesn't sound too bad but yes it's slightly slower than expected",
              "score": 2,
              "created_utc": "2026-02-16 02:53:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nkr0s",
                  "author": "flavio_geo",
                  "text": "Yep, using --fit -on gets me about the same results as trying regex with -ot, which is 12.2 tokens/s.\n\nUsing llama.cpp latest docker rocm img.",
                  "score": 2,
                  "created_utc": "2026-02-16 09:07:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l9fsg",
          "author": "ozzeruk82",
          "text": "Do we have a way of knowing the quality loss from say using the Q3 model suggested in the tutorial? People seem unsure as to whether we're losing 25% quality, or 5% quality, or very minimal loss. It would be great for some of the main benchmarks to test some of these quantised versions.",
          "score": 2,
          "created_utc": "2026-02-15 23:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mxdw7",
          "author": "johndeuff",
          "text": "Impressive!!!",
          "score": 2,
          "created_utc": "2026-02-16 05:36:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i1vdl",
          "author": "Orlandocollins",
          "text": "Been enjoying 2.5. Seems to snappier and pretty intelligent!",
          "score": 2,
          "created_utc": "2026-02-20 22:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ibuw7",
          "author": "Local-Cartoonist3723",
          "text": "I cincerely feel as a community we‚Äôve almost been to quiet on this model, this size and that strong is ridiculous.\n\nThanks unsloth team for making this avail!",
          "score": 2,
          "created_utc": "2026-02-15 14:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iqfor",
          "author": "Lesteriax",
          "text": "Will it work on rtx pro 6000 and 96gb ram?",
          "score": 2,
          "created_utc": "2026-02-15 15:27:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j5qvn",
              "author": "yoracale",
              "text": "Yes of course! Run a bigger one and it'll be even faster",
              "score": 1,
              "created_utc": "2026-02-15 16:42:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iqpdu",
          "author": "Porespellar",
          "text": "Amazing job guys! Thanks so much for getting your quants out so quickly. Is there any chance that you guys could start including the quant perplexity information with your releases so it‚Äôs easier to compare your quants with Ubergarm‚Äôs quants and others who include this information?",
          "score": 1,
          "created_utc": "2026-02-15 15:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mabqz",
              "author": "yoracale",
              "text": "Perplexity is a poor measurement for accuracy and one of the worst. See: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs#calibration-dataset-overfitting](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs#calibration-dataset-overfitting)\n\nPerplexity is just so popular because it's the easiest to benchmark",
              "score": 2,
              "created_utc": "2026-02-16 02:51:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jo6qf",
          "author": "alex_bit_",
          "text": "4 x 3090s will do it?",
          "score": 1,
          "created_utc": "2026-02-15 18:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mafub",
              "author": "yoracale",
              "text": "No, unfortunately it doesn't fit. But you can run a slightly smaller one. How much extra RAM do you have?",
              "score": 2,
              "created_utc": "2026-02-16 02:52:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nogsc",
                  "author": "alex_bit_",
                  "text": "256GB of DDR4 in a X299 system.",
                  "score": 1,
                  "created_utc": "2026-02-16 09:42:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5mxkqg",
              "author": "johndeuff",
              "text": "Man I wish",
              "score": 1,
              "created_utc": "2026-02-16 05:37:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k9xb3",
          "author": "rytheguy88",
          "text": "What size context window and tokens per second would be possible on an M3 ultra with 128 or 256GB ram?",
          "score": 1,
          "created_utc": "2026-02-15 19:58:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mahe9",
              "author": "yoracale",
              "text": "For 128 maybe 20K context? 256 can do 100K",
              "score": 1,
              "created_utc": "2026-02-16 02:52:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tgnk4",
          "author": "WonderfulNectarine87",
          "text": "Got M3 Max with 96gb RAM and 40 GPU.. guess it can work on my device?",
          "score": 1,
          "created_utc": "2026-02-17 05:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vrc6c",
          "author": "mr_chillout",
          "text": "Amazing work guys! thanks for doing this and sharing!  \nWhat Macc should I buy (the lowest price possible ;) ) so that I can run it against big (200 files) iOS project codebase? ",
          "score": 1,
          "created_utc": "2026-02-17 15:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xof55",
              "author": "yoracale",
              "text": "128gb ram id say",
              "score": 1,
              "created_utc": "2026-02-17 21:11:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5xom6c",
                  "author": "mr_chillout",
                  "text": "256 GB would be I assume a room for future growth? ",
                  "score": 1,
                  "created_utc": "2026-02-17 21:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5imzqm",
          "author": "Mysterious_Value_219",
          "text": "Would it work with the 128GB GX10 or the ryzen ai max+ 395?",
          "score": 1,
          "created_utc": "2026-02-15 15:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j3u4p",
              "author": "etcetera0",
              "text": "I tried smaller quantization models but it doesn't load for me. I'll try again today",
              "score": 1,
              "created_utc": "2026-02-15 16:32:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5jzd8f",
              "author": "StardockEngineer",
              "text": "You're going to have to use q3 quants or wait for REAP.",
              "score": 1,
              "created_utc": "2026-02-15 19:06:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iu39g",
          "author": "jellydn",
          "text": "Do you know if that's running okay on a Mac M1/M2 laptop? Or what is the recommended spec for the Mac Mini to run it smoothly? I really appreciate any help you can provide.",
          "score": 1,
          "created_utc": "2026-02-15 15:45:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j5l99",
              "author": "yoracale",
              "text": "If it fits it should mostly be ok. I have a M3 and get 20 toks",
              "score": 1,
              "created_utc": "2026-02-15 16:41:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5j8lw6",
                  "author": "jellydn",
                  "text": "Cool. Thanks, let me try and get back if any issues.",
                  "score": 1,
                  "created_utc": "2026-02-15 16:55:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6we96",
      "title": "All Qwen3.5-397B-A17B GGUFs are up!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/8ah65mf7gzjg1.jpeg",
      "author": "yoracale",
      "created_utc": "2026-02-17 04:30:55",
      "score": 180,
      "num_comments": 37,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r6we96/all_qwen35397ba17b_ggufs_are_up/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5tbknc",
          "author": "AppleBottmBeans",
          "text": "I remember in 1999 having to beg my best friends mom to drive me to CompUSA because after 3 months, I finally saved up enough money to buy a new GPU and finally play Everquest. It required at least 4MB of VRAM and that's the card I got. \n\nI wonder if in 10 years we will look back and look at something like 462GB VRAM and laugh at how small that ends up being. ",
          "score": 25,
          "created_utc": "2026-02-17 04:51:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tx081",
              "author": "DertekAn",
              "text": "Hmm, that's already the case with servers, but for home use... forget it.....\n\nIf you look at home users and upcoming technologies like the Steam Machine, which has 16GB of RAM...\n\nMy first PC from 2015 already had 32GB of RAM. And as for VRAM, my GTX 980 had 4GB, and after that I had an RTX 3060 Ti with 8GB, and now in 2026, 8GB is still very common, and it's becoming more prevalent again due to the RAM shortage.\n\nLuckily, I upgraded to an RX 9060 XT 16GB card at the end of last year, right when they were cheapest.",
              "score": 3,
              "created_utc": "2026-02-17 07:50:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o61gc8v",
                  "author": "rorykoehler",
                  "text": "China will come in with oversupply. They have already started to plug the gap. There is no way they will let this opportunity slip. They are probably also preparing subsidies for when the bubble pops to derisk the scale up for companies. We will have TB of ram for pennies in 3 years. ",
                  "score": 2,
                  "created_utc": "2026-02-18 12:12:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5tgv16",
          "author": "BABA_yaaGa",
          "text": "1 bit still needs more than 100GB ü•≤ü•≤ü•≤",
          "score": 8,
          "created_utc": "2026-02-17 05:31:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tigf8",
              "author": "yoracale",
              "text": "Yep unfortunately, you're better off running MiniMax-2.5 until Qwen releases a smaller model: https://unsloth.ai/docs/models/minimax-2.5",
              "score": 7,
              "created_utc": "2026-02-17 05:43:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5yw801",
                  "author": "garlic-silo-fanta",
                  "text": "Can I get a 0.1-bit? Maybe that‚Äôll fit",
                  "score": 2,
                  "created_utc": "2026-02-18 00:59:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o68v9t2",
                  "author": "RedParaglider",
                  "text": "Wouldn't it be better if we have 128gb like 125gb useable vram to just run qwen3 coder next in a Q5 or Q6?  Would there be a benefit to running minimax in a Q3?  If there is I just found my new fun task for tonight.",
                  "score": 1,
                  "created_utc": "2026-02-19 14:27:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5tfw38",
          "author": "nok01101011a",
          "text": "How nice! I could theoretically run the TQ1_0 Model, lol. Is there actually any thinkable scenario where someone would/could benefit from any of those Q1/Q2 models?",
          "score": 3,
          "created_utc": "2026-02-17 05:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tidyu",
              "author": "yoracale",
              "text": "Usually Q1 or Q2 is very effective for large models. Qwen3.5 is ok ish to run since it's 400b. You can see benchmarks for dynamic 1 and 2-bit here: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs",
              "score": 3,
              "created_utc": "2026-02-17 05:43:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tt7mh",
                  "author": "alhinai_03",
                  "text": "Is the unsloth qwen coder next Q2_K_XL worth it, or am I better off running the 60B REAM at Q4?",
                  "score": 1,
                  "created_utc": "2026-02-17 07:15:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5tdwby",
          "author": "LA_rent_Aficionado",
          "text": "Edit: user error",
          "score": 2,
          "created_utc": "2026-02-17 05:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tia4x",
              "author": "yoracale",
              "text": "Weird can you try another quant and see?",
              "score": 1,
              "created_utc": "2026-02-17 05:42:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tkoon",
                  "author": "LA_rent_Aficionado",
                  "text": "Same results with IQ4\\_NL, I even tried with the mmproj from [https://huggingface.co/DevQuasar/Qwen.Qwen3.5-397B-A17B-GGUF](https://huggingface.co/DevQuasar/Qwen.Qwen3.5-397B-A17B-GGUF) to no avail",
                  "score": 1,
                  "created_utc": "2026-02-17 06:01:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5tkx8e",
              "author": "EbbNorth7735",
              "text": "Mine nailed image analysis perfectly. Like scary perfectly. Way better than qwen3 vl which made occasional mistakes. I have the mxfp4 version I think it's called from unsloth. Using the mmproj 32. Latest llama.cpp release on cuda 13.1",
              "score": 1,
              "created_utc": "2026-02-17 06:03:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5trdl9",
                  "author": "LA_rent_Aficionado",
                  "text": "I accidently fat fingered batch size at 1028 vs 1024 and apparently that was enough to break everything... problem solved ",
                  "score": 4,
                  "created_utc": "2026-02-17 06:58:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5unlp1",
          "author": "timbo2m",
          "text": "Anyone have a single 512 Mac studio benchmark for each quant for each context size?",
          "score": 2,
          "created_utc": "2026-02-17 11:53:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tzqor",
          "author": "iTh0R-y",
          "text": "Need mlx‚Äôs",
          "score": 1,
          "created_utc": "2026-02-17 08:16:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u6lgk",
              "author": "yoracale",
              "text": "Maybe in the future",
              "score": 1,
              "created_utc": "2026-02-17 09:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vwef7",
                  "author": "ProtoSkutR",
                  "text": "it‚Äôs already out",
                  "score": 1,
                  "created_utc": "2026-02-17 16:06:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vwoui",
          "author": "ProtoSkutR",
          "text": "I tried running this with the latest VLLM nightly on macOS, that‚Äôs 14.X, I also tried it on VLLM on Cuda, again the most recent nightly wheel, 16.0rc1, still got all of these errors!!!",
          "score": 1,
          "created_utc": "2026-02-17 16:07:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63so7q",
          "author": "Artistic_Ladder9570",
          "text": "maybe i am not understanding ... who runs these models? who has a b200 gpu to run this? ",
          "score": 1,
          "created_utc": "2026-02-18 19:05:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65gk2t",
              "author": "yoracale",
              "text": "It's also for people with a lot of ram or people who have a lot of unified memory like MacS or DGX Spark etc",
              "score": 1,
              "created_utc": "2026-02-18 23:52:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o69ousn",
                  "author": "Artistic_Ladder9570",
                  "text": "yea i just learned that... (sigh's in broke)... i just learned a mac studio has 512gb ram (for like $8k)...",
                  "score": 1,
                  "created_utc": "2026-02-19 16:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5u7xv9",
          "author": "Impossible_Art9151",
          "text": "under llama.cpp I tried:  \nMXFP4\\_MOE, UD-Q4\\_K\\_XL, Q4\\_K\\_M  \n  \nRunning with   \n llama-server     --model unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf     --alias \"unsloth/Qwen3.5-397B-A17B\"     --temp 0.6     --top-p 0.95     --ctx-size 261000     --top-k 20     --min-p 0.00     --port 8090 --jinja --no-mmap -fa on\n\nRunning without SSL init: using 23 threads for HTTP server start: binding port with default address family main: loading model srv load\\_model: loading model 'unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf' common\\_init\\_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on llama\\_model\\_load: error loading model: error loading model architecture: unknown model architecture: 'qwen35moe' llama\\_model\\_load\\_from\\_file\\_impl: failed to load model llama\\_params\\_fit: encountered an error while trying to fit params to free device memory: failed to load model llama\\_params\\_fit: fitting params to free memory took 0.27 seconds llama\\_model\\_load\\_from\\_file\\_impl: using device CUDA0 (NVIDIA RTX A6000) (0000:00:10.0) - 48299 MiB free llama\\_model\\_loader: additional 5 GGUFs metadata loaded. llama\\_model\\_loader: loaded meta data with 48 key-value pairs and 1098 tensors from unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf (version GGUF V3 (latest)) llama\\_model\\_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. llama\\_model\\_loader: - kv 0: general.architecture str = qwen35moe .... \\[deleted due to size\\] llama\\_model\\_loader: - type mxfp4: 352 tensors print\\_info: file format = GGUF V3 (latest) print\\_info: file type = Q4\\_K - Medium print\\_info: file size = 199.66 GiB (4.33 BPW) llama\\_model\\_load: error loading model: error loading model architecture: unknown model architecture: 'qwen35moe' llama\\_model\\_load\\_from\\_file\\_impl: failed to load model common\\_init\\_from\\_params: failed to load model 'unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf' srv load\\_model: failed to load model, 'unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf' srv operator(): operator(): cleaning up before exit... main: exiting due to model loading error\n\nAny hints what I am doing wrong?  \n",
          "score": 0,
          "created_utc": "2026-02-17 09:35:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ugir3",
              "author": "yoracale",
              "text": "Did you use the latest version of llama.cpp?",
              "score": 1,
              "created_utc": "2026-02-17 10:53:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ugunl",
                  "author": "Impossible_Art9151",
                  "text": "Yes - I did, or at least I thought, I did. BUT DUE TO AN UNDERDOSIS OF COFFEE - I failed using the actual version. :-(",
                  "score": 1,
                  "created_utc": "2026-02-17 10:56:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5uya3e",
          "author": "tist20",
          "text": "Does it run on Strix Halo?",
          "score": -1,
          "created_utc": "2026-02-17 13:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vpg7p",
              "author": "yoracale",
              "text": "Yes if you ahve enough ram",
              "score": 1,
              "created_utc": "2026-02-17 15:32:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5w45bk",
              "author": "starkruzr",
              "text": "feel like it's hard for that math to math when the thing has almost 400 parameters and the machine has a max total RAM of 128GB unless you're quantizing the hell out of it",
              "score": 1,
              "created_utc": "2026-02-17 16:45:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5xld5t",
              "author": "doingmarvelous",
              "text": "You could run it on two strix halo machines with 128GB RAM each.  Using rdma network cards you can get good scaling with vllm.\n\nhttps://youtu.be/nnB8a3OHS2E?si=0eyLSi4JjaUv_hja\n\nStill need a quantized version though.",
              "score": 0,
              "created_utc": "2026-02-17 20:57:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r85r5x",
      "title": "You can now train LLMs in VS Code for free via Colab!",
      "subreddit": "unsloth",
      "url": "https://v.redd.it/k5cjdy4ss9kg1",
      "author": "yoracale",
      "created_utc": "2026-02-18 15:19:57",
      "score": 125,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r85r5x/you_can_now_train_llms_in_vs_code_for_free_via/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o62orpj",
          "author": "Reivaj640",
          "text": "Thank you so much, it's super interesting to do with VS Code",
          "score": 6,
          "created_utc": "2026-02-18 16:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66pi40",
              "author": "danielhanchen",
              "text": "Yes VS Code + Colab GPUs are pretty cool! You can also launch multiple Colabs as well!",
              "score": 1,
              "created_utc": "2026-02-19 04:16:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67pgvl",
          "author": "Big-Balance-6426",
          "text": "Thank you for this guide!",
          "score": 2,
          "created_utc": "2026-02-19 09:17:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a2qyk",
          "author": "bad_gambit",
          "text": "Thanks for the guide (and the quants and your amazing finetuning library)! Is google service (BigQuery, Drive, etc) and other quirks (github lfs integration was janky, iirc) finally fixed now? Last time i used the extension it was feeling like its half-baked and quite a hassle without those features",
          "score": 1,
          "created_utc": "2026-02-19 18:00:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pxbu",
      "title": "Qwen3.5 GGUF Evaluation Results",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/0kqfgy0b1mkg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-20 08:30:37",
      "score": 103,
      "num_comments": 14,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r9pxbu/qwen35_gguf_evaluation_results/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6e47uh",
          "author": "LegacyRemaster",
          "text": "I'm using this model every day. I'm curious about Q2 quantization because I'm having fun asking sonnet 4.6 the exact same questions and comparing the answers, and I haven't seen any hallucinations yet. On the contrary. While sonnet was telling me \"the directory is wrong, here's a script to edit the Python file and adjust the paths,\" qwen was telling me \"rename the directory and the problem is solved.\" In this particular case, I smiled because I then asked sonnet: \"But isn't it better to simply rename the directory to fix the references?\" And sonnet: \"Yes, you're right!\"",
          "score": 11,
          "created_utc": "2026-02-20 08:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e23pd",
          "author": "getmevodka",
          "text": "Wait why is a normal margin of error producing better results in a q3 than in a q4?",
          "score": 7,
          "created_utc": "2026-02-20 08:36:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e2kw1",
              "author": "yoracale",
              "text": "Yes, as for statistics, there is always going to be a margin of error. To cancel it out you usually have to run the test like 10 times and get the median/average. Even then, margin of errors still can occur.\n\nOr coincidentally, it could be possible that the Q3 is in fact better than the Q4 but that is unlikely.",
              "score": 13,
              "created_utc": "2026-02-20 08:40:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ez7sf",
                  "author": "simracerman",
                  "text": "There‚Äôs a small catch her not explaining the behavior.\n\nI believe it‚Äôs the evaluation. Different prompts will reveal more cracks in the Q3 one.\n",
                  "score": 1,
                  "created_utc": "2026-02-20 13:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ezwmb",
          "author": "ciprianveg",
          "text": "can you please also test the MXFP4_MOE version?",
          "score": 4,
          "created_utc": "2026-02-20 13:11:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6izwdt",
              "author": "yoracale",
              "text": "You'll need to ask Benjamin for this: [https://x.com/bnjmn\\_marie/status/2024533203239788851](https://x.com/bnjmn_marie/status/2024533203239788851)",
              "score": 3,
              "created_utc": "2026-02-21 01:15:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6j8pci",
              "author": "Dry_Mortgage_4646",
              "text": "Please ask Benjamin. I want to know too",
              "score": 2,
              "created_utc": "2026-02-21 02:11:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k79at",
          "author": "alexp702",
          "text": "Q8 on Mac Studio subjectively feels very good - much better than iq4_NL of Qwen Coder 480.",
          "score": 2,
          "created_utc": "2026-02-21 06:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fagru",
          "author": "GCoderDCoder",
          "text": "I had an online debate with someone on this topic. I see the perplexity hits a cliff at q4 where the token accuracy compared to the original decreases dramatically BUT that doesnt mean the model is putting bad tokens, it just means the model is using a different token than the original would have. How do those affects compare to temperature changes? Xcreate shows that these models do indeed have different output but it seems to be that they lose nuance as they compress more which makes perfect sense given the way compression works. I would love to see more standardized testing around this stuff. Model providers wont even compare the same benchmarks at full size let alone quants...",
          "score": 1,
          "created_utc": "2026-02-20 14:09:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fneqj",
              "author": "yoracale",
              "text": "Mmm perplexity is generally not a good measurement for accuracy. Benjamin utilized actual prompts which are much more real world use-case based",
              "score": 2,
              "created_utc": "2026-02-20 15:15:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fq6jn",
                  "author": "GCoderDCoder",
                  "text": "Agreed! Just pointing out seeing the data about token accuracy doesn't necessarily translate 1:1 into model performance but we never get standardized data on things like quantized performance despite the fact that most of us are using quantized versions.",
                  "score": 1,
                  "created_utc": "2026-02-20 15:28:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hiotb",
          "author": "JorG941",
          "text": "Can you share a google notebook to do this?",
          "score": 1,
          "created_utc": "2026-02-20 20:28:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6izw2f",
              "author": "yoracale",
              "text": "You'll need to ask Benjamin for this: [https://x.com/bnjmn\\_marie/status/2024533203239788851](https://x.com/bnjmn_marie/status/2024533203239788851)",
              "score": 0,
              "created_utc": "2026-02-21 01:15:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r908nn",
      "title": "New r/unsloth User Flairs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/s14byfp2lgkg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-19 14:11:58",
      "score": 27,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r908nn/new_runsloth_user_flairs/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o698enw",
          "author": "im_datta0",
          "text": "Why did you pick the sad sloth though LOL",
          "score": 2,
          "created_utc": "2026-02-19 15:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c5lng",
              "author": "yoracale",
              "text": "Because it's my favorite sloth emoji üò≠",
              "score": 1,
              "created_utc": "2026-02-20 00:20:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ae124",
          "author": "danielhanchen",
          "text": "I'm using the Heart Sloth! :)",
          "score": 2,
          "created_utc": "2026-02-19 18:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lk0c3",
          "author": "LegacyRemaster",
          "text": "all I need",
          "score": 1,
          "created_utc": "2026-02-21 13:37:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r993q3",
      "title": "Any good model that can even run on 0.5 GB of RAM (512 MB of RAM)?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r993q3/any_good_model_that_can_even_run_on_05_gb_of_ram/",
      "author": "Ok-Type-7663",
      "created_utc": "2026-02-19 19:40:20",
      "score": 10,
      "num_comments": 15,
      "upvote_ratio": 0.78,
      "text": "I'm testing local AI limits. Also recommend a OS :3 and Hugging Face repo and great quant :D",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r993q3/any_good_model_that_can_even_run_on_05_gb_of_ram/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6aqhcs",
          "author": "RudeboyRudolfo",
          "text": "functiongemma. But it's more for running structured output and tool calling. It's not really a LLM.",
          "score": 11,
          "created_utc": "2026-02-19 19:53:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b3ya2",
              "author": "Apart_Refrigerator27",
              "text": "What are the use cases for functiongemma?\nit is really worth the time?",
              "score": 3,
              "created_utc": "2026-02-19 20:59:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b7j5i",
                  "author": "RudeboyRudolfo",
                  "text": "That's your imagination. There is a lot possible with it. Pretty sure you can get it over ollama. So not much time wasted. I built a small assistant with it, which can open and edit files and search man pages.",
                  "score": 4,
                  "created_utc": "2026-02-19 21:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ave8q",
          "author": "PlayerWell",
          "text": "I think the Gemma 3 270m will work. It's not great, but it can be successful if fine-tuned",
          "score": 7,
          "created_utc": "2026-02-19 20:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bxr6f",
              "author": "--Spaci--",
              "text": "this is the only real correct option tbf qwen3 0.6b is to large with a context window",
              "score": 1,
              "created_utc": "2026-02-19 23:34:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ar209",
          "author": "Jan49_",
          "text": "No good model can run on half a GB of RAM... Lol\n\nThe smallest LLM, that I know of, that can barely form sentences is Qwen3 0.6B. The q2 quant from unsloth is sub 300mb in size. But then you would still need RAM for context and general overhead.\n\nDoes your system only have 512mb RAM? Then the OS would probably take up the whole RAM on its own. Try Linux XFCE or even better no DE at all.",
          "score": 4,
          "created_utc": "2026-02-19 19:55:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bun9h",
              "author": "logos_flux",
              "text": "It's not a system it's a ti-83",
              "score": 1,
              "created_utc": "2026-02-19 23:16:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ayjfh",
          "author": "BenniB99",
          "text": "You could try the 2bit dynamic quants of LFM2-1.2B but that is probably going to be a vegetable",
          "score": 3,
          "created_utc": "2026-02-19 20:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b7wc8",
          "author": "Significant_Fig_7581",
          "text": "Qwen 0.6B",
          "score": 3,
          "created_utc": "2026-02-19 21:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ccw9x",
          "author": "host3000",
          "text": "Qwen2.5-0.5b",
          "score": 3,
          "created_utc": "2026-02-20 01:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b3dnv",
          "author": "DuckyBlender",
          "text": "Gemma 3 270M",
          "score": 2,
          "created_utc": "2026-02-19 20:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6civqc",
          "author": "catplusplusok",
          "text": "BitNet?",
          "score": 2,
          "created_utc": "2026-02-20 01:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gm2b6",
          "author": "GeneralComposer5885",
          "text": "Sentence Bert",
          "score": 1,
          "created_utc": "2026-02-20 17:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p3qhq",
          "author": "Mice_With_Rice",
          "text": "There are lots of good models in that size. The real question is what sort of model are you looking for, and what is the use case.\n\nFor example, KittenTTS is <25MB. But do you want a TTS model? There are good embedding models if you want to vectorize data. If you want a custom model for a specific use you can make your own as well. Lots of options, but just asking for \"good\" is open to a lot of interpretation.",
          "score": 1,
          "created_utc": "2026-02-22 01:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ar1om",
          "author": "supportend",
          "text": "Kitten TTS, 14, 40 and 80M.",
          "score": 1,
          "created_utc": "2026-02-19 19:55:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7dcdm",
      "title": "Best Unsloth model for 12GB RAM + GTX 1050 (3GB VRAM) for inference only?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r7dcdm/best_unsloth_model_for_12gb_ram_gtx_1050_3gb_vram/",
      "author": "Ok-Type-7663",
      "created_utc": "2026-02-17 17:55:24",
      "score": 8,
      "num_comments": 16,
      "upvote_ratio": 0.83,
      "text": "I‚Äôm trying to run a local LLM using Unsloth for inference only (NOT finetuning), and I want the best model my hardware can handle smoothly.\n\n**My specs:**\n\n* RAM: 12GB\n* GPU: GTX 1050 (3GB VRAM)\n* OS: Linux\n* Goal: inference/chat, not training\n* Prefer GGUF or Unsloth-compatible models\n\n**Priorities:**\n\n* Best quality possible within my limits\n* Stable inference (no crashes / OOM)\n* Good reasoning and instruction following\n* Fast enough to be usable\n\n**Questions:**\n\n1. What is the BEST model size I can realistically run? (1B, 3B, 4B, etc)\n2. Which specific Unsloth model do you recommend?\n3. What quant should I use? (Q4\\_K\\_M, Q5\\_K\\_M, etc)\n4. Should I use GPU offloading or pure CPU with my 3GB VRAM?\n\nIf possible, please recommend exact HF model IDs.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r7dcdm/best_unsloth_model_for_12gb_ram_gtx_1050_3gb_vram/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5wktmj",
          "author": "--Spaci--",
          "text": "lfm 1.2b",
          "score": 2,
          "created_utc": "2026-02-17 18:06:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wkx4z",
              "author": "--Spaci--",
              "text": "LiquidAI/LFM2.5-1.2B-Thinking",
              "score": 2,
              "created_utc": "2026-02-17 18:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wlnzw",
                  "author": "Ok-Type-7663",
                  "text": "which quant?",
                  "score": 1,
                  "created_utc": "2026-02-17 18:10:05",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5y5bq3",
                  "author": "ctanna5",
                  "text": "This.",
                  "score": 1,
                  "created_utc": "2026-02-17 22:32:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wqigx",
          "author": "Fresh_Finance9065",
          "text": "You will want models under 1.5B on gpu, or under 3B on cpu.\n\nFor gpu, lfm2.5-1.2b is as good as it gets. You wouldn't really chat with it through, more of 1 shotting before you go oom.\n\nFor cpu, you can try granite 4 micro. It is not very smart for its size, but its also very hard to oom with it. You don't have much space to reason with. Qwen 3VL 4B and Ministral 3B are smarter for the suze but goodluck with oom.\n\nFor gpu, use Q8_K_XL from unsloth or Q8_0. For cpu, use Q6_K_XL from unsloth or any Q6_K imatrix.",
          "score": 2,
          "created_utc": "2026-02-17 18:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wu22l",
              "author": "Crafty_Ball_8285",
              "text": "I had to use iq4 instead of iq6 on a 3b model on Mac because q6 was chugging the full vram",
              "score": 1,
              "created_utc": "2026-02-17 18:48:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xncwb",
                  "author": "Fresh_Finance9065",
                  "text": "I find models this size lose too much instruction following ability and hallucinate more if they are quantized too far, thats why i prefer larger quants than other ppl",
                  "score": 1,
                  "created_utc": "2026-02-17 21:06:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o605a1i",
          "author": "Mabuse046",
          "text": "I'm not exactly following what you're asking, so just to make sure we're on the same page, Unsloth is a python library for working with / training transformers models - the primary format you find on Huggingface. It's not like an app you load the model in. You write python code and inside the code you call unsloth/transformers/trl to load the model and then you can do whatever you want to it with your code. A GGUF is a custom format used by llama.cpp - and llama.cpp comes with the python script to convert a transformers model to a GGUF and another script to Quantize it. An \"Unsloth model\" is just a normal model that the unsloth team tuned up, but it's still in the standard transformers format and then optionally converted into a handful of other proprietary formats like GGUF. So my question is, are you asking for models that you can load using the unsloth library in python, or models that have been tweaked by the unsloth team?",
          "score": 2,
          "created_utc": "2026-02-18 05:25:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ds60r",
              "author": "eagledoto",
              "text": "He's fs asking about the quantized models",
              "score": 1,
              "created_utc": "2026-02-20 07:03:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nvswp",
          "author": "souna06",
          "text": "Had similar VRAM constraints before upgrading. If you want to quickly check what fits, advisor.forwardcompute.ai shows the VRAM breakdown per model at different quantizations. With 3GB you're pretty much limited to sub-2B models on GPU, which tracks with what others said here.",
          "score": 1,
          "created_utc": "2026-02-21 20:55:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7c9ip",
      "title": "Creating Dynamic 2.0 quants",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r7c9ip/creating_dynamic_20_quants/",
      "author": "de4dee",
      "created_utc": "2026-02-17 17:17:46",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "How do I create Unsloth Dynamic 2.0 quants (UD-Q4\\_K\\_XL ...) ?\n\nThanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r7c9ip/creating_dynamic_20_quants/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5xh2l3",
          "author": "yoracale",
          "text": "Utilize llama.cpp, utilize the imatrix files and investigate layers which are quantized or updated to create ones for the certain model yourself.",
          "score": 2,
          "created_utc": "2026-02-17 20:36:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rapr95",
      "title": "Subject: Seeking Validation: Strategy for Multi-LoRA Behavioral Fine-Tuning on Micro-Datasets (50-100 rows)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rapr95/subject_seeking_validation_strategy_for_multilora/",
      "author": "Scouserleemc",
      "created_utc": "2026-02-21 12:18:35",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "\n\nHi Folks,\n\nI am currently building a composite agentic system for my PhD dissertation (a Design-Based Research project). The system is a \"Purposeful Agent\" designed to act as a professional executive coach. It uses a multi-agent RAG architecture with a vLLM backend routing to multiple specialized LoRA adapters (e.g., an `adapter_empathy`, `adapter_scaffolding`, `adapter_planner`) based on the user's real-time emotional state (Valence-Arousal-Dominance).\n\nBecause my research relies on highly authentic, expert-validated facilitation transcripts, my dataset is incredibly constrained. Based on the *LIMA (Less Is More for Alignment)* hypothesis, I am attempting to do purely behavioral/stylistic fine-tuning using extremely small, highly curated datasets‚Äîspecifically **only 50 to 100 rows of data per adapter**.\n\nMy goal is not to teach the model new knowledge, but to teach it a very specific facilitative stance (e.g., asking open-ended questions, mirroring, and strictly avoiding giving direct advice).\n\nGiven the high risk of catastrophic overfitting with such a small dataset, I have developed the following training strategy using Unsloth. I would love your expert feedback on whether this is viable and if there are any Unsloth-specific optimizations I should apply:\n\n**1. Data Structure: Multi-Turn ChatML Threads** Instead of single-turn Q&A pairs, I am formatting my 50-100 rows as multi-turn conversational histories (User -> Assistant -> User -> Assistant) using standard ChatML. The theory is that this will provide enough linguistic density for the attention mechanism to learn the temporal pacing of a coaching intervention (e.g., when to validate vs. when to probe) rather than just acting like a reactive search engine.\n\n**2. Data Composition: \"Hard Negatives\" to counter RLHF** Base instruction models (like Llama-3-8B-Instruct) are heavily biased toward sycophancy and immediate problem-solving due to their RLHF training. To overwrite this urge to give \"helpful advice,\" roughly 20% of my micro-dataset consists of \"hard negative\" interactions, where the user explicitly begs for advice, and the assistant actively deflects and returns agency to the user.\n\n**3. Hyperparameter Adjustments for Micro-Datasets** To prevent the loss curve from instantly crashing to zero and the model simply memorizing the 50 transcripts, I am planning the following hyperparameter constraints:\n\n* **LoRA Rank (r) & Alpha:** Very low rank (r=4 or 8) with Alpha=16 to restrict the adapter's capacity and force generalization over memorization.\n* **Dropout:** Increasing LoRA dropout to `0.05` or `0.10`.\n* **Learning Rate:** Lowering to `2e-5` for a gentler update to the stylistic weights.\n* **Epochs:** Capping at 3 to 4 epochs, utilizing a small holdout set to closely monitor Validation Loss. If validation loss spikes while training loss drops, I will trigger early stopping.\n\n**My Questions:**\n\n1. Given Unsloth's underlying optimizations, is this micro-dataset strategy (50-100 multi-turn rows) mathematically viable for behavioral cloning, or is that simply too little data for the optimizer to find a meaningful gradient?\n2. Are there any specific Unsloth arguments, parameters, or configurations (e.g., specific target modules, gradient accumulation steps, or learning rate schedulers) you would highly recommend when the dataset is this tiny?\n3. Have you seen success with multi-turn ChatML formatting in Unsloth when trying to teach conversational pacing rather than just instruction following?\n\nThank you so much for your time and for building such an incredible tool for the open-source community!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rapr95/subject_seeking_validation_strategy_for_multilora/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6magef",
          "author": "wildyam",
          "text": "I am too dumb to add value but am commenting to show support! Sounds fascinating - good luck!",
          "score": 3,
          "created_utc": "2026-02-21 16:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nappe",
          "author": "Rhinoseri0us",
          "text": "This is really exciting to me and my current focus of work/research. Would you be open to a message/connecting? My focus is partly on training small edge models and your dissertation seems extremely compelling to my line of thinking. Not trying to gas you up just saying üòÜ",
          "score": 2,
          "created_utc": "2026-02-21 19:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mahg2",
          "author": "wildyam",
          "text": "I am too dumb to add value but am commenting to show support! Sounds fascinating - good luck!",
          "score": 1,
          "created_utc": "2026-02-21 16:05:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5b5gu",
      "title": "How can I train a small model to self-correct without encouraging it to deliberately answer wrong at first?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r5b5gu/how_can_i_train_a_small_model_to_selfcorrect/",
      "author": "PlayerWell",
      "created_utc": "2026-02-15 10:13:52",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I want to finetune a small model which is Gemma 3 1b, to do some tasks and learn how to make self correction. I'm training it using conversation-style examples in two formats:\n\n\n\n**Plain task examples:**\n\n`User: Task question`\n\n`Model: Output`\n\n\n\n**Self-correction examples:**\n\n`User: Task question`\n\n`Model: Output`\n\n`User: Please correct the output using these steps. The output is wrong.`\n\n`Model: New Output`\n\n\n\nWill training with these \"self-correction\" dialogues cause the model to intentionally produce wrong initial outputs just to trigger corrections later? If that's a possible failure, how can I avoid it while still teaching reliable self-correction?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r5b5gu/how_can_i_train_a_small_model_to_selfcorrect/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5hk6kl",
          "author": "Responsible-Stock462",
          "text": "You can only change the model with fine tuning. If that works depends on the base knowledge of the model and the task you have.",
          "score": 3,
          "created_utc": "2026-02-15 10:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hpfa8",
          "author": "aaronr_90",
          "text": "Masking‚Ä¶it would be something similar to Unsloth‚Äôs ‚Äútrain on responses only‚Äù method but would need a custom implementation to mask everything before the New Output.",
          "score": 3,
          "created_utc": "2026-02-15 11:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5k7a63",
              "author": "PlayerWell",
              "text": "Yes. Thank you. I'll try to do some custom masking",
              "score": 1,
              "created_utc": "2026-02-15 19:45:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}