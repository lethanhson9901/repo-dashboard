{
  "metadata": {
    "last_updated": "2026-02-11 17:29:57",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 8,
    "total_comments": 24,
    "file_size_bytes": 38601
  },
  "items": [
    {
      "id": "1qwp508",
      "title": "We created a Tool Calling Guide for LLMs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/ttfrqhl48phg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-05 16:00:39",
      "score": 242,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qwp508/we_created_a_tool_calling_guide_for_llms/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3uat10",
          "author": "fat_fun_xox",
          "text": "Kudos to you and team",
          "score": 5,
          "created_utc": "2026-02-06 03:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uznm8",
              "author": "yoracale",
              "text": "Thank you appreciate the support :)",
              "score": 2,
              "created_utc": "2026-02-06 06:44:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3szflf",
          "author": "RMK137",
          "text": "This is awesome. I've been wanting to get into this and build my own custom functions for Nemotron nano and Devstral small 2.",
          "score": 3,
          "created_utc": "2026-02-05 23:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t83s6",
          "author": "paul_tu",
          "text": "Great",
          "score": 2,
          "created_utc": "2026-02-05 23:52:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o495pen",
          "author": "JMowery",
          "text": "Unsloth team... Any idea why GLM 4.7 Flash does not work in Pydantic AI? It is the only model out of like 15 I have tested that fails. Even models at a fraction of its size work fine.\n\nGemini (which I was using to help me diagnose the issue) has proposed that 4.7 Flash refuses to adopt Pydantic AI tool calling methods/structure. But it is just strange that it is the only model that suffers from that.",
          "score": 1,
          "created_utc": "2026-02-08 14:10:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r10d9l",
      "title": "GLM-4.7-Flash is now the #1 most downloaded model on Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/n9wlycyu2oig1.png",
      "author": "yoracale",
      "created_utc": "2026-02-10 13:12:55",
      "score": 233,
      "num_comments": 38,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r10d9l/glm47flash_is_now_the_1_most_downloaded_model_on/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4m748w",
          "author": "RedKnightRG",
          "text": "*Raises a toast to everyone who bought 128GB of RAM and dual 3090s or similar thousands of dollars ago*",
          "score": 14,
          "created_utc": "2026-02-10 14:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7sdz",
              "author": "yoracale",
              "text": "You don't need that much to run the 30B model. 24GB is enough to get it working well with quality",
              "score": 7,
              "created_utc": "2026-02-10 14:09:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m8f6l",
                  "author": "RedKnightRG",
                  "text": "Oh my B I saw GLM but read Qwen and thought this was the 80B model.  You're right 1 3090 is fine.   So I'll raise a toast to the guys who bought their single 3090 hundreds of dollars ago!  üòÖ",
                  "score": 2,
                  "created_utc": "2026-02-10 14:13:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ohfii",
              "author": "timbo2m",
              "text": "So mad I didn't stack my machine with 256GB ram when I bought it a couple of years ago and it was like a couple of hundred bucks for 32GB. 20/20 hindsight!",
              "score": 2,
              "created_utc": "2026-02-10 20:35:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz42d",
          "author": "Far-Donut-1177",
          "text": "I envy folks with the hardware that could run 100B above. The best I could do is 30 and that's stretching it.",
          "score": 10,
          "created_utc": "2026-02-10 13:21:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m0j7v",
              "author": "Significant_Fig_7581",
              "text": "Heyyy let's just be happy cause this post just proved that most of us are and these download numbers are surely going to be an encouragement for most of them to release good models at that size, Qwen is releasing a 35B MOE btw...",
              "score": 8,
              "created_utc": "2026-02-10 13:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4m3n6z",
                  "author": "ismaelgokufox",
                  "text": "Indeed. Being able to run this at good speeds in 16GB VRAM is great!",
                  "score": 3,
                  "created_utc": "2026-02-10 13:47:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mujj1",
                  "author": "ScoreUnique",
                  "text": "35B MoE sounds like a dream after seeing qwen 3 next coder.",
                  "score": 3,
                  "created_utc": "2026-02-10 16:04:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m5rwq",
          "author": "MrMrsPotts",
          "text": "I need a version I can squeeze into 12GB of VRAM.",
          "score": 8,
          "created_utc": "2026-02-10 13:58:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ming1",
              "author": "eesnimi",
              "text": "Unload the expert layers to CPU and it will be a nice squeeze :) I have squeezed a Q6\\_K\\_XL quant with 130k token window to 11GB VRAM with all the expert layers unloaded to CPU, getting around 11-13t/s that's quite usable.",
              "score": 6,
              "created_utc": "2026-02-10 15:06:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mqv58",
                  "author": "MrMrsPotts",
                  "text": "Is there something I can read about how to do that?",
                  "score": 2,
                  "created_utc": "2026-02-10 15:46:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4o7yqv",
                  "author": "ethereal_intellect",
                  "text": "Commenting so I'll remember to try this later. I'm on lm studio out of convenience, which app are you on",
                  "score": 1,
                  "created_utc": "2026-02-10 19:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4thuda",
                  "author": "Confident-Ad-2688",
                  "text": "Can you share the command with all parameters I ,also have 12gb vram but not able to run a good speed .",
                  "score": 1,
                  "created_utc": "2026-02-11 16:04:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4m8sty",
              "author": "NorthEastCalifornia",
              "text": "glm-4.7-flash REAP 50%",
              "score": 1,
              "created_utc": "2026-02-10 14:15:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ma897",
                  "author": "paq85",
                  "text": "I've tried the REAP version and couldn't make it work with open code at all... As if it end in some loop, even though I run it with same llamacpp params as the full version.",
                  "score": 3,
                  "created_utc": "2026-02-10 14:22:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m9zsu",
          "author": "paq85",
          "text": "This model is definitely the best 30b agentic coding model I've seen so far.",
          "score": 7,
          "created_utc": "2026-02-10 14:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mml3u",
          "author": "alfpacino2020",
          "text": "[https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) mucho mejor este mas lento pero   no falla como loco   no se tilda como gml 4.7",
          "score": 3,
          "created_utc": "2026-02-10 15:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mr3bb",
              "author": "MrMrsPotts",
              "text": "I would need the 2 bit quantisation though which might not be good.",
              "score": 1,
              "created_utc": "2026-02-10 15:47:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4msb5x",
                  "author": "alfpacino2020",
                  "text": "https://preview.redd.it/k4kc0it8voig1.png?width=974&format=png&auto=webp&s=5c2f0a31e47adbbb79d2bae65ac01c10de4a04be\n\nejemplo en iq4 me da casi  23 tokens x seg el qwen me da 15 x seg osea no gran diferencia usando Qwen3-Coder-Next-MXFP4\\_MOE.gguf",
                  "score": 1,
                  "created_utc": "2026-02-10 15:53:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4muk1k",
                  "author": "alfpacino2020",
                  "text": "tengo 5070 16gb vram 48ram  aca el qwen \n\nhttps://preview.redd.it/916th7phxoig1.png?width=1024&format=png&auto=webp&s=78fb7ae7e7bc1dd6ca8581429839edb61bc221e0\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 16:04:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4q1vqh",
          "author": "PassageInfamous3639",
          "text": "Everyone‚Äôs chasing 100B+ like it‚Äôs the promised land and I‚Äôm over here just wanting a 7‚Äì30B model that doesn‚Äôt hallucinate my whole repo üò≠\n\nIf GLM-4.7-Flash is actually dependable, that‚Äôs a huge W.",
          "score": 3,
          "created_utc": "2026-02-11 01:31:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qyfsm",
              "author": "yoracale",
              "text": "Very soon there will be more!",
              "score": 1,
              "created_utc": "2026-02-11 05:00:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nk2o0",
          "author": "Megalion75",
          "text": "What's the best model for local SWE?  Or is there a model that can run locally that is reliable?",
          "score": 1,
          "created_utc": "2026-02-10 18:01:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r13pk4",
      "title": "Faster MoE LLM Training now in Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/qfoq8mirjoig1.png",
      "author": "yoracale",
      "created_utc": "2026-02-10 15:25:44",
      "score": 115,
      "num_comments": 17,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r13pk4/faster_moe_llm_training_now_in_unsloth/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4mp3bh",
          "author": "joninco",
          "text": "Multi gpu?",
          "score": 5,
          "created_utc": "2026-02-10 15:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mqz3p",
              "author": "yoracale",
              "text": "Works already but still preliminary. It's out soon.\n\nGuide for now: [https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth](https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth)",
              "score": 5,
              "created_utc": "2026-02-10 15:47:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4muop1",
          "author": "arman-d0e",
          "text": "Transformers v5 fully optimized? Do the optimizations apply to Qwen3-Coder-Next?",
          "score": 5,
          "created_utc": "2026-02-10 16:04:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mvode",
              "author": "danielhanchen",
              "text": "Not yet - we plan to add support for Qwen3-Next next!",
              "score": 11,
              "created_utc": "2026-02-10 16:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mwszz",
                  "author": "arman-d0e",
                  "text": "Much appreciated, thanks for the updates.",
                  "score": 3,
                  "created_utc": "2026-02-10 16:14:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4qyfj9",
                  "author": "Desperate-Sir-5088",
                  "text": "Would you make 4bit-bnb quant for the Qwen3-Coder-Next model? I expected QLoRA finetuing of this model with only one H100 80GB or RTX PRO 6000 96GB",
                  "score": 1,
                  "created_utc": "2026-02-11 05:00:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n0rp8",
          "author": "sterby92",
          "text": "Will we also get vulkan or rocm support at some point? ",
          "score": 3,
          "created_utc": "2026-02-10 16:32:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n5cn4",
              "author": "yoracale",
              "text": "Yes, we're announcing very soon. As soon as this month. It already works: https://unsloth.ai/docs/get-started/install/amd",
              "score": 5,
              "created_utc": "2026-02-10 16:53:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nobiz",
                  "author": "sterby92",
                  "text": "Amazing, thank you üôå",
                  "score": 1,
                  "created_utc": "2026-02-10 18:21:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qmmrc",
          "author": "Old-Nobody-2010",
          "text": "What is the minimum VRAM required to fine-tune GLM-4.7-Flash with Unsloth      30b a3b model",
          "score": 2,
          "created_utc": "2026-02-11 03:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qs17q",
              "author": "yoracale",
              "text": "I think around 20GB VRAM",
              "score": 1,
              "created_utc": "2026-02-11 04:14:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sv1cm",
          "author": "Mac_NCheez_TW",
          "text": "What did we do to deserve this work üò≠. You folks are like world heros to poor ram serfs, the Robin hood to us low end peasents stealing from the Nvidia's tax demands and giving us another couple years on our garbage rigs.¬†",
          "score": 1,
          "created_utc": "2026-02-11 14:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pj0od",
          "author": "codeblockzz",
          "text": "Train or fine tune?",
          "score": 0,
          "created_utc": "2026-02-10 23:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ptl9r",
              "author": "yoracale",
              "text": "Both, works for prettaining, FFT and Lora",
              "score": 5,
              "created_utc": "2026-02-11 00:42:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4q3e6p",
          "author": "PassageInfamous3639",
          "text": "12x faster + less VRAM is actually insane. This is exactly the kind of ‚Äúmake local practical‚Äù progress I love seeing.\n\nAlso shoutout to the RTX 3090 mention ‚Äî my GPU just felt seen üòÇ",
          "score": 0,
          "created_utc": "2026-02-11 01:40:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvvks9",
      "title": "TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF ¬∑ Hugging Face",
      "subreddit": "unsloth",
      "url": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF",
      "author": "No-Intention-5521",
      "created_utc": "2026-02-04 17:54:22",
      "score": 47,
      "num_comments": 19,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qvvks9/teichaiglm47flashclaudeopus45highreasoningdistillg/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o3kkfeq",
          "author": "KvAk_AKPlaysYT",
          "text": "Just 250 examples enough for a valuable distillation? Where are the benches?",
          "score": 11,
          "created_utc": "2026-02-04 18:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lh2i4",
              "author": "Zyguard7777777",
              "text": "Thinking the same, would be nice to have even a small bench to compare",
              "score": 4,
              "created_utc": "2026-02-04 20:38:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mlias",
                  "author": "arman-d0e",
                  "text": "I was thinking HumanEval and maybe LiveCodeBench? Or would you suggest others",
                  "score": 1,
                  "created_utc": "2026-02-05 00:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ml4hv",
              "author": "arman-d0e",
              "text": "Our goal with this model (as with all our others) has been to distill CoT not knowledge. 250 is still small don‚Äôt get me wrong, however it‚Äôs surprisingly proven to still successfully teach the student model to ‚Äúthink like‚Äù the teacher (given the diverse types of prompts). I‚Äôm currently looking at rentable gpus to do some benchmarks. Feel free to suggest what benchmarks you‚Äôd like to see for this model.\n\nI wouldn‚Äôt have much expectation though as the model could use some form of RL to better adapt to it‚Äôs new CoT",
              "score": 3,
              "created_utc": "2026-02-05 00:01:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sivsf",
                  "author": "Frequent-Mud8705",
                  "text": "have you tried training on a bunch of claude code logs? I have all of my history with the tool saved, do you think it would make it noticeably better?",
                  "score": 1,
                  "created_utc": "2026-02-05 21:40:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3lnl5z",
              "author": "ethereal_intellect",
              "text": "Afaik the whole idea was to lessen the abysmally long thinking, opus thinks 50x less. But yeah benches would be nice to see for a lot of these :/",
              "score": 2,
              "created_utc": "2026-02-04 21:09:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3md9kz",
                  "author": "ClimateBoss",
                  "text": "or try it can it even code bruh ?",
                  "score": 1,
                  "created_utc": "2026-02-04 23:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mw0be",
              "author": "CoruNethronX",
              "text": "Check this: [Less Is More for Reasoning](https://arxiv.org/abs/2502.03387)",
              "score": 2,
              "created_utc": "2026-02-05 01:01:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o4z9d",
                  "author": "Thrumpwart",
                  "text": "Then check this: [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)",
                  "score": 3,
                  "created_utc": "2026-02-05 05:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o46e3bw",
              "author": "[deleted]",
              "text": "The guy was a rookie... it's not a real distill. Just some amature listing crap on HF. \n\n100,000 minimum for a distill. Get this clown out of there. ",
              "score": 1,
              "created_utc": "2026-02-08 01:11:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mypb8",
          "author": "CoruNethronX",
          "text": "Checked the actual dataset and looks, like it's 90% filled with the same question \"what is requerments\", rephrased in different ways. But also includes more practical questions, like \"how to subtract 47 from 89\". Not sure that it's really useful for programming and especially for agentic skills, that are strongest sides of glm 4.7 flash in comparison to other open models of the similar size.",
          "score": 7,
          "created_utc": "2026-02-05 01:17:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nsvrg",
          "author": "Successful_Bit7710",
          "text": "Anyone use and perform benchmarks?",
          "score": 1,
          "created_utc": "2026-02-05 04:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oz7pu",
              "author": "zoyer2",
              "text": "Tested it, it seems promising, does things well but always messes up something. I'm not sure how it's trained etc but this looks very similar to the first version of glm 4.7 flash from unsloth which made silly mistakes",
              "score": 3,
              "created_utc": "2026-02-05 10:17:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ofk9b",
              "author": "kironlau",
              "text": "worst than original one, tested MXFP4 quant, a few days ago.",
              "score": 2,
              "created_utc": "2026-02-05 07:09:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3oalfu",
              "author": "TokenRingAI",
              "text": "Not needed, falls apart under basic testing.",
              "score": 1,
              "created_utc": "2026-02-05 06:26:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qx1ye4",
      "title": "koute/GLM-4.7-Flash-Derestricted",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qx1ye4/kouteglm47flashderestricted/",
      "author": "Witty_Mycologist_995",
      "created_utc": "2026-02-05 23:56:38",
      "score": 28,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "https://huggingface.co/koute/GLM-4.7-Flash-Derestricted",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qx1ye4/kouteglm47flashderestricted/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qw0ch3",
      "title": "FSDP with Unsloth",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qw0ch3/fsdp_with_unsloth/",
      "author": "Potential_Nerve_4381",
      "created_utc": "2026-02-04 20:43:51",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I'm trying to load Qwen3-30B-A3B model on my g5.12xlarge GPU. I need to shard the model as it doesn't fit in one GPU. Does anyone have an example working script that runs FSDP with Unsloth and Hugging face Trainer? I can't seem to find one anywhere ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qw0ch3/fsdp_with_unsloth/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o3ot2ct",
          "author": "rjtannous",
          "text": "unsloth optimized FSDP is coming soon , however FSDP should still work if you disable torch compile by setting os.environ\\[\"TORCH\\_COMPILE\\_DISABLE\"\\] = \"1\"",
          "score": 1,
          "created_utc": "2026-02-05 09:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xl44b",
              "author": "Potential_Nerve_4381",
              "text": "I tried. I couldn't make it work. It'd be great if you share a working script¬†",
              "score": 1,
              "created_utc": "2026-02-06 17:10:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45pp0f",
          "author": "Mac_NCheez_TW",
          "text": "Call me ignorant and put lipnstick on me, but what is FSDP? Nvm I'll find it and learn. I'm running multigpu setup am I missing something? I'm sure I am I never research enough. brb.¬†",
          "score": 1,
          "created_utc": "2026-02-07 22:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45q2bt",
              "author": "Mac_NCheez_TW",
              "text": "Okay it's for training, I haven't even started down that path I just built my EPYC system after struggling to afford ram.¬†",
              "score": 2,
              "created_utc": "2026-02-07 22:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyopks",
      "title": "When replacing embed_tokens and lm_head with those from another model, is this implementation correct?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qyopks/when_replacing_embed_tokens_and_lm_head_with/",
      "author": "choco132134",
      "created_utc": "2026-02-07 20:38:05",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "In the KnitLM paper ([https://openreview.net/forum?id=2uctT30vTS](https://openreview.net/forum?id=2uctT30vTS)), they train a LoRA adapter on a **base** model and then merge/apply that adapter onto an **instruct** model. To keep the two models consistent, they replace the base model‚Äôs token embeddings (and also the LM head if it is not tied to the embeddings) with those from the instruct model.\n\nI‚Äôm trying to implement this with **Qwen3-8B**, and I‚Äôd like to ask whether the implementation below looks correct. I ran this on **Google Colab with an A100**. When I tried the same thing on an **L4**, I ran into OOM-related issues and ended up getting **meta tensors**, so it didn‚Äôt work properly.\n\nAlso, as far as I understand, **Qwen3-8B uses** `tie_word_embeddings = False`, so the input embeddings and `lm_head` are *not* tied, which is why I‚Äôm copying both.\n\n`%%capture`\n\n`import os, re`\n\n`if \"COLAB_\" not in \"\".join(os.environ.keys()):`\n\n`!pip install unsloth`\n\n`else:`\n\n`# Do this only in Colab notebooks! Otherwise use pip install unsloth`\n\n`import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)`\n\n`xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")`\n\n`!pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo`\n\n`!pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer`\n\n`!pip install --no-deps unsloth`\n\n`!pip install transformers==4.56.2`\n\n`!pip install --no-deps trl==0.22.2`\n\n`# =============================================================================`\n\n`# Hyperparameter configuration`\n\n`# =============================================================================`\n\n`LORA_R = 16`\n\n`LORA_ALPHA = 16`\n\n`PER_DEVICE_TRAIN_BATCH_SIZE = 16`\n\n`GRADIENT_ACCUMULATION_STEPS = 1`\n\n`PACKING = True`\n\n`NUM_TRAIN_EPOCHS = 1`\n\n`LEARNING_RATE = 2e-4`\n\n`MAX_SEQ_LENGTH = 2048`\n\n`# Model configuration`\n\n`BASE_MODEL = \"unsloth/Qwen3-8B-Base\"`\n\n`INSTRUCT_MODEL = \"unsloth/Qwen3-8B\"`\n\n`USE_INSTRUCT_EMBEDDINGS = True`\n\n`from unsloth import FastLanguageModel`\n\n`import torch`\n\n`# 1. Load the Base LLM`\n\n`print(\"[1/4] Loading Base LLM (backbone)...\")`\n\n`base_model, base_tokenizer = FastLanguageModel.from_pretrained(`\n\n`model_name = BASE_MODEL,`\n\n`max_seq_length = MAX_SEQ_LENGTH,`\n\n`load_in_4bit = False,`\n\n`)`\n\n`# 2. Load the Instruct LLM`\n\n`print(\"[2/4] Loading Instruct LLM (for embeddings)...\")`\n\n`instruct_model, instruct_tokenizer = FastLanguageModel.from_pretrained(`\n\n`model_name = INSTRUCT_MODEL,`\n\n`max_seq_length = MAX_SEQ_LENGTH,`\n\n`load_in_4bit = False,`\n\n`)`\n\n`def _is_meta(t: torch.Tensor) -> bool:`\n\n`return hasattr(t, \"device\") and t.device.type == \"meta\"`\n\n`def copy_qwen_embed_and_lm_head_exact(base_model, instruct_model, *, verbose: bool = True):`\n\n`\"\"\"`\n\n`Assumptions:`\n\n`- The Base and Instruct models have identical vocab_size / hidden_size (exact match).`\n\n`- For Qwen-style models where embeddings are NOT tied, copy both \\embed_tokens\\` and \\`lm_head\\`.\\``\n\n`What it does:`\n\n`- Prints the parameter shapes.`\n\n`- Copies weights in-place under torch.no_grad() (does NOT use .data).`\n\n`\"\"\"`\n\n`base_in = base_model.get_input_embeddings() # nn.Embedding`\n\n`inst_in = instruct_model.get_input_embeddings()`\n\n`base_out = base_model.get_output_embeddings() # nn.Linear (lm_head)`\n\n`inst_out = instruct_model.get_output_embeddings()`\n\n`if base_in is None or inst_in is None:`\n\n`raise ValueError(\"get_input_embeddings() returned None. Please check the model implementation.\")`\n\n`if base_out is None or inst_out is None:`\n\n`raise ValueError(\"get_output_embeddings() returned None. Please make sure this is a CausalLM.\")`\n\n`# Meta guard (prevents copying from tensors with no real storage)`\n\n`if _is_meta(inst_in.weight) or _is_meta(inst_out.weight):`\n\n`raise RuntimeError(\"instruct_model weights are on the 'meta' device (likely not fully loaded yet).\")`\n\n`# Get shapes`\n\n`base_in_shape = tuple(base_in.weight.shape)`\n\n`inst_in_shape = tuple(inst_in.weight.shape)`\n\n`base_out_shape = tuple(base_out.weight.shape)`\n\n`inst_out_shape = tuple(inst_out.weight.shape)`\n\n`# Print shapes`\n\n`if verbose:`\n\n`print(\"[Shapes]\")`\n\n`print(f\" base input_embeddings : {base_in_shape}\")`\n\n`print(f\" inst input_embeddings : {inst_in_shape}\")`\n\n`print(f\" base lm_head : {base_out_shape}\")`\n\n`print(f\" inst lm_head : {inst_out_shape}\")`\n\n`# Enforce exact match`\n\n`if base_in_shape != inst_in_shape:`\n\n`raise ValueError(f\"Input embedding shape mismatch: base={base_in_shape}, inst={inst_in_shape}\")`\n\n`if base_out_shape != inst_out_shape:`\n\n`raise ValueError(f\"LM head shape mismatch: base={base_out_shape}, inst={inst_out_shape}\")`\n\n`# Copy weights`\n\n`with torch.no_grad():`\n\n`base_in.weight.copy_(inst_in.weight)`\n\n`base_out.weight.copy_(inst_out.weight)`\n\n`if verbose:`\n\n`print(\"‚úì Copied input_embeddings and lm_head weights (exact match).\")`\n\n`return base_model`\n\n`copy_qwen_embed_and_lm_head_exact(base_model, instruct_model, verbose=True)`\n\n`# KnitLM-style assumption: use the Instruct tokenizer`\n\n`tokenizer = instruct_tokenizer`\n\n`print(f\"[Tokenizer] using instruct tokenizer. len(tokenizer)={len(tokenizer)}, vocab_size={tokenizer.vocab_size}\")`\n\n`# Safety check: ensure tokenizer IDs fit within the embedding matrix`\n\n`print(\"max token id (instruct tokenizer):\", max(instruct_tokenizer.get_vocab().values()))`\n\n`print(\"embedding rows:\", base_model.get_input_embeddings().weight.shape[0])`\n\nOutput:  \n\\[Shapes\\]\n\nbase  input\\_embeddings : (151936, 4096)\n\ninst  input\\_embeddings : (151936, 4096)\n\nbase  lm\\_head          : (151936, 4096)\n\ninst  lm\\_head          : (151936, 4096)\n\n‚úì Copied input\\_embeddings and lm\\_head weights (exact match).\n\n\\[Tokenizer\\] using instruct tokenizer. len(tokenizer)=151669, vocab\\_size=151643\n\nmax token id (instruct tokenizer): 151668\n\nembedding rows: 151936\n\nIf you think anything is missing, please let me know.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qyopks/when_replacing_embed_tokens_and_lm_head_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r116zw",
      "title": "Finetuning query for gpt-oss 20b model",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r116zw/finetuning_query_for_gptoss_20b_model/",
      "author": "Double_Tourist3600",
      "created_utc": "2026-02-10 13:47:55",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "We are facing a **thinking-loop issue** after fine-tuning a reasoning-enabled model and would appreciate guidance.\n\n**Setup**\n\n* Created a custom medical dataset and prepared it using the OpenAI Harmony format\n* Fine-tuned using Unsloth (analysis samples included)\n* Converted to GGUF via `llama.cpp`, quantized to **Q4\\_K\\_M**, and deployed with Ollama\n* For short/simple prompts, outputs are correct; however, as conversation context grows, the model remains in continuous reasoning (‚Äúthinking‚Äù) and does not produce the final response\n\n**Questions**\n\n1. What are the common causes of this behavior (chat template mismatch, stop-token issues, reasoning token configuration, RLHF override during SFT, etc.)?\n2. What checks or precautions should be taken during fine-tuning, GGUF conversion, quantization, and Ollama model file setup to prevent reasoning loops?\n3. Are there recommended template or stop-sequence configurations specifically for reasoning-enabled models to ensure the model exits the thinking phase properly?\n\nAny debugging checklist or best practices would be very helpful.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r116zw/finetuning_query_for_gptoss_20b_model/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o4mspu4",
          "author": "arman-d0e",
          "text": "Train on assistant only loss, include stop tokens (training with harmony has been weird for me, but this helped a lot) but most of all just try lowering temperature with inference you‚Äôll see better results",
          "score": 2,
          "created_utc": "2026-02-10 15:55:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r0dr3",
              "author": "Double_Tourist3600",
              "text": "    trainer = train_on_responses_only(\n            trainer,\n            instruction_part=\"<|start|>user<|message|>\",\n            response_part=\"<|start|>assistant\",\n            force_match=True,\n        )\n    I used this to apply loss on assistant messages/channels, Included stop tokens in each examples as <|end|> for channel and EOS = <|return|>\n    Lowering the temperature works for short queries but as context grows it start repeating the thinking..\n    This are the above measures that I have taken please let me know if any inputs are there..",
              "score": 1,
              "created_utc": "2026-02-11 05:15:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}