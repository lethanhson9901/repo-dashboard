{
  "metadata": {
    "last_updated": "2026-01-04 04:31:53",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 10,
    "total_comments": 49,
    "file_size_bytes": 70262
  },
  "items": [
    {
      "id": "1pzjisa",
      "title": "Unsloth just hit 50,000 GitHub stars! ‚≠êü¶•",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/80o4jxiipcag1.png",
      "author": "yoracale",
      "created_utc": "2025-12-30 14:29:32",
      "score": 172,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pzjisa/unsloth_just_hit_50000_github_stars/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwqkrse",
          "author": "Educational_Rent1059",
          "text": "Thanks for all the contributions to the OSS community this year!! Team unsloth <3",
          "score": 7,
          "created_utc": "2025-12-30 14:50:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqlz97",
              "author": "yoracale",
              "text": "Thank you for the constant support! \\^\\^",
              "score": 2,
              "created_utc": "2025-12-30 14:56:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqx43l",
          "author": "WarmAd6505",
          "text": "Amazing project! Deserves so much support. Helped me so much.",
          "score": 2,
          "created_utc": "2025-12-30 15:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx4xg",
              "author": "yoracale",
              "text": "Thank you for using us and the support! Glad the package helped you! üôè",
              "score": 1,
              "created_utc": "2025-12-30 21:30:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwrkn2r",
          "author": "Arindam_200",
          "text": "Congratulationsüî•",
          "score": 2,
          "created_utc": "2025-12-30 17:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx68p",
              "author": "yoracale",
              "text": "Thank you! üôèüòä",
              "score": 2,
              "created_utc": "2025-12-30 21:30:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwrx43h",
          "author": "NoPresentation7366",
          "text": "Fully deserved! thank you brothers for your works and dedication üòéüíï",
          "score": 2,
          "created_utc": "2025-12-30 18:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx869",
              "author": "yoracale",
              "text": "Thanks so much for the support! ü•∞üôè",
              "score": 2,
              "created_utc": "2025-12-30 21:30:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nws1y4h",
          "author": "LegacyRemaster",
          "text": "Well deserved",
          "score": 2,
          "created_utc": "2025-12-30 19:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx282",
              "author": "yoracale",
              "text": "Thank you! üôèü•∞",
              "score": 1,
              "created_utc": "2025-12-30 21:29:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxqp4q",
      "title": "All GLM 4.7, GLM 4.6 and GLM 4.6V-Flash GGUFs are now updated!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pxqp4q/all_glm_47_glm_46_and_glm_46vflash_ggufs_are_now/",
      "author": "yoracale",
      "created_utc": "2025-12-28 12:54:48",
      "score": 124,
      "num_comments": 20,
      "upvote_ratio": 1.0,
      "text": "Hey guys, we did a refresh of quants (quality of life updates) for GLM 4.5, 4.6, 4.6V-Flash and 4.7\n\nllama.cpp and other inference engines like LM Studio now support more features including but not limited to:\n\n1. Non ascii decoding for tools (affects non English languages) For eg before the default (ensure\\_ascii=True) would cause \"caf√©\" ‚Üí \"caf\\\\u00e9\", whilst now ensure\\_ascii=False would tokenize \"caf√©\" ‚Üí \"caf√©\". I would re-download our quants if you use languages other than English.\n2. Converts reasoning content parsing to original \\[0\\], \\[-1\\] from our changes of |first and |last. We used to change \\[0\\] to |first and \\[-1\\] to |last so we be compatible with LM Studio and llama-cli. With the upgrade of llama-cli to use llama-server, we can revert this. llama-server also didn't like |first, so we fixed it as well.\n3. Many of you reported Chinese thinking with the GLM-4.6V-Flash GGUFs. After investigating, we confirmed the same behavior appears in all uploads regardless of uploader (e.g., LM Studio and bartowski). LM Studio‚Äôs Q8\\_0, bartowski‚Äôs BF16, and our BF16 all produce Chinese ‚Äúthinking,‚Äù so this is just the way Z . ai intended for the model and is not unique to our uploads. [See our investigation here.](https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/discussions/4#694bdbac91a48021d8b210a1)\n\nAlso other changes:\n\n1. **Added lot of tool calls in our calibration dataset - makes tool calling better, especially for smaller quants.**\n2. **A bit more calibration data for GLM models., adding a teeny tiny bit more accuracy overall.**\n\n**This does mean you need to re-download them to use the latest changes**\n\nGGUFs which received Quality of Life updates:\n\n* [https://huggingface.co/unsloth/GLM-4.6V-GGUF](https://huggingface.co/unsloth/GLM-4.6V-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.7-GGUF](https://huggingface.co/unsloth/GLM-4.7-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.6-GGUF](https://huggingface.co/unsloth/GLM-4.6-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.5-GGUF](https://huggingface.co/unsloth/GLM-4.5-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.5-Air-GGUF](https://huggingface.co/unsloth/GLM-4.5-Air-GGUF)\n\nOur guides are all in our docs or model cards: [https://unsloth.ai/docs/models/glm-4.7](https://unsloth.ai/docs/models/glm-4.7)\n\nThanks so much guys! :)",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1pxqp4q/all_glm_47_glm_46_and_glm_46vflash_ggufs_are_now/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwe2k2y",
          "author": "kiwibonga",
          "text": "\"Sorry guys, it just wants to think in Chinese, but it understands everything you say.\"\n\nI have that exact problem with my bilingual toddler",
          "score": 4,
          "created_utc": "2025-12-28 17:08:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxs6q",
          "author": "keypa_",
          "text": "Thanks a lot, being a french it's been a nightmare trying to understand why all accents are not being properly displayed !",
          "score": 3,
          "created_utc": "2025-12-28 16:44:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfmsm1",
          "author": "elsung",
          "text": "awesome! now i just need to figure out /wait for conversion of these into MLX =)",
          "score": 3,
          "created_utc": "2025-12-28 21:38:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdyahx",
          "author": "Magnus114",
          "text": "Should we still specify the jinja from this pr?\n\nhttps://github.com/ggml-org/llama.cpp/pull/16932",
          "score": 2,
          "created_utc": "2025-12-28 16:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjjpy0",
              "author": "yoracale",
              "text": "Our current jinja chat template should work fine as is",
              "score": 1,
              "created_utc": "2025-12-29 13:38:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwegwto",
          "author": "Sensitive_Sweet_1850",
          "text": "nice",
          "score": 2,
          "created_utc": "2025-12-28 18:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nweqpdi",
          "author": "LegacyRemaster",
          "text": "thx sir!",
          "score": 2,
          "created_utc": "2025-12-28 19:03:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgq8gj",
          "author": "RedditUsr2",
          "text": "Is unsloth/GLM-4.6V-Flash-GGUF usable with cline?",
          "score": 2,
          "created_utc": "2025-12-29 01:05:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhhpy4",
              "author": "yoracale",
              "text": "It should be yes",
              "score": 2,
              "created_utc": "2025-12-29 03:44:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwd6kjn",
          "author": "Brave-Hold-9389",
          "text": "Why not glm 4.6v?",
          "score": 4,
          "created_utc": "2025-12-28 14:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe8kv6",
              "author": "molbal",
              "text": "https://huggingface.co/unsloth/GLM-4.6V-GGUF",
              "score": 7,
              "created_utc": "2025-12-28 17:38:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwgqgky",
              "author": "yoracale",
              "text": "Glm4.6v is also updated",
              "score": 3,
              "created_utc": "2025-12-29 01:06:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfobor",
          "author": "gofiend",
          "text": "Thank you for your amazing work! \n\nOne request - I know that when you do your quantization you run some sort of quick validation test to ensure you got a functional quant. Could you please share those scores (whatever it is KL divergence, perplexity - anything) consistantly when you release a set of quants?\n\nI'm constantly trying to get a feel for what tradeoff to make around the Q3-Q6 range (64GB VRAM) and it would be amazing to have some sort of signal. I fully understand that \"real usability\" will be different and non-linearly related to the scores you report. \n\n(I also suspect, long term, across multiple model families, we'll be able to model \"real usibility\" differences starting from the scores + the model architecture).",
          "score": 1,
          "created_utc": "2025-12-28 21:46:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhi41k",
              "author": "yoracale",
              "text": "To be honest it is a lot of work to do it for every quant. In general we recommend using at least 2-bit for large quants and 3-bit for medium sized and 4-bit for small sized.\n\nWe did do Aider benchmarks for example DeepSeek-V3.1: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)\n\nAider is one of the top 3 benchmarks for real use-case benchmarks",
              "score": 2,
              "created_utc": "2025-12-29 03:47:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwhp74a",
                  "author": "gofiend",
                  "text": "Is there some limited version (say only K\\_M) that is possible?\n\nAlso I‚Äôm definitely not asking for accurate benchmarking (like the terrific Aider effort). I‚Äôve found that fairly small bits of text converge on perplexity etc. pretty quickly. I‚Äôm thinking of whatever 60-120 seconds of compute per under 96GB quant can provide in terms of signal. Basically automate a quick perplexity / KL estimate and auto publish. Even better if it is whatever sense check you use to make sure the quant process didn‚Äôt fail.",
                  "score": 1,
                  "created_utc": "2025-12-29 04:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwiusop",
                  "author": "thedarkbobo",
                  "text": "Awesome, imagine we had a table for filter model scores i.e. coding then sort by size with different quants.",
                  "score": 1,
                  "created_utc": "2025-12-29 10:22:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhxz0h",
          "author": "Aggressive-Dingo-993",
          "text": "Wait, is this why I get dramatically improved performance when I re-tested GLM 4.5 air yesterday????",
          "score": 1,
          "created_utc": "2025-12-29 05:31:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwicypz",
              "author": "yoracale",
              "text": "Yes could definitely be possible",
              "score": 1,
              "created_utc": "2025-12-29 07:36:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxf4q10",
          "author": "No_Art8851",
          "text": "glm-4.6v-flash/GLM-4.6V-Flash-Q8\\_0.gguf bullshit still not working in lm studio with vision",
          "score": 1,
          "created_utc": "2026-01-03 11:00:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfmpjh",
              "author": "yoracale",
              "text": "Did you try llama.cpp? works for me",
              "score": 1,
              "created_utc": "2026-01-03 13:18:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q09435",
      "title": "Qwen-Image-2512 is released! New SOTA text-to-image model. üíú",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/9kixwe5weiag1.png",
      "author": "yoracale",
      "created_utc": "2025-12-31 09:37:57",
      "score": 114,
      "num_comments": 9,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1q09435/qwenimage2512_is_released_new_sota_texttoimage/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwxx5c6",
          "author": "AdPristine1358",
          "text": "Awesome work releasing quantized versions the same day it's launched! \n\nQuestion - How is quality of Q2 or Q3? \n\nWondering if I could use with 8-10GB VRAM",
          "score": 2,
          "created_utc": "2025-12-31 17:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0zliw",
              "author": "yoracale",
              "text": "You could but it's best to use at least Q3. Do you have RAM as well? You can do offloading",
              "score": 2,
              "created_utc": "2026-01-01 03:36:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwyr92",
          "author": "Sensitive_Sweet_1850",
          "text": "how many parameters?",
          "score": 1,
          "created_utc": "2025-12-31 14:05:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwzksc",
              "author": "yoracale",
              "text": "It's around 20B parameters",
              "score": 2,
              "created_utc": "2025-12-31 14:10:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwx171z",
          "author": "Dramatic-Rub-7654",
          "text": "Is it compatible with stable-diffusion.cpp?",
          "score": 1,
          "created_utc": "2025-12-31 14:19:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0zfqg",
              "author": "yoracale",
              "text": "Yes it is, we made a guide just for it here: [https://unsloth.ai/docs/models/qwen-image-2512/stable-diffusion.cpp](https://unsloth.ai/docs/models/qwen-image-2512/stable-diffusion.cpp)",
              "score": 3,
              "created_utc": "2026-01-01 03:35:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ygls",
          "author": "soyelborja",
          "text": "hi thanks for this, but im having a error when runing the workflow  \n  \n TextEncodeQwenImageEditPlus\n\nmat1 and mat2 shapes cannot be multiplied (784x1280 and 3840x1280)\n\nwhat could be wrong?\n\nthanks",
          "score": 1,
          "created_utc": "2026-01-01 17:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiiiwl",
          "author": "StillNearby",
          "text": "notebook?",
          "score": 1,
          "created_utc": "2026-01-03 21:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkcie4",
          "author": "Vusiwe",
          "text": "If not GGUF, how much VRAM does the normal 2512 use?",
          "score": 1,
          "created_utc": "2026-01-04 03:44:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pya97t",
      "title": "Progressive LoRA Merging - complete model identity replacement on consumer hardware",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pya97t/progressive_lora_merging_complete_model_identity/",
      "author": "TastyWriting8360",
      "created_utc": "2025-12-29 02:38:39",
      "score": 46,
      "num_comments": 43,
      "upvote_ratio": 0.8,
      "text": "I'm here to democratize model creation. After 3+ months of development, I've figured out how to **completely replace a model's weights while preserving the architecture**.\n\nThis means you can take Qwen3, Llama, or any open model - reuse the millions of dollars they spent on pretraining - and replace the identity for a few bucks on consumer hardware.\n\n**How it works:**\n\n1. Train a LoRA adapter on your data\n2. Merge the LoRA into the base model permanently (in BF16, not quantized)\n3. The merged model becomes your new base\n4. Apply a **fresh** LoRA and train again\n5. Repeat\n\nEach merge **dissolves** the adapter into the weights. The next cycle starts with fresh random LoRA weights on the new base. This is not stacking - it's sequential replacement.\n\n**Why this works:**\n\nWe deliberately use catastrophic forgetting to erase the base model's identity while preserving your injected patterns through dataset mixing (50% new data / 50% historical).\n\nAfter enough cycles, the model stops saying \"I am Qwen\" and fully adopts your identity, reasoning style, and knowledge.\n\n---\n\n**Resources:**\n\n- Paper & Code: [https://huggingface.co/hitonet/progressive-lora-merging](https://huggingface.co/hitonet/progressive-lora-merging)\n- GitHub: [https://github.com/antibitcoin/progressive-lora-merging](https://github.com/antibitcoin/progressive-lora-merging)\n- Working demo: [https://chat.hitonet.com](https://chat.hitonet.com) (try Hito-small - it was Qwen 8B)\n- Example model: [https://huggingface.co/hitonet/hito-1.7b](https://huggingface.co/hitonet/hito-1.7b)\n\n---\n\n**FAQ:**\n\n**Q: Isn't this just LoRA stacking? Won't errors compound like (a+b)¬≤ √ó (a+b)¬≤?**\n\nNo. After each merge, the LoRA adapter is **dissolved** into the base weights via `merge_and_unload()` and ceases to exist. The next cycle initializes a **fresh LoRA with random weights**. There is no stacking. After 100 cycles, you have ONE model with 100 sequential weight modifications, not 100 stacked adapters.\n\n**Q: Won't quantization errors accumulate?**\n\nNot if you merge correctly. We train in 4-bit/8-bit (memory efficient), but merge in **BF16 full precision** (error-free). This asymmetric precision prevents error accumulation.\n\n**Q: Won't this cause catastrophic forgetting?**\n\nYes - that's the goal. We selectively forget the base model's identity while preserving yours through dataset mixing.\n\n**Q: How is this different from full fine-tuning?**\n\nSame result, 10-100x cheaper. Full fine-tuning needs 4-8x A100s. This runs on a single 24GB GPU.\n\n**Q: How many cycles until identity replacement?**\n\n- 25 cycles: Noticeable shift (~40%)\n- 50 cycles: Fundamentally different (~70%)\n- 100 cycles: Near-complete replacement (~93%)\n\n---\n\n**Citation:**\n\n    @article{drissi2024bodysnatching,\n      title={Body Snatching: Complete Model Identity Replacement via Progressive LoRA Merging},\n      author={Drissi, Ouissam Said},\n      year={2024},\n      url={https://github.com/antibitcoin/progressive-lora-merging}\n    }\n\n---\n\nThe math, code, and working models are all public. Try it before theorizing why it can't work.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pya97t/progressive_lora_merging_complete_model_identity/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwhkp1g",
          "author": "zmarty",
          "text": "I am confused how this does not destroy the model and its capabilities. Have you run the standard benchmarks?",
          "score": 7,
          "created_utc": "2025-12-29 04:03:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhqols",
              "author": "TastyWriting8360",
              "text": "**Why it doesn't destroy capabilities:**\n\nThe architecture stays intact - attention patterns, layer structure, everything that makes the model \"work\" is preserved. What changes is the *\\*content\\** of the weights, not the *\\*structure\\**.\n\nThink of it like this: the model learned \"how to reason\" during pretraining. That's baked into the architecture and weight relationships. We're replacing *\\*what\\** it reasons about and *\\*how it identifies itself\\**, not its ability to reason. The key is the dataset.\n\nIf you train on garbage, you get garbage. If you train on high-quality synthetic data with proper reasoning chains, the model retains (and can even improve) its capabilities while adopting a new identity.\n\n**Benchmarks:** Honestly? No, I haven't run standard benchmarks. I built this for production use, not for leaderboard chasing. \n\nWhat I can tell you: - It works in my production environment daily\n\n \\- The models reason coherently, follow instructions, and don't hallucinate more than base models\n\n \\- You can try it yourself: [https://chat.hitonet.com](https://chat.hitonet.com) (Hito-small was Qwen 8B) If someone wants to run benchmarks, the models are public: - [https://huggingface.co/hitonet/hito-1.7b](https://huggingface.co/hitonet/hito-1.7b) I'd actually be curious to see the results. But \"works in production\" matters more to me than MMLU scores.",
              "score": 0,
              "created_utc": "2025-12-29 04:40:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhhbzs",
          "author": "vichustephen",
          "text": "Sorry I'm noob but can't we just use DPO to change the identity? Why do all this ?",
          "score": 7,
          "created_utc": "2025-12-29 03:42:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhjcom",
              "author": "TastyWriting8360",
              "text": "\nDPO changes *preferences* (what the model prefers to say), not *identity* (what the model fundamentally is).\n\n**DPO:**\n- Trains the model to prefer response A over response B\n- The base model's knowledge, reasoning patterns, and self-concept remain intact\n- Qwen + DPO = Qwen that prefers certain outputs\n\n**Progressive LoRA Merging:**\n- Rewrites the actual weights over many cycles\n- The base model's identity is progressively erased and replaced\n- Qwen + PLM = Not Qwen anymore\n\nThink of it this way:\n- DPO is like teaching someone to give different answers\n- PLM is like replacing the person entirely\n\nDPO is great for alignment and steering. But if you want the model to genuinely *be* something else - different reasoning style, different knowledge, different self-identity - you need to replace the weights, not just the preferences.\n\nAlso, DPO still requires significant compute for large models. PLM runs on a single 24GB GPU.",
              "score": 3,
              "created_utc": "2025-12-29 03:54:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwhpjrk",
                  "author": "vichustephen",
                  "text": "Though it sounds interesting do you know what you are doing? I smell chatgpt ahhh reply. Also I've done lots of lora fine-tuning in 6gb vram using unsloth",
                  "score": 9,
                  "created_utc": "2025-12-29 04:33:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwijs0t",
                  "author": "__Maximum__",
                  "text": "I feel like you contradict your other comment here. Which is it, the models attention patterns stay the same and only the identity changes, which is equivalent to DPO or the model completely changes (different reasoning style, different knowledge, different self-identity), which means you should run benchmarks (your own or public) to see if it degraded or not.\n\nI appreciate the open source nature of your work, don't get me wrong, and whatever the outcome is, you found a way to fine-tune a model with way less memory, trading of time.",
                  "score": 4,
                  "created_utc": "2025-12-29 08:39:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwievg1",
          "author": "Ok_Appearance3584",
          "text": "Duh? This is called finetuning.",
          "score": 6,
          "created_utc": "2025-12-29 07:53:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwifmdc",
              "author": "TastyWriting8360",
              "text": "Fine-tuning adds a layer on top. The base model is still there underneath - that's why fine-tuned models still say \"I am Qwen\" or revert to base behavior on edge cases.  \n  \nThis erases and replaces the base model. After 100 merge cycles, there is no \"Qwen underneath\" anymore. The weights that made it Qwen are gone, overwritten by sequential modifications.  \n  \nIt's the difference between putting a new coat of paint on a car vs replacing the engine, transmission, and interior piece by piece until nothing original remains.  \n  \nSame architecture, completely different model. That's not fine-tuning, that's replacement.  \n  \nBut you need a good quality dataset. Check the repo for details. Garbage data = garbage model, same as any training method.",
              "score": -1,
              "created_utc": "2025-12-29 08:00:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwim9st",
                  "author": "Ok_Appearance3584",
                  "text": "That's incorrect, the end result does not change if you merge lora to base model. Lora is just a diff layer to the base. During inference it's behaving exactly as if the lora was merged to the base model.¬†\n\n\nYou can merge your lora back to base if you wish to release a standalone finetuned model. If you want to swap different loras during runtime, it's better to have one big base model and multiple adapters as opposed to multiple big models.\n\n\n\nYour method could be streamlined to simply finetune one lora for a hundred epochs.\n\n\nBut as a concept, yes, if you finetune a model (as opposed to designing your own + training from scratch) you get to keep the benefits of large scale pretraining and engineering and make it your own via finetuning.",
                  "score": 5,
                  "created_utc": "2025-12-29 09:02:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwj3ts0",
          "author": "Agreeable-Market-692",
          "text": "Congratulations on \"discovering\" iterative fine-tuning with extra steps... \n\nYou fell prey to one of the classic blunders, you believed ChatGPT when it glazed you. You have achieved dimensional shifting Star Seed! If you thought ChatGPT was going to make you smarter or make you sound smarter you played yourself. It in fact made several people here dumber and made the people who actually read papers annoyed. Your post is pure spam.\n\nThe data mixing you described is just replay based continual learning, not very novel at all and is well-researched. \n\nThe percentages (40%, 70%, 93%) are almost certainly made up. There's no citation, no metric definition, no evaluation methodology. What does \"93% identity replacement\" even mean?\n\nThese are not the only problems with this post but these are the ones I cared to type out before I got too annoyed to keep going.\n\nIf you have some kind of ADHD issue that means you can't focus long enough to get through papers or get motivated enough to start get it handled now and you'll live a much better life, medication is effective. You are responsible for your own actions though, you can't lean on excuses in the real world.   \n  \n\"Independent researcher\" is a fun way to say unemployed. And if you keep posting stuff like this you're gonna stay \"independent\". Interviewers can see through the pretending and would be much harsher than I have been here. If you did this kind of thing in an academic setting you'd rightfully be trotted down to the dean's office to explain yourself.",
          "score": 5,
          "created_utc": "2025-12-29 11:43:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj9mgx",
              "author": "vichustephen",
              "text": "All his replies are from chatgpt ü§£ü§£",
              "score": 3,
              "created_utc": "2025-12-29 12:29:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjag9q",
                  "author": "Agreeable-Market-692",
                  "text": "he also followed me to one of my posts in another sub, classic narcissistic injury",
                  "score": 2,
                  "created_utc": "2025-12-29 12:35:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj45n3",
              "author": "TastyWriting8360",
              "text": "How is that related to my research or work? are you speaking about your own experiance, did that happen to you? I am sorry but I spent 3 months to come up with this even if I summerized it with AI, that does not matter. your post is very toxic, unethical, unprofessional and not related to the topic, you are welcome to run any benchmarks everything is public.",
              "score": 1,
              "created_utc": "2025-12-29 11:45:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwj4ham",
                  "author": "Agreeable-Market-692",
                  "text": "I actually work in the industry and your post smacks of slop. Take it personally if you want, but I'm just telling you what other experienced people are going to think but may or may not say to your face. Address my critiques or you're just conceding what you did here.",
                  "score": 3,
                  "created_utc": "2025-12-29 11:48:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwj9qov",
                  "author": "vichustephen",
                  "text": "Bro this is your only reply others are chatgpt . I don't understand how people fell for your trap ü§£",
                  "score": 3,
                  "created_utc": "2025-12-29 12:30:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwjjpep",
                  "author": "Agreeable-Market-692",
                  "text": "Really pathetic behavior. You are simply outing yourself at this point. I feel secondhand embarassment for you.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:38:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuo2vg",
              "author": "TheRiddler79",
              "text": "On a side note, your entire premise hinges on your own failures and inability to carefully read more than just the post.\n\nIt's an equally stupid \"burn\" to pretend to know about LLM tuning, but also clown on someone using AI to make a clearly stated post.\n\nFinally, you claim the percentages are made up, but also can't prove they can't be true.\n\nIn summary, your entire post was dripping with amateur attacks and flawed logic based on your own personal shortcomings.",
              "score": 0,
              "created_utc": "2025-12-31 03:12:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvbzqe",
                  "author": "Agreeable-Market-692",
                  "text": "\"but also can't prove they can't be true\"\n\nthis is the lowest grade bait",
                  "score": 1,
                  "created_utc": "2025-12-31 05:53:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhq9m1",
          "author": "SpaceNinjaDino",
          "text": "Do you think that the planned progression should end on the most important as it will be the most influential?",
          "score": 2,
          "created_utc": "2025-12-29 04:38:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhr27o",
              "author": "TastyWriting8360",
              "text": "Good intuition, but I'd actually argue the opposite based on how the method works.\n\nEarly cycles are most influential for identity. The first 25-50 cycles cause the biggest drift from the base model. This is when the model is most malleable because you're breaking its original identity. Whatever you train early gets reinforced across all subsequent cycles.\n\nLater cycles are for refinement. By cycle 50+, the model has largely adopted your identity. Later training is fine-tuning your own model at that point, not fighting against Qwen/Llama anymore.\n\nMy approach:\n\nEarly cycles (1-25): Core identity, personality, self-concept. This is where you establish \"I am X, not Qwen\"\n\nMid cycles (25-50): Reasoning style, knowledge patterns, how it thinks\n\nLate cycles (50-100): Edge cases, specific behaviors, polish\n\nThink of it like painting. You lay down the base colors first, then add details. If you save your most important stuff for last, the earlier paint might show through.\n\nThat said, the 50/50 dataset mixing means important early data keeps getting reinforced throughout. So it's not like early stuff gets forgotten. It's more about what gets the most total exposure across all cycles.\n\nWhat's your use case? That might change the recommendation.",
              "score": 1,
              "created_utc": "2025-12-29 04:43:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkuf8i",
          "author": "sirfitzwilliamdarcy",
          "text": "You can usually achieve the same effect with more steps during fine-tuning. Would be interested in seeing Evals before and after this process though. It may have unique advantages.",
          "score": 2,
          "created_utc": "2025-12-29 17:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsun2y",
              "author": "TastyWriting8360",
              "text": "Yep one of the key advantages is new ways of reasoning, you are not stuck with the model default, you are not finetuning you are basically rewriting the model, SFT on a budget but more accurate, remember when we merge and dissolve the lora into the new base we actually do it on the full wieghts of the model not the quantized.\n\nthe following is nested reasoning my own invention have you seen sideways reasoning before?\n\nhttps://preview.redd.it/x21tgfqwqeag1.png?width=950&format=png&auto=webp&s=55de2f0ea77eebc32d6e5a3ea42541eb5c30d16a\n\nyou can test it out by downloading the hito 1.7b which was created from qwen3 1.7b I have trained full embeddings with my own custom tokens and applied styling to make it clear you can use it without downloading the model at [chat.hitonet.com](http://chat.hitonet.com) just make sure to signup cuz the non signed up version is bullshit demo. just remember to make good stuff, you have to forget about following tutorials, generate your own high quality dataset, real dataset not what everyone is using. I spent a good amount of time on mine,",
              "score": 1,
              "created_utc": "2025-12-30 21:18:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp2k8r",
          "author": "schlammsuhler",
          "text": "How to overfit",
          "score": 2,
          "created_utc": "2025-12-30 07:52:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwssndt",
              "author": "TastyWriting8360",
              "text": "It does not overfit read the paper. already solved that problem long ago, download the model, test it your self, try it on qwen3-0.6b.",
              "score": 1,
              "created_utc": "2025-12-30 21:09:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwha6cd",
          "author": "SlavaSobov",
          "text": "Funny you mention this, I've been doing exactly this for my companion AI. \n\nFirst was a LoRA adapter on the base personality, to become the ground truth. \n\nThen further fine tuning on other tokens and behavior I wanted to introduce.",
          "score": 3,
          "created_utc": "2025-12-29 03:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhao9e",
              "author": "TastyWriting8360",
              "text": "Finally I am happy to see a smart fellow adventurer can you please clarify that it works here for this people if you don't mind of course. [https://www.reddit.com/r/LocalLLaMA/comments/1py8yyw/r\\_progressive\\_lora\\_merging\\_complete\\_model/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1py8yyw/r_progressive_lora_merging_complete_model/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
              "score": 1,
              "created_utc": "2025-12-29 03:03:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhzqel",
          "author": "ZealousidealShoe7998",
          "text": "wait until people realize that are literally models out there that super small and efficient and could literally work great for very niche scenarios and it would take is just to train it a bit.   \nbeing a generalist takes lots of parameters but niche knowledge not so much.",
          "score": 1,
          "created_utc": "2025-12-29 05:44:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwidzyb",
              "author": "TastyWriting8360",
              "text": "Well said, Exactly! meanwhile people at r/LocalLLaMA  is down voting me saying it wont work, even tho I provided a working model, the math, the paper and the code lol, it was a mistake sharing there, this is the correct place to share this, people who actually technical and do finetuning and machine learning, not just users.",
              "score": 1,
              "created_utc": "2025-12-29 07:45:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi67an",
          "author": "ramendik",
          "text": "What do you mean by \"model identity\"?",
          "score": 1,
          "created_utc": "2025-12-29 06:37:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwidgfh",
              "author": "TastyWriting8360",
              "text": "The patterns that make the model \"itself\":\n\n\\- How it responds to \"who are you\" / \"what are you\"\n\n\\- Its default reasoning style and thought patterns\n\n\\- Built-in safety responses and refusals\n\n\\- Knowledge it considers core vs peripheral\n\n\\- Personality traits baked in during RLHF\n\nA fresh Qwen will say \"I am Qwen, developed by Alibaba\" and reason in a specific way. After PLM, it says \"I am \\[your model name\\]\" and reasons the way YOUR training data taught it.\n\nIt's not just changing a system prompt. The weights themselves no longer encode \"I am Qwen.\" That information is gone, replaced by your training data.\n\nIts still finetuning but the end result is a fully trained model not related to the original anymore.",
              "score": 1,
              "created_utc": "2025-12-29 07:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiggat",
          "author": "Thick-Protection-458",
          "text": "How is that different from ReLoRA?",
          "score": 1,
          "created_utc": "2025-12-29 08:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwih2ve",
              "author": "TastyWriting8360",
              "text": "ReLoRA is for pretraining from scratch. You start with random weights and want to train a new model efficiently. The problem it solves: full-rank pretraining is expensive, so use iterative low-rank updates to approximate full-rank training with less memory.  \n  \nReLoRA requirements:  \n\\- Warm start: 25% of training must be full-rank before switching to LoRA  \n\\- Jagged LR scheduler: learning rate resets after each merge to prevent divergence  \n\\- Partial optimizer reset: prune 99% of optimizer state by magnitude after each merge  \n\\- Multi-GPU setup: paper uses multiple A100s  \n\\- Tested up to 1.3B, attempts to scale beyond that were unsuccessful  \n  \nPLM is for identity replacement in pretrained models. You start with Qwen/Llama/etc and want to make it into YOUR model. The problem it solves: you want to leverage billions of dollars of pretraining but completely change what the model is.  \n  \nPLM requirements:  \n\\- No warm start: you inherit the pretrained weights as-is  \n\\- Standard training: no special schedulers or optimizer resets  \n\\- Single 24GB GPU: train in 4-bit, merge in BF16 on CPU  \n\\- Dataset mixing: 50% new / 50% historical to preserve your identity while erasing base  \n\\- Tested on 14B, scales higher with CPU offload for merge  \n  \nReLoRA: random weights ‚Üí efficient pretraining ‚Üí new model (up to 1.3B)  \nPLM: pretrained model ‚Üí identity replacement ‚Üí your model (14B+)  \n  \nSame mechanic, opposite directions, different scale.",
              "score": 2,
              "created_utc": "2025-12-29 08:13:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhdxzd",
          "author": "Ok-Adhesiveness-4141",
          "text": "Am most interested in this, is there anything on GitHub?",
          "score": 1,
          "created_utc": "2025-12-29 03:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhemjd",
              "author": "TastyWriting8360",
              "text": "Yes there is in the link",
              "score": 1,
              "created_utc": "2025-12-29 03:26:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhj2dl",
          "author": "ANTIVNTIANTI",
          "text": "omfgoddamn this sounds freaking amazing! MUCH LOVES!!!",
          "score": -1,
          "created_utc": "2025-12-29 03:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhji5r",
              "author": "TastyWriting8360",
              "text": "Thank you I am glad you like enjoy!",
              "score": 1,
              "created_utc": "2025-12-29 03:55:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0mcf4",
      "title": "Am I calculating this wrong ? AWS H100 vs Decentralized 4090s (Cost of Iteration)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q0mcf4/am_i_calculating_this_wrong_aws_h100_vs/",
      "author": "yz0011",
      "created_utc": "2025-12-31 20:15:03",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "I'm building a cost model for fine tuning Llama 3 70B and I found a weird crossover point where consumer swarms beat H100s on time, not just cost. I want to check if my constants align with your experience.\n\nThe constants I'm using:\n\n* AWS H100: $4.50/hr. Setup time (Driver install + 140GB download): around 45 mins.\n* WAN Swarm (4090s): $2.00/hr. Setup time (Hot-loaded): 5 mins.\n* Latency penalty: I'm assuming the Swarm is 1.6x slower on pure compute due to WAN bandwidth.\n\nThe Result: For a single production run (long training), AWS wins on speed. But for research cycles (e.g., 3 runs of 10k samples to test hyperparams), the math says the Swarm is actually cheaper AND competitive on total time because you don't pay the 45 minute \"setup tax\" three times.\n\nThe question: For those of you fine-tuning 70B models:\n\n1. Is my 45 minute setup estimate for AWS spot instances accurate, or do you have faster persistent environments ?\n2. Is a 1.6x slowdown on training speed a dealbreaker if the cost is $2/hr vs $4.50/hr?\n\n(Note: I built a calculator to visualize this, but I want to validate the constants first).",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q0mcf4/am_i_calculating_this_wrong_aws_h100_vs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nx33tzw",
          "author": "Final-Rush759",
          "text": "You can pay less for h100, h200 at vast.ai or lambda.ai",
          "score": 6,
          "created_utc": "2026-01-01 14:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6kqp6",
          "author": "MoistGovernment9115",
          "text": "Your math doesn‚Äôt look crazy to me, especially for research loops. That 30‚Äì45 min AWS setup tax is very real unless you‚Äôre running warm AMIs with everything pre-baked, and even then spot interruptions can blow up your assumptions fast. For short iterative runs, setup time matters more than raw FLOPs.\n\nI‚Äôve been running similar experiments on Gcore‚Äôs GPU cloud, and what helped was having fast-provisioned GPUs without the heavyweight AWS bootstrap. \n\nYou still get serious cards, but without rebuilding the environment every cycle. It makes iteration feel closer to ‚Äúhot-loaded‚Äù than traditional hyperscalers.\n\nExtra thought: iteration velocity is underrated. If faster feedback leads to better hyperparams sooner, the effective cost per insight drops way more than people model.",
          "score": 6,
          "created_utc": "2026-01-02 02:04:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8mvsl",
              "author": "yz0011",
              "text": "Effective cost per insight. I am stealing that phrase. That is exactly the metric I was trying to optimize for, but couldn't name.\n\n\n‚ÄãYou're right that smaller providers like Gcore (or Lambda/vast) generally have faster provisioning than the AWS behemoth. The friction I usually hit with them isn't the boot time, but the availability.\n\n\n‚Äã- AWS: Slow setup, but infinite supply (if you pay).\n- ‚ÄãGcore/Lambda: Fast setup, but often no GPUs available in the specific region I need.\n\n\n‚ÄãMy goal with the swarm is to make the remote cluster feel like localhost. I want the model weights to live in VRAM (or RAM) across the network permanently, so when I hit Enter, the feedback loop starts instantly even if the actual token generation is slower.\n\n\n‚ÄãOn Gcore, are you using their standard on-demand instances, or do they have a serverless/endpoint product that keeps the environment warm for you?",
              "score": 1,
              "created_utc": "2026-01-02 11:49:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwzc23a",
          "author": "wektor420",
          "text": "On aws use a disk to avoid download every time you start up a vm",
          "score": 2,
          "created_utc": "2025-12-31 21:27:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2ic6f",
              "author": "yz0011",
              "text": "That works perfectly for on-demand, but it breaks with spot instances (which I use to keep costs down).\n\nEBS volumes are locked to a specific Availability Zone. If my spot instance gets reclaimed and the next cheapest availability is in antoher zone, I can't mount that existing disk without snapshotting and restoring it first, which kills the speed. Unless you're paying the premium for on-demand to guarantee the AZ.",
              "score": 2,
              "created_utc": "2026-01-01 12:10:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx10dg0",
          "author": "eleqtriq",
          "text": "Big fat assumption in the middle of your calculations.  Going to have to prove it out to be sure.",
          "score": 1,
          "created_utc": "2026-01-01 03:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2im5m",
              "author": "yz0011",
              "text": "100%. The 1.6x latency penalty is the most fragile variable in the model. It assumes we can mask network latency using pipeline parallelism (similar to how Petals does it), but if the stragglers are too slow, that multiplier could easily blow out to 3x or 4x, and then the cost savings vanish.\n\nThat's honestly why I'm running this benchmark instead of just trusting the paper math. I'm trying to find the break point where the latency makes the swarm unusable.  \n  \nHave you messed with WAN training enough to have a gut feeling on what the real penalty usually looks like?",
              "score": 1,
              "created_utc": "2026-01-01 12:13:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2dw2g",
      "title": "Can someone explain this MedGemma variant on Unsloth's page?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q2dw2g/can_someone_explain_this_medgemma_variant_on/",
      "author": "Hot-Comb-4743",
      "created_utc": "2026-01-02 23:15:27",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "Can you help me with any info about the datasets used for finetuning [this particular (Unsloth's) MedGemma](https://huggingface.co/unsloth/medgemma-27b-text-it) from its predecessor, the original MedGemma? And also about the differences between Unsloth's MedGemma and the Google's original MedGemma?\n\n\n\nhttps://preview.redd.it/jf32d8d1p0bg1.png?width=1601&format=png&auto=webp&s=19fdb07201613d8ab6b75012db5e9d0224dbfd4d\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q2dw2g/can_someone_explain_this_medgemma_variant_on/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nxcectq",
          "author": "yoracale",
          "text": "There's no difference between the original MedGemma, it's just a reupload however we do sometimes do bugfixes for models so you can download from our page just to be safe",
          "score": 7,
          "created_utc": "2026-01-02 23:35:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf0el7",
              "author": "Hot-Comb-4743",
              "text": "Many thanks.",
              "score": 1,
              "created_utc": "2026-01-03 10:24:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcb9gq",
          "author": "Zc5Gwu",
          "text": "Medgemma has a vision component. Maybe that's the text component without vision?",
          "score": 1,
          "created_utc": "2026-01-02 23:18:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf02cu",
              "author": "Hot-Comb-4743",
              "text": "No, that's another MedGemma. There are 2 Medgemma 27B models, one with vision and the other only text.",
              "score": 1,
              "created_utc": "2026-01-03 10:21:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcf9qu",
          "author": "qustrolabe",
          "text": "The only thing I see so far is that I don't need to request access to Unsloth version unlike Google one",
          "score": 1,
          "created_utc": "2026-01-02 23:40:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q13jsu",
      "title": "Fine tune 9bn params model for tools use.",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q13jsu/fine_tune_9bn_params_model_for_tools_use/",
      "author": "RokasRaulinaitis",
      "created_utc": "2026-01-01 12:52:42",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hello, I'm currently working on fine-tuning LLM to generate tool requests. My model does not support tools calling and I have a workaround with Langgraph agent that parses output and completes actions, but the result is not what I want. Ideally I would like to fine-tune my model with unsloth and \"teach\" my model to generate ChatML and Hermes tools calling format nativaly so my model would be better optimized.\n\nLLM i'm using is EuroLLM 9bn params.\n\nMy current goal is simple: Generate dataset (200-3000 entries), both human written and synthetic data, but I'm facing the issue where i don't really know what should be included into the dataset. Should I include roles: System, User, Assistant, Tool? Maybe some of you already have some data that could greatly help me.\n\nExample I came up with:\n\n    {\n      \"conversations\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"System prompt...\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"User request...\"\n        },\n        {\n          \"role\": \"assistant\",\n          \"content\": \"<tool_call>\\n{JSON}\\n</tool_call>\"\n        },\n        {\n          \"role\": \"tool\",\n          \"content\": \"{JSON result}\",\n          \"tool_call_id\": \"call_X\"\n        },\n        {\n          \"role\": \"assistant\",\n          \"content\": \"Natural response...\"\n        }\n      ]\n    }\n\nI will build my own dataset and it will be in my native language (Lithuanian). Ideally I would prefer to run my model via Ollama.\n\n**If anyone is familiar with fine-tuning for this purpose, please write a comment bellow or drop me a PM. Thank you a ton!**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q13jsu/fine_tune_9bn_params_model_for_tools_use/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nx3m98s",
          "author": "StardockEngineer",
          "text": "Another thread about this here https://www.reddit.com/r/LocalLLaMA/s/r8OOpLj55X",
          "score": 2,
          "created_utc": "2026-01-01 16:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5pepg",
          "author": "Ok-LLama-Ok",
          "text": "Check out deepfabric and the work those folks are doing. They have discord which I used to get quite a lot of support. It works well with unsloth training books\n\n[https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq](https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq)",
          "score": 1,
          "created_utc": "2026-01-01 23:03:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py5j8p",
      "title": "Can't load Ministral-3 models for finetuning. Config file issue ?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1py5j8p/cant_load_ministral3_models_for_finetuning_config/",
      "author": "LostBejamin",
      "created_utc": "2025-12-28 23:10:53",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "**EDIT : I corrected the problem by installing transformers library via github with this command:**\n\n    pip install git+https://github.com/huggingface/transformers.git@bf3f0ae70d0e902efab4b8517fce88f6697636ce\n\n\\---\n\nI tried loading Ministral-3 models (bnb-4bit and basic versions of all size) locally, but I was unable to do so as It get me this error:\n\n`RuntimeError: Unsloth: No config file found - are you sure the \\`model\\_name\\` is correct?\\`\n\nI also tried with other models like [unsloth/functiongemma-270m-it-unsloth-bnb-4bit](https://huggingface.co/unsloth/functiongemma-270m-it-unsloth-bnb-4bit) and [unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit](https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit), and they seem to work just fine.\n\nDoes anyone has this problem or know how to deal with it ? Here the code I used:\n\n    from unsloth import FastLanguageModel\n    \n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Ministral-3-14B-Instruct-2512\",\n        load_in_4bit=True,\n    )\n\n(PS: I also wrote an issue ticket on [Github](https://github.com/unslothai/unsloth/issues/3788).)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1py5j8p/cant_load_ministral3_models_for_finetuning_config/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwjm7co",
          "author": "7h3_50urc3",
          "text": "You need the newest unsloth Version with transformers 5.0 lib. There is no official docker image from unsloth yet which fulfills these requirements, so you have to install it yourself or install all required libs into the the official docker image.",
          "score": 2,
          "created_utc": "2025-12-29 13:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpc8cb",
              "author": "LostBejamin",
              "text": "I installed transformers library via github with this command:\n\n    pip install git+https://github.com/huggingface/transformers.git@bf3f0ae70d0e902efab4b8517fce88f6697636ce\n\nAnd now the model load without any issues !",
              "score": 1,
              "created_utc": "2025-12-30 09:22:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwizdx0",
          "author": "im_datta0",
          "text": "Hey u/LostBejamin Thanks for creating a github issue for this. I am looking into it as we speak and would prefer to communicate there itself.",
          "score": 1,
          "created_utc": "2025-12-29 11:04:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2s8vk",
      "title": "GGUF conversion and quantization for IQuest coder models",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q2s8vk/gguf_conversion_and_quantization_for_iquest_coder/",
      "author": "Hot-Comb-4743",
      "created_utc": "2026-01-03 11:20:02",
      "score": 6,
      "num_comments": 10,
      "upvote_ratio": 0.87,
      "text": "EDIT: Apparently, IQuest Coder models have been debunked as **benchmaxxing garbage**. \n\nOld, obsolete post: \n\nThese 4 new [IQuest coder](https://huggingface.co/IQuestLab) models seem very promising. Can Unsloth kindly quantize and GGUF-convert them?\n\nhttps://preview.redd.it/ovn090ty94bg1.png?width=4199&format=png&auto=webp&s=23d98b9758c3ec098eaf94353157af78007373b4\n\nTheir original SafeTensors version is in BF16 format (not FP16), so I hope their GGUF-conversion (quantization) into ***full-size*** BF16 GGUFs would cause no performance loss. üòç\n\nI mean these 4 IQuest models:\n\n1. [https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base)\n2. [https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base-Stage1](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base-Stage1)\n3. [https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct)\n4. [https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q2s8vk/gguf_conversion_and_quantization_for_iquest_coder/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nxfp19h",
          "author": "Familiar_Wish1132",
          "text": "Yes please !!! Always looking forward for interesting models fixed by unsloth :D",
          "score": 4,
          "created_utc": "2026-01-03 13:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfyt4p",
              "author": "Hot-Comb-4743",
              "text": "Exactly üòÅ Their GGUFs are awesome.\n\nI don't know if official Unsloth team members monitor this sub. I hope u/yoracle is an official Unslothian. Otherwise, I should post this suggestion on their Github page too.",
              "score": 1,
              "created_utc": "2026-01-03 14:29:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgkv7m",
          "author": "doradus_novae",
          "text": "Wasnt this model debunked as benchmaxxing garbage? Anyone care to dispute? Anyone actually using this with feedback?",
          "score": 4,
          "created_utc": "2026-01-03 16:21:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgmckt",
              "author": "Hot-Comb-4743",
              "text": "Thanks for the heads-up. I didn't know that.",
              "score": 1,
              "created_utc": "2026-01-03 16:28:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxh4dxs",
                  "author": "doradus_novae",
                  "text": "No worries! I read it yesterday and really was just waiting on seeing if people were actually using this or not, but it seemed like the model had some drama associated with it from what I recall reading.",
                  "score": 1,
                  "created_utc": "2026-01-03 17:52:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgn4hi",
              "author": "Hot-Comb-4743",
              "text": "I added the heads-up to the post to warn others. If you can give me the link where they debunked it, I would add the link too.",
              "score": 1,
              "created_utc": "2026-01-03 16:32:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxg34v9",
          "author": "streppelchen",
          "text": "I tried another GGUF and found horrible performance (2tps on rtx 5090 at q4)",
          "score": 2,
          "created_utc": "2026-01-03 14:53:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxg71z6",
              "author": "Hot-Comb-4743",
              "text": "By horrible performance, you mean speed-wise? (because you mentioned your setup)\n\nOr you meant its coding ability was horrible (and those benchmarks are just BS)?",
              "score": 1,
              "created_utc": "2026-01-03 15:14:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxg761q",
                  "author": "streppelchen",
                  "text": "I haven‚Äôt tested further, at 2 tps it would take forever",
                  "score": 1,
                  "created_utc": "2026-01-03 15:14:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2smw0",
      "title": "assert len(weights) == expected_node_count error with AMD MI100",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q2smw0/assert_lenweights_expected_node_count_error_with/",
      "author": "regstuff",
      "created_utc": "2026-01-03 11:42:52",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Have an AMD MI100 with rocm 6.4.3 on a Ubuntu 22.04 VM. The MI100 is passthrough and works fine as in rocm-smi etc show what is expected.\n\nllama.cpp also works and uses the gpu.\n\nAm following the guide to install unsloth here: [https://unsloth.ai/docs/new/fine-tuning-llms-on-amd-gpus-with-unsloth](https://unsloth.ai/docs/new/fine-tuning-llms-on-amd-gpus-with-unsloth)\n\nEverything works fine till I get to the last step:\n\n`pip install \"unsloth[amd] @ git+`[`https://github.com/unslothai/unsloth`](https://github.com/unslothai/unsloth)`\"`\n\nThen I get this error\n\n`Collecting exceptiongroup>=1.0.2`\n\n`Using cached exceptiongroup-1.3.1-py3-none-any.whl (16 kB)`\n\n`ERROR: Exception:`\n\n`Traceback (most recent call last):`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 165, in exc_logging_wrapper`\n\n`status = run_func(*args)`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 205, in wrapper`\n\n`return func(self, options, args)`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 389, in run`\n\n`to_install = resolver.get_installation_order(requirement_set)`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 188, in get_installation_order`\n\n`weights = get_topological_weights(`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 276, in get_topological_weights`\n\n`assert len(weights) == expected_node_count`\n\n`AssertionError`\n\nCan anyone help?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q2smw0/assert_lenweights_expected_node_count_error_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    }
  ]
}