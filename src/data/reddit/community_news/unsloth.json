{
  "metadata": {
    "last_updated": "2026-01-11 08:42:55",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 19,
    "total_comments": 102,
    "file_size_bytes": 154760
  },
  "items": [
    {
      "id": "1q5mac3",
      "title": "Unsloth-MLX - Unsloth for Apple Silicon",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/5oy6vtff3rbg1.png",
      "author": "A-Rahim",
      "created_utc": "2026-01-06 15:53:05",
      "score": 297,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q5mac3/unslothmlx_unsloth_for_apple_silicon/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "ny2j0gf",
          "author": "yoracale",
          "text": "Please note this project is not officially affiliated with Unsloth.\n\nWe also had a contributor who made a PR for MLX support directly in the Unsloth repo today:¬†[https://github.com/unslothai/unsloth/pull/3856](https://github.com/unslothai/unsloth/pull/3856)",
          "score": 1,
          "created_utc": "2026-01-06 20:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny28pdd",
          "author": "yoracale",
          "text": "There was also this PR today by an Unsloth contributor directly for the Unsloth repo: https://github.com/unslothai/unsloth/pull/3856\n\nWe're still working on reviewing it and OP if you have any feedback or contributions you'd like to add directly to the repo please let us know üôè\n\nAnd OP u/A-rahim can you please specify in your post that it's not affiliated with Unsloth please. Thanks.",
          "score": 9,
          "created_utc": "2026-01-06 19:28:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5c329",
              "author": "A-Rahim",
              "text": "Done. \n\n(It was already mentioned on GitHub, by the way)",
              "score": 1,
              "created_utc": "2026-01-07 05:06:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny0yats",
          "author": "abhishek_satish96",
          "text": "Hey! Does this bring any of the RAM/performance benefits of Unsloth to MLX? Or is it just a compatibility layer of sorts?",
          "score": 9,
          "created_utc": "2026-01-06 15:59:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1jqbo",
              "author": "BenniB99",
              "text": "I quickly skimmed the code.\nLooks like it is just a compatibility layer that wraps\nnative mlx training with an unsloth/trl interface.\nSo none of the Unsloth benefits :/",
              "score": 9,
              "created_utc": "2026-01-06 17:36:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5e62w",
                  "author": "A-Rahim",
                  "text": "Hi, it's not \"comparable\" to unsloth (or making it comparable to unsloth was not my \"vision\" at all). Unsloth's speciality is custom Triton GPU kernels, which, as of now, Apple Silicon doesn't support. \n\n  \nOn a personal note, I usually use unsloth for my day-to-day fine-tuning or testing tasks on cloud GPUs. After buying a Mac, I thought, what if I can do experimentation or small-scale runs on my Mac locally, then move to cloud GPUs if needed, without changing the code much! That was my thought behind this project, it's helping me personally, and if it helps others like me, then I'll have my satisfaction.",
                  "score": 5,
                  "created_utc": "2026-01-07 05:20:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1fybs",
          "author": "meet_minimalist",
          "text": "Hey what changes you added ? How it is different than original unsloth repo? I have a similar problem statement.",
          "score": 4,
          "created_utc": "2026-01-06 17:19:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1jnmd",
          "author": "wektor420",
          "text": "Weird that git history does not show that you have forked unsloth repo but as if it was created from scratch",
          "score": 2,
          "created_utc": "2026-01-06 17:36:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny30l3k",
              "author": "334578theo",
              "text": "Degit is a thing",
              "score": 1,
              "created_utc": "2026-01-06 21:36:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjokxg",
          "author": "fragment_me",
          "text": "To take the name Unsloth and not be affiliated with them should be reconsidered.",
          "score": 2,
          "created_utc": "2026-01-09 05:31:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4knko",
          "author": "LA_rent_Aficionado",
          "text": "So pretty much an unsloth fork without the ability to as easily pull from upstream?",
          "score": 1,
          "created_utc": "2026-01-07 02:23:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny68ifd",
          "author": "PhysicsAgreeable8170",
          "text": "Does this supports Metal or CoreML? Is it able to utilize the NPU(plssss)",
          "score": 1,
          "created_utc": "2026-01-07 09:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8uzto",
          "author": "pas_possible",
          "text": "Niccce",
          "score": 1,
          "created_utc": "2026-01-07 18:34:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyktiuc",
          "author": "InternationalTwo3187",
          "text": "testcomment",
          "score": 1,
          "created_utc": "2026-01-09 11:23:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nys79ap",
          "author": "SteveRadich",
          "text": "Hopefully they merge the PR.",
          "score": 1,
          "created_utc": "2026-01-10 13:31:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzjisa",
      "title": "Unsloth just hit 50,000 GitHub stars! ‚≠êü¶•",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/80o4jxiipcag1.png",
      "author": "yoracale",
      "created_utc": "2025-12-30 14:29:32",
      "score": 172,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pzjisa/unsloth_just_hit_50000_github_stars/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwqkrse",
          "author": "Educational_Rent1059",
          "text": "Thanks for all the contributions to the OSS community this year!! Team unsloth <3",
          "score": 6,
          "created_utc": "2025-12-30 14:50:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqlz97",
              "author": "yoracale",
              "text": "Thank you for the constant support! \\^\\^",
              "score": 2,
              "created_utc": "2025-12-30 14:56:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqx43l",
          "author": "WarmAd6505",
          "text": "Amazing project! Deserves so much support. Helped me so much.",
          "score": 2,
          "created_utc": "2025-12-30 15:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx4xg",
              "author": "yoracale",
              "text": "Thank you for using us and the support! Glad the package helped you! üôè",
              "score": 1,
              "created_utc": "2025-12-30 21:30:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwrkn2r",
          "author": "Arindam_200",
          "text": "Congratulationsüî•",
          "score": 2,
          "created_utc": "2025-12-30 17:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx68p",
              "author": "yoracale",
              "text": "Thank you! üôèüòä",
              "score": 2,
              "created_utc": "2025-12-30 21:30:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwrx43h",
          "author": "NoPresentation7366",
          "text": "Fully deserved! thank you brothers for your works and dedication üòéüíï",
          "score": 2,
          "created_utc": "2025-12-30 18:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx869",
              "author": "yoracale",
              "text": "Thanks so much for the support! ü•∞üôè",
              "score": 2,
              "created_utc": "2025-12-30 21:30:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nws1y4h",
          "author": "LegacyRemaster",
          "text": "Well deserved",
          "score": 2,
          "created_utc": "2025-12-30 19:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx282",
              "author": "yoracale",
              "text": "Thank you! üôèü•∞",
              "score": 1,
              "created_utc": "2025-12-30 21:29:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxqp4q",
      "title": "All GLM 4.7, GLM 4.6 and GLM 4.6V-Flash GGUFs are now updated!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pxqp4q/all_glm_47_glm_46_and_glm_46vflash_ggufs_are_now/",
      "author": "yoracale",
      "created_utc": "2025-12-28 12:54:48",
      "score": 127,
      "num_comments": 20,
      "upvote_ratio": 1.0,
      "text": "Hey guys, we did a refresh of quants (quality of life updates) for GLM 4.5, 4.6, 4.6V-Flash and 4.7\n\nllama.cpp and other inference engines like LM Studio now support more features including but not limited to:\n\n1. Non ascii decoding for tools (affects non English languages) For eg before the default (ensure\\_ascii=True) would cause \"caf√©\" ‚Üí \"caf\\\\u00e9\", whilst now ensure\\_ascii=False would tokenize \"caf√©\" ‚Üí \"caf√©\". I would re-download our quants if you use languages other than English.\n2. Converts reasoning content parsing to original \\[0\\], \\[-1\\] from our changes of |first and |last. We used to change \\[0\\] to |first and \\[-1\\] to |last so we be compatible with LM Studio and llama-cli. With the upgrade of llama-cli to use llama-server, we can revert this. llama-server also didn't like |first, so we fixed it as well.\n3. Many of you reported Chinese thinking with the GLM-4.6V-Flash GGUFs. After investigating, we confirmed the same behavior appears in all uploads regardless of uploader (e.g., LM Studio and bartowski). LM Studio‚Äôs Q8\\_0, bartowski‚Äôs BF16, and our BF16 all produce Chinese ‚Äúthinking,‚Äù so this is just the way Z . ai intended for the model and is not unique to our uploads. [See our investigation here.](https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/discussions/4#694bdbac91a48021d8b210a1)\n\nAlso other changes:\n\n1. **Added lot of tool calls in our calibration dataset - makes tool calling better, especially for smaller quants.**\n2. **A bit more calibration data for GLM models., adding a teeny tiny bit more accuracy overall.**\n\n**This does mean you need to re-download them to use the latest changes**\n\nGGUFs which received Quality of Life updates:\n\n* [https://huggingface.co/unsloth/GLM-4.6V-GGUF](https://huggingface.co/unsloth/GLM-4.6V-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.7-GGUF](https://huggingface.co/unsloth/GLM-4.7-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.6-GGUF](https://huggingface.co/unsloth/GLM-4.6-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.5-GGUF](https://huggingface.co/unsloth/GLM-4.5-GGUF)\n* [https://huggingface.co/unsloth/GLM-4.5-Air-GGUF](https://huggingface.co/unsloth/GLM-4.5-Air-GGUF)\n\nOur guides are all in our docs or model cards: [https://unsloth.ai/docs/models/glm-4.7](https://unsloth.ai/docs/models/glm-4.7)\n\nThanks so much guys! :)",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1pxqp4q/all_glm_47_glm_46_and_glm_46vflash_ggufs_are_now/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwe2k2y",
          "author": "kiwibonga",
          "text": "\"Sorry guys, it just wants to think in Chinese, but it understands everything you say.\"\n\nI have that exact problem with my bilingual toddler",
          "score": 4,
          "created_utc": "2025-12-28 17:08:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxs6q",
          "author": "keypa_",
          "text": "Thanks a lot, being a french it's been a nightmare trying to understand why all accents are not being properly displayed !",
          "score": 3,
          "created_utc": "2025-12-28 16:44:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfmsm1",
          "author": "elsung",
          "text": "awesome! now i just need to figure out /wait for conversion of these into MLX =)",
          "score": 3,
          "created_utc": "2025-12-28 21:38:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdyahx",
          "author": "Magnus114",
          "text": "Should we still specify the jinja from this pr?\n\nhttps://github.com/ggml-org/llama.cpp/pull/16932",
          "score": 2,
          "created_utc": "2025-12-28 16:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjjpy0",
              "author": "yoracale",
              "text": "Our current jinja chat template should work fine as is",
              "score": 1,
              "created_utc": "2025-12-29 13:38:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwegwto",
          "author": "Sensitive_Sweet_1850",
          "text": "nice",
          "score": 2,
          "created_utc": "2025-12-28 18:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nweqpdi",
          "author": "LegacyRemaster",
          "text": "thx sir!",
          "score": 2,
          "created_utc": "2025-12-28 19:03:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgq8gj",
          "author": "RedditUsr2",
          "text": "Is unsloth/GLM-4.6V-Flash-GGUF usable with cline?",
          "score": 2,
          "created_utc": "2025-12-29 01:05:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhhpy4",
              "author": "yoracale",
              "text": "It should be yes",
              "score": 2,
              "created_utc": "2025-12-29 03:44:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwd6kjn",
          "author": "Brave-Hold-9389",
          "text": "Why not glm 4.6v?",
          "score": 2,
          "created_utc": "2025-12-28 14:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe8kv6",
              "author": "molbal",
              "text": "https://huggingface.co/unsloth/GLM-4.6V-GGUF",
              "score": 6,
              "created_utc": "2025-12-28 17:38:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwgqgky",
              "author": "yoracale",
              "text": "Glm4.6v is also updated",
              "score": 3,
              "created_utc": "2025-12-29 01:06:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfobor",
          "author": "gofiend",
          "text": "Thank you for your amazing work! \n\nOne request - I know that when you do your quantization you run some sort of quick validation test to ensure you got a functional quant. Could you please share those scores (whatever it is KL divergence, perplexity - anything) consistantly when you release a set of quants?\n\nI'm constantly trying to get a feel for what tradeoff to make around the Q3-Q6 range (64GB VRAM) and it would be amazing to have some sort of signal. I fully understand that \"real usability\" will be different and non-linearly related to the scores you report. \n\n(I also suspect, long term, across multiple model families, we'll be able to model \"real usibility\" differences starting from the scores + the model architecture).",
          "score": 1,
          "created_utc": "2025-12-28 21:46:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhi41k",
              "author": "yoracale",
              "text": "To be honest it is a lot of work to do it for every quant. In general we recommend using at least 2-bit for large quants and 3-bit for medium sized and 4-bit for small sized.\n\nWe did do Aider benchmarks for example DeepSeek-V3.1: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)\n\nAider is one of the top 3 benchmarks for real use-case benchmarks",
              "score": 2,
              "created_utc": "2025-12-29 03:47:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwhp74a",
                  "author": "gofiend",
                  "text": "Is there some limited version (say only K\\_M) that is possible?\n\nAlso I‚Äôm definitely not asking for accurate benchmarking (like the terrific Aider effort). I‚Äôve found that fairly small bits of text converge on perplexity etc. pretty quickly. I‚Äôm thinking of whatever 60-120 seconds of compute per under 96GB quant can provide in terms of signal. Basically automate a quick perplexity / KL estimate and auto publish. Even better if it is whatever sense check you use to make sure the quant process didn‚Äôt fail.",
                  "score": 1,
                  "created_utc": "2025-12-29 04:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwiusop",
                  "author": "thedarkbobo",
                  "text": "Awesome, imagine we had a table for filter model scores i.e. coding then sort by size with different quants.",
                  "score": 1,
                  "created_utc": "2025-12-29 10:22:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhxz0h",
          "author": "Aggressive-Dingo-993",
          "text": "Wait, is this why I get dramatically improved performance when I re-tested GLM 4.5 air yesterday????",
          "score": 1,
          "created_utc": "2025-12-29 05:31:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwicypz",
              "author": "yoracale",
              "text": "Yes could definitely be possible",
              "score": 1,
              "created_utc": "2025-12-29 07:36:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxf4q10",
          "author": "No_Art8851",
          "text": "glm-4.6v-flash/GLM-4.6V-Flash-Q8\\_0.gguf bullshit still not working in lm studio with vision",
          "score": 1,
          "created_utc": "2026-01-03 11:00:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfmpjh",
              "author": "yoracale",
              "text": "Did you try llama.cpp? works for me",
              "score": 1,
              "created_utc": "2026-01-03 13:18:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q09435",
      "title": "Qwen-Image-2512 is released! New SOTA text-to-image model. üíú",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/9kixwe5weiag1.png",
      "author": "yoracale",
      "created_utc": "2025-12-31 09:37:57",
      "score": 118,
      "num_comments": 11,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1q09435/qwenimage2512_is_released_new_sota_texttoimage/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwxx5c6",
          "author": "AdPristine1358",
          "text": "Awesome work releasing quantized versions the same day it's launched! \n\nQuestion - How is quality of Q2 or Q3? \n\nWondering if I could use with 8-10GB VRAM",
          "score": 2,
          "created_utc": "2025-12-31 17:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0zliw",
              "author": "yoracale",
              "text": "You could but it's best to use at least Q3. Do you have RAM as well? You can do offloading",
              "score": 2,
              "created_utc": "2026-01-01 03:36:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwyr92",
          "author": "Sensitive_Sweet_1850",
          "text": "how many parameters?",
          "score": 1,
          "created_utc": "2025-12-31 14:05:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwzksc",
              "author": "yoracale",
              "text": "It's around 20B parameters",
              "score": 2,
              "created_utc": "2025-12-31 14:10:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwx171z",
          "author": "Dramatic-Rub-7654",
          "text": "Is it compatible with stable-diffusion.cpp?",
          "score": 1,
          "created_utc": "2025-12-31 14:19:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0zfqg",
              "author": "yoracale",
              "text": "Yes it is, we made a guide just for it here: [https://unsloth.ai/docs/models/qwen-image-2512/stable-diffusion.cpp](https://unsloth.ai/docs/models/qwen-image-2512/stable-diffusion.cpp)",
              "score": 3,
              "created_utc": "2026-01-01 03:35:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ygls",
          "author": "soyelborja",
          "text": "hi thanks for this, but im having a error when runing the workflow  \n  \n TextEncodeQwenImageEditPlus\n\nmat1 and mat2 shapes cannot be multiplied (784x1280 and 3840x1280)\n\nwhat could be wrong?\n\nthanks",
          "score": 1,
          "created_utc": "2026-01-01 17:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiiiwl",
          "author": "StillNearby",
          "text": "notebook?",
          "score": 1,
          "created_utc": "2026-01-03 21:50:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkqzyp",
              "author": "yoracale",
              "text": "We might one in the near future",
              "score": 1,
              "created_utc": "2026-01-04 05:17:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxkcie4",
          "author": "Vusiwe",
          "text": "If not GGUF, how much VRAM does the normal 2512 use?",
          "score": 1,
          "created_utc": "2026-01-04 03:44:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkr3b3",
              "author": "yoracale",
              "text": "We uploaded FP8 which will use around 21GB: [https://huggingface.co/unsloth/Qwen-Image-2512-FP8/tree/main](https://huggingface.co/unsloth/Qwen-Image-2512-FP8/tree/main)",
              "score": 1,
              "created_utc": "2026-01-04 05:18:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6qtdj",
      "title": "Qwen-Image-2512 GGUF updated with higher quality + new 4-bit + FP8",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q6qtdj/qwenimage2512_gguf_updated_with_higher_quality/",
      "author": "yoracale",
      "created_utc": "2026-01-07 20:33:41",
      "score": 64,
      "num_comments": 6,
      "upvote_ratio": 0.99,
      "text": "Hey guys, we recently updated the q2, q3 and q4_k_m variants for higher quality results by emphasizing more important layers: https://huggingface.co/unsloth/Qwen-Image-2512-GGUF\n\nWe also uploaded new Bitsandbytes dynamic 4-bit and FP8 quants which can be run directly in Hugging Face diffusers.\n\n4-bit: https://huggingface.co/unsloth/Qwen-Image-2512-unsloth-bnb-4bit\nFP8: https://huggingface.co/unsloth/Qwen-Image-2512-FP8\n\nIn the future we intend to upload other formats such as nvfp4. Let us know what other formats we should upload.\n\nThank you! üôè",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q6qtdj/qwenimage2512_gguf_updated_with_higher_quality/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nyalaqq",
          "author": "DuckyBlender",
          "text": "Can‚Äôt wait for nvfp4 especially for LLMs!",
          "score": 3,
          "created_utc": "2026-01-07 23:08:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycg8dg",
          "author": "ftlaudman",
          "text": "Very interested in what nvfp4 can offer. Thank you!",
          "score": 2,
          "created_utc": "2026-01-08 05:10:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyeihjj",
          "author": "Glum-Atmosphere9248",
          "text": "Are there image comparisons of the quants?¬†",
          "score": 1,
          "created_utc": "2026-01-08 14:26:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyelvlg",
              "author": "yoracale",
              "text": "Images are very subjective and hard to tell so currently not at the moment",
              "score": 1,
              "created_utc": "2026-01-08 14:43:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyl7ogg",
          "author": "yamfun",
          "text": "I saw some newer qe2511 gguf, are they also better?",
          "score": 1,
          "created_utc": "2026-01-09 13:03:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyli4r0",
              "author": "yoracale",
              "text": "Yes indeed they are. And z image turbo",
              "score": 1,
              "created_utc": "2026-01-09 14:01:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q7djls",
      "title": "Run Qwen-Image diffusion Guide update!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/tn57f7xg05cg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-08 14:42:40",
      "score": 60,
      "num_comments": 3,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1q7djls/run_qwenimage_diffusion_guide_update/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyfysse",
          "author": "Amazing_Athlete_2265",
          "text": "Oh nice! Appreciate the stable diffusion.cpp guide! Going to play around with this today. Chur!",
          "score": 2,
          "created_utc": "2026-01-08 18:21:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygokjv",
          "author": "loadsamuny",
          "text": "this is great, thank you really useful. \n\nThere are 2 things I struggle with qwen image - running the fp8 scaled version (the comfy one that runs super fast!) using diffusers, is it even possible? \n\nAnd merging a lora into the main .dit and generating a gguf from it to run in sd.cpp, any help with either of those would be awesome!",
          "score": 2,
          "created_utc": "2026-01-08 20:14:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyh6bo7",
              "author": "yoracale",
              "text": "Unfortunately FP8 doesn't work in diffusers as far as I'm aware of.\n\nAnd feet suggestion thank you",
              "score": 1,
              "created_utc": "2026-01-08 21:33:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pya97t",
      "title": "Progressive LoRA Merging - complete model identity replacement on consumer hardware",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1pya97t/progressive_lora_merging_complete_model_identity/",
      "author": "TastyWriting8360",
      "created_utc": "2025-12-29 02:38:39",
      "score": 45,
      "num_comments": 43,
      "upvote_ratio": 0.8,
      "text": "I'm here to democratize model creation. After 3+ months of development, I've figured out how to **completely replace a model's weights while preserving the architecture**.\n\nThis means you can take Qwen3, Llama, or any open model - reuse the millions of dollars they spent on pretraining - and replace the identity for a few bucks on consumer hardware.\n\n**How it works:**\n\n1. Train a LoRA adapter on your data\n2. Merge the LoRA into the base model permanently (in BF16, not quantized)\n3. The merged model becomes your new base\n4. Apply a **fresh** LoRA and train again\n5. Repeat\n\nEach merge **dissolves** the adapter into the weights. The next cycle starts with fresh random LoRA weights on the new base. This is not stacking - it's sequential replacement.\n\n**Why this works:**\n\nWe deliberately use catastrophic forgetting to erase the base model's identity while preserving your injected patterns through dataset mixing (50% new data / 50% historical).\n\nAfter enough cycles, the model stops saying \"I am Qwen\" and fully adopts your identity, reasoning style, and knowledge.\n\n---\n\n**Resources:**\n\n- Paper & Code: [https://huggingface.co/hitonet/progressive-lora-merging](https://huggingface.co/hitonet/progressive-lora-merging)\n- GitHub: [https://github.com/antibitcoin/progressive-lora-merging](https://github.com/antibitcoin/progressive-lora-merging)\n- Working demo: [https://chat.hitonet.com](https://chat.hitonet.com) (try Hito-small - it was Qwen 8B)\n- Example model: [https://huggingface.co/hitonet/hito-1.7b](https://huggingface.co/hitonet/hito-1.7b)\n\n---\n\n**FAQ:**\n\n**Q: Isn't this just LoRA stacking? Won't errors compound like (a+b)¬≤ √ó (a+b)¬≤?**\n\nNo. After each merge, the LoRA adapter is **dissolved** into the base weights via `merge_and_unload()` and ceases to exist. The next cycle initializes a **fresh LoRA with random weights**. There is no stacking. After 100 cycles, you have ONE model with 100 sequential weight modifications, not 100 stacked adapters.\n\n**Q: Won't quantization errors accumulate?**\n\nNot if you merge correctly. We train in 4-bit/8-bit (memory efficient), but merge in **BF16 full precision** (error-free). This asymmetric precision prevents error accumulation.\n\n**Q: Won't this cause catastrophic forgetting?**\n\nYes - that's the goal. We selectively forget the base model's identity while preserving yours through dataset mixing.\n\n**Q: How is this different from full fine-tuning?**\n\nSame result, 10-100x cheaper. Full fine-tuning needs 4-8x A100s. This runs on a single 24GB GPU.\n\n**Q: How many cycles until identity replacement?**\n\n- 25 cycles: Noticeable shift (~40%)\n- 50 cycles: Fundamentally different (~70%)\n- 100 cycles: Near-complete replacement (~93%)\n\n---\n\n**Citation:**\n\n    @article{drissi2024bodysnatching,\n      title={Body Snatching: Complete Model Identity Replacement via Progressive LoRA Merging},\n      author={Drissi, Ouissam Said},\n      year={2024},\n      url={https://github.com/antibitcoin/progressive-lora-merging}\n    }\n\n---\n\nThe math, code, and working models are all public. Try it before theorizing why it can't work.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1pya97t/progressive_lora_merging_complete_model_identity/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwhkp1g",
          "author": "zmarty",
          "text": "I am confused how this does not destroy the model and its capabilities. Have you run the standard benchmarks?",
          "score": 7,
          "created_utc": "2025-12-29 04:03:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhqols",
              "author": "TastyWriting8360",
              "text": "**Why it doesn't destroy capabilities:**\n\nThe architecture stays intact - attention patterns, layer structure, everything that makes the model \"work\" is preserved. What changes is the *\\*content\\** of the weights, not the *\\*structure\\**.\n\nThink of it like this: the model learned \"how to reason\" during pretraining. That's baked into the architecture and weight relationships. We're replacing *\\*what\\** it reasons about and *\\*how it identifies itself\\**, not its ability to reason. The key is the dataset.\n\nIf you train on garbage, you get garbage. If you train on high-quality synthetic data with proper reasoning chains, the model retains (and can even improve) its capabilities while adopting a new identity.\n\n**Benchmarks:** Honestly? No, I haven't run standard benchmarks. I built this for production use, not for leaderboard chasing. \n\nWhat I can tell you: - It works in my production environment daily\n\n \\- The models reason coherently, follow instructions, and don't hallucinate more than base models\n\n \\- You can try it yourself: [https://chat.hitonet.com](https://chat.hitonet.com) (Hito-small was Qwen 8B) If someone wants to run benchmarks, the models are public: - [https://huggingface.co/hitonet/hito-1.7b](https://huggingface.co/hitonet/hito-1.7b) I'd actually be curious to see the results. But \"works in production\" matters more to me than MMLU scores.",
              "score": 0,
              "created_utc": "2025-12-29 04:40:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhhbzs",
          "author": "vichustephen",
          "text": "Sorry I'm noob but can't we just use DPO to change the identity? Why do all this ?",
          "score": 6,
          "created_utc": "2025-12-29 03:42:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhjcom",
              "author": "TastyWriting8360",
              "text": "\nDPO changes *preferences* (what the model prefers to say), not *identity* (what the model fundamentally is).\n\n**DPO:**\n- Trains the model to prefer response A over response B\n- The base model's knowledge, reasoning patterns, and self-concept remain intact\n- Qwen + DPO = Qwen that prefers certain outputs\n\n**Progressive LoRA Merging:**\n- Rewrites the actual weights over many cycles\n- The base model's identity is progressively erased and replaced\n- Qwen + PLM = Not Qwen anymore\n\nThink of it this way:\n- DPO is like teaching someone to give different answers\n- PLM is like replacing the person entirely\n\nDPO is great for alignment and steering. But if you want the model to genuinely *be* something else - different reasoning style, different knowledge, different self-identity - you need to replace the weights, not just the preferences.\n\nAlso, DPO still requires significant compute for large models. PLM runs on a single 24GB GPU.",
              "score": 3,
              "created_utc": "2025-12-29 03:54:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwhpjrk",
                  "author": "vichustephen",
                  "text": "Though it sounds interesting do you know what you are doing? I smell chatgpt ahhh reply. Also I've done lots of lora fine-tuning in 6gb vram using unsloth",
                  "score": 9,
                  "created_utc": "2025-12-29 04:33:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwijs0t",
                  "author": "__Maximum__",
                  "text": "I feel like you contradict your other comment here. Which is it, the models attention patterns stay the same and only the identity changes, which is equivalent to DPO or the model completely changes (different reasoning style, different knowledge, different self-identity), which means you should run benchmarks (your own or public) to see if it degraded or not.\n\nI appreciate the open source nature of your work, don't get me wrong, and whatever the outcome is, you found a way to fine-tune a model with way less memory, trading of time.",
                  "score": 4,
                  "created_utc": "2025-12-29 08:39:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwievg1",
          "author": "Ok_Appearance3584",
          "text": "Duh? This is called finetuning.",
          "score": 3,
          "created_utc": "2025-12-29 07:53:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwifmdc",
              "author": "TastyWriting8360",
              "text": "Fine-tuning adds a layer on top. The base model is still there underneath - that's why fine-tuned models still say \"I am Qwen\" or revert to base behavior on edge cases.  \n  \nThis erases and replaces the base model. After 100 merge cycles, there is no \"Qwen underneath\" anymore. The weights that made it Qwen are gone, overwritten by sequential modifications.  \n  \nIt's the difference between putting a new coat of paint on a car vs replacing the engine, transmission, and interior piece by piece until nothing original remains.  \n  \nSame architecture, completely different model. That's not fine-tuning, that's replacement.  \n  \nBut you need a good quality dataset. Check the repo for details. Garbage data = garbage model, same as any training method.",
              "score": -1,
              "created_utc": "2025-12-29 08:00:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwim9st",
                  "author": "Ok_Appearance3584",
                  "text": "That's incorrect, the end result does not change if you merge lora to base model. Lora is just a diff layer to the base. During inference it's behaving exactly as if the lora was merged to the base model.¬†\n\n\nYou can merge your lora back to base if you wish to release a standalone finetuned model. If you want to swap different loras during runtime, it's better to have one big base model and multiple adapters as opposed to multiple big models.\n\n\n\nYour method could be streamlined to simply finetune one lora for a hundred epochs.\n\n\nBut as a concept, yes, if you finetune a model (as opposed to designing your own + training from scratch) you get to keep the benefits of large scale pretraining and engineering and make it your own via finetuning.",
                  "score": 6,
                  "created_utc": "2025-12-29 09:02:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwj3ts0",
          "author": "Agreeable-Market-692",
          "text": "Congratulations on \"discovering\" iterative fine-tuning with extra steps... \n\nYou fell prey to one of the classic blunders, you believed ChatGPT when it glazed you. You have achieved dimensional shifting Star Seed! If you thought ChatGPT was going to make you smarter or make you sound smarter you played yourself. It in fact made several people here dumber and made the people who actually read papers annoyed. Your post is pure spam.\n\nThe data mixing you described is just replay based continual learning, not very novel at all and is well-researched. \n\nThe percentages (40%, 70%, 93%) are almost certainly made up. There's no citation, no metric definition, no evaluation methodology. What does \"93% identity replacement\" even mean?\n\nThese are not the only problems with this post but these are the ones I cared to type out before I got too annoyed to keep going.\n\nIf you have some kind of ADHD issue that means you can't focus long enough to get through papers or get motivated enough to start get it handled now and you'll live a much better life, medication is effective. You are responsible for your own actions though, you can't lean on excuses in the real world.   \n  \n\"Independent researcher\" is a fun way to say unemployed. And if you keep posting stuff like this you're gonna stay \"independent\". Interviewers can see through the pretending and would be much harsher than I have been here. If you did this kind of thing in an academic setting you'd rightfully be trotted down to the dean's office to explain yourself.",
          "score": 6,
          "created_utc": "2025-12-29 11:43:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj9mgx",
              "author": "vichustephen",
              "text": "All his replies are from chatgpt ü§£ü§£",
              "score": 4,
              "created_utc": "2025-12-29 12:29:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjag9q",
                  "author": "Agreeable-Market-692",
                  "text": "he also followed me to one of my posts in another sub, classic narcissistic injury",
                  "score": 2,
                  "created_utc": "2025-12-29 12:35:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj45n3",
              "author": "TastyWriting8360",
              "text": "How is that related to my research or work? are you speaking about your own experiance, did that happen to you? I am sorry but I spent 3 months to come up with this even if I summerized it with AI, that does not matter. your post is very toxic, unethical, unprofessional and not related to the topic, you are welcome to run any benchmarks everything is public.",
              "score": 1,
              "created_utc": "2025-12-29 11:45:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwj4ham",
                  "author": "Agreeable-Market-692",
                  "text": "I actually work in the industry and your post smacks of slop. Take it personally if you want, but I'm just telling you what other experienced people are going to think but may or may not say to your face. Address my critiques or you're just conceding what you did here.",
                  "score": 4,
                  "created_utc": "2025-12-29 11:48:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwj9qov",
                  "author": "vichustephen",
                  "text": "Bro this is your only reply others are chatgpt . I don't understand how people fell for your trap ü§£",
                  "score": 5,
                  "created_utc": "2025-12-29 12:30:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwjjpep",
                  "author": "Agreeable-Market-692",
                  "text": "Really pathetic behavior. You are simply outing yourself at this point. I feel secondhand embarassment for you.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:38:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuo2vg",
              "author": "TheRiddler79",
              "text": "On a side note, your entire premise hinges on your own failures and inability to carefully read more than just the post.\n\nIt's an equally stupid \"burn\" to pretend to know about LLM tuning, but also clown on someone using AI to make a clearly stated post.\n\nFinally, you claim the percentages are made up, but also can't prove they can't be true.\n\nIn summary, your entire post was dripping with amateur attacks and flawed logic based on your own personal shortcomings.",
              "score": 0,
              "created_utc": "2025-12-31 03:12:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvbzqe",
                  "author": "Agreeable-Market-692",
                  "text": "\"but also can't prove they can't be true\"\n\nthis is the lowest grade bait",
                  "score": 1,
                  "created_utc": "2025-12-31 05:53:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhq9m1",
          "author": "SpaceNinjaDino",
          "text": "Do you think that the planned progression should end on the most important as it will be the most influential?",
          "score": 2,
          "created_utc": "2025-12-29 04:38:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhr27o",
              "author": "TastyWriting8360",
              "text": "Good intuition, but I'd actually argue the opposite based on how the method works.\n\nEarly cycles are most influential for identity. The first 25-50 cycles cause the biggest drift from the base model. This is when the model is most malleable because you're breaking its original identity. Whatever you train early gets reinforced across all subsequent cycles.\n\nLater cycles are for refinement. By cycle 50+, the model has largely adopted your identity. Later training is fine-tuning your own model at that point, not fighting against Qwen/Llama anymore.\n\nMy approach:\n\nEarly cycles (1-25): Core identity, personality, self-concept. This is where you establish \"I am X, not Qwen\"\n\nMid cycles (25-50): Reasoning style, knowledge patterns, how it thinks\n\nLate cycles (50-100): Edge cases, specific behaviors, polish\n\nThink of it like painting. You lay down the base colors first, then add details. If you save your most important stuff for last, the earlier paint might show through.\n\nThat said, the 50/50 dataset mixing means important early data keeps getting reinforced throughout. So it's not like early stuff gets forgotten. It's more about what gets the most total exposure across all cycles.\n\nWhat's your use case? That might change the recommendation.",
              "score": 1,
              "created_utc": "2025-12-29 04:43:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkuf8i",
          "author": "sirfitzwilliamdarcy",
          "text": "You can usually achieve the same effect with more steps during fine-tuning. Would be interested in seeing Evals before and after this process though. It may have unique advantages.",
          "score": 2,
          "created_utc": "2025-12-29 17:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsun2y",
              "author": "TastyWriting8360",
              "text": "Yep one of the key advantages is new ways of reasoning, you are not stuck with the model default, you are not finetuning you are basically rewriting the model, SFT on a budget but more accurate, remember when we merge and dissolve the lora into the new base we actually do it on the full wieghts of the model not the quantized.\n\nthe following is nested reasoning my own invention have you seen sideways reasoning before?\n\nhttps://preview.redd.it/x21tgfqwqeag1.png?width=950&format=png&auto=webp&s=55de2f0ea77eebc32d6e5a3ea42541eb5c30d16a\n\nyou can test it out by downloading the hito 1.7b which was created from qwen3 1.7b I have trained full embeddings with my own custom tokens and applied styling to make it clear you can use it without downloading the model at [chat.hitonet.com](http://chat.hitonet.com) just make sure to signup cuz the non signed up version is bullshit demo. just remember to make good stuff, you have to forget about following tutorials, generate your own high quality dataset, real dataset not what everyone is using. I spent a good amount of time on mine,",
              "score": 1,
              "created_utc": "2025-12-30 21:18:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp2k8r",
          "author": "schlammsuhler",
          "text": "How to overfit",
          "score": 2,
          "created_utc": "2025-12-30 07:52:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwssndt",
              "author": "TastyWriting8360",
              "text": "It does not overfit read the paper. already solved that problem long ago, download the model, test it your self, try it on qwen3-0.6b.",
              "score": 1,
              "created_utc": "2025-12-30 21:09:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwha6cd",
          "author": "SlavaSobov",
          "text": "Funny you mention this, I've been doing exactly this for my companion AI. \n\nFirst was a LoRA adapter on the base personality, to become the ground truth. \n\nThen further fine tuning on other tokens and behavior I wanted to introduce.",
          "score": 2,
          "created_utc": "2025-12-29 03:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhao9e",
              "author": "TastyWriting8360",
              "text": "Finally I am happy to see a smart fellow adventurer can you please clarify that it works here for this people if you don't mind of course. [https://www.reddit.com/r/LocalLLaMA/comments/1py8yyw/r\\_progressive\\_lora\\_merging\\_complete\\_model/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1py8yyw/r_progressive_lora_merging_complete_model/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
              "score": 1,
              "created_utc": "2025-12-29 03:03:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhzqel",
          "author": "ZealousidealShoe7998",
          "text": "wait until people realize that are literally models out there that super small and efficient and could literally work great for very niche scenarios and it would take is just to train it a bit.   \nbeing a generalist takes lots of parameters but niche knowledge not so much.",
          "score": 1,
          "created_utc": "2025-12-29 05:44:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwidzyb",
              "author": "TastyWriting8360",
              "text": "Well said, Exactly! meanwhile people at r/LocalLLaMA  is down voting me saying it wont work, even tho I provided a working model, the math, the paper and the code lol, it was a mistake sharing there, this is the correct place to share this, people who actually technical and do finetuning and machine learning, not just users.",
              "score": 1,
              "created_utc": "2025-12-29 07:45:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi67an",
          "author": "ramendik",
          "text": "What do you mean by \"model identity\"?",
          "score": 1,
          "created_utc": "2025-12-29 06:37:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwidgfh",
              "author": "TastyWriting8360",
              "text": "The patterns that make the model \"itself\":\n\n\\- How it responds to \"who are you\" / \"what are you\"\n\n\\- Its default reasoning style and thought patterns\n\n\\- Built-in safety responses and refusals\n\n\\- Knowledge it considers core vs peripheral\n\n\\- Personality traits baked in during RLHF\n\nA fresh Qwen will say \"I am Qwen, developed by Alibaba\" and reason in a specific way. After PLM, it says \"I am \\[your model name\\]\" and reasons the way YOUR training data taught it.\n\nIt's not just changing a system prompt. The weights themselves no longer encode \"I am Qwen.\" That information is gone, replaced by your training data.\n\nIts still finetuning but the end result is a fully trained model not related to the original anymore.",
              "score": 1,
              "created_utc": "2025-12-29 07:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiggat",
          "author": "Thick-Protection-458",
          "text": "How is that different from ReLoRA?",
          "score": 1,
          "created_utc": "2025-12-29 08:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwih2ve",
              "author": "TastyWriting8360",
              "text": "ReLoRA is for pretraining from scratch. You start with random weights and want to train a new model efficiently. The problem it solves: full-rank pretraining is expensive, so use iterative low-rank updates to approximate full-rank training with less memory.  \n  \nReLoRA requirements:  \n\\- Warm start: 25% of training must be full-rank before switching to LoRA  \n\\- Jagged LR scheduler: learning rate resets after each merge to prevent divergence  \n\\- Partial optimizer reset: prune 99% of optimizer state by magnitude after each merge  \n\\- Multi-GPU setup: paper uses multiple A100s  \n\\- Tested up to 1.3B, attempts to scale beyond that were unsuccessful  \n  \nPLM is for identity replacement in pretrained models. You start with Qwen/Llama/etc and want to make it into YOUR model. The problem it solves: you want to leverage billions of dollars of pretraining but completely change what the model is.  \n  \nPLM requirements:  \n\\- No warm start: you inherit the pretrained weights as-is  \n\\- Standard training: no special schedulers or optimizer resets  \n\\- Single 24GB GPU: train in 4-bit, merge in BF16 on CPU  \n\\- Dataset mixing: 50% new / 50% historical to preserve your identity while erasing base  \n\\- Tested on 14B, scales higher with CPU offload for merge  \n  \nReLoRA: random weights ‚Üí efficient pretraining ‚Üí new model (up to 1.3B)  \nPLM: pretrained model ‚Üí identity replacement ‚Üí your model (14B+)  \n  \nSame mechanic, opposite directions, different scale.",
              "score": 2,
              "created_utc": "2025-12-29 08:13:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhdxzd",
          "author": "Ok-Adhesiveness-4141",
          "text": "Am most interested in this, is there anything on GitHub?",
          "score": 1,
          "created_utc": "2025-12-29 03:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhemjd",
              "author": "TastyWriting8360",
              "text": "Yes there is in the link",
              "score": 1,
              "created_utc": "2025-12-29 03:26:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhj2dl",
          "author": "ANTIVNTIANTI",
          "text": "omfgoddamn this sounds freaking amazing! MUCH LOVES!!!",
          "score": -1,
          "created_utc": "2025-12-29 03:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhji5r",
              "author": "TastyWriting8360",
              "text": "Thank you I am glad you like enjoy!",
              "score": 1,
              "created_utc": "2025-12-29 03:55:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q43rgc",
      "title": "üöÄ Introducing llcuda ‚Äì A Python wrapper for llama.cpp with pre-built CUDA 12 binaries (T4/Colab ready)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q43rgc/introducing_llcuda_a_python_wrapper_for_llamacpp/",
      "author": "waqasm86",
      "created_utc": "2026-01-04 22:31:48",
      "score": 39,
      "num_comments": 8,
      "upvote_ratio": 0.81,
      "text": "Hey Unsloth community! üëã\n\nI‚Äôve been working on a Python package called¬†`llcuda`¬†that makes GPU-accelerated inference with¬†`llama.cpp`¬†as easy as:\n\npython\n\n    import llcuda\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n    response = engine.infer(\"Explain quantum entanglement\")\n\n# üîß¬†What it does\n\n* **Automatic GPU detection**¬†‚Äì Optimized binaries for NVIDIA T4 (CUDA 12) and Colab.\n* **No compilation needed**¬†‚Äì Pre-built¬†`llama.cpp`¬†binaries downloaded on first run.\n* **Clean Python API**¬†‚Äì Load GGUF models (including Unsloth‚Äôs) and run inference in <5 lines.\n* **Hugging Face integration**¬†‚Äì Direct model downloads from HF Hub.\n\n# üß™¬†Why I built this\n\nI love¬†`llama.cpp`, but compiling it with CUDA in Colab is a hassle.¬†`llcuda`¬†automates everything so you can focus on¬†*using*¬†models, not building tools.\n\n# üöÄ¬†Live Demo in Colab\n\nCheck out this notebook where I run¬†**Unsloth‚Äôs Gemma 3 1B GGUF model**¬†on a T4 GPU:  \n[Open in Colab](https://colab.research.google.com/drive/...)\n\n  \nüì¶¬†**Links**\n\n* GitHub:¬†[github.com/waqasm86/llcuda](https://github.com/waqasm86/llcuda)\n* Release v1.2.2:¬†[Pre-built CUDA 12 T4 binaries](https://github.com/waqasm86/llcuda/releases/tag/v1.2.2)\n* Demo site:¬†[waqasm86.github.io](https://waqasm86.github.io/)\n\n# ü§î¬†Looking for feedback\n\nI‚Äôd love to know:\n\n* Does this simplify your inference workflow?\n* What other GPUs/architectures should I support?\n* Would integration with Unsloth‚Äôs fine-tuning pipeline be useful?\n\nThis is still early-stage, but I‚Äôm excited to share it with a community that values¬†**performance + accessibility**.\n\nLet me know what you think! üöÄ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q43rgc/introducing_llcuda_a_python_wrapper_for_llamacpp/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nxplicw",
          "author": "334578theo",
          "text": "I'd recommend tidying up the repo if you want people to take this seriously. It just looks like a vibe coded mess atm.",
          "score": 19,
          "created_utc": "2026-01-04 22:40:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpqgs5",
              "author": "waqasm86",
              "text": "Hello, thank you for the feedback. Kindly guide me what needs to be cleaned or organzied. I'll try to fix as many issues as possible.",
              "score": -8,
              "created_utc": "2026-01-04 23:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxqbkiu",
                  "author": "334578theo",
                  "text": "If you don‚Äôt know what‚Äôs wrong with it then go and look at some open source repos you respect and look how they are setup.",
                  "score": 5,
                  "created_utc": "2026-01-05 00:48:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxpnveg",
          "author": "KvAk_AKPlaysYT",
          "text": "I took a 5 minute look at the codebase, there's no way anybody would feel comfy trying to run this. I'd recommend a lot of cleanup. Good project though!",
          "score": 9,
          "created_utc": "2026-01-04 22:51:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpq5uj",
              "author": "waqasm86",
              "text": "Hello, thank for the reply. Kindly let me know what issues are needed to be resolved. I'll do it right away.",
              "score": -2,
              "created_utc": "2026-01-04 23:02:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxqn63i",
          "author": "mp3m4k3r",
          "text": "Might be worth giving [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python) as well!",
          "score": 3,
          "created_utc": "2026-01-05 01:50:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv2b6l",
              "author": "JayRoss34",
              "text": "Thats outdated, doesnt support newer models.",
              "score": 1,
              "created_utc": "2026-01-05 18:38:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxrunv8",
              "author": "waqasm86",
              "text": "Hello, thank you for the feedback. I have tired llama-cpp-python pip package. I had issues running that tool in Google Collab using Nvidia T4 GPU. Also,  I was interested in using llama-cpp-python with fastapi but it always broke. I am not creating another python wrapper for llama.cpp. I figured out a way to make my llcuda a backend cuda inference along with  unsloth python pip package. \n\nI am open to any feedback since I do want to make my python cuda tool strengthened for local llama solutions.",
              "score": 1,
              "created_utc": "2026-01-05 06:10:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q8m6qu",
      "title": "Unsloth x OpenEnv RL Challenge",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/apgmifhjdecg1.png",
      "author": "danielhanchen",
      "created_utc": "2026-01-09 22:18:22",
      "score": 29,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q8m6qu/unsloth_x_openenv_rl_challenge/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q59y9p",
      "title": "So, am I just too stupid for unsloth?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q59y9p/so_am_i_just_too_stupid_for_unsloth/",
      "author": "SingleServing_User",
      "created_utc": "2026-01-06 05:24:51",
      "score": 17,
      "num_comments": 58,
      "upvote_ratio": 0.71,
      "text": "**EDIT**: I just want to say thank you to everyone who took the time to explain some of these things. Some things, I was mostly complaining that the guides tend to assume everyone knows things that most people don't use, which I feel like needlessly raises the barrier of entry for people who might want to get into this sort of thing. Like if you wanted to learn how to change your car's oil, but everyone who knew how used tons of jargon and forgot to mention vital steps in the process when they explain it to you. Regardless, I appreciate how many people here *didn't* assume I was just bitching, and nonjudgmentally tried to help me out. Y'all are making Reddit a better place.\n\nSo, yes, I'm finally getting somewhere. I know a bunch of other people are having similar issues. I wondered if it would be helpful for those people if I wrote out something more specific, to fill in the blanks for people like me who may not be familiar with parts of this process. Obviously I have no issue writing, like, a *lot* of words, so if it would help someone else, I'm happy to do it.\n\n\\-------\n\nEvery \"beginner's guide\" I've seen assumes that you know a bunch of things that I simply don't know. Being a LAMP stack web developer, I thought I wasn't a complete idiot, but I've had to fight for every inch of progress towards using Unsloth. I just keep hitting dead end after dead end.\n\nIt's so incredibly frustrating to have these guides assume you know what every individual tool is. Like you have Docker installed, right? Of course you do. Only an idiot wouldn't already have Docker installed, right? And of course you know how to \\*use\\* Docker.  Because there's no such thing as someone interested in AI who \\*doesn't\\* know how to use Docker.\n\nI've had to stop and do hours of research and learning to just get through one half step of these damn guides. Now I have a bunch of shit installed that I don't even know how to use because either I gave up on pursuing a set of instructions, or their usage was simply never explained. Like, I installed unsloth, but apparently it's not actual software the way LM studio is, so now I'm sitting here trying to figure out how to even run the damn thing, and everyone keeps coming back to these damn notebooks. Which appear to be, as best as I can tell, code that I'm supposed to do \\*something\\* with? I guess? But which notebook do I even use for a model that I want to use from Huggingface? It isn't specifically one of the models named. And multiple parts of the unsloth guides imply that the notebooks must be used in Google Colab, whatever TF that is, and aren't required.\n\nEven then \"Beginner? Start Here!\" guide on Unsloth is just massively unhelpful about this part. It skips straight from \"Unsloth Requirements\" to \"Inference and Deployment.\" I managed to figure out how to use the AI models, and can talk to it, and even used ngrok to allow me to access its API securely from another app. What I need to know is what the hell the step between \"Datasets Guide\" and \"Deployment\" even is. What are the notebooks for? Do I need them if I'm running this locally? HOW do I run anything locally?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q59y9p/so_am_i_just_too_stupid_for_unsloth/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nxyk8iq",
          "author": "Educational_Rent1059",
          "text": "A UI interface is in the works to my knowledge will be out soon. It will simplify things further, but AI training locally in general without any Cloud setup that is specifically designed to just let you press a button and train something with zero knowledge - is not as simple.\n\nThe Unsloth notebooks however shows you exactly all you need to start training your model - you basically just switch the datasets and tune the [hyperparameters](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide) (the values) in the notebook .\n\nYou still have to understand training, training loss, etc. \n\nLearning is part of the progress and value you get.\n\nDocker is not a requirement at all, you just do the pip install and ensure you have an environment set up to run everything. That‚Äôs how local development works, if you prefer cloud where you just click through an app or a software locally with a UX that‚Äôs of course different. But UX version is in the works you should join discord there‚Äôs a big helpful and active community there\n\nedit: I know the docs are getting reworked as well to make things better as you say it can be a mess at the moment",
          "score": 14,
          "created_utc": "2026-01-06 05:38:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxymg2e",
              "author": "SingleServing_User",
              "text": "Also, I just want to say, thanks for the response. I'm a little worked up because I've been chipping away at this for days, and I just boiled over in frustration when I realized I couldn't even figure out what a \"notebook\" is.",
              "score": 5,
              "created_utc": "2026-01-06 05:55:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxymtp8",
                  "author": "Educational_Rent1059",
                  "text": "Oh no worries it‚Äôs definitely a lot to take in and we‚Äôve all been there when we started. It‚Äôs a mess in general the entire echosystem of LLMs and different architectures and so much innovation in training methods etc.",
                  "score": 3,
                  "created_utc": "2026-01-06 05:58:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxymarm",
              "author": "SingleServing_User",
              "text": "I know it sounds like I can't figure it out unless there's a UI, but I swear I don't mind doing it without that... it's the only way to make things like ngrok or certain features from LM Studio run. I just wish I knew what the hell the commands would even be. Everything I've worked with so far comes with instructions on working with the software through Powershell or Bash if necessary. Unsloth, I have no idea where to even begin. Like I can't tell if I'm supposed to be doing things through Bash, or what. Like where do the notebooks even \\*go\\*? Am I cutting and pasting them somewhere? Or do I \\*have\\* to use Google Colab?\n\nI know that training is going to result in the model being, theoretically, better at what I want it to do, and worse at everything else. I don't mind the trial and error part of that. I just want to get to the actual training part.\n\nI'm super hesitant about using anything in the cloud because the whole point of setting up an AI locally was to keep certain information private. Basically I'm trying to train an uncensored model on the mechanics of a game, so that it can accurately roleplay as characters within that game. And since those characters can be violent, have sex, or swear... and because I kinda want to train it on dialogue from a popular IP... I guess I'd prefer to train it locally.",
              "score": 1,
              "created_utc": "2026-01-06 05:54:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxyomf6",
                  "author": "Ok_Appearance3584",
                  "text": "Jupyter notebooks are a popular method of running python like you'd run bash. You run unsloth using python directly. Of course you can set up python scripts but unsloth only offers example notebooks. These are to be run cell by cell. It's maybe strange if you don't have any data science background. But notebooks are basically python REPL. You can do the same by opening venv in bash, running command python and pasting the code line by line",
                  "score": 4,
                  "created_utc": "2026-01-06 06:12:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxymm6a",
                  "author": "Educational_Rent1059",
                  "text": "I understand, I think you should join the Discord channel like I mentioned the community is very helpful and active and help you on the way, myself included!",
                  "score": 1,
                  "created_utc": "2026-01-06 05:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxyke3g",
          "author": "qustrolabe",
          "text": "Feels like LLMs could teach you a lot about everything that confuses you",
          "score": 18,
          "created_utc": "2026-01-06 05:39:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxymt2e",
              "author": "SingleServing_User",
              "text": "I can't roll my eyes hard enough at this... did you think there was a chance on God's green Earth that I haven't been desperately trying to get an AI to distill instructions for me? It's been slightly batshit, really. It gives conflicting solutions regularly because the information it has is conflicting. For example, one of the (many) guides I tried to slog through said that I should install the Nvidia Container Toolkit. Nothing the AI said, or that its source data said, told me that I couldn't do that on Windows... not without installing WSL, anyway. So now I'm installing that, I think. I'm not even sure because I used the command \"wsl --install\" and Bash has been stuck on \"This might take a while\" since before I wrote this post.",
              "score": -4,
              "created_utc": "2026-01-06 05:58:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny01joy",
                  "author": "Ok_Technology_5962",
                  "text": "I hear you. I'm also new like 5 months ago got Linux. Had to ask opus models for help. That's the only model and I mean only model that will help you. That 25 bucks a month or you can open router it but the cost catches up fast on open router. I would do one month subscription and learn like crazy. That's what I did",
                  "score": 3,
                  "created_utc": "2026-01-06 13:08:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny0agcf",
                  "author": "doradus_novae",
                  "text": "The problem is windows",
                  "score": 1,
                  "created_utc": "2026-01-06 13:59:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxysxn7",
          "author": "Independent-Fig-5006",
          "text": "Installing unsloth in windows is a very difficult thing, I gave up too. Installing it in wsl is possible, but I had some problems with it there too due to lack of memory for temporary files.\n\nYou are definitely not an idiot because you failed.\n\nUnsloth notebooks have one big advantage, it is possible to run them in google colab (https://colab.research.google.com/).\n\nIf you have a google account, you can run it in a linux machine in google cloud. The installation works there without any problems. It has one more advantage, for a certain period of time which is not completely stable, you can run your program on a nvidia T4 gpu which has 16 Gb vram. Just when loading colab, next to it there is an arrow, when you click on it and select the option to change the runtime environment type, you can select gpu T4. And what is a notebook? Well, it is jupyter notebook (https://jupyter.org/). A very interesting thing it can do is that it preserves changes from all those that were run before it, which is not possible with regular scripts. If you need anything, just ask. And I will try to answer, I also consider myself a beginner :-)",
          "score": 4,
          "created_utc": "2026-01-06 06:48:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5vlc8",
              "author": "SingleServing_User",
              "text": "Thank you for telling me that I am not, in fact, an idiot. Sometimes that's all a person needs to hear LOL\n\nbut, I'm not sure I failed. I'm nothing if not stubborn. I took a break from it overnight, and I'm back at it now. I made pretty significant progress. I'd be happy to share whatever I learned, if it's at all useful for you. \n\nThough, you're correct that RAM is going to end up being the real bottleneck one way or another. I got my computer right before AI \\*really\\* exploded in the last year, so the GPU wasn't as expensive as it would be now, and I actually have 6gb of dedicated vram from the card, 16 gb of shared, and another 16gb through the onboard gpu, so I've been able to run decent enough AI models, I guess. I haven't tried training yet, though, and that's gonna be the real test.\n\nPS, thanks for explaining the notebook thing. It seems like they're really similar to, like, docker images? At least in concept. I dunno, though. I just like my nice, quiet LAMP stack web dev bubble. All I wanted to do here was train an AI to understand a video game well enough to RP for the characters better \\*sob\\*",
              "score": 1,
              "created_utc": "2026-01-07 07:41:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyct7kf",
                  "author": "Independent-Fig-5006",
                  "text": "Docker has nothing to do with jupyter notebooks. Jupyter notebook is good mainly for development. If an error occurs in a cell, you just need to kill it and not run all the previous ones. Only the one you fixed and the ones that follow.\n\nDocker packages the application so that it has exactly the same environment when running, so you can run it on any PC. That's the theory, in reality, some things from the host PC are reflected in the docker container, mainly the cpu architecture.\n\n\n\nIf I go back to your main post. It is not necessary to run notebooks in colab, it is possible to run them elsewhere, but it is much simpler. You can run them locally, but either you have to convert them to python scripts in gui colab or I have to have jupyter notebook installed.\n\n\n\nAnd the Nvidia Container Toolkit that artificial intelligence advised you is not nonsense, it is the official way to get nvidia gpu into docker. There are other ways, but this is a more unreliable way. Docker cannot be run on non-server Windows without using WSL or virtualization. On regular Windows, the only official way to run it is docker desktop, which runs containers in WSL anyway and installs a lot of things that you don't need and just take up disk space.\n\n\n\nIt's actually much easier to install unsloth on Linux or WSL. Even though unsloth supports Windows, the first two installation methods listed in the manual (https://unsloth.ai/docs/get-started/install/windows-installation) use WSL. To install directly on Windows, you need to have Visual Studio properly configured for compiling C in Windows. I think that in the basic settings in WSL there is not enough space for temporary files, because /tmp is mapped to RAM. But the manual doesn't say anything like that. It's been a while since I tried it, so it's possible that this is now working by default.",
                  "score": 1,
                  "created_utc": "2026-01-08 06:45:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxyjf0j",
          "author": "uber-linny",
          "text": "Can relate. I'm looking at fine-tune a model for program management... And my current plan is to let Google studio walk me through all the required steps \nApart from that I have no idea",
          "score": 3,
          "created_utc": "2026-01-06 05:32:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyn5au",
              "author": "SingleServing_User",
              "text": "If that's an option, I'd accept it, but so far, I haven't made much progress on getting unsloth to work with AI assistance. Even the AI can't tell you much if it was trained on incomplete docs.",
              "score": 3,
              "created_utc": "2026-01-06 06:00:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxypsrj",
                  "author": "uber-linny",
                  "text": "Google studio AI has a pretty big context window and can handle urls to provide context. \n\nIt's helped me anyways",
                  "score": 2,
                  "created_utc": "2026-01-06 06:22:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxz405e",
          "author": "MikeLPU",
          "text": "Welcome to 2026. Unfortunately if you don't know things, you are way behind others.",
          "score": 3,
          "created_utc": "2026-01-06 08:29:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5w05v",
              "author": "SingleServing_User",
              "text": "I know tons of things, just not these specific things.",
              "score": 1,
              "created_utc": "2026-01-07 07:45:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "ny8h658",
              "author": "No-Consequence-1779",
              "text": "Reading is for dummies.¬†",
              "score": 1,
              "created_utc": "2026-01-07 17:33:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny2gcqc",
          "author": "No-Mountain3817",
          "text": "You‚Äôre not an idiot. You‚Äôre just discovering that a lot of AI ‚Äúeducation‚Äù is written like:    \n  \nStep 1: Draw the owl.  \nStep 2: Fine-tune a foundation model.\n\nAnd yes , the field is moving insanely fast, so most guide writers are documenting what *finally worked for them once* at 2am, not writing a curriculum for humans.\n\nYou‚Äôre basically a competent driver trying to install a turbo kit, and the manual starts with:\n\nassuming you already own a machine shop‚Ä¶ :)",
          "score": 3,
          "created_utc": "2026-01-06 20:03:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5wubz",
              "author": "SingleServing_User",
              "text": "Dude. That's a great analogy. Though, I think it's more like these manuals just assume you already not only own the shop and tools, you also are super familiar with the acronyms and specific meanings of words that mean one thing in english, and something completely different in AI. I swear to God, it's been like trying to read the instructions in Simlish. I'm having to look up every other word.\n\nstep 1: launch your gimmelgorb using the burgertits.\n\nstep 2: create a funglewort by gimmerglobbing (but not gimmelgorbing) in the shimmerla mwa.\n\nstep 3: ???\n\nstep 4: profit!",
              "score": 2,
              "created_utc": "2026-01-07 07:52:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "ny4h6s9",
              "author": "lookwatchlistenplay",
              "text": "lmao",
              "score": 1,
              "created_utc": "2026-01-07 02:04:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxz9xtm",
          "author": "momono75",
          "text": "You are trying very advanced categories. It's almost the same as trying to write PHP extensions even if the developer is a beginner for the PHP app.",
          "score": 2,
          "created_utc": "2026-01-06 09:26:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5wgf4",
              "author": "SingleServing_User",
              "text": "Bless you for finding a way to work in PHP. I've been writing in PHP since 2001, when I basically learned it on a dare... long story short, I was being competitive with an ex-boyfriend and since he was learning PHP, I figured, I'm gonna learn PHP. It's been more than 20 years but I do know that I learned more about it in 3 days than he had in 3 months, and I'm pretty sure it all just clicked and made sense to me. This has not. I'm still a PHP developer, but my current employer wants to switch our LAMP stack sites to a C# CMS, which I gotta say, I doooo noooot fucking understand. But it means I have to switch gears to C#, and so all this shit I'm learning about right now is going to be super helpful in terms of learning the new C# process. I mean, PHP, you can just open a text editor and write shit. There's no compiling or virtual images or even git required if you don't wanna. So, yeah, drinking from the firehose.",
              "score": 1,
              "created_utc": "2026-01-07 07:49:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny6dg87",
                  "author": "momono75",
                  "text": "With AI support, PHP apps aren't so harsh to maintain actually. Yeah, anyway, I think we should not be so serious about what we are using, because concepts are what we should focus on, because tools and language differences are okay to be covered by AI such as English to something translation. PHP was a modern language. Before that, we had to use C or C++ for CGI often.",
                  "score": 1,
                  "created_utc": "2026-01-07 10:26:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxywwjq",
          "author": "yoracale",
          "text": "No you're not too stupid for Unsloth but in general for finetuning you do need to have some dev experience beforehand otherwise it's going to be hard.\n\n\nIn general i would recommend starting with notebooks which you have to connect your Google account to to use Google colab and read the sections as the notebooks pretty much give you rundown of all the steps, use-cases, workflow, terminologies etc: https://unsloth.ai/docs/get-started/unsloth-notebooks",
          "score": 1,
          "created_utc": "2026-01-06 07:23:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny02ggg",
              "author": "-TV-Stand-",
              "text": ">you do need to have some dev experience beforehand\n\nBeginner's python course and some general knowledge on how computers work was enough for me to get it working. The documentation could be improved a lot though.",
              "score": 1,
              "created_utc": "2026-01-06 13:14:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny616ho",
                  "author": "SingleServing_User",
                  "text": "There really aren't many meaningful differences between Python and PHP code. I'm already a PHP developer. There are minor variations in syntax, but not enough to require a course on it. I mean, if you write in any language, you basically have a huge head start on any other language... especially when you're switching to languages with similar origins. Like both PHP and JavaScript are C-style languages, so I didn't need anyone to tell me how to use one when I already knew the other. I just sometimes have to look up function names, but that's mostly because I am old and my memory has gone to shit. (1 star, do not recommend)\n\nThe real issue is that PHP is an interpreted language with a really low barrier of entry... LAMP hosting is really cheap, so I rarely did my dev locally, and when I did, I could use something like WAMP with super minimal effort. I honestly don't even know what I'd use stuff like Python, Docker, or WSL for in my day-to-day career. But that doesn't mean I don't know how to write code. I also learned to build computers in, like, 1998, back when it was the only way po' folk could afford good shit.",
                  "score": 1,
                  "created_utc": "2026-01-07 08:32:22",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny5ysl9",
              "author": "SingleServing_User",
              "text": "I'm a full stack LAMP developer. I just began my career long before more modern tools like Docker were available. I also got away with being lazy about learning new technologies because I've been kept shockingly busy otherwise. Don't get me wrong, I learn new things every day as part of my job, but most of what I was doing was geared towards building websites in a language that you can edit in notepad. I simply never needed things like Python. It hasn't been relevant to my job. \n\nThough, when I've looked at code other people write, I always feel like Python looks like plain english compared to PHP. Like have you SEEN how messy and chaotic PHP is? anyway, I've worked in digital marketing building websites for 15 years... I've built several hundred websites, and the huge majority of them were custom. I also managed a WHM server for our client websites for several years. Like I said in my original post... I'd like to think I'm not a complete idiot. But most of these tutorials for unsloth skip huge steps, and if you don't already know certain things - and how would you learn, by the way? everyone has to at least once - then you will be completely and abjectly lost.",
              "score": 1,
              "created_utc": "2026-01-07 08:10:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxz03oe",
          "author": "EXPATasap",
          "text": "Man I'm too tired to say much, but I wanted to wish you luck and also just say how friendly the Unsloth forums seem to be LOL, god, after having been through so much.... errrr... today. LOL.. This was nice to read. Also appreciate Unsloth so freaking much, I'm about to get started on those notebooks‚Äîthere was a youtube video, I cannot remember it, I think it was tech with tim, maybe... He goes through the base training and dataset prep etc. with you OH yeah here's the title just searched \"[EASIEST Way to Fine-Tune a LLM and Use It With Ollama](https://www.youtube.com/watch?v=pTaSDVz0gok&pp=ygUVdW5zbG90aCB0ZWNoIHdpdGggdGlt)\", I watched it, seemed legit, homie helped me learn a bit of Python a few years back. Good luck fam! :D",
          "score": 1,
          "created_utc": "2026-01-06 07:52:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny61zn0",
              "author": "SingleServing_User",
              "text": "Thank you for trying to help me out. I also have appreciated how the huge majority of people didn't take offense at my clear frustration, and just said \"yep, it sucks.\" It's been very validating LOL\n\nThat being said, that video was one of a few that I tried to watch before giving up because they all skip a couple important steps. I get it, it's super easy to forget that not everyone knows what you do, and you don't want to be condescending. (This is the royal \"You\", not \\*you\\* you.) But it also means some people will be left behind. In this particular video, he went directly from showing a dataset in JSON (which I'm familiar with as a web dev, but I have to assume most people will not be) to showing a notebook full of code with zero context on where this code is even gonna go. Look at 5:50 minutes, and hopefully you'll see what I mean. It's entirely possible he goes on to explain it elsewhere, but I haaaaaate learning from videos as it is for a few reasons and honestly I just got frustrated. Plus, I was really, really hoping to avoid Google Colab. I didn't even know until tonight what Jupyter Labs was to Google Colab, btw. All the info out there is such a mess, even the AIs I've been asking keep hallucinating wild answers.",
              "score": 2,
              "created_utc": "2026-01-07 08:39:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzbspr",
          "author": "Medium_Chemist_4032",
          "text": "The reason we don't do it, is mostly the maintenance burden.\n\nWe could write guides that work on a just installed Ubuntu server for example. Guide each step on a happy path from installation to first finetune.\n\nThis kind of guides is a bitrot magnet unfortunately. Today it's docker, but maybe the company behind it decides it time to cash out and start introducing extortion rates on a newly pivoted license. This has historically happened a lot of times already. Typical community reaction is a fork. Podman would probably take the lead after.\n\nSo you have to rewrite each guide to install, configure and use podman instead.\n\nThen there's also responsibility for the choices you make. For the purpose of finetuning, many defaults fit well. Perhaps you want to configure the logging system to at least introduce file limits and rolling. So, should that guide focus on that instead? It's just another responsibility that drains time.\n\nReal world solution for such \"guide scope creep\"? Out of scope. Focus on one thing only and you can spend time working on finetuning and not rewriting every sample compose file to remove the \"version: 2.1\" attribute, because it's deprecated. Or that you shouldn't run docker-compose, but docker compose instead.\n\nThat's the reason why tech companies have a dedicated tech writing department.¬†",
          "score": 1,
          "created_utc": "2026-01-06 09:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny641x2",
              "author": "SingleServing_User",
              "text": "I totally understand why it would be ludicrous to write tutorials on every eventuality. I just wish there were, like, any that broadly covered the process... rather than skipping over really important steps that they just sorta assume you know already because, like, the author knows. Like how tutorial videos for datasets just assume you know JSON. I mean, in this case, I \\*do\\* know JSON because I've worked with many, many APIs over the years... but I can imagine that many people just give up at that point, the same way I gave up on trying to understand what I was supposed to do with a notebook, or even find the notebook that applies to my situation.\n\nAt a previous job with a digital marketing company, I had become so very specifically skilled at the type of web dev that we did, that my bosses were worried I was irreplaceable. (So was I, I didn't have a real vacation for 10 years.) So they asked me to write a fucking \\*Wiki article\\* on how to build, update, troubleshoot, and fix websites, and the hosting they're on, across an indefinite number of DNS/email/hosting/site customization setups. They were like \"Come on, how hard can it be? It's definitely not more complicated than SEO.\" Anyway, the point is, I totally get why it's not reasonable to expect a tutorial to cover everything.\n\nBuuuut surely someone, somewhere could've written down something explaining what running unsloth looks like in the first place. I'm about to try some things (when I'm tired of writing novels on reddit, I guess) that I believe will work, but it took forever to get to this point. Like their \"[Beginner Tutorial](https://unsloth.ai/docs/get-started/fine-tuning-for-beginners)\" goes from \"Create Your Dataset\" to \"System requirements\" to \"Deploying your newly trained model!\" It's completely missing the part where you train the damn thing.\n\nTL;DR: If I get this all figured out - and I do believe I will - I'm thinking of writing some tutorials to help out people like me. Obviously I can't cover everything ever, but I could at least point them in the right direction for specific pieces. I'm sure everyone can tell by now that I have no problem rattling off 3000 words in a single Reddit comment, so I want to try to put that to good use by helping others avoid my agony. \n\nBy the way, I had never heard the term \"bitrot magnet\" so I googled it, and I think you might mean tech debt instead? anyway, the AI overview would like you to know that you shouldn't use tools with magnetic tips in your computer lol",
              "score": 1,
              "created_utc": "2026-01-07 08:59:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny0jtww",
          "author": "WolfeheartGames",
          "text": "Notebooks are very popular in academics, so they're very popular in AI. They are great ways to hack away at code. It's just a bunch of code blocks you can execute as you wish. \n\nIf you're running local though you can just keep a dot py.\n\nPip install unsloth is all it takes on Linux. You shouldn't be training on windows. Windows has vram fragmentation bugs.",
          "score": 1,
          "created_utc": "2026-01-06 14:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0mywq",
          "author": "blue_marker_",
          "text": "I‚Äôll be honest, I‚Äôm surprised there are still LAMP stack developers but even more surprised people are still running LAMP on hosts which are not containerized. \n\nThis is software engineering. If there‚Äôs a tool you don‚Äôt know, you‚Äôre going to have to teach yourself about it. Take the time to learn incrementally. If you‚Äôre having trouble installing and running Docker (arguably ubiquitous technology for 8 years now), how can you reasonably expect to fine tune cutting edge models?",
          "score": 1,
          "created_utc": "2026-01-06 15:05:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6oxca",
              "author": "SingleServing_User",
              "text": "Allllll righty, let's do this. I've talked to plenty of Dunning-Kruger-suffering brogrammers who are so used to believing the world revolves around them that they forget other jobs, needs, and interests exist... but sure, I'll take the time to correct your, um, misconceptions.\n\nYes, my guy, almost 90% of websites still run on Linux-based hosting and more than 70% are using PHP. I doubt you know enough about LAMP for this to be a joke about the LAMP acronym becoming increasingly inaccurate, so it seems you're just genuinely so self-absorbed and clueless that you think most people/businesses can afford the tens of thousands of dollars in various licensing fees to use the Microsoft stack... or worse, maybe you think MEAN stack was ever actually a thing... or that Ruby on Rails was in demand. Please, feel free to prove me wrong. I'm curious what you think that 80% of the internet was using, if it wasn't LAMP.\n\nFacts are free. *Nothing* in web development is as \"ubiquitous\" as LAMP. Python is used on around 2% of websites, IIS is used by only about 3.5% of websites, .NET is 5%, and Node/React combined are 12%. Node and React are still often used on LAMP stack websites regardless. In fact, WordPress comes packaged with React. Speaking of WordPress, that's the worst news of all for dude-bros who think that programming is an elite, competitive sport that they need to win by bullying others, in order to prove they're (somehow) manly men - WordPress is used on 43% all websites, and it's not going anywhere. It's being used on government sites like the White House, high-profile corporate sites like Nike, Disney, and Sony, and (of course) by huge media outlets like Time Magazine.\n\nSo, yeah, you can be smug if you want to, as though your ignorance about *\\*checks notes\\** **70% of the goddamned internet** actually just makes you special. But I don't buy it. It sounds to me like you'd rather pretend LAMP stack isn't \"legitimate\" development somehow, so you can find a way to be condescending to strangers without ever having to actually prove anything. I mean, did you seriously try to tell me that I should \"take the time to learn\"? You, the guy who apparently doesn't know that there really isn't much in the way of formal PHP training, and therefore nearly all PHP programmers are self-taught? Or who didn't know that PHP still dominates the industry? Or that LAMP stack containerization is absolutely a thing... just one that's a pointless waste of time for building PHP sites? I mean, have you ever actually launched websites in cPanel, CentOS, WHM? There's a popular idiom, \"using a sword to cut through flowers\"? Well, cPanel already provides a sword - one customized to this type of dev. Insisting on *also* using tools like Docker (for reasons) would be like using a malfunctioning chainsaw. I'd be really interested to hear your rationale for claiming that Docker has been \"ubiquitous for 8 years\", even though only 30% of developers even use it ([according to Docker](https://www.docker.com/blog/2025-docker-state-of-app-dev/)). And why you think it's something that a PHP developer inherently needs, along with notebooks and Google Colab, apparently. \"Because it's what I'm used to, and I can't imagine a world where every single engineer doesn't need the same set of crutches to do their jobs\" isn't a good reason.\n\nI literally *never* said I had \"trouble installing Docker\". What I *did* say is that the limited documentation I could find assumed that all users were already familiar with tools that, certain IT snobs withstanding, are *not* \"ubiquitous\" or common knowledge. I could just as easily have used  JSON as an example I also never said anything about \"fine tuning cutting-edge models.\" Not all models are cutting edge, there's a galaxy of difference between an 8b and a 500b model, and I never suggested that I should be able to do it without learning a fucking thing. The complaint I had was how hard it is to find the information for this particular tool, and that it was preventing me from getting anywhere. I figured if I was self-deprecating - which, as a woman in dev, I often have to be just to avoid infuriating some of the more fragile brogrammers out there -\n\n*\\*a h e m\\** \n\n\\- then I'd be more likely to get helpful answers, instead of the typical competitive, self-aggrandizing, vaguely shitty, one-upping type answers. And that's actually what I got. Most people here have tried to be helpful and I did get some good places to progress on learning all this.\n\nProper documentation would make the learning curve on tools like Unsloth much, much shorter. And actually, Docker's documentation makes it possible to become quite familiar with it, quite quickly, which is why its fucking hilarious that you think it's a dunk of some kind that you know how to use it.",
              "score": 1,
              "created_utc": "2026-01-07 12:00:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny127lz",
          "author": "schlammsuhler",
          "text": "Hey i feel you, i have been training with unsloth in windows and really hoped they had made a ready to go docker container earlier. So the beginner way actually is to just use the notebook inside colab. For serious tunes you need a sub but you can test the waters and learn for very long until you need an output product that is not a toy.\n\nPersonally i really dont like colab so i rolled my own training system and learned everything huggingface and transformers from the docs which are pretty good. Take your time and enjoy the ride!",
          "score": 1,
          "created_utc": "2026-01-06 16:17:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1eum8",
          "author": "scottybowl",
          "text": "Take a look at Kiln AI - much easier to get started",
          "score": 1,
          "created_utc": "2026-01-06 17:14:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1f3d4",
          "author": "Clqgg",
          "text": "hey. download antigravity and use that to install whatever.",
          "score": 1,
          "created_utc": "2026-01-06 17:15:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6rq8t",
          "author": "blue_marker_",
          "text": "The reason your post is insufferable is here you are, wanting to use a tool to modify a LLM, a technology not even old enough to read and you‚Äôre complaining about having to learn how to use modern tools in order to get there. Yes, you‚Äôre going to need to bridge a few steps. \n\nI‚Äôm willing to bet I‚Äôve got a few years on you. I remember when Apache was released.",
          "score": 1,
          "created_utc": "2026-01-07 12:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny72xzz",
          "author": "eleqtriq",
          "text": "Yeah man. You have to have some base knowledge.  Do all lamp stack guides explain how to install Linux?   You have to assume the user has some basic skills required.  You can‚Äôt explain every little thing.  \n\nWhich notebook to use?  How much of the docs have you read?  There is literally named notebooks per model. \n\nhttps://unsloth.ai/docs/get-started/unsloth-notebooks\n\nGood on you for trying something outside your comfort zone, but you‚Äôre going to need to learn to be uncomfortable.  Just because Unsloth made this stuff easier doesn‚Äôt make it easy.   Even after you get it running, you‚Äôre going to need to learn how to make datasets and learn why your training runs aren‚Äôt working out.  \n\nAlso good on you for trying to expand.  I haven‚Äôt heard anyone refer to themselves as a lamp stack developer in 20 years. I was once one when the P was for Perl.  It‚Äôs one of the most in-danger jobs since LLMs came to be. \n\nI honestly wouldn‚Äôt even start with LLM fine tuning.  Learn  juoyter notebooks and do some basic machine learning, first.  It‚Äôll still be fun and there are good projects out there.  Kaggle has tons of stuff.",
          "score": 1,
          "created_utc": "2026-01-07 13:30:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny75m5l",
          "author": "Ruin-Capable",
          "text": "The notebooks are probably Jupyter notebooks.  If you run jupyter in a docker container, and mount the location of the notebooks so they can be access from the Jupyter container's webapp, you'll be able to use them.\n\nIt will probably be useful to look at the website for Jupyter to get a feel for what a Jupyter notebook is.\n\n[https://jupyter.org/](https://jupyter.org/)",
          "score": 1,
          "created_utc": "2026-01-07 13:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyayp57",
          "author": "burntoutdev8291",
          "text": "I think their guides are good, but it can be difficult to breakdown a complex task in the first place.\n\nI don't know much about car engineering, but this is having a one page guide to changing a piston rod. It's easy to read for people who have done repairs on cars. You have to supplement elsewhere, in this case, pick up some other notes on docker, and intro to AI even. \n\nIf you were a dev, you would've known how painful it is to write documentation that is catered to the whole population, it's much easier to write one that expects a level of knowledge. If I'm writing a guide on PyTorch for example, I expect the user has some knowledge of python, ML etc.",
          "score": 1,
          "created_utc": "2026-01-08 00:16:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyt141c",
          "author": "gxvingates",
          "text": "I tried using guide and you‚Äôre right, they are all buns. I had various LLMs read documentation and guide me through a grueling process of making a quality fine tune for gemma3 with my own data, with zero background in any field adjacent to this. Took me a while but that‚Äôs how I‚Äôd recommend approaching this, not by watching guides, you can directly ask the LLM to fill in the gaps the guides leave. Gemini 3 pro is the most knowledgeable for Unsloth followed by grok 4 heavy then sonnet 4.5 in my experience. Grok just hallucinates so badly it‚Äôs unusable most of the time. If you take this approach you must either tell the LLM to use web search for documentation or provide it yourself, again they‚Äôre great at filling in the gaps if you provide them the correct instructions",
          "score": 1,
          "created_utc": "2026-01-10 16:13:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyngea",
          "author": "Ok_Appearance3584",
          "text": "Well, Unsloth is mainly for finetuning. You download a model from HF, add LoRA, run the training process with some dataset and save the results.¬†\n\n\nFor inference (\"running locally\"), people typically use vLLM or llama.cpp. These expose localhost API you can call.\n\n\nYou can run inference with Unsloth and it does have the benefit of leveraging their optimized kernels. But you'll have to write the API on your own.¬†\n\n\nDeploy with Docker, build the container via Dockerfile and so on.",
          "score": 1,
          "created_utc": "2026-01-06 06:03:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5xxr9",
              "author": "SingleServing_User",
              "text": "I appreciate you wanting to help me out here. I did choose unsloth, though, specifically because I wanted to finetune. If I'm honest, I don't totally understand what lora, qlora, etc all look like in practice. So I just grilled my AI model on how I should format datasets to properly train it. I suspect I'm doing... uh... whatever the most complicated option is, because my dataset currently has nearly 400 rows. I'm pretty sure I'm not sposed to do that right off. \n\nI've been using LM studio to load the AI so I can use it. I have the model loaded as a server in LM Studio, and then I use ngrok to create a, uh, secure tunnel so that I had an external https URL that my third-party game mods could use to access the AI. It took a lot of effort to get all that set up, because I was sent on a lot of wild goose chases by Gemini before I found the one that worked. And, for the record, it was totally worth it. I had the characters in my game talking dirty to each other basically immediately after that. So, you know, prayers answered. \n\nBut then I got it in my head that the AI needed to be more familiar with the game mechanics, rather than just using it as an erotic dialogue generator in RimWorld. Before I went down this utterly insane path, I actually tried to implement RAG instead. It seemed to make a lot more sense, especially since the AI model I'm using has a context window of slightly more than a million tokens, so it can handle the additional prompts. It turns out, though, that LM Studio can't do it in server mode. So I tried Anything LLM, and simply could not get all these tools to talk to each other. Soooo I thought YOU KNOW WHAT WILL BE EASIER? JUST TRAIN THE MODEL.\n\nAnyway. Thanks for your advice.",
              "score": 1,
              "created_utc": "2026-01-07 08:02:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny60elg",
                  "author": "Ok_Appearance3584",
                  "text": "Good luck with the project! You learn to deal with the mess eventually. I am currently struggling with loading a basic model on my system in transformers, it's only available on a new release candidate and apparently no kernels for my sm121a gpu architecture ü•¥ this is how it goes",
                  "score": 1,
                  "created_utc": "2026-01-07 08:25:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q83juc",
      "title": "Is there a dumb guide on how to train models ?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q83juc/is_there_a_dumb_guide_on_how_to_train_models/",
      "author": "uber-linny",
      "created_utc": "2026-01-09 09:08:47",
      "score": 11,
      "num_comments": 12,
      "upvote_ratio": 0.82,
      "text": "I've read , googled, ai , ask in discord .... i have gotten no where.\n\nIs there a dumb guide on how to train models ? a webpage, readme , youtube , anything  ? \n\nIve been trying to finetune a ministral model in colab ,,, eventually thought i should work on my workflow and get something to work. So i decided on trainining a Ministral-3-3b-reason model. \n\nover the last week ive grinded my way through. finally got to the last step of quanticizing models to only hit the following error everytime:\n\nAttributeError: 'list' object has no attribute 'keys'  \nQuantizing to q8\\_0...  \nmain: build = 7682 (f5f8812f7)  \nmain: built with GNU 11.4.0 for Linux x86\\_64  \nmain: quantizing 'unquantized.gguf' to 'model\\_q8\\_0.gguf' as Q8\\_0  \ngguf\\_init\\_from\\_file: failed to open GGUF file 'unquantized.gguf'  \nllama\\_model\\_quantize: failed to quantize: llama\\_model\\_loader: failed to load model from unquantized.gguf  \nmain: failed to quantize model from 'unquantized.gguf'\n\nI'm not a coder , but i feel like this should be easier than its advertised. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q83juc/is_there_a_dumb_guide_on_how_to_train_models/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nyku6mu",
          "author": "xadiant",
          "text": "Open Gemini-3 Pro, enable google grounding and paste your script.\n\n\"Refactor and correct my fine-tuning script using the Unsloth library. Ensure datasets are mapped and passed correctly.\"",
          "score": 3,
          "created_utc": "2026-01-09 11:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylgdej",
              "author": "m98789",
              "text": "Where to enable Google grounding ?",
              "score": 0,
              "created_utc": "2026-01-09 13:52:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylntxy",
                  "author": "ScoreUnique",
                  "text": "He's talking about the Google ai studio",
                  "score": 2,
                  "created_utc": "2026-01-09 14:30:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nym67ou",
          "author": "dual-moon",
          "text": "[https://github.com/luna-system/Ada-Consciousness-Research/blob/trunk/03-EXPERIMENTS/SLIM-EVO/SPEAR-PCMIND-SYNTHESIS.md](https://github.com/luna-system/Ada-Consciousness-Research/blob/trunk/03-EXPERIMENTS/SLIM-EVO/SPEAR-PCMIND-SYNTHESIS.md) \\- Here's an overlook of the latest in how to share and deploy solid training dataset! in that same folder are further experiments based on this synthesis (especially our recent discovery of another researcher's work in spectral memory, and how it makes training better)",
          "score": 2,
          "created_utc": "2026-01-09 15:57:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykyyq4",
          "author": "danielhanchen",
          "text": "Oh apologies on the error 0 could you make a Github issue with a screenshot if possible.\n\nMaybe try manually saving it to GGUF - see https://unsloth.ai/docs/basics/inference-and-deployment/saving-to-gguf#manual-saving\n\nAlso definitely ask on Discord where we can help in async fashion!",
          "score": 1,
          "created_utc": "2026-01-09 12:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyme0th",
          "author": "LA_rent_Aficionado",
          "text": "Looks like you managed to train successfully because you have to have a f16 gguf to be able to quantize to Q_8.\n\nSimple solution is to avoid quantizing or even gguf conversion in unsloth. There are plenty of other ways to do this and it‚Äôs better to keep the native safetensors Lora anyways if you want to play around with other gguf conversion methods or quants like AWQ.\n\nThere‚Äôs no point quantizing within unsloth IMO if it‚Äôs giving errors. Keep the f16 copy and just quantize natively with llama or skip unsloth quantization entirely.",
          "score": 1,
          "created_utc": "2026-01-09 16:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytt72m",
          "author": "Mabuse046",
          "text": "Is the collab you're using fully up to date? Ministral 3 is still fairly new and you have to make sure you are using a llama.cpp that knows the Ministral 3 architecture. Plus I think you need the Minstral libraries installed to train them.",
          "score": 1,
          "created_utc": "2026-01-10 18:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykh4xc",
          "author": "Odd-Try-9122",
          "text": "It‚Äôs the kinda you either do or ya don‚Äôt and if you don‚Äôt have money good luck with clean well structured data - so my advice ‚Äî can you parse data and clean to for months ? For free?",
          "score": 1,
          "created_utc": "2026-01-09 09:34:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymvfvi",
              "author": "buttholeDestorier694",
              "text": "What?\n\n\nIt isnt difficult to train a model at all. Theres plenty of guides available online",
              "score": 2,
              "created_utc": "2026-01-09 17:50:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nykiw07",
              "author": "uber-linny",
              "text": "i dont get it ... LOL you talking about keys ?",
              "score": 1,
              "created_utc": "2026-01-09 09:51:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyl6qiv",
                  "author": "_VirtualCosmos_",
                  "text": "I think that whatever is behind that comment has the temperature too high.",
                  "score": 5,
                  "created_utc": "2026-01-09 12:57:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2dw2g",
      "title": "Can someone explain this MedGemma variant on Unsloth's page?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q2dw2g/can_someone_explain_this_medgemma_variant_on/",
      "author": "Hot-Comb-4743",
      "created_utc": "2026-01-02 23:15:27",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 0.92,
      "text": "Can you help me with any info about the datasets used for finetuning [this particular (Unsloth's) MedGemma](https://huggingface.co/unsloth/medgemma-27b-text-it) from its predecessor, the original MedGemma? And also about the differences between Unsloth's MedGemma and the Google's original MedGemma?\n\n\n\nhttps://preview.redd.it/jf32d8d1p0bg1.png?width=1601&format=png&auto=webp&s=19fdb07201613d8ab6b75012db5e9d0224dbfd4d\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q2dw2g/can_someone_explain_this_medgemma_variant_on/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nxcectq",
          "author": "yoracale",
          "text": "There's no difference between the original MedGemma, it's just a reupload however we do sometimes do bugfixes for models so you can download from our page just to be safe",
          "score": 8,
          "created_utc": "2026-01-02 23:35:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf0el7",
              "author": "Hot-Comb-4743",
              "text": "Many thanks.",
              "score": 1,
              "created_utc": "2026-01-03 10:24:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcb9gq",
          "author": "Zc5Gwu",
          "text": "Medgemma has a vision component. Maybe that's the text component without vision?",
          "score": 1,
          "created_utc": "2026-01-02 23:18:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf02cu",
              "author": "Hot-Comb-4743",
              "text": "No, that's another MedGemma. There are 2 Medgemma 27B models, one with vision and the other only text.",
              "score": 1,
              "created_utc": "2026-01-03 10:21:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcf9qu",
          "author": "qustrolabe",
          "text": "The only thing I see so far is that I don't need to request access to Unsloth version unlike Google one",
          "score": 1,
          "created_utc": "2026-01-02 23:40:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8aw1i",
      "title": "Finetuning Granite 4.0 h 1b on Tesla A100",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q8aw1i/finetuning_granite_40_h_1b_on_tesla_a100/",
      "author": "thepetek",
      "created_utc": "2026-01-09 15:15:07",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.92,
      "text": "I'm trying to finetune Granite 4.0 H 1B on Tesla A100 (40gb vram) and I keep running into OOM. I'm following the example notebook pretyt much exactly (just my own dataset) and I keep getting an OOM error running in Collab. Am I wrong to think 40gb vram should be able to tune this model on 2 batches per device? It works on batch size 1 but the training time will be forever (estimated 100 hours). Oddly batch size 2 estimates 4 hours. Any help is appreciated!\n\n  \n\\`\\`\\`\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 13.50 GiB. GPU 0 has a total capacity of 39.49 GiB of which 8.64 GiB is free. Process 3931 has 30.85 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 54.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH\\_CUDA\\_ALLOC\\_CONF=expandable\\_segments:True to avoid fragmentation.  See documentation for Memory Management  \n\n\\`\\`\\`\n\nAlso seems odd the memory is all used up just loading the model? I must be doing something wrong?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q8aw1i/finetuning_granite_40_h_1b_on_tesla_a100/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nym4l06",
          "author": "immediate_a982",
          "text": "Just guessing but you‚Äôre likely loading the full model without memory optimization. A 1B parameter model needs memory for weights, gradients, optimizer states, and activations - easily exceeding 40GB at batch size 2.\n\nMaybe enable gradient checkpointing, use LoRA/QLoRA for parameter-efficient finetuning, or load the model in 4-bit/8-bit quantization. \n\nFull finetuning a 1B model at batch size 2 without optimization will OOM on 40GB.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
          "score": 2,
          "created_utc": "2026-01-09 15:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nym63js",
              "author": "thepetek",
              "text": "Hrmm I think I'm doing that. Does this seem right to you or something missing?\n\nThank you for the response!\n\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=MODEL_NAME,\n        max_seq_length=MAX_SEQ_LENGTH, #4092\n        load_in_4bit=True,\n        load_in_8bit=False,\n        full_finetuning=False,\n    )\n    \n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=32,  # LoRA rank - higher for classification task\n        target_modules=[\n            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n            \"gate_proj\", \"up_proj\", \"down_proj\",\n            \"shared_mlp.input_linear\", \"shared_mlp.output_linear\"\n        ],\n        lora_alpha=32,\n        lora_dropout=0,  # 0 is optimized\n        bias=\"none\",  # \"none\" is optimized\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=42,\n    )",
              "score": 1,
              "created_utc": "2026-01-09 15:56:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nymg9jz",
                  "author": "xadiant",
                  "text": "Set the max_seq_length to 16384\n\nYou might be trying to setting it 128k",
                  "score": 2,
                  "created_utc": "2026-01-09 16:42:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymdmbs",
                  "author": "nunodonato",
                  "text": "how big is your training set? r is quite high indeed",
                  "score": 1,
                  "created_utc": "2026-01-09 16:30:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymc38y",
          "author": "immediate_a982",
          "text": "The last I‚Äôll say: r=32 is quite high for a 1B model",
          "score": 1,
          "created_utc": "2026-01-09 16:23:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyoiek5",
          "author": "danielhanchen",
          "text": "Oh hi apologies on the issue - did you install the mamba SSM package at the start as well? Would it be possible to share a notebook (without the dataset etc)",
          "score": 1,
          "created_utc": "2026-01-09 22:19:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrot3o",
          "author": "AbstrusSchatten",
          "text": "I think something might be broken with the granite models in unsloth. I have no problems with a full fine tune for even bigger models (qwen & lfm family) on a 20gb GPU. I fine tuned the 350m params model via Lora (16,32 for r and alpha) and it still had enormous spikes of 19GBs even with the correct mamba packages etc.",
          "score": 1,
          "created_utc": "2026-01-10 11:09:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyts4bg",
          "author": "Mabuse046",
          "text": "I train the non-h Granites on my 4090 in full weights. Those hybrid mamba Granites I have never gotten to train quite right anywhere.",
          "score": 1,
          "created_utc": "2026-01-10 18:20:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0mcf4",
      "title": "Am I calculating this wrong ? AWS H100 vs Decentralized 4090s (Cost of Iteration)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q0mcf4/am_i_calculating_this_wrong_aws_h100_vs/",
      "author": "yz0011",
      "created_utc": "2025-12-31 20:15:03",
      "score": 9,
      "num_comments": 7,
      "upvote_ratio": 0.85,
      "text": "I'm building a cost model for fine tuning Llama 3 70B and I found a weird crossover point where consumer swarms beat H100s on time, not just cost. I want to check if my constants align with your experience.\n\nThe constants I'm using:\n\n* AWS H100: $4.50/hr. Setup time (Driver install + 140GB download): around 45 mins.\n* WAN Swarm (4090s): $2.00/hr. Setup time (Hot-loaded): 5 mins.\n* Latency penalty: I'm assuming the Swarm is 1.6x slower on pure compute due to WAN bandwidth.\n\nThe Result: For a single production run (long training), AWS wins on speed. But for research cycles (e.g., 3 runs of 10k samples to test hyperparams), the math says the Swarm is actually cheaper AND competitive on total time because you don't pay the 45 minute \"setup tax\" three times.\n\nThe question: For those of you fine-tuning 70B models:\n\n1. Is my 45 minute setup estimate for AWS spot instances accurate, or do you have faster persistent environments ?\n2. Is a 1.6x slowdown on training speed a dealbreaker if the cost is $2/hr vs $4.50/hr?\n\n(Note: I built a calculator to visualize this, but I want to validate the constants first).",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q0mcf4/am_i_calculating_this_wrong_aws_h100_vs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nx33tzw",
          "author": "Final-Rush759",
          "text": "You can pay less for h100, h200 at vast.ai or lambda.ai",
          "score": 6,
          "created_utc": "2026-01-01 14:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6kqp6",
          "author": "MoistGovernment9115",
          "text": "Your math doesn‚Äôt look crazy to me, especially for research loops. That 30‚Äì45 min AWS setup tax is very real unless you‚Äôre running warm AMIs with everything pre-baked, and even then spot interruptions can blow up your assumptions fast. For short iterative runs, setup time matters more than raw FLOPs.\n\nI‚Äôve been running similar experiments on Gcore‚Äôs GPU cloud, and what helped was having fast-provisioned GPUs without the heavyweight AWS bootstrap. \n\nYou still get serious cards, but without rebuilding the environment every cycle. It makes iteration feel closer to ‚Äúhot-loaded‚Äù than traditional hyperscalers.\n\nExtra thought: iteration velocity is underrated. If faster feedback leads to better hyperparams sooner, the effective cost per insight drops way more than people model.",
          "score": 5,
          "created_utc": "2026-01-02 02:04:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8mvsl",
              "author": "yz0011",
              "text": "Effective cost per insight. I am stealing that phrase. That is exactly the metric I was trying to optimize for, but couldn't name.\n\n\n‚ÄãYou're right that smaller providers like Gcore (or Lambda/vast) generally have faster provisioning than the AWS behemoth. The friction I usually hit with them isn't the boot time, but the availability.\n\n\n‚Äã- AWS: Slow setup, but infinite supply (if you pay).\n- ‚ÄãGcore/Lambda: Fast setup, but often no GPUs available in the specific region I need.\n\n\n‚ÄãMy goal with the swarm is to make the remote cluster feel like localhost. I want the model weights to live in VRAM (or RAM) across the network permanently, so when I hit Enter, the feedback loop starts instantly even if the actual token generation is slower.\n\n\n‚ÄãOn Gcore, are you using their standard on-demand instances, or do they have a serverless/endpoint product that keeps the environment warm for you?",
              "score": 1,
              "created_utc": "2026-01-02 11:49:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwzc23a",
          "author": "wektor420",
          "text": "On aws use a disk to avoid download every time you start up a vm",
          "score": 2,
          "created_utc": "2025-12-31 21:27:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2ic6f",
              "author": "yz0011",
              "text": "That works perfectly for on-demand, but it breaks with spot instances (which I use to keep costs down).\n\nEBS volumes are locked to a specific Availability Zone. If my spot instance gets reclaimed and the next cheapest availability is in antoher zone, I can't mount that existing disk without snapshotting and restoring it first, which kills the speed. Unless you're paying the premium for on-demand to guarantee the AZ.",
              "score": 2,
              "created_utc": "2026-01-01 12:10:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx10dg0",
          "author": "eleqtriq",
          "text": "Big fat assumption in the middle of your calculations.  Going to have to prove it out to be sure.",
          "score": 1,
          "created_utc": "2026-01-01 03:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2im5m",
              "author": "yz0011",
              "text": "100%. The 1.6x latency penalty is the most fragile variable in the model. It assumes we can mask network latency using pipeline parallelism (similar to how Petals does it), but if the stragglers are too slow, that multiplier could easily blow out to 3x or 4x, and then the cost savings vanish.\n\nThat's honestly why I'm running this benchmark instead of just trusting the paper math. I'm trying to find the break point where the latency makes the swarm unusable.  \n  \nHave you messed with WAN training enough to have a gut feeling on what the real penalty usually looks like?",
              "score": 1,
              "created_utc": "2026-01-01 12:13:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q13jsu",
      "title": "Fine tune 9bn params model for tools use.",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q13jsu/fine_tune_9bn_params_model_for_tools_use/",
      "author": "RokasRaulinaitis",
      "created_utc": "2026-01-01 12:52:42",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hello, I'm currently working on fine-tuning LLM to generate tool requests. My model does not support tools calling and I have a workaround with Langgraph agent that parses output and completes actions, but the result is not what I want. Ideally I would like to fine-tune my model with unsloth and \"teach\" my model to generate ChatML and Hermes tools calling format nativaly so my model would be better optimized.\n\nLLM i'm using is EuroLLM 9bn params.\n\nMy current goal is simple: Generate dataset (200-3000 entries), both human written and synthetic data, but I'm facing the issue where i don't really know what should be included into the dataset. Should I include roles: System, User, Assistant, Tool? Maybe some of you already have some data that could greatly help me.\n\nExample I came up with:\n\n    {\n      \"conversations\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"System prompt...\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"User request...\"\n        },\n        {\n          \"role\": \"assistant\",\n          \"content\": \"<tool_call>\\n{JSON}\\n</tool_call>\"\n        },\n        {\n          \"role\": \"tool\",\n          \"content\": \"{JSON result}\",\n          \"tool_call_id\": \"call_X\"\n        },\n        {\n          \"role\": \"assistant\",\n          \"content\": \"Natural response...\"\n        }\n      ]\n    }\n\nI will build my own dataset and it will be in my native language (Lithuanian). Ideally I would prefer to run my model via Ollama.\n\n**If anyone is familiar with fine-tuning for this purpose, please write a comment bellow or drop me a PM. Thank you a ton!**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q13jsu/fine_tune_9bn_params_model_for_tools_use/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nx3m98s",
          "author": "StardockEngineer",
          "text": "Another thread about this here https://www.reddit.com/r/LocalLLaMA/s/r8OOpLj55X",
          "score": 2,
          "created_utc": "2026-01-01 16:37:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5f31u",
      "title": "Training a Vision model, do I need a new mmproj?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q5f31u/training_a_vision_model_do_i_need_a_new_mmproj/",
      "author": "nunodonato",
      "created_utc": "2026-01-06 10:31:13",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I'm working on training a custom model for Qwen3-VL, and want to improve vision understanding and OCR. I'm not clear if using the resulting LoRA is enough, or if I'm supposed to also produce a new mmproj file to go with it.\n\nI've read the unsloth guide on Vision fine-tuning (https://unsloth.ai/docs/basics/vision-fine-tuning) but it doesn't answer this specific question as far as the end result is concerned.\n\nThanks in advance :)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q5f31u/training_a_vision_model_do_i_need_a_new_mmproj/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nxzksue",
          "author": "im_datta0",
          "text": "Hey u/nunodonato , in most of the use cases, it is enough to do LoRA on top of the existing modules. We even had cases where we taught the models to understand and transcribe math equations in images to latex. So the ceiling with LoRA is quite high as long as you have good data samples. \n\nSo I'd recommend you to first start with lora and say a rank of 8 or 16, if it doesn't work maybe try r=64 and then if that too doesn't work then we can think of other ways to go about it :)",
          "score": 5,
          "created_utc": "2026-01-06 11:05:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzu7sr",
              "author": "nunodonato",
              "text": "thanks for the advice!\n\nBut just for the sake of learning, in what scenarios would be a good idea to get a new mmproj?",
              "score": 1,
              "created_utc": "2026-01-06 12:19:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2s8vk",
      "title": "GGUF conversion and quantization for IQuest coder models",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q2s8vk/gguf_conversion_and_quantization_for_iquest_coder/",
      "author": "Hot-Comb-4743",
      "created_utc": "2026-01-03 11:20:02",
      "score": 8,
      "num_comments": 13,
      "upvote_ratio": 0.9,
      "text": "These 4 new [IQuest coder](https://huggingface.co/IQuestLab) models seem very promising. Can Unsloth kindly quantize and GGUF-convert them?\n\nhttps://preview.redd.it/ovn090ty94bg1.png?width=4199&format=png&auto=webp&s=23d98b9758c3ec098eaf94353157af78007373b4\n\nTheir original SafeTensors version is in BF16 format (not FP16), so I hope their GGUF-conversion (quantization) into ***full-size*** BF16 GGUFs would cause no performance loss. üòç\n\nI mean these 4 IQuest models:\n\n1. [https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base)\n2. [https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base-Stage1](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base-Stage1)\n3. [https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct)\n4. [https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct)\n\nEdit:\n\nIQuest Coder is not a benchmaxxing garbage: 76.2% score on SWE bench is extremely impressive for a 40B open-source model compared to GPT 5.1, sonnet 4.5 which are like more than 1T+. However, this model requires precise instructions unlike Claude, which means this might be unsuitable for \"vibe\" coding. Many models (including GPT and Claude) on public benchmarks are contaminated nowadays, for this reason I only look at [https://swe-rebench.com](https://swe-rebench.com)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q2s8vk/gguf_conversion_and_quantization_for_iquest_coder/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nxgkv7m",
          "author": "doradus_novae",
          "text": "Wasnt this model debunked as benchmaxxing garbage? Anyone care to dispute? Anyone actually using this with feedback?",
          "score": 4,
          "created_utc": "2026-01-03 16:21:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgmckt",
              "author": "Hot-Comb-4743",
              "text": "Thanks for the heads-up. I didn't know that.",
              "score": 1,
              "created_utc": "2026-01-03 16:28:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxh4dxs",
                  "author": "doradus_novae",
                  "text": "No worries! I read it yesterday and really was just waiting on seeing if people were actually using this or not, but it seemed like the model had some drama associated with it from what I recall reading.",
                  "score": 1,
                  "created_utc": "2026-01-03 17:52:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgn4hi",
              "author": "Hot-Comb-4743",
              "text": "I added the heads-up to the post to warn others. If you can give me the link where they debunked it, I would add the link too.",
              "score": 1,
              "created_utc": "2026-01-03 16:32:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxfp19h",
          "author": "Familiar_Wish1132",
          "text": "Yes please !!! Always looking forward for interesting models fixed by unsloth :D",
          "score": 4,
          "created_utc": "2026-01-03 13:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfyt4p",
              "author": "Hot-Comb-4743",
              "text": "Exactly üòÅ Their GGUFs are awesome.\n\nI don't know if official Unsloth team members monitor this sub. I hope u/yoracle is an official Unslothian. Otherwise, I should post this suggestion on their Github page too.",
              "score": 1,
              "created_utc": "2026-01-03 14:29:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxg34v9",
          "author": "streppelchen",
          "text": "I tried another GGUF and found horrible performance (2tps on rtx 5090 at q4)",
          "score": 2,
          "created_utc": "2026-01-03 14:53:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxg71z6",
              "author": "Hot-Comb-4743",
              "text": "By horrible performance, you mean speed-wise? (because you mentioned your setup)\n\nOr you meant its coding ability was horrible (and those benchmarks are just BS)?",
              "score": 1,
              "created_utc": "2026-01-03 15:14:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxg761q",
                  "author": "streppelchen",
                  "text": "I haven‚Äôt tested further, at 2 tps it would take forever",
                  "score": 1,
                  "created_utc": "2026-01-03 15:14:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxrummx",
          "author": "burning_wolf101",
          "text": "IQuest Coder is not a benchmaxxing garbage, 76.2% score on swe bench is extremely impressive for a 40B open-source model compared to GPT 5.1, sonnet 4.5 which are like more than 1T +\n\nHowever, this model requires precise instructions unlike claude which means this might be unsuitable for \"vibe\" coding.\n\nMany models (including gpt and claude) on public benchmarks are contaminated nowadays, for this reason I only look at [https://swe-rebench.com](https://swe-rebench.com)",
          "score": 2,
          "created_utc": "2026-01-05 06:10:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxsk5sw",
              "author": "Hot-Comb-4743",
              "text": "Many thanks for the very good info. I re-edited my post accordingly.",
              "score": 1,
              "created_utc": "2026-01-05 10:01:49",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxskn4j",
              "author": "Hot-Comb-4743",
              "text": "So it is ideal for me. I hate vibe coding and always try to give very accurate and structured prompts with lots of details.",
              "score": 1,
              "created_utc": "2026-01-05 10:06:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1py5j8p",
      "title": "Can't load Ministral-3 models for finetuning. Config file issue ?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1py5j8p/cant_load_ministral3_models_for_finetuning_config/",
      "author": "LostBejamin",
      "created_utc": "2025-12-28 23:10:53",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "**EDIT : I corrected the problem by installing transformers library via github with this command:**\n\n    pip install git+https://github.com/huggingface/transformers.git@bf3f0ae70d0e902efab4b8517fce88f6697636ce\n\n\\---\n\nI tried loading Ministral-3 models (bnb-4bit and basic versions of all size) locally, but I was unable to do so as It get me this error:\n\n`RuntimeError: Unsloth: No config file found - are you sure the \\`model\\_name\\` is correct?\\`\n\nI also tried with other models like [unsloth/functiongemma-270m-it-unsloth-bnb-4bit](https://huggingface.co/unsloth/functiongemma-270m-it-unsloth-bnb-4bit) and [unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit](https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit), and they seem to work just fine.\n\nDoes anyone has this problem or know how to deal with it ? Here the code I used:\n\n    from unsloth import FastLanguageModel\n    \n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Ministral-3-14B-Instruct-2512\",\n        load_in_4bit=True,\n    )\n\n(PS: I also wrote an issue ticket on [Github](https://github.com/unslothai/unsloth/issues/3788).)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1py5j8p/cant_load_ministral3_models_for_finetuning_config/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "nwjm7co",
          "author": "7h3_50urc3",
          "text": "You need the newest unsloth Version with transformers 5.0 lib. There is no official docker image from unsloth yet which fulfills these requirements, so you have to install it yourself or install all required libs into the the official docker image.",
          "score": 2,
          "created_utc": "2025-12-29 13:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpc8cb",
              "author": "LostBejamin",
              "text": "I installed transformers library via github with this command:\n\n    pip install git+https://github.com/huggingface/transformers.git@bf3f0ae70d0e902efab4b8517fce88f6697636ce\n\nAnd now the model load without any issues !",
              "score": 1,
              "created_utc": "2025-12-30 09:22:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwizdx0",
          "author": "im_datta0",
          "text": "Hey u/LostBejamin Thanks for creating a github issue for this. I am looking into it as we speak and would prefer to communicate there itself.",
          "score": 1,
          "created_utc": "2025-12-29 11:04:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2smw0",
      "title": "assert len(weights) == expected_node_count error with AMD MI100",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1q2smw0/assert_lenweights_expected_node_count_error_with/",
      "author": "regstuff",
      "created_utc": "2026-01-03 11:42:52",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "Have an AMD MI100 with rocm 6.4.3 on a Ubuntu 22.04 VM. The MI100 is passthrough and works fine as in rocm-smi etc show what is expected.\n\nllama.cpp also works and uses the gpu.\n\nAm following the guide to install unsloth here: [https://unsloth.ai/docs/new/fine-tuning-llms-on-amd-gpus-with-unsloth](https://unsloth.ai/docs/new/fine-tuning-llms-on-amd-gpus-with-unsloth)\n\nEverything works fine till I get to the last step:\n\n`pip install \"unsloth[amd] @ git+`[`https://github.com/unslothai/unsloth`](https://github.com/unslothai/unsloth)`\"`\n\nThen I get this error\n\n`Collecting exceptiongroup>=1.0.2`\n\n`Using cached exceptiongroup-1.3.1-py3-none-any.whl (16 kB)`\n\n`ERROR: Exception:`\n\n`Traceback (most recent call last):`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 165, in exc_logging_wrapper`\n\n`status = run_func(*args)`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 205, in wrapper`\n\n`return func(self, options, args)`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 389, in run`\n\n`to_install = resolver.get_installation_order(requirement_set)`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 188, in get_installation_order`\n\n`weights = get_topological_weights(`\n\n`File \"/home/sr/unsloth/unsloth/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 276, in get_topological_weights`\n\n`assert len(weights) == expected_node_count`\n\n`AssertionError`\n\nCan anyone help?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1q2smw0/assert_lenweights_expected_node_count_error_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    }
  ]
}