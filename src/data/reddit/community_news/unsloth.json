{
  "metadata": {
    "last_updated": "2026-02-08 08:47:11",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 9,
    "total_comments": 84,
    "file_size_bytes": 95082
  },
  "items": [
    {
      "id": "1quvrmn",
      "title": "Qwen3-Coder-Next is released! üíú",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/7kswd313pahg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-03 15:59:43",
      "score": 580,
      "num_comments": 116,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1quvrmn/qwen3codernext_is_released/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3d2cke",
          "author": "danielhanchen",
          "text": "MXFP4 MoE and FP8-Dynamic quants are still converting!",
          "score": 28,
          "created_utc": "2026-02-03 16:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dabs3",
              "author": "GlobalLadder9461",
              "text": "How do you rate MXFP4 vs UD Q4 K XL in terms of quality and speed ?\n\nAny chance of getting KL divergence graph. Between them also adding q4_1. These are new quants added.\n\nHopefully we get a reply",
              "score": 9,
              "created_utc": "2026-02-03 16:48:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dr16r",
                  "author": "sourceholder",
                  "text": "Related question: is \"UD Q4 K XL\" able to leverage fast Blackwell 4-bit registers or does it fallback to 8-bits?   The primary appeal of MXFP4¬†is 4-bit native acceleration.",
                  "score": 6,
                  "created_utc": "2026-02-03 18:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dtk12",
              "author": "yoracale",
              "text": "They're all up now!",
              "score": 4,
              "created_utc": "2026-02-03 18:15:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3de1qg",
              "author": "StardockEngineer",
              "text": "Let‚Äôs gooooooooooooo üëè üéâ",
              "score": 1,
              "created_utc": "2026-02-03 17:05:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3hf121",
              "author": "debackerl",
              "text": "Awesome guys? Would it be possible to get MXFP4 on vLLM? vLLM is never getting as many quantization options as llama.cpp I find, but as shown by GPT OSS, it actually helps a lot to use MXFP4, even on an engine such as vLLM.",
              "score": 1,
              "created_utc": "2026-02-04 05:49:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3l7zwz",
              "author": "SomeAcanthocephala17",
              "text": "Has anyone tried benchmarking the MXFP4 model to see how much i scores compared tot he full FP16 model?",
              "score": 1,
              "created_utc": "2026-02-04 19:55:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d2100",
          "author": "qwen_next_gguf_when",
          "text": "This is perfection ü§©",
          "score": 8,
          "created_utc": "2026-02-03 16:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dekui",
          "author": "Effective_Head_5020",
          "text": "Thank you so much!\n\n\nI always wondered about the VRAM requirement. If I have 64gb of RAM only, will it work or will I have performance degradation?",
          "score": 5,
          "created_utc": "2026-02-03 17:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhj1f",
              "author": "yoracale",
              "text": "Absolutely you can. More VRAM will just make it faster.",
              "score": 2,
              "created_utc": "2026-02-03 17:21:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3hxr9z",
                  "author": "CatEatsDogs",
                  "text": "How I can do that? Was trying to run qwen 80b on 16+16vram and 64ram. It was failing to load models under ollama and lm studio.",
                  "score": 1,
                  "created_utc": "2026-02-04 08:32:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dthbw",
          "author": "brhkim",
          "text": "Okay I've never attempted to run an agentic coding LLM locally before -- seemed totally out of reach and not worth it v. paying for Claude. But this is WILD.\n\nHow do the hardware requirements scale when you're running subagents? If you have 3 separate subagents running with their own context (in addition to an orchestrator agent you're interacting with directly), how much more RAM/VRAM do you need to make things continue to run smoothly? Does that make sense? I assume tok/sec gen gets spread across parallel running subagents, and the added context per session means there's a lot more RAM usage just to context. But the model can be loaded \"centrally\", right? Or can it not run parallel sessions at all, they'd end up being sequential by query?",
          "score": 6,
          "created_utc": "2026-02-03 18:15:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jndl4",
              "author": "txgsync",
              "text": "At least on Mac, ‚Äúbatch inference‚Äù ‚Äî re-using the same model with multiple KV caches ‚Äî can allow a model that runs at dozens of tokens per second to thousands. Each slows down a tad but aggregate performance is wild. \n\nI‚Äôve been experimenting with handing the same model slightly different prompts and then having the model evaluate the best answer to a baseline prompt. This kind of ‚Äúswarm programming‚Äù seems to lead to better outcomes than rolling the dice with a single context. \n\nBut my harness is quite primitive.",
              "score": 2,
              "created_utc": "2026-02-04 15:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3k3pl3",
                  "author": "brhkim",
                  "text": "Huh, that's super interesting and extremely unintuitive. How could it be that they calculate totally independently??? I don't doubt what you're saying, it just is really hard to make sense of from an underlying technical perspective",
                  "score": 1,
                  "created_utc": "2026-02-04 16:50:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3gsiuh",
              "author": "not-really-adam",
              "text": "This is a really solid question. I haven‚Äôt thought about sub agents in the LLM context. My experience has been so poor compared to Opus 4.5 that I haven‚Äôt been bothered to push past just getting it setup.\n\nIt‚Äôs just so slow. And I have an M3 Ultra with 256GB. \n\nI hope this model works well and I can push it with some subagents. Might even consider running different versions of this model for primary vs subagents.",
              "score": 1,
              "created_utc": "2026-02-04 03:18:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hxu7o",
                  "author": "ImOutOfIceCream",
                  "text": "Fwiw i am running the same hardware as you and have had good results with opencode using qwen3-coder-30b for code gen/small model and qwen3 next as the reasoning model. There are definitely some quirks in behavior that differ from opus 4.5 but with a well configured set of instructions, skills, hooks, etc you can accomplish a lot. I‚Äôm excited to try this one.",
                  "score": 1,
                  "created_utc": "2026-02-04 08:33:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3i8xyx",
                  "author": "Street-Buyer-2428",
                  "text": "use vllm-mlx\n\n",
                  "score": 1,
                  "created_utc": "2026-02-04 10:18:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d2jpi",
          "author": "msrdatha",
          "text": "Thanks for the quick release...! \n\nWill there be an IQ4\\_NL Quant also?",
          "score": 5,
          "created_utc": "2026-02-03 16:11:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d5cew",
              "author": "yoracale",
              "text": "Yes it's converting right now!",
              "score": 2,
              "created_utc": "2026-02-03 16:24:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d3hyp",
          "author": "oldassveteran",
          "text": "Let‚Äôs gooooo!",
          "score": 5,
          "created_utc": "2026-02-03 16:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dzqq3",
          "author": "flavio_geo",
          "text": "Great performance on single XTX 7900 + Ryzen 7 9700X CPU 2x48gb 6000 MT/s with Q4\\_K\\_XL\n\n29.5 tokens/s (21.5 GB VRAM used)\n\nllama.cpp config:\n\n\"-ot\", \"blk.(2\\[0-9\\]|\\[3-4\\]\\[0-9\\]).ffn\\_.\\*\\_exps.=CPU\",\n\nUsing K and V type q8\\_0 and 64k token context setup\n\nNow lets go test the model in the daily works",
          "score": 5,
          "created_utc": "2026-02-03 18:43:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fsezt",
              "author": "BigYoSpeck",
              "text": "You should try the MXFP4 and if you aren't already ROCm for the backend. I'm on a Ryzen 9 5900X but only use 8 threads as performance caps out, 64gb DDR4 and a 16gb RX 6800 XT\n\nhttps://preview.redd.it/wlrgal8nadhg1.png?width=1368&format=png&auto=webp&s=e16215d54da118c756b7dea63c5c038f405f3179",
              "score": 3,
              "created_utc": "2026-02-03 23:56:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hxiif",
                  "author": "flavio_geo",
                  "text": "Thank you for the tip.\n\nJust tried the MXFP4 quant and got 32.7 tokens/s on same config. Using only \\~20.1GB VRAM.\n\nTried different option using Unsloth guide:\n\n\"-ot\", \"\\\\\\\\.(2\\[1-9\\]|\\[3-4\\]\\[0-9\\])\\\\\\\\.ffn\\_(gate|up|down)\\_exps.=CPU\",\n\nGot 34.9 tokens/s using 21.0GB VRAM\n\n\\*Feels like there is still room for improvement\n\n\\---\n\nAlso, for update: yesterday I tested the model inside my personal assistant platform (which is not for coding) and decided to try his coding skills just to check, and he just decide by himself (no instructions) to use my obsidian (which he has access too since he create tasks and notes for me), to create a note for tracking his coding task, and write the code with versions inside obsidian. That seems, at first, to indicate a very strong alignment towards agentic behavior. The code was very simple dinossaur pygame, so i cant say anything yet about his coding skills.",
                  "score": 2,
                  "created_utc": "2026-02-04 08:30:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mdtnk",
              "author": "usofrob",
              "text": "FYI, I tried keeping the kv cache unset through lm studio and saw a slight improvement in throughput with minimal impact on vram use.\n\nI've been using this model all day, and it's better than my ~70 GB versions of M2.1 and GLM 4.7 and any other models in this size that I've tested. I'm using it for python, json, html stuff today. I would run into jinja prompt template issues after 30k to 80k tokens until I removed \"| safe\" from the default template. I've gotten over 130k token usage without explicit errors, but it is having trouble with my current task. So, I may reset it soon to get a clean start again.\n\nBtw, I'm using opencode through lmstudio to 88GB of Amd VRAM.",
              "score": 2,
              "created_utc": "2026-02-04 23:21:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3t1f3y",
                  "author": "usofrob",
                  "text": "Lmstudio fixed the bug with the default template in their beta release today. I'm still using this model. vulkan is a little quicker than rocm 48 vs 42 tok/s. Prompt processing is about the same. Also, disabling FA uses less VRAM, but slightly slower pp.",
                  "score": 1,
                  "created_utc": "2026-02-05 23:14:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d5pit",
          "author": "Zeranor",
          "text": "Nice, let's see how this does compared to GLM 4.7 flash and Devstral 2 Small. But quick question: WHERE can I find the MXFP4 quants? :D I only find the \"regular\" quants.",
          "score": 3,
          "created_utc": "2026-02-03 16:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d6edn",
              "author": "yoracale",
              "text": "Sorry they're still converting ahaha will let u know once theyre up\n\nEdit: they're out now: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF?show_file_info=Qwen3-Coder-Next-MXFP4_MOE.gguf",
              "score": 7,
              "created_utc": "2026-02-03 16:29:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3d6vem",
                  "author": "Zeranor",
                  "text": "ahh yes, nice, I see, sorry for being too excited ;)",
                  "score": 2,
                  "created_utc": "2026-02-03 16:31:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3imz7c",
                  "author": "1-a-n",
                  "text": "Thanks! What sort of tps is expected on Blackwell 6000, 1st simple test with LMStudio and the MXFP4 guff only managed \\~44tps? Utilisation \\~60% power max 200W.",
                  "score": 1,
                  "created_utc": "2026-02-04 12:14:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d6xxq",
          "author": "FullstackSensei",
          "text": "Guess it's still uploading? Q8 isn't there yet üòÇ",
          "score": 2,
          "created_utc": "2026-02-03 16:32:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dlpuy",
              "author": "yoracale",
              "text": "Should be all up now!",
              "score": 2,
              "created_utc": "2026-02-03 17:40:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3dnovw",
                  "author": "FullstackSensei",
                  "text": "Thanks! Already halfway through the download\n\nWas checking the page every couple of mins üòÇ",
                  "score": 2,
                  "created_utc": "2026-02-03 17:49:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d7nn2",
              "author": "yoracale",
              "text": "You're right lol, I just realised. Will need to wait a few more mins xD",
              "score": 1,
              "created_utc": "2026-02-03 16:35:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dmeep",
          "author": "ChopSticksPlease",
          "text": "Any comparision to Devstral small 2, Qwen3 coder and GLM-4.7-Flash ?",
          "score": 2,
          "created_utc": "2026-02-03 17:43:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hj41y",
              "author": "gtrak",
              "text": "It's much better",
              "score": 1,
              "created_utc": "2026-02-04 06:22:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l6ttc",
                  "author": "SomeAcanthocephala17",
                  "text": "SPEED or Quality?",
                  "score": 1,
                  "created_utc": "2026-02-04 19:49:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ewzku",
          "author": "Suitable-Program-181",
          "text": "Damn this is so gooooood!",
          "score": 2,
          "created_utc": "2026-02-03 21:18:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fz8z3",
              "author": "Motor_Ocelot_1547",
              "text": "more story pz",
              "score": 1,
              "created_utc": "2026-02-04 00:33:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3if40k",
          "author": "timbo2m",
          "text": "Oh wow, on my i9 rig with a 4090 with only 32GB of RAM I can get 32 tokens per second.\n\nAMAZING\n\nhttps://preview.redd.it/evieatpynghg1.png?width=977&format=png&auto=webp&s=316dd91249530bb544c780283491d3eeeab1d129",
          "score": 2,
          "created_utc": "2026-02-04 11:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d3eu2",
          "author": "PrefersAwkward",
          "text": "Can anyone recommend a good tool that can use a local LLM like this for development?¬†",
          "score": 2,
          "created_utc": "2026-02-03 16:15:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d4u53",
              "author": "yoracale",
              "text": "We made a guide for Codex and Claude Code which you can view here: [https://unsloth.ai/docs/models/qwen3-coder-next#improving-generation-speed](https://unsloth.ai/docs/models/qwen3-coder-next#improving-generation-speed)",
              "score": 9,
              "created_utc": "2026-02-03 16:22:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3dt2m4",
              "author": "synth_mania",
              "text": "Aider's community fork, \"cecli\" is a good bet.\n\n[https://github.com/dwash96/cecli](https://github.com/dwash96/cecli)",
              "score": 3,
              "created_utc": "2026-02-03 18:13:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3i22ab",
              "author": "timbo2m",
              "text": "For llama.cpp:  \nUI: [https://github.com/ggml-org/llama.cpp/discussions/16938](https://github.com/ggml-org/llama.cpp/discussions/16938)  \nAPI: [https://github.com/ggml-org/llama.cpp/tree/master/tools/server](https://github.com/ggml-org/llama.cpp/tree/master/tools/server)",
              "score": 2,
              "created_utc": "2026-02-04 09:13:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d6q62",
              "author": "dsartori",
              "text": "Cline recommends qwen3-coder and they work really well together. This should be good too.",
              "score": 1,
              "created_utc": "2026-02-03 16:31:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gra2l",
              "author": "stuckinmotion",
              "text": "I've had good experience with roo code",
              "score": 1,
              "created_utc": "2026-02-04 03:11:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ddua6",
          "author": "Fox-Lopsided",
          "text": "I wonder how fast it would be with 16 VRAM and 32DRAM",
          "score": 1,
          "created_utc": "2026-02-03 17:04:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhs4v",
              "author": "yoracale",
              "text": "10+ tokens/s",
              "score": 1,
              "created_utc": "2026-02-03 17:22:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3de3wz",
          "author": "KillerX629",
          "text": "Is there any chance of getting a QAD version? Very interested in looking at how that performs",
          "score": 1,
          "created_utc": "2026-02-03 17:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dtnfl",
              "author": "yoracale",
              "text": "QAD or QAT?",
              "score": 1,
              "created_utc": "2026-02-03 18:16:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3frk96",
                  "author": "KillerX629",
                  "text": "QAD, the one recently proposed by nvidia",
                  "score": 1,
                  "created_utc": "2026-02-03 23:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dfd7k",
          "author": "Proper_Taste_6778",
          "text": "Could you make your version of Step 3.5 Flash?",
          "score": 1,
          "created_utc": "2026-02-03 17:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhl5b",
              "author": "yoracale",
              "text": "I'm not sure if llama.cpp supports it tbh",
              "score": 2,
              "created_utc": "2026-02-03 17:21:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3diwpy",
                  "author": "Proper_Taste_6778",
                  "text": "They're working on it probablyüòÖ thx for fast answer \n\nhttps://github.com/stepfun-ai/Step-3.5-Flash/blob/main/llama.cpp/docs/step3.5-flash.md",
                  "score": 2,
                  "created_utc": "2026-02-03 17:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3eke2o",
                  "author": "Mr_Back",
                  "text": "[https://github.com/ggml-org/llama.cpp/pull/19283](https://github.com/ggml-org/llama.cpp/pull/19283)",
                  "score": 1,
                  "created_utc": "2026-02-03 20:19:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dil5r",
          "author": "alfpacino2020",
          "text": "funvionara  con 16vram y 48 ram?  en llm studio?",
          "score": 1,
          "created_utc": "2026-02-03 17:26:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dkolh",
              "author": "yoracale",
              "text": "Yes it works, just follow our guide: [https://unsloth.ai/docs/models/qwen3-coder-next](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 1,
              "created_utc": "2026-02-03 17:35:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dv2tk",
          "author": "milkipedia",
          "text": "Nice that there is an MXFP4 quant in there! Going to give this a try soon",
          "score": 1,
          "created_utc": "2026-02-03 18:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dy30q",
          "author": "fernando782",
          "text": "Any benchmark comparison with Claude Sonnet 4.5, Claude Opus 4.5? those are the best coding models out there!\n\n",
          "score": 1,
          "created_utc": "2026-02-03 18:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3etnod",
          "author": "turbomedoqa",
          "text": "I tried the MXFP4 version and it flies at 50 t/s on 48GB VRAM. I am using it at Temperature 0.1. Is there any reason why would I run it at 1.0 for coding or instructions?",
          "score": 1,
          "created_utc": "2026-02-03 21:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3euuvh",
          "author": "TheSpicyBoi123",
          "text": "Do you have images of the spectrogram of the generated music? Would be very interresting what it actually makes. Additionally, on which data was it trained? Its not exactly a \\*garden variety\\* project to find \\~thousands of hours of genuine lossless music. Either way, solid job!",
          "score": 1,
          "created_utc": "2026-02-03 21:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f1kxl",
          "author": "Skt_97",
          "text": "Has anyone had a chance to compare it with a 3.5 step flash? It would be interesting to see which of the two is better.",
          "score": 1,
          "created_utc": "2026-02-03 21:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3grgcl",
              "author": "stuckinmotion",
              "text": "In my preliminary testing step flash was struggling and qwen was doing well",
              "score": 1,
              "created_utc": "2026-02-04 03:12:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hlf0h",
                  "author": "Skt_97",
                  "text": "It's crazy how the Step 3.5 Flash benchmarks are so much higher (probably \"maxed\"?)\nWhat did you test with? I'd like to see how it performs with Opencode.",
                  "score": 1,
                  "created_utc": "2026-02-04 06:42:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fa3do",
          "author": "Status_Contest39",
          "text": "The open-source model supports the first echelon of speed, and this operation is so great that it takes off directly!",
          "score": 1,
          "created_utc": "2026-02-03 22:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fxlfx",
          "author": "LittleBlueLaboratory",
          "text": "Anyone else getting this error when¬† trying to use Q6_K_XL in llama.cpp?\n\n\nLlama_model_load: error loading model: missing tensor 'blk.0.ssm_in.weight'\n\n\nI have downloaded the model twice already thinking I just got a corrupted download or something but it keeps happening.",
          "score": 1,
          "created_utc": "2026-02-04 00:24:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gyu9e",
              "author": "yoracale",
              "text": "Can you try another quant and see if it still happens?",
              "score": 1,
              "created_utc": "2026-02-04 03:57:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ju5ho",
                  "author": "LittleBlueLaboratory",
                  "text": "I just tried Q2_K_XL and confirmed the exact same error. Must be something with my environment? Any suggestions on what I should look at to fix it? I just did a git pull on my llama.cpp right before trying this.",
                  "score": 1,
                  "created_utc": "2026-02-04 16:06:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3g4ljy",
          "author": "DaringNinja",
          "text": "I am definitely doing something wrong seeing everyone else's token numbers. Using a 3090 and 128gb RAM only seeing 7 tokens/s with MXFP4 on LM Studio.",
          "score": 1,
          "created_utc": "2026-02-04 01:02:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gwnta",
              "author": "yoracale",
              "text": "Did you try using llama.cpp isntead and follow our guide? it's more optimized",
              "score": 1,
              "created_utc": "2026-02-04 03:43:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jzrv0",
                  "author": "DaringNinja",
                  "text": "I hadn't, but finally set it up last night based on the guide. Around 28 t/s now! Totally usable, especially for a model that doesn't fully fit on vram.\n\n12900K, RTX 3090, 128gb DDR4 3300.",
                  "score": 1,
                  "created_utc": "2026-02-04 16:32:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hjdij",
              "author": "gtrak",
              "text": "I can get 30 t/s on lm studio with a 4090.",
              "score": 1,
              "created_utc": "2026-02-04 06:24:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3i64nf",
              "author": "turbomedoqa",
              "text": "I have 48gb VRAM (5000 blackwell) and 192GB ram. It runs completely on VRAM with 50t/s.",
              "score": 1,
              "created_utc": "2026-02-04 09:52:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3grzzf",
          "author": "ab2377",
          "text": "i have 48gb, on mb, tell me in your opinion which gguf quant will be best, or is there not much hope!",
          "score": 1,
          "created_utc": "2026-02-04 03:15:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hho1f",
          "author": "UfuomaBabatunde",
          "text": "*cries in 12 GB VRAM",
          "score": 1,
          "created_utc": "2026-02-04 06:10:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hmyhk",
          "author": "Zeranor",
          "text": "So, talking configuration: Would this be a model with which I should chose to \"offload MoE experts to CPU\"? (16 GB VRAM / 128 GB RAM) :)",
          "score": 1,
          "created_utc": "2026-02-04 06:55:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i0b0h",
          "author": "Calm-Republic9370",
          "text": "how much context would be able to experience if i have 48gb vram\n\n?",
          "score": 1,
          "created_utc": "2026-02-04 08:56:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i60il",
              "author": "turbomedoqa",
              "text": "Around 140.000, at least in my case. And it's fast, 50t/s.",
              "score": 1,
              "created_utc": "2026-02-04 09:50:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3if3tv",
          "author": "Spiritual_Leg_7683",
          "text": "Can this shit run on my RTX 3090?",
          "score": 1,
          "created_utc": "2026-02-04 11:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iqmq2",
          "author": "Americanuu",
          "text": "I might not ask this in the right place but what agentic code works decent on 32gb of RAM  and 8GB VRAM ?",
          "score": 1,
          "created_utc": "2026-02-04 12:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j3gqa",
          "author": "dwrz",
          "text": "/u/danielhanchen -- sorry to ping you directly, but with `llama.cpp` the model seems to constantly hallucinate missing closing braces. Seems like this is happening to others as well: https://www.reddit.com/r/LocalLLaMA/comments/1quvqs9/comment/o3edjam/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button. Do you have any insights on this? I'm using the Q8_0 GGUF.",
          "score": 1,
          "created_utc": "2026-02-04 13:54:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3juvem",
          "author": "zp-87",
          "text": "I will try to run it on 2x 5060TI 16GB + 96GB RAM. I hope it will work",
          "score": 1,
          "created_utc": "2026-02-04 16:10:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lnske",
              "author": "zp-87",
              "text": "It does work in LM Studio with RooCode (I had to edit prompt template and remove |safe).  \nGPU offload: 22, Context length: 100 000.  \n\\- Prompt Evaluation (Input): \\~48 tokens/second.  \n\\- Generation (Output): \\~3.6 tokens/second.  \nQuite slow but it works.\n\nEdit: with \"GPU Offload\" set to 48, \"Number of layers for MoE weights onto CPU\" set to 48 and K and V quantization set to Q4\\_0, I get 13.4 tokens/second.",
              "score": 1,
              "created_utc": "2026-02-04 21:10:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k3c2t",
          "author": "Shoddy_Bed3240",
          "text": "I‚Äôm using Unsloth Q8\\_K\\_XL (93.4 GB) across two GPUs with 56 GB total VRAM. Generation speed is about 35 tokens/sec, which is totally usable.",
          "score": 1,
          "created_utc": "2026-02-04 16:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nbsae",
          "author": "FartOnYourBoofMound",
          "text": "cool - is that why i was getting weird stuff like this?\n\nhttps://preview.redd.it/ov5s9ir38lhg1.png?width=1711&format=png&auto=webp&s=c9911275e8f92ac2542e5bc0a6c95d6daaf2b8a8",
          "score": 1,
          "created_utc": "2026-02-05 02:32:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o6b9h",
              "author": "yoracale",
              "text": "Yes you need to update llama.cpp and redownload our quants",
              "score": 1,
              "created_utc": "2026-02-05 05:51:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oehli",
          "author": "Proximity_afk",
          "text": "Hey, just a beginner here, what exact quantized model can I  run on 48gb VRAM (typically over an Agentic rag system)???",
          "score": 1,
          "created_utc": "2026-02-05 07:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3prqco",
          "author": "Creepy-Bell-4527",
          "text": "Anyone benchmark it on mlx yet?\n\nAlso is speculative decoding working yet with mlx?",
          "score": 1,
          "created_utc": "2026-02-05 13:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qk327",
          "author": "shrug_hellifino",
          "text": "Was there a bugged version, and we need to re-download?",
          "score": 1,
          "created_utc": "2026-02-05 16:09:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3r05qf",
              "author": "yoracale",
              "text": "Yes you'll need to redownload and update llama.cpp",
              "score": 2,
              "created_utc": "2026-02-05 17:24:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o481ngw",
          "author": "BackUpBiii",
          "text": "Wait till you try this on my newly released ide on GitHub GitHub.com/itsmehrawrxd repo RawrXD",
          "score": 1,
          "created_utc": "2026-02-08 08:40:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dwej6",
          "author": "No_Afternoon_4260",
          "text": "How benchmaxxed is it?",
          "score": 1,
          "created_utc": "2026-02-03 18:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i1qtg",
              "author": "RealisticPrimary8",
              "text": "probably a ton, no way it outperforms kimi k2.5 in the real world.",
              "score": 1,
              "created_utc": "2026-02-04 09:10:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d0tei",
          "author": "Otherwise_Wave9374",
          "text": "That 256K context + \"agentic coding\" angle is really interesting, local agent setups get way more usable once you can keep a lot of repo + docs in context without constant chunking. Have you noticed any gotchas with tool calling or long horizon tasks (like refactors) vs quick one shot codegen?\n\nIm always looking for notes on building coding agents and evaling them, a few posts Ive bookmarked are here: https://www.agentixlabs.com/blog/",
          "score": -5,
          "created_utc": "2026-02-03 16:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dd177",
              "author": "pokemonplayer2001",
              "text": "You‚Äôre such a shill, FO.",
              "score": 7,
              "created_utc": "2026-02-03 17:00:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k475p",
          "author": "Oxffff0000",
          "text": "How do we build machines with that amount of VRAM? The cards I know are only 24Gb. Does that mean, you'll have to install multiple nvidia cards?",
          "score": 0,
          "created_utc": "2026-02-04 16:52:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kln4x",
              "author": "kkazakov",
              "text": "Not necessarily. I have ADA 6000 and I plan to try it.",
              "score": 1,
              "created_utc": "2026-02-04 18:13:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pvffd",
                  "author": "Impossible_Art9151",
                  "text": "I am running llama.cpp, qwen3-next-coder-q8, --ctx-size 256000 -parallel 2 with an rtx A6000/48GB  \ngetting \\~20t/s  \nWhat is your setup/speed?",
                  "score": 1,
                  "created_utc": "2026-02-05 14:06:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3viflx",
              "author": "LizardViceroy",
              "text": "Mac Studio Max / Ultra, AMD Strix Halo / Point, or just lots of regular RAM + CPU or memory hotswapping. Speed won't be perfect but it'll run.",
              "score": 1,
              "created_utc": "2026-02-06 09:39:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3daw4d",
          "author": "getmevodka",
          "text": "How big is that ? I have 96gb vram available üòäüòÖüëç",
          "score": -4,
          "created_utc": "2026-02-03 16:50:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dg7gg",
              "author": "some_user_2021",
              "text": "The answer is on the post",
              "score": 2,
              "created_utc": "2026-02-03 17:15:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dgb02",
                  "author": "getmevodka",
                  "text": "Yes it is indeed. Thanks",
                  "score": 3,
                  "created_utc": "2026-02-03 17:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwp508",
      "title": "We created a Tool Calling Guide for LLMs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/ttfrqhl48phg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-05 16:00:39",
      "score": 215,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qwp508/we_created_a_tool_calling_guide_for_llms/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3uat10",
          "author": "fat_fun_xox",
          "text": "Kudos to you and team",
          "score": 4,
          "created_utc": "2026-02-06 03:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uznm8",
              "author": "yoracale",
              "text": "Thank you appreciate the support :)",
              "score": 2,
              "created_utc": "2026-02-06 06:44:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3szflf",
          "author": "RMK137",
          "text": "This is awesome. I've been wanting to get into this and build my own custom functions for Nemotron nano and Devstral small 2.",
          "score": 2,
          "created_utc": "2026-02-05 23:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t83s6",
          "author": "paul_tu",
          "text": "Great",
          "score": 2,
          "created_utc": "2026-02-05 23:52:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvt6qy",
      "title": "Qwen3-Coder-Next GGUFs updated - now produces much better outputs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/yoeghkey7ihg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-04 16:28:56",
      "score": 200,
      "num_comments": 58,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qvt6qy/qwen3codernext_ggufs_updated_now_produces_much/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3kl6da",
          "author": "Nieles1337",
          "text": "Hey, just wanted to check the update and noticed the MXFP4\\_MOE is still from yesterday, is this still cooking? Or is this the \"This week we'll also release a new MoE update\" you mention? Thanks!",
          "score": 7,
          "created_utc": "2026-02-04 18:10:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ks81r",
              "author": "GlobalLadder9461",
              "text": "Also q8 are not updated",
              "score": 3,
              "created_utc": "2026-02-04 18:42:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3limxk",
                  "author": "NebulaBetter",
                  "text": "I am waiting for this one too",
                  "score": 1,
                  "created_utc": "2026-02-04 20:46:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m7tb3",
                  "author": "Zc5Gwu",
                  "text": "Welp, that was a waste of bandwidth, sorry huggingface.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:48:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3n65m3",
                  "author": "danielhanchen",
                  "text": "Q8_0 and Q8_K_XL should be fine - updating llama.cpp is all you need - sorry should have mentioned it earlier",
                  "score": 1,
                  "created_utc": "2026-02-05 02:00:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m3ic7",
              "author": "StardockEngineer",
              "text": "u/yorascale Is mxfp4 ok or?",
              "score": 2,
              "created_utc": "2026-02-04 22:26:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n69fs",
                  "author": "danielhanchen",
                  "text": "MXFP4 is fine - just a llama.cpp update!",
                  "score": 1,
                  "created_utc": "2026-02-05 02:00:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3n62px",
              "author": "danielhanchen",
              "text": "Oh hey so sorry - MXFP4-MoE is fine - just a llama.cpp update would work!",
              "score": 1,
              "created_utc": "2026-02-05 01:59:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3qs32k",
              "author": "StardockEngineer",
              "text": "Looks like it updated overnight.",
              "score": 1,
              "created_utc": "2026-02-05 16:46:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lpe5v",
          "author": "zoyer2",
          "text": "Many thanks! The IQ4\\_XS now performs really good compared to before! This is for sure a great replacement for GPT-OSS 120B for coding\n\nhttps://preview.redd.it/51hzzo02ojhg1.png?width=1578&format=png&auto=webp&s=701d6334c31ff311ca5504e4a1d3910ac9a0ed45\n\n",
          "score": 4,
          "created_utc": "2026-02-04 21:18:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6jjv",
              "author": "danielhanchen",
              "text": "Fantastic!",
              "score": 1,
              "created_utc": "2026-02-05 02:02:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ko1fk",
          "author": "DocWolle",
          "text": "was the update needed? For me it seems fixed after updating llama.cpp.",
          "score": 3,
          "created_utc": "2026-02-04 18:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6bnq",
              "author": "danielhanchen",
              "text": "Yes imatrix ones still need them - BF16, Q8_0, Q8_K_XL, MXFP4 are fine, but the rest need updating",
              "score": 2,
              "created_utc": "2026-02-05 02:01:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3roa8r",
          "author": "turbomedoqa",
          "text": "2 days ago I run Qwen Coder Next MXFP4 and I was impressed (it run 50 t/s). Blackwell 5000 RTX PRO was utilized by 100%. I had LM Studio 0.4.1 and Llama 2.01 runtime.\n\nToday, I updated LM Studio to 0.4.2 and I downloaded a fixed Qwen Coder Next MXFP4 model. I also plugged in 3060tu on my PCI4 slot and moved my monitors off blackwell which now runs on 16x PCI5. So my Blackwell is totally dedicated for LLM work.\n\nNow the inference speed is 46 t/s, but utilization of Blackwell is 50-60%. How can I run it at 100%? Is this a bug or something else? \n\nI loaded Qwen 3 Coder 32b Q8 and it runs at \\~100% utlization with 150 t/s speed. I wonder why is blackwell under utilized on Blackwell and MXFP4 Qwen Coder Next model?\n\nHere are model configs: \n\nhttps://preview.redd.it/mida3ocs6qhg1.png?width=390&format=png&auto=webp&s=3c500fecd3029fb2e7a97352a4017b894554db4d\n\nAnyone experiencing something like that? It looks like something else might be the bottleneck or runtime issues? Thanks for any hints.",
          "score": 3,
          "created_utc": "2026-02-05 19:14:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k87c2",
          "author": "FinalsMVPZachZarba",
          "text": "Did the bug affect qwen3-next too?",
          "score": 2,
          "created_utc": "2026-02-04 17:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3koujm",
              "author": "DocWolle",
              "text": "not sure if new ggufs were needed.¬†\nThe prompt from¬†https://github.com/ggml-org/llama.cpp/issues/19305\n\n\nworks by just updating llama.cpp",
              "score": 3,
              "created_utc": "2026-02-04 18:27:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n6hj1",
                  "author": "danielhanchen",
                  "text": "Yes! But imatrix needs updating - I'll need to redo the other Qwen3-Next ones as well",
                  "score": 1,
                  "created_utc": "2026-02-05 02:02:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kikud",
              "author": "DocWolle",
              "text": "I think so",
              "score": 2,
              "created_utc": "2026-02-04 17:59:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kjisa",
          "author": "CogahniMarGem",
          "text": "Sorry but how about the tool call, are there anyone test it.",
          "score": 2,
          "created_utc": "2026-02-04 18:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lsg8t",
              "author": "zoyer2",
              "text": "tested IQ4\\_XS now webgl and javascript repo. No looping, loaded 100k context. Actually performs really well",
              "score": 2,
              "created_utc": "2026-02-04 21:33:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n6cg2",
                  "author": "danielhanchen",
                  "text": "Oh nice!",
                  "score": 1,
                  "created_utc": "2026-02-05 02:01:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3kknyn",
          "author": "[deleted]",
          "text": "Is it possible to make torrents or diffs to prevent having to redownload gigs of data again? Or are the models changed that much?",
          "score": 2,
          "created_utc": "2026-02-04 18:08:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6j2u",
              "author": "danielhanchen",
              "text": "Sadly imatrix changed a lot",
              "score": 1,
              "created_utc": "2026-02-05 02:02:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3nnn49",
              "author": "illkeepthatinmind",
              "text": "Boggles my mind torrents arent a thing with models.",
              "score": 1,
              "created_utc": "2026-02-05 03:41:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lxk05",
          "author": "Zeranor",
          "text": "Oh boy.... this is an excellent model for (minor) local coding tasks. This is working WAY better than my recent attempts with GLM 4.7 flash, Devstral 2 Small or Qwen3 Next .... nice",
          "score": 2,
          "created_utc": "2026-02-04 21:57:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6p6p",
              "author": "danielhanchen",
              "text": "Yes the model is really good! Qwen really cooked this time!",
              "score": 1,
              "created_utc": "2026-02-05 02:03:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o40pswp",
          "author": "Ok-Shower7286",
          "text": "Oh. Thanks a lot. ",
          "score": 2,
          "created_utc": "2026-02-07 03:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0ck9",
          "author": "boatbomber",
          "text": "Why does Qwen refuse to contribute to prevent these problems? It makes their models look worse on release when they underperform with community implementations of their architecture",
          "score": 2,
          "created_utc": "2026-02-04 16:35:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k1yb5",
              "author": "yoracale",
              "text": "This isn't related to them not contributing or not. There was no need for custom/new implementation because it's just using Qwen3 next arch which was already supported in llama.cpp\n\nIt was a normal human accident that caused this issue and it happens and was fixed. Even if Qwen helped for this release, this would've likely happened as it was a needle in a haystack situation.\n\nQwen team also previously contributed to llama.cpp for Qwen3 release. They need to allocate their time for smaller model releases like Qwen3-next otherwise there would a million repos to implement and contribute to\n\nI'm guessing they're working with the llama.cpp team for the next important Qwen model release.",
              "score": 22,
              "created_utc": "2026-02-04 16:42:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3k4oiw",
                  "author": "fancyrocket",
                  "text": "Is it worth downloading now or will i have to re-download it again in the near future for updates? Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-04 16:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lrdxn",
          "author": "kryptkpr",
          "text": "In case anyone hasn't learned this lesson yet you should wait a week before downloading any new model the first implementation is broke 95% of the tome",
          "score": 1,
          "created_utc": "2026-02-04 21:27:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ly9yg",
              "author": "ServiceOver4447",
              "text": "probably too much AI to release it\n\nokay, i'll leave myself out now.",
              "score": 1,
              "created_utc": "2026-02-04 22:00:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3n6lf4",
              "author": "danielhanchen",
              "text": "Yes it's best probably to wait 3 days ish - we also do bug fixes on day 1 or 2",
              "score": 1,
              "created_utc": "2026-02-05 02:02:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3qwg30",
              "author": "TokenRingAI",
              "text": "This bug affected the original Qwen Next which was released months ago",
              "score": 1,
              "created_utc": "2026-02-05 17:06:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3rkuzy",
              "author": "TaroOk7112",
              "text": "And miss all the fun?!",
              "score": 1,
              "created_utc": "2026-02-05 18:59:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mju9f",
          "author": "lukepacman",
          "text": "the version i downloaded about 12h ago generated at speed of 18 tok/s on apple silicon m1  \n  \nim on llama.cpp 7930 and IQ3\\_XXS\n\nwould the new version improve the speed?",
          "score": 1,
          "created_utc": "2026-02-04 23:54:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6n2e",
              "author": "danielhanchen",
              "text": "Speed not so much, but definitely accuracy",
              "score": 1,
              "created_utc": "2026-02-05 02:02:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nryk2",
                  "author": "lukepacman",
                  "text": "yeah im re-downloading the model and will retry to see how it's going then.\n\njust curious, since this is also a MoE 3B active params model, do you know why it's significantly slower than the others in the same class?",
                  "score": 1,
                  "created_utc": "2026-02-05 04:09:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mqqjl",
          "author": "iadanos",
          "text": "Does this change affect previous Qwen-Next models?",
          "score": 1,
          "created_utc": "2026-02-05 00:32:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6nll",
              "author": "danielhanchen",
              "text": "Yes sadly I have to reupload",
              "score": 3,
              "created_utc": "2026-02-05 02:03:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n99o2",
                  "author": "iadanos",
                  "text": "Would you kindly give a notice once it's done?",
                  "score": 1,
                  "created_utc": "2026-02-05 02:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pzyvs",
          "author": "wisepal_app",
          "text": "In your site says: \"If your quant fully fits on your device, expect 20+ tokens/s. If it doesn't fit, it'll still work by offloading but it will be slower.\"  \nWhat do you mean with \"fully fit\"? Fully fit to VRAM or fully fit to \"disk space + RAM + VRAM\"? And with which quant and context window 20+ tokens/s we get? (i have a 16 gb vram+96 gb ddr5 ram laptop)",
          "score": 1,
          "created_utc": "2026-02-05 14:31:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qjex7",
          "author": "Rand_o",
          "text": "This update actually broke things for me. Previous version didnt have issues, this new one has many issues with tool calls failing",
          "score": 1,
          "created_utc": "2026-02-05 16:06:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rfnyp",
          "author": "iadanos",
          "text": "Sorry for asking each time, but, did anyone check that these fixes don't improve Qwen3-30B-A3B and the respective Coder model? Llama.cpp had some important fixes during last months and it might improve old arch models as well. (or, please correct me if I'm wrong)",
          "score": 1,
          "created_utc": "2026-02-05 18:35:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zfhlz",
              "author": "yoracale",
              "text": "No these changes only affect Qwen3-Next and qwen3nextcoder",
              "score": 1,
              "created_utc": "2026-02-06 22:36:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44w9jj",
                  "author": "iadanos",
                  "text": "Good. Thanks.",
                  "score": 1,
                  "created_utc": "2026-02-07 20:04:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o46o00l",
          "author": "raiffuvar",
          "text": "46gb ram? Can it run on CPU?",
          "score": 1,
          "created_utc": "2026-02-08 02:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47w2s6",
              "author": "yoracale",
              "text": "Yes it can yes",
              "score": 1,
              "created_utc": "2026-02-08 07:48:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qt7q4c",
      "title": "I bullied my dual 3060s into ruinning GLM-4.7-Flash  500+ T/s @ 70k Context on a Ryzen 2500 Potato. (Two Configs: \"Daily Driver\" vs. \"The Diesel Factory\")",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/gallery/1qt7q4c",
      "author": "MohammedGomaa",
      "created_utc": "2026-02-01 19:15:29",
      "score": 70,
      "num_comments": 46,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qt7q4c/i_bullied_my_dual_3060s_into_ruinning_glm47flash/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o310uy4",
          "author": "mukz_mckz",
          "text": "Please stop writing posts with AI.",
          "score": 19,
          "created_utc": "2026-02-01 19:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o312ok4",
              "author": "MohammedGomaa",
              "text": "i am not a native english speaker , i used ai to polish it",
              "score": 3,
              "created_utc": "2026-02-01 20:03:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31gwup",
                  "author": "--Spaci--",
                  "text": "Id rather struggle to read your paragraph, than have to read an AI's attempt at conveying information",
                  "score": 14,
                  "created_utc": "2026-02-01 21:13:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o34s89y",
              "author": "marko_mavecki",
              "text": "Huh? Maybe he can't? I am here for information. Not for formatting or the beauty of words. Besides, in few months you will no longer be able to differentiate between human and machine. Will it then be acceptable for you?",
              "score": 1,
              "created_utc": "2026-02-02 10:19:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3277zm",
          "author": "i_wayyy_over_think",
          "text": "Thanks for the detailed run configs. What do you use to actually use 64 concurrent quest at once besides running a benchmark? Can moltbot use that many concurrent requests effectively to get actual useful work done? I‚Äôve not tried it yet.",
          "score": 3,
          "created_utc": "2026-02-01 23:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o329d4z",
              "author": "MohammedGomaa",
              "text": "A swarm of AI agents plus multiple instance open code and the other coding agents",
              "score": 2,
              "created_utc": "2026-02-01 23:38:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o348hst",
          "author": "Previous_Nature_5319",
          "text": "good job, I think it will be useful for many people. thank you!",
          "score": 3,
          "created_utc": "2026-02-02 07:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37j6xu",
          "author": "Shivam_R_A",
          "text": "This is just awesome man!",
          "score": 3,
          "created_utc": "2026-02-02 19:25:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33h8sb",
          "author": "Soft_Syllabub_3772",
          "text": "Can modify to use with rtx3090 x 2 and 192gb ram and 1tb nvme?",
          "score": 2,
          "created_utc": "2026-02-02 03:47:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33hp44",
              "author": "MohammedGomaa",
              "text": "Go fo it , the sky is the limit",
              "score": 2,
              "created_utc": "2026-02-02 03:50:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38kj0z",
          "author": "SectionCrazy5107",
          "text": "i have not been able to get vllm to work with glm 4.7 flash on my v100+3080 so far, does sglang work out of the box? or any specific cuda + python + .. required?",
          "score": 2,
          "created_utc": "2026-02-02 22:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b3u9r",
              "author": "MohammedGomaa",
              "text": "uv pip install sglang==0.3.2.dev9039+pr-17247.g90c446848 --extra-index-url [https://sgl-project.github.io/whl/pr/](https://sgl-project.github.io/whl/pr/)\n\nuv pip install git+https://github.com/huggingface/transformers.git@76732b4e7120808ff989edbd16401f61fa6a0afa",
              "score": 2,
              "created_utc": "2026-02-03 08:07:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o316ecp",
          "author": "heaven00",
          "text": "Interesting stuff will have to try out",
          "score": 2,
          "created_utc": "2026-02-01 20:21:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31mixg",
          "author": "AbheekG",
          "text": "How do you handle the cache going stale? Doesn‚Äôt that happen fairly frequently with a codebase?",
          "score": 1,
          "created_utc": "2026-02-01 21:40:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31nnv4",
              "author": "MohammedGomaa",
              "text": "Sglang finds previously computed tokens and reuses it , very useful for ai agents  play books , system prompts and skills, i ran scheduled script    that removes chunks not accessed in the last 7 days",
              "score": 4,
              "created_utc": "2026-02-01 21:45:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31nsth",
                  "author": "AbheekG",
                  "text": "Very cool, thanks!",
                  "score": 3,
                  "created_utc": "2026-02-01 21:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34ef1f",
          "author": "Aggravating_Bee3757",
          "text": "what model did you use? can i use it in my single 5060 ti 16/16?",
          "score": 1,
          "created_utc": "2026-02-02 08:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34lc83",
              "author": "MohammedGomaa",
              "text": "**GLM-4.7-Flash**¬†(the¬†`QuantTrio-AWQ`¬†flavor) , i dont think so , not without c\\[u offloading tanking performance",
              "score": 2,
              "created_utc": "2026-02-02 09:12:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34u3r2",
          "author": "jannemansonh",
          "text": "impressive setup for agent swarms... curious though - for production agent workflows, ended up using needle app since you just describe what you want vs configuring cuda graphs and cache layers. kept the self-hosted setup for experimenting but way easier when agents need to understand code/docs",
          "score": 1,
          "created_utc": "2026-02-02 10:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o355g99",
              "author": "MohammedGomaa",
              "text": "sorry , i dont realy get what you are asking about , BTW setting \n\n    --cuda-graph-bs 4 16 32  # makes sure that single requist get 20-70 t/s , depending on context cach , ie single  agent get 20-70 t/s depending on caching context and concurancy , with 450 + t/s on max concurancy",
              "score": 2,
              "created_utc": "2026-02-02 12:12:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3icgpi",
          "author": "Flkhuo",
          "text": "ŸäÿßÿπŸÖ ŸÇŸÑÿ™ŸÑŸÉ ÿπŸÑŸä ÿßŸÑŸÅŸäÿ≥ ÿ®ŸàŸÉ ŸÖÿ™ŸÉÿ™ÿ®ÿ¥ ÿ®ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿßŸà ÿπŸÑŸä ÿßŸÑÿßŸÇŸÑ ŸÖÿ™ÿÆŸÑŸäŸáŸàÿ¥ ŸäŸÉÿ™ÿ® ŸÉÿ™Ÿäÿ± ŸÉŸÑÿßŸÖ ŸÉŸÑŸàÿ¥ ŸÑÿßÿ≤ŸÖŸá ÿπŸÑÿ¥ÿßŸÜ ÿ∞ÿßŸÉÿ±ÿ™ŸÜÿß ÿßÿµÿ®ÿ≠ÿ™ ŸÖÿ´ŸÑ ÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑÿ≥ŸÖŸÉŸá ŸÖŸÜ ŸÉÿ´ÿ± ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ÿå ÿßŸÑŸÜÿßÿ≥ ŸÉŸÑŸáÿß ÿπÿ±ÿ® Ÿàÿßÿ¨ÿßŸÜÿ® ÿßÿπÿ∑ŸàŸÉ ŸÅŸäÿØ ÿ®ÿßŸÉ ÿßŸÜŸÉ ŸÖÿ™ŸÉÿ™ÿ®ÿ¥ ŸÉÿ™Ÿäÿ± ÿ®ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸäÿå Ÿäÿßÿ±ÿ® ÿ™ÿßÿÆÿ∞ Ÿáÿ∞ÿß ÿßŸÑŸÉŸàŸÖŸÜÿ™ ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿßŸäÿ¨ÿßŸäŸá Ÿàÿ¥ÿ∫ŸÑŸÉ ÿ¨ŸÖŸäŸÑŸÉ ŸàÿßŸÜ ÿ¥ÿßÿ° ÿßŸÑŸÑŸá ŸÖŸÖŸÉŸÜ ÿ™ÿ∑Ÿàÿ± ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿßÿ≠ÿ≥ŸÜ ŸÖŸÜ ŸÉÿØÿß ÿ®ÿßŸÑÿ™ŸàŸÅŸäŸÇ",
          "score": 1,
          "created_utc": "2026-02-04 10:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j5qoh",
              "author": "MohammedGomaa",
              "text": "ÿ®ŸÉÿ±Ÿá ŸàŸÑÿß ÿ®ÿπÿØŸá ŸÖÿ¥ ŸáŸäŸÅÿ±ŸÇ ŸÉÿ™Ÿäÿ± ŸÑŸÖÿß ÿßŸÉÿ™ÿ® ÿ®ÿßŸäÿØŸä 5 % Ÿà Ÿäÿ∑ŸÑ ÿ®ÿ¨ŸàÿØŸá 90%  ÿßÿ≠ÿ≥ŸÜ ŸÖÿßŸÉÿ™ÿ® 100% ÿ®ÿ¨ŸàÿØŸá 100% .... ÿßŸÜÿß ÿßŸàŸÑŸä ÿ®ŸàŸÇÿ™Ÿä ÿßŸÜÿ™ ÿ≠ÿßÿ® ÿ™ÿ≥ÿ™ŸÅŸäÿØ ŸÖŸÜ ÿßŸÑŸÉŸÑÿßŸÖ ÿßÿ¥ÿ∑ÿß ŸÖÿ¥ ÿ≠ÿßÿ®ÿ® ÿ®ÿ±ÿ∂Ÿàÿß ÿßÿ¥ÿ∑ÿß .... ÿßŸÜÿß ÿßŸÜÿ¥ÿ± ŸÅŸä ŸÉÿ∞ÿß ŸÖŸÉÿßŸÜ .. ÿßŸÑŸä ÿ±ŸÉÿ≤Ÿàÿß ŸÅŸä ÿßŸÑŸÅÿ±ÿπ Ÿàÿ≥ÿßÿ®Ÿàÿß ÿßŸÑÿßÿµŸÑ 2-3 ŸÖŸÜ  Ÿà ÿßŸÑŸä ŸÜÿßŸÇÿ¥Ÿàÿß ÿ≥ÿßŸÑŸä ÿπŸÑŸä ÿ™ŸÅÿßÿµŸÑ 200+ ÿ∂ÿπŸÅ",
              "score": 1,
              "created_utc": "2026-02-04 14:07:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3j67vy",
              "author": "MohammedGomaa",
              "text": "Ÿà ÿßŸÑŸä ŸÖÿ¥ ÿπÿßÿ¨ÿ® ÿπÿßÿ¨ÿ® ÿ∫Ÿäÿ± : [https://www.reddit.com/r/LocalLLM/comments/1qt5l53/comment/o3gpvom/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLM/comments/1qt5l53/comment/o3gpvom/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)Ÿäÿßÿ±Ÿäÿ®ÿ™ ŸÜÿ±ŸÉÿ≤ ŸÅŸä ÿßŸÑŸÖŸáŸÖ",
              "score": 1,
              "created_utc": "2026-02-04 14:09:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ruc6l",
          "author": "ajmusic15",
          "text": "Bruh, I'v 29 tok/s per stream with Qwen3 Coder Next Q4\\_K\\_M at 128K context in my R9 9950X + RTX 5080 (16 GB) setup, and i try my best to increase that throughput.",
          "score": 1,
          "created_utc": "2026-02-05 19:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43ltq3",
          "author": "portlandstreet2",
          "text": "Does this code as well as claude?",
          "score": 1,
          "created_utc": "2026-02-07 16:12:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31gezn",
          "author": "alhinai_03",
          "text": "no one will bother reading AI produced text.",
          "score": 0,
          "created_utc": "2026-02-01 21:10:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32d7hy",
              "author": "MohammedGomaa",
              "text": "You know you are on an AI focused subreddit almost 99% of AI mentioned here argued towards Text generation",
              "score": 7,
              "created_utc": "2026-02-01 23:59:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32gt6c",
                  "author": "alhinai_03",
                  "text": "Brother, this doesn't mean I'm okay with reading AI generated text. I get it if it's not your first language but I'd rather read human written content no matter how bad the grammar is. I'm sure you have good intentions but people are generally getting tired of it.",
                  "score": 2,
                  "created_utc": "2026-02-02 00:19:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvvks9",
      "title": "TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF ¬∑ Hugging Face",
      "subreddit": "unsloth",
      "url": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF",
      "author": "No-Intention-5521",
      "created_utc": "2026-02-04 17:54:22",
      "score": 45,
      "num_comments": 19,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qvvks9/teichaiglm47flashclaudeopus45highreasoningdistillg/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o3kkfeq",
          "author": "KvAk_AKPlaysYT",
          "text": "Just 250 examples enough for a valuable distillation? Where are the benches?",
          "score": 8,
          "created_utc": "2026-02-04 18:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lh2i4",
              "author": "Zyguard7777777",
              "text": "Thinking the same, would be nice to have even a small bench to compare",
              "score": 4,
              "created_utc": "2026-02-04 20:38:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mlias",
                  "author": "arman-d0e",
                  "text": "I was thinking HumanEval and maybe LiveCodeBench? Or would you suggest others",
                  "score": 1,
                  "created_utc": "2026-02-05 00:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ml4hv",
              "author": "arman-d0e",
              "text": "Our goal with this model (as with all our others) has been to distill CoT not knowledge. 250 is still small don‚Äôt get me wrong, however it‚Äôs surprisingly proven to still successfully teach the student model to ‚Äúthink like‚Äù the teacher (given the diverse types of prompts). I‚Äôm currently looking at rentable gpus to do some benchmarks. Feel free to suggest what benchmarks you‚Äôd like to see for this model.\n\nI wouldn‚Äôt have much expectation though as the model could use some form of RL to better adapt to it‚Äôs new CoT",
              "score": 3,
              "created_utc": "2026-02-05 00:01:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sivsf",
                  "author": "Frequent-Mud8705",
                  "text": "have you tried training on a bunch of claude code logs? I have all of my history with the tool saved, do you think it would make it noticeably better?",
                  "score": 1,
                  "created_utc": "2026-02-05 21:40:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3lnl5z",
              "author": "ethereal_intellect",
              "text": "Afaik the whole idea was to lessen the abysmally long thinking, opus thinks 50x less. But yeah benches would be nice to see for a lot of these :/",
              "score": 2,
              "created_utc": "2026-02-04 21:09:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3md9kz",
                  "author": "ClimateBoss",
                  "text": "or try it can it even code bruh ?",
                  "score": 1,
                  "created_utc": "2026-02-04 23:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mw0be",
              "author": "CoruNethronX",
              "text": "Check this: [Less Is More for Reasoning](https://arxiv.org/abs/2502.03387)",
              "score": 2,
              "created_utc": "2026-02-05 01:01:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o4z9d",
                  "author": "Thrumpwart",
                  "text": "Then check this: [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)",
                  "score": 3,
                  "created_utc": "2026-02-05 05:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o46e3bw",
              "author": "Embarrassed_Reply92",
              "text": "The guy was a rookie... it's not a real distill. Just some amature listing crap on HF. \n\n100,000 minimum for a distill. Get this clown out of there. ",
              "score": 2,
              "created_utc": "2026-02-08 01:11:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mypb8",
          "author": "CoruNethronX",
          "text": "Checked the actual dataset and looks, like it's 90% filled with the same question \"what is requerments\", rephrased in different ways. But also includes more practical questions, like \"how to subtract 47 from 89\". Not sure that it's really useful for programming and especially for agentic skills, that are strongest sides of glm 4.7 flash in comparison to other open models of the similar size.",
          "score": 9,
          "created_utc": "2026-02-05 01:17:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nsvrg",
          "author": "Successful_Bit7710",
          "text": "Anyone use and perform benchmarks?",
          "score": 1,
          "created_utc": "2026-02-05 04:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oz7pu",
              "author": "zoyer2",
              "text": "Tested it, it seems promising, does things well but always messes up something. I'm not sure how it's trained etc but this looks very similar to the first version of glm 4.7 flash from unsloth which made silly mistakes",
              "score": 3,
              "created_utc": "2026-02-05 10:17:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ofk9b",
              "author": "kironlau",
              "text": "worst than original one, tested MXFP4 quant, a few days ago.",
              "score": 2,
              "created_utc": "2026-02-05 07:09:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3oalfu",
              "author": "TokenRingAI",
              "text": "Not needed, falls apart under basic testing.",
              "score": 1,
              "created_utc": "2026-02-05 06:26:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qx1ye4",
      "title": "koute/GLM-4.7-Flash-Derestricted",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qx1ye4/kouteglm47flashderestricted/",
      "author": "Witty_Mycologist_995",
      "created_utc": "2026-02-05 23:56:38",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "https://huggingface.co/koute/GLM-4.7-Flash-Derestricted",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qx1ye4/kouteglm47flashderestricted/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qw0ch3",
      "title": "FSDP with Unsloth",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qw0ch3/fsdp_with_unsloth/",
      "author": "Potential_Nerve_4381",
      "created_utc": "2026-02-04 20:43:51",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "I'm trying to load Qwen3-30B-A3B model on my g5.12xlarge GPU. I need to shard the model as it doesn't fit in one GPU. Does anyone have an example working script that runs FSDP with Unsloth and Hugging face Trainer? I can't seem to find one anywhere ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qw0ch3/fsdp_with_unsloth/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o3ot2ct",
          "author": "rjtannous",
          "text": "unsloth optimized FSDP is coming soon , however FSDP should still work if you disable torch compile by setting os.environ\\[\"TORCH\\_COMPILE\\_DISABLE\"\\] = \"1\"",
          "score": 1,
          "created_utc": "2026-02-05 09:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xl44b",
              "author": "Potential_Nerve_4381",
              "text": "I tried. I couldn't make it work. It'd be great if you share a working script¬†",
              "score": 1,
              "created_utc": "2026-02-06 17:10:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45pp0f",
          "author": "Mac_NCheez_TW",
          "text": "Call me ignorant and put lipnstick on me, but what is FSDP? Nvm I'll find it and learn. I'm running multigpu setup am I missing something? I'm sure I am I never research enough. brb.¬†",
          "score": 1,
          "created_utc": "2026-02-07 22:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45q2bt",
              "author": "Mac_NCheez_TW",
              "text": "Okay it's for training, I haven't even started down that path I just built my EPYC system after struggling to afford ram.¬†",
              "score": 1,
              "created_utc": "2026-02-07 22:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qurswa",
      "title": "Vllm is not supported in Asus GX10 machine",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qurswa/vllm_is_not_supported_in_asus_gx10_machine/",
      "author": "maayon",
      "created_utc": "2026-02-03 13:23:20",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "The SFT notebooks run properly in ASUS gx10 but when i try to run GRPO the vLLM installation corrupts the venv installations.\n\nIs there anyway to run GRPO notebooks without vllm ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qurswa/vllm_is_not_supported_in_asus_gx10_machine/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o3c9b5p",
          "author": "Final-Rush759",
          "text": "Pip install accelerator, I think.",
          "score": 2,
          "created_utc": "2026-02-03 13:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ccir8",
              "author": "eleqtriq",
              "text": "This sounds right",
              "score": 1,
              "created_utc": "2026-02-03 14:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3c7knx",
          "author": "eleqtriq",
          "text": "Can you be more specific?  What notebook?  What is the error?",
          "score": 1,
          "created_utc": "2026-02-03 13:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3c831y",
              "author": "maayon",
              "text": "[https://github.com/unslothai/unsloth/issues/3976](https://github.com/unslothai/unsloth/issues/3976)",
              "score": 1,
              "created_utc": "2026-02-03 13:37:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3c8c1j",
                  "author": "eleqtriq",
                  "text": "You‚Äôre in a container?  What container did you use?",
                  "score": 1,
                  "created_utc": "2026-02-03 13:38:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qyopks",
      "title": "When replacing embed_tokens and lm_head with those from another model, is this implementation correct?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qyopks/when_replacing_embed_tokens_and_lm_head_with/",
      "author": "choco132134",
      "created_utc": "2026-02-07 20:38:05",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "In the KnitLM paper ([https://openreview.net/forum?id=2uctT30vTS](https://openreview.net/forum?id=2uctT30vTS)), they train a LoRA adapter on a **base** model and then merge/apply that adapter onto an **instruct** model. To keep the two models consistent, they replace the base model‚Äôs token embeddings (and also the LM head if it is not tied to the embeddings) with those from the instruct model.\n\nI‚Äôm trying to implement this with **Qwen3-8B**, and I‚Äôd like to ask whether the implementation below looks correct. I ran this on **Google Colab with an A100**. When I tried the same thing on an **L4**, I ran into OOM-related issues and ended up getting **meta tensors**, so it didn‚Äôt work properly.\n\nAlso, as far as I understand, **Qwen3-8B uses** `tie_word_embeddings = False`, so the input embeddings and `lm_head` are *not* tied, which is why I‚Äôm copying both.\n\n`%%capture`\n\n`import os, re`\n\n`if \"COLAB_\" not in \"\".join(os.environ.keys()):`\n\n`!pip install unsloth`\n\n`else:`\n\n`# Do this only in Colab notebooks! Otherwise use pip install unsloth`\n\n`import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)`\n\n`xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")`\n\n`!pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo`\n\n`!pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer`\n\n`!pip install --no-deps unsloth`\n\n`!pip install transformers==4.56.2`\n\n`!pip install --no-deps trl==0.22.2`\n\n`# =============================================================================`\n\n`# Hyperparameter configuration`\n\n`# =============================================================================`\n\n`LORA_R = 16`\n\n`LORA_ALPHA = 16`\n\n`PER_DEVICE_TRAIN_BATCH_SIZE = 16`\n\n`GRADIENT_ACCUMULATION_STEPS = 1`\n\n`PACKING = True`\n\n`NUM_TRAIN_EPOCHS = 1`\n\n`LEARNING_RATE = 2e-4`\n\n`MAX_SEQ_LENGTH = 2048`\n\n`# Model configuration`\n\n`BASE_MODEL = \"unsloth/Qwen3-8B-Base\"`\n\n`INSTRUCT_MODEL = \"unsloth/Qwen3-8B\"`\n\n`USE_INSTRUCT_EMBEDDINGS = True`\n\n`from unsloth import FastLanguageModel`\n\n`import torch`\n\n`# 1. Load the Base LLM`\n\n`print(\"[1/4] Loading Base LLM (backbone)...\")`\n\n`base_model, base_tokenizer = FastLanguageModel.from_pretrained(`\n\n`model_name = BASE_MODEL,`\n\n`max_seq_length = MAX_SEQ_LENGTH,`\n\n`load_in_4bit = False,`\n\n`)`\n\n`# 2. Load the Instruct LLM`\n\n`print(\"[2/4] Loading Instruct LLM (for embeddings)...\")`\n\n`instruct_model, instruct_tokenizer = FastLanguageModel.from_pretrained(`\n\n`model_name = INSTRUCT_MODEL,`\n\n`max_seq_length = MAX_SEQ_LENGTH,`\n\n`load_in_4bit = False,`\n\n`)`\n\n`def _is_meta(t: torch.Tensor) -> bool:`\n\n`return hasattr(t, \"device\") and t.device.type == \"meta\"`\n\n`def copy_qwen_embed_and_lm_head_exact(base_model, instruct_model, *, verbose: bool = True):`\n\n`\"\"\"`\n\n`Assumptions:`\n\n`- The Base and Instruct models have identical vocab_size / hidden_size (exact match).`\n\n`- For Qwen-style models where embeddings are NOT tied, copy both \\embed_tokens\\` and \\`lm_head\\`.\\``\n\n`What it does:`\n\n`- Prints the parameter shapes.`\n\n`- Copies weights in-place under torch.no_grad() (does NOT use .data).`\n\n`\"\"\"`\n\n`base_in = base_model.get_input_embeddings() # nn.Embedding`\n\n`inst_in = instruct_model.get_input_embeddings()`\n\n`base_out = base_model.get_output_embeddings() # nn.Linear (lm_head)`\n\n`inst_out = instruct_model.get_output_embeddings()`\n\n`if base_in is None or inst_in is None:`\n\n`raise ValueError(\"get_input_embeddings() returned None. Please check the model implementation.\")`\n\n`if base_out is None or inst_out is None:`\n\n`raise ValueError(\"get_output_embeddings() returned None. Please make sure this is a CausalLM.\")`\n\n`# Meta guard (prevents copying from tensors with no real storage)`\n\n`if _is_meta(inst_in.weight) or _is_meta(inst_out.weight):`\n\n`raise RuntimeError(\"instruct_model weights are on the 'meta' device (likely not fully loaded yet).\")`\n\n`# Get shapes`\n\n`base_in_shape = tuple(base_in.weight.shape)`\n\n`inst_in_shape = tuple(inst_in.weight.shape)`\n\n`base_out_shape = tuple(base_out.weight.shape)`\n\n`inst_out_shape = tuple(inst_out.weight.shape)`\n\n`# Print shapes`\n\n`if verbose:`\n\n`print(\"[Shapes]\")`\n\n`print(f\" base input_embeddings : {base_in_shape}\")`\n\n`print(f\" inst input_embeddings : {inst_in_shape}\")`\n\n`print(f\" base lm_head : {base_out_shape}\")`\n\n`print(f\" inst lm_head : {inst_out_shape}\")`\n\n`# Enforce exact match`\n\n`if base_in_shape != inst_in_shape:`\n\n`raise ValueError(f\"Input embedding shape mismatch: base={base_in_shape}, inst={inst_in_shape}\")`\n\n`if base_out_shape != inst_out_shape:`\n\n`raise ValueError(f\"LM head shape mismatch: base={base_out_shape}, inst={inst_out_shape}\")`\n\n`# Copy weights`\n\n`with torch.no_grad():`\n\n`base_in.weight.copy_(inst_in.weight)`\n\n`base_out.weight.copy_(inst_out.weight)`\n\n`if verbose:`\n\n`print(\"‚úì Copied input_embeddings and lm_head weights (exact match).\")`\n\n`return base_model`\n\n`copy_qwen_embed_and_lm_head_exact(base_model, instruct_model, verbose=True)`\n\n`# KnitLM-style assumption: use the Instruct tokenizer`\n\n`tokenizer = instruct_tokenizer`\n\n`print(f\"[Tokenizer] using instruct tokenizer. len(tokenizer)={len(tokenizer)}, vocab_size={tokenizer.vocab_size}\")`\n\n`# Safety check: ensure tokenizer IDs fit within the embedding matrix`\n\n`print(\"max token id (instruct tokenizer):\", max(instruct_tokenizer.get_vocab().values()))`\n\n`print(\"embedding rows:\", base_model.get_input_embeddings().weight.shape[0])`\n\nOutput:  \n\\[Shapes\\]\n\nbase  input\\_embeddings : (151936, 4096)\n\ninst  input\\_embeddings : (151936, 4096)\n\nbase  lm\\_head          : (151936, 4096)\n\ninst  lm\\_head          : (151936, 4096)\n\n‚úì Copied input\\_embeddings and lm\\_head weights (exact match).\n\n\\[Tokenizer\\] using instruct tokenizer. len(tokenizer)=151669, vocab\\_size=151643\n\nmax token id (instruct tokenizer): 151668\n\nembedding rows: 151936\n\nIf you think anything is missing, please let me know.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qyopks/when_replacing_embed_tokens_and_lm_head_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    }
  ]
}