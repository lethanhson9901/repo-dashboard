{
  "metadata": {
    "last_updated": "2026-01-23 02:26:24",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 10,
    "total_comments": 50,
    "file_size_bytes": 71585
  },
  "items": [
    {
      "id": "1qhscts",
      "title": "Run GLM-4.7-Flash locally Guide! (24GB RAM)",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/hgng1lc5ufeg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-20 05:22:23",
      "score": 198,
      "num_comments": 45,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qhscts/run_glm47flash_locally_guide_24gb_ram/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mccxs",
          "author": "TaroOk7112",
          "text": "Thanks!!!  \nDoes llama.cpp need to add some code to improve support or with this PR it's all supported?  \n[https://github.com/ggml-org/llama.cpp/pull/18936](https://github.com/ggml-org/llama.cpp/pull/18936)",
          "score": 8,
          "created_utc": "2026-01-20 05:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mdiub",
              "author": "danielhanchen",
              "text": "It should work in llama.cpp main now!",
              "score": 5,
              "created_utc": "2026-01-20 06:00:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0n9xr7",
                  "author": "goniz",
                  "text": "Does the repetitions happen on Q8 and BF16 GGUFs as well? Or just lower quants?",
                  "score": 3,
                  "created_utc": "2026-01-20 10:51:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0q5ocg",
                  "author": "TaroOk7112",
                  "text": "Be aware that flash attention degrades speed a lot, there are several PR in the works to improve support for  GLM-4.7-Flash in llama.cpp. It works, but is slower than even gpt-oss 120, so for now not too interesting.   \n[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/5#696f93fcbc8028d97e6fdf2b](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/5#696f93fcbc8028d97e6fdf2b)",
                  "score": 1,
                  "created_utc": "2026-01-20 19:59:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzq84",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:35:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0o83m1",
          "author": "maxpayne07",
          "text": "So far, yes",
          "score": 2,
          "created_utc": "2026-01-20 14:35:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzs2f",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mldpf",
          "author": "RMK137",
          "text": "Great turnaround! I just got my 5090 so this is perfect timing.",
          "score": 2,
          "created_utc": "2026-01-20 07:05:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzs82",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:36:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mjz8z",
          "author": "Psyko38",
          "text": "On a GPU with 16GB of VRAM, we're good at Q3?",
          "score": 1,
          "created_utc": "2026-01-20 06:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mma9p",
              "author": "Conscious_Chef_3233",
              "text": "you should go higher, i run qwen3 30b q4 on my 4070 12g",
              "score": 1,
              "created_utc": "2026-01-20 07:13:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mmg8z",
                  "author": "Psyko38",
                  "text": "But the weights, where do you put them in the RAM and you put the MoE in the VRAM?",
                  "score": 1,
                  "created_utc": "2026-01-20 07:14:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0tzsmj",
                  "author": "yoracale",
                  "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
                  "score": 1,
                  "created_utc": "2026-01-21 10:36:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzsdw",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u46lj",
                  "author": "Psyko38",
                  "text": "I already downloaded the model yesterday with your recommended settings and the result was not too bad. I'll try the new ones again, but in Q4 with the MoE weights on the GPU and the unused ones on the CPU.",
                  "score": 1,
                  "created_utc": "2026-01-21 11:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mmhfv",
          "author": "Unlikely_Database_87",
          "text": "In lm studio the model with those parameters you provided, cannot even give any clear response and it infinitely generates nonsense. Simple prompt I use - write java method  to merge two sorted arrays, no tests no explanation just code. My config 5080 and 64GB RAM",
          "score": 1,
          "created_utc": "2026-01-20 07:14:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mohp7",
              "author": "yoracale",
              "text": "You need to use dry multiplier which has the biggest impact, because LM Studio does not have it, you need to disable repeat penalty entirely.",
              "score": 3,
              "created_utc": "2026-01-20 07:32:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0mq2e5",
                  "author": "Unlikely_Database_87",
                  "text": "Thanks that helps",
                  "score": 1,
                  "created_utc": "2026-01-20 07:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tztmu",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mnfbw",
          "author": "Prudent-Ad4509",
          "text": "I'm not sure what they mean by full precision. The original seems to require 64Gb even with near zero context.",
          "score": 1,
          "created_utc": "2026-01-20 07:23:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n1ywr",
          "author": "Kirito_5",
          "text": "Thank you as always! \nWill try to see how well it compares to others on my 3090.",
          "score": 1,
          "created_utc": "2026-01-20 09:38:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nfyub",
              "author": "siggystabs",
              "text": "I really like GLM 4.7 Flash, but i can only fit a small context (like 5000 chars) on a single 3090 ‚Äî Not using System RAM. I end up having to split it across two 3090s to use it with decent context, and then it isn‚Äôt as fast.\n\nBut GLM 4.7 Flash is really exciting. The quality is excellent. If I didn‚Äôt have specific needs for my self-hosted apps, I‚Äôd probably use that exclusively for chatting. It‚Äôs definitely smarter than Qwen3 30B A3B in my usage.\n\nFor my workhorse applications I went back to Qwen3 30B A3B. Reluctantly. It‚Äôs just faster and better on VRAM, and ‚Äúgood enough‚Äù.",
              "score": 2,
              "created_utc": "2026-01-20 11:42:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tzu6e",
                  "author": "yoracale",
                  "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
                  "score": 1,
                  "created_utc": "2026-01-21 10:36:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzu0f",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nh57o",
          "author": "xanduonc",
          "text": "FP8 is half precision, original weights from glm are BF16",
          "score": 1,
          "created_utc": "2026-01-20 11:51:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nvq0g",
              "author": "yoracale",
              "text": "Yes thanks I edited my post",
              "score": 1,
              "created_utc": "2026-01-20 13:28:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nl0u3",
          "author": "maxpayne07",
          "text": "On LM studio, on the last step of reasoning, its starts a loop of repetition.  \n\n||\n|:-|",
          "score": 1,
          "created_utc": "2026-01-20 12:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzuge",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:36:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0no5zm",
              "author": "yoracale",
              "text": "Did you disable repeat penalty?",
              "score": 1,
              "created_utc": "2026-01-20 12:41:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0np93r",
                  "author": "maxpayne07",
                  "text": "yes, just put it to value 1, solved!!! Thanks",
                  "score": 1,
                  "created_utc": "2026-01-20 12:49:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pkbck",
          "author": "DuckyBlender",
          "text": "Will dynamic nvfp4 quants come out?",
          "score": 1,
          "created_utc": "2026-01-20 18:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0q1qym",
              "author": "Opposite-Station-337",
              "text": "https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4/tree/main\n\nAlready are. \n\n:edit: sorry, missed dynamic.",
              "score": 1,
              "created_utc": "2026-01-20 19:41:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tzur2",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 0,
              "created_utc": "2026-01-21 10:37:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qiwhk",
          "author": "Dramatic-Rub-7654",
          "text": "Is this model actually dumber than Qwen 3 Coder Flash, or is it just overly sensitive? To the point that with the --n-cpu-moe flag it gets stuck in an infinite loop repeating a single word, and without that flag it keeps creating endless files, all with errors, until the window runs out?",
          "score": 1,
          "created_utc": "2026-01-20 21:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rc1ba",
              "author": "yoracale",
              "text": "It's very sensitive. Did you try using our recommended parameters? It seems to be a must for the this model",
              "score": 2,
              "created_utc": "2026-01-20 23:23:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rm20t",
                  "author": "Dramatic-Rub-7654",
                  "text": "I tried using the parameters recommended on the Hugging Face page for llama-b7782/llama-server:\n\n-m GLM-4.7-Flash-Q4_K_M.gguf --host 0.0.0.0 --n-gpu-layers 999 -fa on -t 14 -n -1 -c 16384 --jinja --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1 \n\nThe only changes I experimented with were adding the --n-cpu-moe flag, which caused the model to bug out with severe repetition issues, and increasing the temperature to 1.0.\n\nAt temperature 1.0, the model‚Äôs reasoning and responses appear coherent, but when I try to use it with tools like Cline, it clearly doesn‚Äôt know what it‚Äôs doing. It can create and edit files and interact with the terminal, but it consistently outputs broken code and introduces errors when editing files.\n\nIn contrast, Qwen, even in version Q4, is capable of providing a fully functional implementation of Flappy Bird from start to finish. based on the tests I ran, the GGUF versions still need further refinement. I tested the model using the version available on OpenRouter, where it performs significantly better than in my GGUF-based tests. However, Coder Flash still demonstrates superior intelligence compared to this model.",
                  "score": 1,
                  "created_utc": "2026-01-21 00:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzv12",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:37:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qiu5w8",
      "title": "GLM-4.7-Flash GGUFs updated - now produces much better outputs!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qiu5w8/glm47flash_ggufs_updated_now_produces_much_better/",
      "author": "yoracale",
      "created_utc": "2026-01-21 10:13:23",
      "score": 142,
      "num_comments": 30,
      "upvote_ratio": 0.99,
      "text": "Hey guys after the issues in the past day or so, llama.cpp has fixed a bug which caused the model to loop and produce poor outputs. Huge thanks to the `llama.cpp` team and all contributors for the fix: [https://github.com/ggml-org/llama.cpp/pull/18980](https://github.com/ggml-org/llama.cpp/pull/18980)\n\nWe‚Äôve reconverted and reuploaded the model, so **you‚Äôll need to re-download it** for the fix to take effect:  \n[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nThe issue was GLM 4.7 Flash did not set `\"scoring_func\": \"sigmoid\"`in the config.json file. We added the metadata in, so no need to reinstall llama.cpp, just re-download the quants.\n\nAfter our testing, outputs are **significantly improved**, and you should be able to use Z.ai‚Äôs recommended sampling settings with great results:\n\n* **General use:** `--temp 1.0 --top-p 0.95`\n* **Tool-calling:** `--temp 0.7 --top-p 1.0`\n* If using llama.cpp, don't forget to set `--min-p 0.01` as the default is 0.1\n\nNo need to update llama.cpp, just redownload the quants.\n\nGuide: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)\n\nLet us know if you notice the improvement!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qiu5w8/glm47flash_ggufs_updated_now_produces_much_better/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0tzhea",
          "author": "PixelatedCaffeine",
          "text": "Thank you!",
          "score": 7,
          "created_utc": "2026-01-21 10:33:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0u095h",
              "author": "danielhanchen",
              "text": ":) Let me know how it goes!",
              "score": 2,
              "created_utc": "2026-01-21 10:40:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0wpnwy",
                  "author": "PixelatedCaffeine",
                  "text": "It's working without any loops now, thanks! I couldn't compare the generation speed, but looking good so far!",
                  "score": 1,
                  "created_utc": "2026-01-21 19:14:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uiuwb",
          "author": "Any_Pressure4251",
          "text": "Confirmed fixed in llama.cpp and the new GLM-4.7-Flash-Q4\\_K\\_M.gguf is sweet. \n\nThe generation speed slowed a lot I don't know which part is responsible.",
          "score": 4,
          "created_utc": "2026-01-21 13:00:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ulii6",
              "author": "yoracale",
              "text": "Awesome, mmm might be a FA issue, i remember theyre still trying to optimize it.",
              "score": 2,
              "created_utc": "2026-01-21 13:16:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0u6kgj",
          "author": "Appropriate_Car_5599",
          "text": "is GLM good model for general reasoning not just for coding tasks? or will qwen3 be better for that case?",
          "score": 2,
          "created_utc": "2026-01-21 11:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ubmfc",
              "author": "danielhanchen",
              "text": "Yes GLM is good for all! I would actually try using it as a speculator for the larger ones as well maybe!",
              "score": 3,
              "created_utc": "2026-01-21 12:12:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0wbhyp",
              "author": "zoyer2",
              "text": "so far code-wise imo its better than **Qwen3 30B A3B instruct**. I would say it might be a bit more stable than **Qwen3-Next-80B-A3B** but **Next** seems to be able to complete more complex tasks but fails a bit more often.",
              "score": 1,
              "created_utc": "2026-01-21 18:12:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0uax2s",
          "author": "alhinai_03",
          "text": "Has the inference speed issue been fixed as well? And can we use ```-fa on``` now?",
          "score": 2,
          "created_utc": "2026-01-21 12:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ubkes",
              "author": "danielhanchen",
              "text": "I think you're referring to https://github.com/ggml-org/llama.cpp/pull/18953 right - there seems to be some issues on the PR :(",
              "score": 1,
              "created_utc": "2026-01-21 12:11:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ud1lc",
                  "author": "alhinai_03",
                  "text": "Yes, I will keep waiting until the PR gets merged. My system always offloads things to system ram for some reason with -fa off which massively degrades performance for me. I really can't wait to try this model, seeing how well it performs while people are running it broken is crazy.",
                  "score": 3,
                  "created_utc": "2026-01-21 12:22:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0x185t",
                  "author": "TokenRingAI",
                  "text": "The PR is working fine for me",
                  "score": 1,
                  "created_utc": "2026-01-21 20:06:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uepwf",
          "author": "zoyer2",
          "text": "Thanks! Works as intended now I believe (not sure if the notable speed drop after some context load is fixable), managed to pass my coding tests! Looks like a solid model",
          "score": 2,
          "created_utc": "2026-01-21 12:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11p9eo",
              "author": "yoracale",
              "text": "Awesome, thanks for trying. have you tried disabling flashattention? might help speed",
              "score": 1,
              "created_utc": "2026-01-22 13:57:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14zk11",
                  "author": "zoyer2",
                  "text": "yep sure helps! but only for a short while :,D waiting for llama.cpp fix i guess",
                  "score": 1,
                  "created_utc": "2026-01-22 23:15:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uzby4",
          "author": "Sad-Masterpiece-4730",
          "text": "Can you explain please what is the right way to understand how much q8_k_xl is better than q4_k_xl? Are there any benchmarks or is there a scientific way to see this info? Regarding glm4.7 flash or in general usecase. Thanks.",
          "score": 2,
          "created_utc": "2026-01-21 14:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v29at",
              "author": "yoracale",
              "text": "Different models will always have difference quantization sensitivity but for now, you can view some analysis and graphs here: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)",
              "score": 2,
              "created_utc": "2026-01-21 14:46:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0x45xp",
                  "author": "NoahFect",
                  "text": "Is there *any* document that explains what the different parts of the filenames mean?  q4/q8 are obvious enough, but what are _k_ and _xl_ and _iq_ and the rest?",
                  "score": 1,
                  "created_utc": "2026-01-21 20:20:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11egz4",
          "author": "marko_mavecki",
          "text": "Works way better now. I am getting 45 t/s on my dual RTX3060 with the following complete command line. Remember that this is for CUDA only and you have to modify \"\\~/models/\" path - this is the place where you have to have the model downloaded to.\n\ndocker run --gpus all -p 11434:11434 -v \\~/models/:/models ghcr.io/ggml-org/llama.cpp:full-cuda --server -m /models/GLM-4.7-Flash-UD-Q4\\_K\\_XL.gguf --jinja --threads -8 --ctx-size 20000 --temp 0.7 --top-p 0.95  --port 11434 --host 0.0.0.0\n\nThreads param is only needed in case CPU needs to take over some calculations. But if the whole thing fits your GPU then it is almost useless.  \nThe model has been downloaded from [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/blob/main/GLM-4.7-Flash-UD-Q4\\_K\\_XL.gguf](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/blob/main/GLM-4.7-Flash-UD-Q4_K_XL.gguf)\n\nKudos to u/danielhanchen for quick reaction to feedback!",
          "score": 2,
          "created_utc": "2026-01-22 12:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11p1xw",
              "author": "yoracale",
              "text": "Amazing thanks so much for trying again! And awesome to hear it works for you",
              "score": 1,
              "created_utc": "2026-01-22 13:56:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0u06sk",
          "author": "danielhanchen",
          "text": "As an example after fixing the `\"scoring_func\": \"sigmoid\"` issue, we tried a long convo:\n\n    Hi\n    What is 2+2\n    Create a Python Flappy Bird game\n    Create a totally different game in Rust\n    Find bugs in both\n    Make the 1st game I mentioned but in a standalone HTML file\n    Find bugs and show the fixed game\n\nAnd we get:\n\nhttps://preview.redd.it/h8ptha1floeg1.png?width=1422&format=png&auto=webp&s=4539e6bc931b10d31df9de9336f2416a244cd2ff",
          "score": 2,
          "created_utc": "2026-01-21 10:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0u1xb4",
              "author": "Medium_Chemist_4032",
              "text": "Which quants?",
              "score": 2,
              "created_utc": "2026-01-21 10:55:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0u1yn2",
                  "author": "yoracale",
                  "text": "Q4\\_K\\_XL and Q2\\_K\\_XL",
                  "score": 3,
                  "created_utc": "2026-01-21 10:55:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0ujmwa",
              "author": "__Maximum__",
              "text": "What about other game in Rust? And 2+2",
              "score": 1,
              "created_utc": "2026-01-21 13:05:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0wc5yq",
              "author": "zoyer2",
              "text": "https://preview.redd.it/4ar6zdf8uqeg1.png?width=1503&format=png&auto=webp&s=fa87585b583bf8112337d2d6b9d19f8fd8bac10a\n\ncreate in one html file using canvas a 2d platformer game, features: camera following the player. Procedural generated world with trees, rocks. Collision system, weather system. Make it complete and complex, fully experience.\n\n\\- worked great, better output than Qwen Next 80B and other 30B models.",
              "score": 1,
              "created_utc": "2026-01-21 18:15:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjy258",
      "title": "Fine-tuning Embedding models in Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/i0ojvxvg9xeg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-22 15:49:36",
      "score": 81,
      "num_comments": 3,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qjy258/finetuning_embedding_models_in_unsloth/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o12jo43",
          "author": "danielhanchen",
          "text": "Some benchmarks for 4bit QLoRA - more in our docs [https://unsloth.ai/docs/new/embedding-finetuning](https://unsloth.ai/docs/new/embedding-finetuning)\n\nhttps://preview.redd.it/jhnixy1pfxeg1.png?width=1109&format=png&auto=webp&s=142af4191cfc72fd509c349b719958fef63c31e4",
          "score": 3,
          "created_utc": "2026-01-22 16:23:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12ovqu",
          "author": "larrytheevilbunnie",
          "text": "Does this work for clip/siglip?",
          "score": 2,
          "created_utc": "2026-01-22 16:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12srjc",
              "author": "danielhanchen",
              "text": "Oh we do support VLMs like Gemma which has a siglip part - so I guess yes? Maybe try loading it and see if it works (any model name works)",
              "score": 3,
              "created_utc": "2026-01-22 17:04:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qf6qvv",
      "title": "Translategemma-27b",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qf6qvv/translategemma27b/",
      "author": "StormrageBG",
      "created_utc": "2026-01-17 07:17:53",
      "score": 20,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "Guys do you plan to release quantisation variants of Translategemma-27b ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qf6qvv/translategemma27b/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o02thsb",
          "author": "danielhanchen",
          "text": "We did investigate it but the chat template sadly is quite specialized for it - I can check again later today, but currently it looks complex to support :(",
          "score": 4,
          "created_utc": "2026-01-17 09:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02u4gf",
              "author": "Purple-Programmer-7",
              "text": "Hugely supportive of everything you guys do.\n\nCan you educate me though? The whole ‚Äúwe fixed the chat template‚Äù didn‚Äôt really pan out IMO under scrutiny. And the copyright statement seems a bit of the anthesis  of everything you guys do to support the community.\n\nIf the chat template is functional and embedded, it‚Äôs not going to prevent a gguf from being made.\n\nAgain, not trying to be adversarial here, just want to understand better your position on all of this.\n\nThank you again for everything you‚Äôve provided (and continue to provide) this community, I genuinely am rooting for your success!\n\nEdit: spelling",
              "score": 2,
              "created_utc": "2026-01-17 09:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o07g10j",
                  "author": "danielhanchen",
                  "text": "Hey! Thanks!\n\n1. Translate Gemma has a specialized chat template that requires you to specify the language you want to translate to - this means it's not a chat model - you will need to specify the chat template kwargs separately, so it can't be loaded well in frontend UIs and make the process just harder for folks\n2. The chat template fixes are wide ranging, and we also do direct model implementation fixes - see the following:\n    * Gemma 1, Gemma 3 bug fixes: https://x.com/karpathy/status/1765473722985771335\n    * Phi 4 fixes: https://simonwillison.net/2025/Jan/11/phi-4-bug-fixes/\n    * Llama 4 fixes: https://www.reddit.com/r/LocalLLaMA/comments/1jxo7lb/llamacpp_got_2_fixes_for_llama_4_rope_wrong_norms/\n    * GPT OSS fixes: https://x.com/danielhanchen/status/1953901104150065544\n    * Kimi K2 bug fixes: https://x.com/danielhanchen/status/1946163064665260486\n    * And many more - we do a lot of our work behind the scenes now, so whatever model that was released with us generally is already fixed.\n3. The copyright header is Apache 2 (fully open source) - we place it because folks would simply copy and paste and rebrand it as their own fixes - we just want attribution that's all. Likewise all fixes before a model releases is licensed as whatever the model provider wants.",
                  "score": 2,
                  "created_utc": "2026-01-18 00:48:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03a9z5",
          "author": "DocWolle",
          "text": "Here are ggufs\n\n[https://huggingface.co/mradermacher/translategemma-27b-it-GGUF](https://huggingface.co/mradermacher/translategemma-27b-it-GGUF)",
          "score": 2,
          "created_utc": "2026-01-17 11:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03hq44",
              "author": "StormrageBG",
              "text": "Unsloth quants are always way better than mradermacher... I don't know what  the magic they did but on my test especially for translating unsloth Gemma 3 quants rulz... So I really hope ü§û that can do it again with this model too...",
              "score": 4,
              "created_utc": "2026-01-17 12:47:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03kj2v",
                  "author": "DocWolle",
                  "text": "this may be true, but the mradermacher gguf for translategemma works fine for me.",
                  "score": 1,
                  "created_utc": "2026-01-17 13:06:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0gp3rl",
          "author": "igvarh",
          "text": "\\+RL please. For subtitles, you need to work with a long context.",
          "score": 1,
          "created_utc": "2026-01-19 12:09:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhbr0p",
      "title": "glm 4.7 flash is out gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-19 18:08:55",
      "score": 19,
      "num_comments": 7,
      "upvote_ratio": 0.81,
      "text": "Guys do you plan to release quantisation variants of GLM-4.7 flash ? Its 30b a3b, unsloth chat template fixes are da best.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0mfece",
          "author": "yoracale",
          "text": "It's out now! GGUF: [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nTweet: [https://x.com/UnslothAI/status/2013482180564132092](https://x.com/UnslothAI/status/2013482180564132092)\n\nGuide: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)",
          "score": 1,
          "created_utc": "2026-01-20 06:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iq727",
          "author": "loadsamuny",
          "text": "architecture looks like a renamed deepseekv3, a pull request is in for it in llama.cpp so maybe tomorrow‚Ä¶.",
          "score": 4,
          "created_utc": "2026-01-19 18:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jnj3f",
          "author": "bobeeeeeeeee8964",
          "text": "i test it by the f16 gguf, and it seems the runing in a good speed, BUT it output garbage instead of proper text. We need waiting and see what going in this PR https://github.com/ggml-org/llama.cpp/pull/18936",
          "score": 3,
          "created_utc": "2026-01-19 20:55:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0k8is8",
              "author": "remghoost7",
              "text": "Pull request was merged and closed about an hour ago.  \nIt seems like they figured it out.",
              "score": 2,
              "created_utc": "2026-01-19 22:38:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0j9v1l",
          "author": "neph1010",
          "text": "[https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF](https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF)",
          "score": 1,
          "created_utc": "2026-01-19 19:51:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jk10j",
              "author": "noctrex",
              "text": "Still needs more work, its looping indefinitely",
              "score": 3,
              "created_utc": "2026-01-19 20:39:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0jzshk",
              "author": "Clqgg",
              "text": "this one is omega broken",
              "score": 2,
              "created_utc": "2026-01-19 21:55:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qi3m95",
      "title": "Is GLM-4.7-Flash still looping / repeating for you?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qi3m95/is_glm47flash_still_looping_repeating_for_you/",
      "author": "yoracale",
      "created_utc": "2026-01-20 15:11:00",
      "score": 18,
      "num_comments": 34,
      "upvote_ratio": 1.0,
      "text": "Hey guys many of you are still experiencing looping/repetition issues.\n\n**Jan 21 UPDATE: llama.cpp has fixed a bug which caused the model to loop and produce poor outputs. We have reconverted and reuploaded the model so outputs should be much much better now.**\n\nYou can now use Z.ai's recommended parameters and get great results:  \n\\- For general use-case:  `--temp 1.0 --top-p 0.95`  \n\\- For tool-calling:  `--temp 0.7 --top-p 1.0`\n\nIf you still experience looping issues even after following all these steps, please let us know!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qi3m95/is_glm47flash_still_looping_repeating_for_you/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0ppvlj",
          "author": "Scared_Mycologist_92",
          "text": "i use lm-studio and try repeat penality 1,2 ..this fixed it for me and i think temp 0,6 or 0,9 or something. anything else beside repeat penality couldnt fix overthinking and those psychotic loops at any answer i tried. the model itself is pretty amazing",
          "score": 1,
          "created_utc": "2026-01-20 18:47:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0raa4l",
              "author": "yoracale",
              "text": "Interesting, for many people, adding repeat penalty didn't do anything for them unfortunately.",
              "score": 1,
              "created_utc": "2026-01-20 23:14:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0tvekv",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0q2llt",
          "author": "zoyer2",
          "text": "No loop or repeating but just in general suck at coding, making small mistakes here and there (Have only tested the quant versions). Something needs to be fixed",
          "score": 1,
          "created_utc": "2026-01-20 19:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tveuj",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u3wbc",
                  "author": "zoyer2",
                  "text": "damn, much better, great work! :)",
                  "score": 2,
                  "created_utc": "2026-01-21 11:12:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0q4czx",
          "author": "epigen01",
          "text": "Yup unsloths are not usable for me gonna wait it out",
          "score": 1,
          "created_utc": "2026-01-20 19:53:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvf1q",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qepz7",
          "author": "Final-Rush759",
          "text": "No problem,  I use mlx version.",
          "score": 1,
          "created_utc": "2026-01-20 20:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvf6d",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 2,
              "created_utc": "2026-01-21 09:56:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s6fur",
          "author": "Calm_Management_5090",
          "text": "Hi, I am using Q5\\_K\\_M on lm studio and get frequent looping with both unsloth and [z.ai](http://z.ai) suggested options above once the context gets a few thousand tokens long. Thank you for all the unsloth work on this and previous models, I will keep my eyes open for updates. Best regards.",
          "score": 1,
          "created_utc": "2026-01-21 02:13:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tba8e",
              "author": "kripper-de",
              "text": "Did you also try Q4?",
              "score": 1,
              "created_utc": "2026-01-21 06:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tvfka",
                  "author": "yoracale",
                  "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
                  "score": 1,
                  "created_utc": "2026-01-21 09:56:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tvff3",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0p3js8",
          "author": "Sensitive_Song4219",
          "text": "Happy to test, have just grabbed *unsloth/GLM-4.7-Flash-GGUF* in LM Studio: what are the recommended settings  for it? (We don't seem to have a dry-multiplier?)",
          "score": 1,
          "created_utc": "2026-01-20 17:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0r9z4b",
              "author": "yoracale",
              "text": "Edit: llamacpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)\nI wrote it in my post. Use the parameters above and disable repeat penalty.",
              "score": 1,
              "created_utc": "2026-01-20 23:12:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rd2hu",
                  "author": "Sensitive_Song4219",
                  "text": "I see that now - thank you! I also see there's *zai-org/glm-4.7-flash* available now in LM Studio as-of an hour or two ago (I assume that's z-ai's release).\n\nWill test them both properly! In my early testing thinking seems to run fine (no loops); performance at small contexts (<4k) on my machine seems similar to Qwen3 30B A3B 2507. Looking forward to seeing how the output quality is (and how performance is at larger contexts!)",
                  "score": 2,
                  "created_utc": "2026-01-20 23:29:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pvp4m",
          "author": "ZeWishmastR",
          "text": "Same issue here",
          "score": 1,
          "created_utc": "2026-01-20 19:13:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvdn5",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0q6gfx",
          "author": "Rektile142",
          "text": "The model is responding well after using the recommended parameters, but throughput is utterly scuffed as context grows.\n\nMaybe the llama.cpp team still needs time to cook.",
          "score": -1,
          "created_utc": "2026-01-20 20:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qbd9j",
              "author": "Mr_Back",
              "text": "I really hope so. Nemotron 3 nano and GPT OSS 120b run ten times faster on my hardware when the context is increased.",
              "score": 1,
              "created_utc": "2026-01-20 20:26:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tvdw1",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pz5ow",
          "author": "Mr_Back",
          "text": "I'm more concerned about performance. I have a weak system, but even with it, similarly sized models perform much better (GPT OSS 120b, Qwen Next q6, Qwen3 Coder 30B, Nemotron 3 Nano). It's not about your specific quantization, but about the model as a whole.  \nI'm using a relatively large context (128k). The speed is still acceptable for small requests, but when I try a request with 35-40k tokens, I don't even want to wait for a response, it's so slow.\n\nMy PC: i5 12400, 96gb ram. 4070 - 12gb vram.",
          "score": -2,
          "created_utc": "2026-01-20 19:29:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tve65",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0xl09d",
                  "author": "Mr_Back",
                  "text": "https://preview.redd.it/x3mu0jnvtreg1.png?width=1280&format=png&auto=webp&s=df7e4507e88931948f20addf4ab66090e654c41c\n\nNemotron 3 nano F16 vs GLM 4.7 air Q4 UD. And yet GLM's response is shit(  \nI will wait for further improvements üí™.",
                  "score": 1,
                  "created_utc": "2026-01-21 21:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qgvg1u",
      "title": "Fine tuning Gpt oss on thinking dataset , which tokens to mask ?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qgvg1u/fine_tuning_gpt_oss_on_thinking_dataset_which/",
      "author": "Hulksulk666",
      "created_utc": "2026-01-19 05:23:52",
      "score": 15,
      "num_comments": 5,
      "upvote_ratio": 0.95,
      "text": "from the official unsloth [notebook ](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb#) for fine tuning Gpt oss 20b ,they used\n\n    from unsloth.chat_templates import train_on_responses_only\n    gpt_oss_kwargs = dict( instruction_part = \"<|start|>user<|message|>\", response_part = \"<|start|>assistant<|channel|>final<|message|>\" )\n     trainer = train_on_responses_only( trainer, **gpt_oss_kwargs, )\n\nBut doesn't this effectively mean the thinking tokens are also being masked ? if so , how is the model actually learning from the thinking tokens of the dataset ? or am i missing something .",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qgvg1u/fine_tuning_gpt_oss_on_thinking_dataset_which/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0g9qyp",
          "author": "im_datta0",
          "text": "Hey u/Hulksulk666  \nThanks for noticing this. Yeah if I read it right, this might be masking \"thinking/analysis\" tokens from the training loss.  \nEdit: Talked to my friends and he mentioned that masking the analysis/thinking seems to produce better down stream results and hence the decision",
          "score": 2,
          "created_utc": "2026-01-19 09:54:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hbime",
              "author": "Hulksulk666",
              "text": "Thanks , i was confused whether or not this masking meant the thinking tokens would not be part of training loss and how not being part of the training loss makes the model learn from the data . From my intuitive understanding it seemed the model should take loss if the data has some ground truth outcome like math if the solution step by step is passed onto thinking , ig i lack some deeper understanding and should resort to some papers",
              "score": 1,
              "created_utc": "2026-01-19 14:29:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0on6kq",
          "author": "WolfeheartGames",
          "text": "The reason thinking works is because it causes a dependency. What comes after thinking depends on the thinking.\n\nIf you train a model's thinking to be too far out of its own distribution, it will degrade.",
          "score": 1,
          "created_utc": "2026-01-20 15:50:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tojyj",
              "author": "Hulksulk666",
              "text": "That makes sense",
              "score": 2,
              "created_utc": "2026-01-21 08:49:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ucp5p",
          "author": "burntoutdev8291",
          "text": "Hmmm doesn't the think tokens come after that response_part? I could be wrong, never fine tuned gpt oss.",
          "score": 0,
          "created_utc": "2026-01-21 12:19:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3ti9",
      "title": "Train Llama-3.2-11b-Vision-Instruct with GRPO",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qj3ti9/train_llama3211bvisioninstruct_with_grpo/",
      "author": "darkwigga",
      "created_utc": "2026-01-21 17:13:39",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "Hi,\n\nI was working on training Llama-3.2-11b-Vision-Instruct with GRPO using unsloth and trl grpotrainer.\n\nAfter starting the training, I am getting the following error\n\n`raise ValueError(\"\\`aspect\\_ratio\\_ids\\` must be provided if \\`pixel\\_values\\` is provided\")\\`\n\nMy trainer code worked for Qwen and Gemma.\n\n  \nCode for trainer and config\n\n    from\n     trl \n    import\n     GRPOConfig, GRPOTrainer\n    \n    \n    training_args = GRPOConfig(\n    ¬† ¬† learning_rate=learning_rate,\n    ¬† ¬† adam_beta1=adam_beta1,\n    ¬† ¬† adam_beta2=adam_beta2,\n    ¬† ¬† weight_decay=weight_decay,\n    ¬† ¬† warmup_ratio=warmup_ratio,\n    ¬† ¬† lr_scheduler_type=lr_scheduler_type,\n    ¬† ¬† optim=optim,\n    ¬† ¬† logging_steps=logging_steps,\n    ¬† ¬† log_completions=log_completions,\n    ¬† ¬† per_device_train_batch_size=per_device_train_batch_size,\n    ¬† ¬† gradient_accumulation_steps=gradient_accumulation_steps, ¬†\n    # Increase to 4 for smoother training\n    ¬† ¬† num_generations=num_generations, ¬†\n    # Decrease if out of memory\n    ¬† ¬† max_prompt_length=max_prompt_length,\n    ¬† ¬† max_completion_length=max_completion_length,\n    ¬† ¬† num_train_epochs=num_train_epochs, ¬†\n    # Set to 1 for a full training run\n    ¬† ¬† \n    # max_steps = 60,\n    ¬† ¬† save_steps=save_steps,\n    ¬† ¬† max_grad_norm=max_grad_norm,\n    ¬† ¬† report_to=report_to, ¬†\n    # Can use Weights & Biases\n    ¬† ¬† output_dir=output_dir,\n    ¬† ¬† \n    # # Below enables GSPO:\n    ¬† ¬† importance_sampling_level=importance_sampling_level,\n    ¬† ¬† mask_truncated_completions=mask_truncated_completions,\n    ¬† ¬† loss_type=loss_type,\n    )\n    \n    \n    from\n     unsloth.trainer \n    import\n     UnslothVisionDataCollator\n    \n    \n    trainer = GRPOTrainer(\n    ¬† ¬† model=model,\n    ¬† ¬† args=training_args,\n    ¬† ¬† \n    # Pass the processor to handle multimodal inputs\n    ¬† ¬† data_collator=UnslothVisionDataCollator(model, processor),\n    ¬† ¬† processing_class=processor,\n    ¬† ¬† reward_funcs=[\n    ¬† ¬† ¬† ¬† get_reward,\n    ¬† ¬† ],\n    ¬† ¬† train_dataset=train_ds,\n    ¬† ¬† eval_dataset=test_ds,\n    )\n    \n    \n    trainer.train()\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qj3ti9/train_llama3211bvisioninstruct_with_grpo/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o10dbtw",
          "author": "im_datta0",
          "text": "Hey u/darkwigga it would be helpful if you can tell us what dataset are you using and if you're doing any pre proessing...\n\nI think you might want to look at this notebook [https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2\\_5\\_7B\\_VL\\_GRPO.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb) for reference.\n\nThis is very possibly some issue with preprocessing the dataset. Would be happy to help if you can provide further information\n\nEdit: If you need any further assistance, feel free to open a github issue or hop on to our discord server",
          "score": 1,
          "created_utc": "2026-01-22 07:44:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10g9ll",
              "author": "im_datta0",
              "text": "For example, if you do something like\n\n\n\n    instruction = \"You are an expert radiographer. Describe accurately what you see in this image.\"\n    # image is a PIL object\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction}\n        ]}\n    ]\n    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n    inputs = tokenizer(\n        image,\n        input_text,\n        add_special_tokens = False,\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n\n\n\nThe inputs would contain input\\_ids, attention\\_mask, pixel\\_values, aspect\\_ratio\\_ids, aspect\\_ratio\\_mask....",
              "score": 1,
              "created_utc": "2026-01-22 08:10:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qk1qy4",
      "title": "Guide to use unsloth on windows",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qk1qy4/guide_to_use_unsloth_on_windows/",
      "author": "LahmeriMohamed",
      "created_utc": "2026-01-22 18:02:54",
      "score": 7,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "hello guys hope i recieve help , i have recently installed unsloth to try it fine-tuning process , but due to dependencies  conflicts i had to remove it , if anyone can help me to fix this issue , my current env \npython 3.11.2\ntorch 2.5.1+cu121 \nif i ran install unsloth , it remove the cuda installation , so i used --no-deps instruction , but when running it , it require vllm , accelerate error .. .\ncan  you provide me with better/compatible versions ? \nthank you",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qk1qy4/guide_to_use_unsloth_on_windows/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o149lq3",
          "author": "Educational_Rent1059",
          "text": "Install WSL2 I highly recommend that. And hook up VSCode and you are good to go with Linux within windows without any overhead. The benchmarks in training vs native Ubuntu is between 1-4% diffs (native ahead) , if you do pure GPU work and ML, it's no diff at all.",
          "score": 2,
          "created_utc": "2026-01-22 21:04:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o149z5e",
              "author": "LahmeriMohamed",
              "text": "since i have 1ssd main system and hdd for env saving , can i save the wls env in the hdd and use it in env-variable to be accessible globaly ?",
              "score": 1,
              "created_utc": "2026-01-22 21:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14ag0l",
                  "author": "Educational_Rent1059",
                  "text": "Your WSL ext4 drive will be a single file that you can move anywhere into any HDD you like in windows. If you prefer to have it on a different disk etc that works as well. Everything you do will be stored within that single file (it's basically your entire OS and all files within it in one file on windows)",
                  "score": 2,
                  "created_utc": "2026-01-22 21:08:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o14aosb",
                  "author": "Educational_Rent1059",
                  "text": "You can also create another \"drive\" (single file) and attach it into linux, it's a bit messy as you need to attach it in windows command shell first, and then go into the linux WsL2 cli and attach it there too, but use GPT for guidance and do that. Be careful as so you don't delete any files by wrong instructions from GPT as they hallucinate alot",
                  "score": 1,
                  "created_utc": "2026-01-22 21:09:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15bmc0",
              "author": "fiery_prometheus",
              "text": "Except that wsl likes to crash under heavy workloads, I've had so many crashes when fine-tuning¬†",
              "score": 0,
              "created_utc": "2026-01-23 00:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15vh4u",
                  "author": "Educational_Rent1059",
                  "text": "I'm running WSL on 2  machines  with 4 gpus never had a single issue in years. I think the issue is either your bad/faulty hardware (probably memory or something else) or your environment/drivers. Yeah, just blaming WSL randomly for a crash without any input into what CAUSED the crash is not helpful at all - rather misleading.",
                  "score": 1,
                  "created_utc": "2026-01-23 02:08:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14t3zp",
          "author": "yoracale",
          "text": "We have this Windows guide which we revamped around a month ago, especially for WSL: [https://unsloth.ai/docs/get-started/install/windows-installation](https://unsloth.ai/docs/get-started/install/windows-installation)",
          "score": 2,
          "created_utc": "2026-01-22 22:41:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13lzmu",
          "author": "immediate_a982",
          "text": "Use virtual environments",
          "score": 1,
          "created_utc": "2026-01-22 19:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o145msj",
              "author": "LahmeriMohamed",
              "text": "did not work",
              "score": 0,
              "created_utc": "2026-01-22 20:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13mp3c",
          "author": "CMPUTX486",
          "text": "Use docker.. I think it save more time",
          "score": 1,
          "created_utc": "2026-01-22 19:18:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o145kpy",
              "author": "LahmeriMohamed",
              "text": "i am low on ram 1 * 8gb , so avoid it",
              "score": 0,
              "created_utc": "2026-01-22 20:45:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qk109a",
      "title": "The best (tiny) model I can run on my phone",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qk109a/the_best_tiny_model_i_can_run_on_my_phone/",
      "author": "gized00",
      "created_utc": "2026-01-22 17:36:05",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I work in ML and I am quite familiar with Llama, fine tuning, etc. but I always work on 10s of billions parameters. \n\nI would like to train a tiny model that I can run on my phone (Pixel 8) and unsloth seems the right place to start with this (but feel free to suggest other solutions). I have some difficulties to identify what can realistically run (with a decent num tokens/s). Is a 1B model a reasonable choice if I am quantizing it?\n\nAny other suggestions?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qk109a/the_best_tiny_model_i_can_run_on_my_phone/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o134ezq",
          "author": "BenniB99",
          "text": "I believe Gemma3n was made specifically with phone and edge usage in mind.  \nIt is also a pretty decent model for its size imo (and multimodal).  \nAfaik it is also possible to switch between 2B and 4B effective parameter on the fly with it.",
          "score": 7,
          "created_utc": "2026-01-22 17:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13ow28",
              "author": "gized00",
              "text": "Nice! I will take a look.\nThank you",
              "score": 2,
              "created_utc": "2026-01-22 19:27:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13flhy",
          "author": "Late_Huckleberry850",
          "text": "If you get the Apollo app that has some. Idk if they have an android version or not though",
          "score": 1,
          "created_utc": "2026-01-22 18:46:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13woxx",
          "author": "maxtheman",
          "text": "I have a pixel 9 and am working on fine-tuning functionalgemma, which is working great, but it really depends on your task. 1B or less can work great on a distilled task, but don't expect 90%+ perf unless you overfit the shit out of it and consider doing multiple types of fine-tuning.\n\n  \nOn pixel the hardest part, for me at least, will be getting it on an api that can actually access your gpu. I am targeting huggingfacejs for now due to the ease of use, but I don't know a better way to deploy than that or get on the google npu.",
          "score": 1,
          "created_utc": "2026-01-22 20:03:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o141j0z",
              "author": "gized00",
              "text": "It's a fairly simple task but it will require a bit of RL",
              "score": 1,
              "created_utc": "2026-01-22 20:26:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14w8qp",
          "author": "Azuriteh",
          "text": "The best model for on-device usage right now is probably [https://huggingface.co/LiquidAI/LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B), with 4 bit quantization it'd probably be alright!",
          "score": 1,
          "created_utc": "2026-01-22 22:58:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o138iyq",
          "author": "Large-Example-1275",
          "text": "You can use the ¬´¬†Locally¬†¬ª app to check supported models on your device, but I don't know if it's available on Android.",
          "score": 0,
          "created_utc": "2026-01-22 18:15:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}