{
  "metadata": {
    "last_updated": "2026-02-24 17:19:14",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 14,
    "total_comments": 105,
    "file_size_bytes": 103345
  },
  "items": [
    {
      "id": "1r9wkxd",
      "title": "100,000+ models trained with Unsloth have been open-sourced",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/6f7zem4yrnkg1.png",
      "author": "danielhanchen",
      "created_utc": "2026-02-20 14:19:29",
      "score": 245,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r9wkxd/100000_models_trained_with_unsloth_have_been/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6fiw48",
          "author": "SnooPuppers4132",
          "text": "wow",
          "score": 8,
          "created_utc": "2026-02-20 14:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fnlqw",
          "author": "immediate_a982",
          "text": "With open weights you get the modelâ€™s parameters ie the actual numbers so you can run and fine-tune the model, you donâ€™t necessarily get the **training data, training code**, needed to reproduce the gguf from scratch.â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹",
          "score": 7,
          "created_utc": "2026-02-20 15:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6isms7",
              "author": "zp-87",
              "text": "So basically in terms of software, you get compiled .exe file but not the source code. So it is not open source, it is free to use",
              "score": 4,
              "created_utc": "2026-02-21 00:32:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qw6t1",
                  "author": "cibernox",
                  "text": "Not really because you can modify an exe to make to work differently. I think that metaphor isnâ€™t right.",
                  "score": 1,
                  "created_utc": "2026-02-22 09:34:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fkc51",
          "author": "Comrade-Porcupine",
          "text": "Basically none of this is \"open source\".  It's open weight. There is an important difference -- apart from AllenAI almost none of the openweight models on huggingface have their \"sources\" open.\n\nUsing the term \"open source\" for these models is an abuse of the term.",
          "score": 7,
          "created_utc": "2026-02-20 15:00:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fpk0j",
              "author": "Rhinoseri0us",
              "text": "I believe you are correct. Whatâ€™s a good AllenAI model to start with?",
              "score": 1,
              "created_utc": "2026-02-20 15:25:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6fogx8",
              "author": "yoracale",
              "text": "I think we're getting stressed over semantics for no reason. The models are released under an opensource license aka APACHE 2.0 and this qualify for it to be open source.",
              "score": 0,
              "created_utc": "2026-02-20 15:20:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6grcsc",
                  "author": "Tema_Art_7777",
                  "text": "Does â€˜sourceâ€™ definition include training data?",
                  "score": 1,
                  "created_utc": "2026-02-20 18:19:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6k23z7",
          "author": "No_Conversation9561",
          "text": "So let me get this straight.\n\n- These guys took weights of an existing model (for free)\n- They used Unslothâ€™s toolkit (for free)\n- They finetuned the existing model using Unslothâ€™s toolkit using their own dataset. (compute not free)\n- They released only the finetuned weights on huggingface but not the dataset.\n\nIf you donâ€™t release the dataset then thereâ€™s no distinction between open source community and big corporations.",
          "score": 2,
          "created_utc": "2026-02-21 05:35:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6prppo",
              "author": "NorthEastCalifornia",
              "text": "Datasets are there. You can check the models description.",
              "score": 2,
              "created_utc": "2026-02-22 03:47:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vkugv",
          "author": "cashtec",
          "text": "How and where is this fine tuned? Who is funding the GPUs to do all this? I am just curious",
          "score": 1,
          "created_utc": "2026-02-23 01:27:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbkbse",
      "title": "Qwen3-Coder-Next GGUF Aider Coding Benchmarks",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/2u2qeza4f1lg1.png",
      "author": "Etherll",
      "created_utc": "2026-02-22 12:12:31",
      "score": 181,
      "num_comments": 33,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rbkbse/qwen3codernext_gguf_aider_coding_benchmarks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6rdogv",
          "author": "Significant_Fig_7581",
          "text": "I said the IQ3 XXS was great and people still don't believe me",
          "score": 23,
          "created_utc": "2026-02-22 12:16:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6riq5y",
              "author": "Look_0ver_There",
              "text": "I found this to also be true of Unsloth's IQ3_XXS quant of MiniMax M2.5 which allows it to fix nicely within the various MiniPC's and Mac's with 128GB of memory.  Personally I've found MiniMax to be a little more reliable than Qwen3-Coder-Next, but I think that just depends on the tasks at hand.  The main take away though is that for larger models(>50B say?) that IQ3_XXS doesn't seem to hurt as much as it does for smaller models.",
              "score": 12,
              "created_utc": "2026-02-22 12:55:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rktb0",
          "author": "define_undefine",
          "text": "Does anyone know why FP8 has a drop in performance compared to Q6 or NVFP4?",
          "score": 10,
          "created_utc": "2026-02-22 13:10:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rm7jy",
              "author": "FullstackSensei",
              "text": "Because it's a limited benchmark. Dig enough and you'll find other benchmarks where the picture is flipped.\n\nIn any case, the only thing that actually matters is whether a quant works for your uses or not.",
              "score": 8,
              "created_utc": "2026-02-22 13:19:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6sfpnx",
                  "author": "steezy13312",
                  "text": ">Â In any case, the only thing that actually matters is whether a quant works for your uses or not.\n\nI understand what this sentence is communicating, but itâ€™s kind of missing the point here. Many of us donâ€™t have the time to determine if a quant or model works for every one of our use cases or not.\n\nImagined you have a friend whoâ€™s interested in buying a car, and you tell them the only way to find out what works best for them is to go test drive every variation of trim and engine package instead of at first looking at car reviews to refine their options.Â ",
                  "score": 6,
                  "created_utc": "2026-02-22 15:56:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rul62",
                  "author": "some_user_2021",
                  "text": "And because inference is a statistical process.",
                  "score": 3,
                  "created_utc": "2026-02-22 14:09:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rngdl",
          "author": "Thrumpwart",
          "text": "How does NVFP4 perform in terms of speed on AMD GPUs? Is Blackwell necessary to have. A good experience?",
          "score": 4,
          "created_utc": "2026-02-22 13:27:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rr6x3",
              "author": "blazze",
              "text": "AMD **RDNA 4**: Supports FP8/BF8 with hardware acceleration and includes improvements in AI compute performance, butÂ does not implement NVFP4Â or similar micro-floating-point formats.",
              "score": 4,
              "created_utc": "2026-02-22 13:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s0yil",
                  "author": "Thrumpwart",
                  "text": "Yeah thatâ€™s too bad. Seems like a solid quant option.",
                  "score": 2,
                  "created_utc": "2026-02-22 14:45:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sazam",
          "author": "siegevjorn",
          "text": "NVPF4 better than BF16? How is it posssible",
          "score": 7,
          "created_utc": "2026-02-22 15:35:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rjciq",
          "author": "LegacyRemaster",
          "text": "wow",
          "score": 3,
          "created_utc": "2026-02-22 13:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6toudd",
          "author": "MaxKruse96",
          "text": "No +- error margins, no simpler quants for q2 q4 q5 q6. kinda whack",
          "score": 3,
          "created_utc": "2026-02-22 19:22:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ro12r",
          "author": "Glittering-Call8746",
          "text": "What card u using 5070ti 16gb ?",
          "score": 2,
          "created_utc": "2026-02-22 13:31:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tddtr",
          "author": "StartupTim",
          "text": "How would this sort on a system with 2x GPUs for 48GB vram and 96GB system ram?\n\nWhich model would you choose, especially when going for long context windows such as 256k or 512k?",
          "score": 2,
          "created_utc": "2026-02-22 18:29:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rlven",
          "author": "Prudent-Ad4509",
          "text": "I wonder why there is NVFP4 quant but no UD-Q4-K-XL quant. Is it \\*that\\* bad ?",
          "score": 4,
          "created_utc": "2026-02-22 13:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rrzfo",
              "author": "yoracale",
              "text": "The contributors who benchmarkered this on Discord did not test the Q4 quants as the gap between Q3 and full BF16 precision is already so close",
              "score": 5,
              "created_utc": "2026-02-22 13:55:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rswjm",
                  "author": "Prudent-Ad4509",
                  "text": "The problem is in the position of the UD-Q6\\_K\\_XL quant relative to the NVFP4 quant. Also, I see two different NVFP4 quants on huggingface, one made with **nvidia-modelopt** and another with **llmcompressor**. It feels like they've missed the elephant in the room. All 3 quants should have been tested I think.\n\nAlso, there is at least one more thread on reddit with this picture where people are reporting issues with Q3.",
                  "score": 2,
                  "created_utc": "2026-02-22 14:00:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rt9x9",
          "author": "Mr_Back",
          "text": "This table is confusing. I don't understand where UD-Q3\\_K\\_XL, UD-Q4\\_K\\_XL, and MXFP4\\_MOE fit in.  \nI always thought that the \"K\\_XL\" configuration offered the best balance of speed and quality â€“ is that not the case?  \nI just tried running UD-IQ3\\_XXS, and it's running a quarter slower than MXFP4, and its speed is comparable to UD-Q8\\_K\\_XL on my machine.",
          "score": 2,
          "created_utc": "2026-02-22 14:02:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rywod",
              "author": "yoracale",
              "text": "This benchmark is comparing REAP non Unsloth GGUFs, vs Unsloth GGUFs vs NVFP4 vs FP8. It is quite confusing. K\\_XL isn't always the best balance of speed, but it is usually quality yes.\n\nThe Q3\\_K\\_XL displayed in this graph is not of Unsloth's but rather the REAP version of the model",
              "score": 3,
              "created_utc": "2026-02-22 14:34:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s4hqw",
                  "author": "Mr_Back",
                  "text": "Regarding the Q3\\_K\\_XL REAP model on the graph â€“ I understand. My question is more about where the Unsloth models UD-Q3\\_K\\_XL, UD-Q4\\_K\\_XL, and MXFP4\\_MOE would be located on this graph.  \nWould they be positioned on the line between UD-IQ3\\_XXS and UD-Q6\\_K\\_XL?  \nI'm currently using MXFP4, which gives me 20 tokens per second (for video transcription and small code edits), and UD-Q8\\_K\\_XL (for agent-based encoding), which gives me 15 tokens per second.  \nLooking at this graph, I thought that UD-IQ3\\_XXS would be very good and faster than MXFP4, while also being almost as accurate as UD-Q6\\_K\\_XL, but its speed is similar to UD-Q8\\_K\\_XL.  \nIs UD-IQ3\\_XXS more accurate than MXFP4?  \nIs MXFP4 particularly fast compared to other quantization methods?  \nIs UD-IQ3\\_XXS quantization slower?  \nWhich quantization method would be best for me, offering a good balance between speed and accuracy for both casual use and more demanding tasks?",
                  "score": 0,
                  "created_utc": "2026-02-22 15:03:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rg2t6",
          "author": "Glittering-Call8746",
          "text": "Can fit consumer gaming cards ?",
          "score": 2,
          "created_utc": "2026-02-22 12:36:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rib3w",
              "author": "yoracale",
              "text": "Only if you have enough RAM. Becuase IQ3XXS is only 32.7GB youll need about 35GB total VRAM + RAM combined.\n\nSo e.g. a 16GB VRAM + 20GB RAM will work quite nicely\n\nMore deets in our guide: [https://unsloth.ai/docs/models/qwen3-coder-next](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 11,
              "created_utc": "2026-02-22 12:52:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rprj8",
                  "author": "1_7xr",
                  "text": "I have a laptop with 8GB VRAM + 24GB RAM. Would I get decent performance if I upgraded the ram to 32GB?",
                  "score": 2,
                  "created_utc": "2026-02-22 13:41:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6wa9ka",
          "author": "AdventurousGold672",
          "text": "what frameworks support running nvfp4?",
          "score": 1,
          "created_utc": "2026-02-23 04:06:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x8evm",
          "author": "FeiX7",
          "text": "link to benchmark? and does they have similar benchmark or other models as well?\n\n",
          "score": 1,
          "created_utc": "2026-02-23 08:57:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ytfb6",
          "author": "AntuaW",
          "text": "So lame we don't get those by default and people have to do that individually. It is such a waste of time because of this info lacking on each quant :(",
          "score": 1,
          "created_utc": "2026-02-23 15:39:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t2ap3",
          "author": "alfons_fhl",
          "text": "NVFP4 is better than bf16? Does I understand it right that the Quantization has better performance as the default bf16? (bf16 is the default LLM of Qwen3-Coder-Next right?)",
          "score": 0,
          "created_utc": "2026-02-22 17:38:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72txjg",
              "author": "eXl5eQ",
              "text": "Happens to be better in this benchmark with the particular hardware and seed OP used.",
              "score": 1,
              "created_utc": "2026-02-24 04:00:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcjrux",
      "title": "Qwen3-Coder-Next is now the #1 most downloaded model on Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/o3qtavcee9lg1.jpeg",
      "author": "yoracale",
      "created_utc": "2026-02-23 15:02:44",
      "score": 178,
      "num_comments": 30,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rcjrux/qwen3codernext_is_now_the_1_most_downloaded_model/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ypidx",
          "author": "mukz_mckz",
          "text": "I wish more companies also released a Lite/Smaller versions of their models so that GPU poor plebs like us can actually use them for agentic workflows. I hope they see this response to GLM Flash and Qwen Next Coder and reconsider dropping a few blazing fast models in the future. I'm hopeful.",
          "score": 11,
          "created_utc": "2026-02-23 15:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrgp2",
              "author": "yoracale",
              "text": "According to Qwen team e.g. Jungyang they are in fact coming very soon",
              "score": 8,
              "created_utc": "2026-02-23 15:30:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o71d547",
              "author": "starkruzr",
              "text": "I wonder what hardware people are running this on. only 3B active parameters means you *can* run it well with a variety of GPUs, *provided* you have a lot of fast system RAM for moving experts in and out of VRAM as needed. I guess it would run very well on STXH.",
              "score": 2,
              "created_utc": "2026-02-23 22:55:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zd87p",
          "author": "flavio_geo",
          "text": "Qwen3-Coder-Next is the main man in my local agentic workflow, very consistent, running on consumer grade accessible hardware, in Q6\\_K\\_XL.\n\n  \nGreat model. Great quant.\n\n  \nThank you for the great work Unsloth, you are making the difference for us.",
          "score": 7,
          "created_utc": "2026-02-23 17:11:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72x3ct",
              "author": "yoracale",
              "text": "Thanks for using our quants we appreciate it! \\^\\^",
              "score": 3,
              "created_utc": "2026-02-24 04:21:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ze6wq",
          "author": "HlddenDreck",
          "text": "This small model performs exceptional!\nI'm using it since unsloth released their quants, it's amazing. Hope the performance with llama.cpp will improve. I get 25t/s tg, this very poor for such a small model.\nRunning GPT-OSS-120B I get 80t/s tg, however GPT is really bad at coding.",
          "score": 4,
          "created_utc": "2026-02-23 17:16:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72x6v1",
              "author": "yoracale",
              "text": "Thank you! Which specific quant are you using btw?",
              "score": 1,
              "created_utc": "2026-02-24 04:22:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o73hvp1",
                  "author": "HlddenDreck",
                  "text": "I always aim for the highest I can run completely from VRAM.\nIn case of this model I'm running Q4_K_XL.\nWell, I could run Q6_K_XL, however benchmarks indicate there's almost no accuracy difference between them, so I can run additionally a smaller model for other tasks.",
                  "score": 1,
                  "created_utc": "2026-02-24 07:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zkdrg",
          "author": "TaroOk7112",
          "text": "With this model, UD_Q8_K_XL, I finally have useful local inference solving real problems. Changing Linux configuration, writing scripts, explaining code, ... Very solid compared with previous <235B models.",
          "score": 4,
          "created_utc": "2026-02-23 17:45:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703wzr",
          "author": "omercelebi00",
          "text": "i get 15tok/s with 6600xt and the model is very good at chat. Not able to use with RooCode yet. Q4\\_K\\_XL",
          "score": 3,
          "created_utc": "2026-02-23 19:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72xbf5",
              "author": "yoracale",
              "text": "Llama.cpp fixed some parsing issues a few days ago, try it now and see if it fixed",
              "score": 1,
              "created_utc": "2026-02-24 04:23:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71ufj5",
          "author": "simracerman",
          "text": "And itâ€™s not even for the coding part (which is awesome too). The model is just amazing at everything Iâ€™ve thrown at it so far.",
          "score": 3,
          "created_utc": "2026-02-24 00:31:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o700ox7",
          "author": "dmter",
          "text": "can confirm it's the best atm - using q8 quant from developers, it just consumes 12-16gb or vram if you fully unload moe so you can use other gpu intensive apps and that's at 15t/s.\n\nalso I tried mm2.5 q4 and was extremely disappointed. the math errors it makes are unbelievable. q3cn looks like a genius in comparison.",
          "score": 2,
          "created_utc": "2026-02-23 18:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708kfn",
          "author": "LegacyRemaster",
          "text": "Question: I'm using Step Fun 3.5 a lot, but I haven't seen Unsloth's gguf. Is there a specific reason?",
          "score": 2,
          "created_utc": "2026-02-23 19:36:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72hoey",
              "author": "yoracale",
              "text": "There were some issues with conversion unfortunately",
              "score": 2,
              "created_utc": "2026-02-24 02:45:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71aw8o",
          "author": "Prudent-Ad4509",
          "text": "Still falling into infinite loops in opencode with UD Q4 quant though.",
          "score": 2,
          "created_utc": "2026-02-23 22:43:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72hk53",
              "author": "yoracale",
              "text": "Have you tried Q6 and see any improvements? It was only recently llama.cpp fixed parsing. Did you update llama.cpp?",
              "score": 1,
              "created_utc": "2026-02-24 02:44:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o73i8w0",
                  "author": "Prudent-Ad4509",
                  "text": "I did various suggested things except moving to higher quant but it just moved the step where the looping starts. So, even if looping stops in one scenario, this does not mean much. Iâ€™ll try mxfp4 quant next, it showed abnormally high results on certain benches and this looping problem just might be the reason. Grasping at straws and all that.",
                  "score": 1,
                  "created_utc": "2026-02-24 07:08:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72981l",
          "author": "OmarBessa",
          "text": "it's an amazing model",
          "score": 2,
          "created_utc": "2026-02-24 01:56:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72bdds",
          "author": "SouvikMandal",
          "text": "Really hope they release a VLM with 80b active 3B params",
          "score": 2,
          "created_utc": "2026-02-24 02:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o738g58",
          "author": "DarkZ3r0o",
          "text": "Because its really amazing ! I stayed up all night testing the model and its on fire ðŸ”¥ it can easily work with claude code and provide great results",
          "score": 2,
          "created_utc": "2026-02-24 05:45:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ytflm",
          "author": "joninco",
          "text": "Can we use unsloth to make an Eagle3 draft model? Asking for a friend.",
          "score": 1,
          "created_utc": "2026-02-23 15:39:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72xeiv",
              "author": "yoracale",
              "text": "Eagle3 draft? We use llama.cpp for all our quants, not unsloth. Unsloth is mainly for training and inference",
              "score": 1,
              "created_utc": "2026-02-24 04:24:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71i0ha",
          "author": "gtrak",
          "text": "This is the best one in my informal testing for 24gb vram and 64gb dram https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF/discussions/2 .  It uses bf16 for the non moe weights and 4 bit for everything else. I think it beats q6 at a smaller size.",
          "score": 1,
          "created_utc": "2026-02-23 23:21:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73hpey",
              "author": "Slow-Ability6984",
              "text": "Can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-24 07:03:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74x2h9",
                  "author": "gtrak",
                  "text": "Read the link",
                  "score": 1,
                  "created_utc": "2026-02-24 13:56:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73wsfe",
          "author": "ZealousidealShoe7998",
          "text": "for anyone who wants to try a code harness, instead of using opencode or claude code try qwen code first. it should work out of the box. ",
          "score": 1,
          "created_utc": "2026-02-24 09:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74c5wa",
          "author": "Legitimate-Track-829",
          "text": "Does the 3-bit work on Apple Silicon 48GB unified RAM? ",
          "score": 1,
          "created_utc": "2026-02-24 11:42:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74hbku",
              "author": "yoracale",
              "text": "You'd rather use the 4-bit which works yes. Our guide uses 4-bit: [https://unsloth.ai/docs/models/qwen3-coder-next](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 2,
              "created_utc": "2026-02-24 12:20:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r85r5x",
      "title": "You can now train LLMs in VS Code for free via Colab!",
      "subreddit": "unsloth",
      "url": "https://v.redd.it/k5cjdy4ss9kg1",
      "author": "yoracale",
      "created_utc": "2026-02-18 15:19:57",
      "score": 133,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r85r5x/you_can_now_train_llms_in_vs_code_for_free_via/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o62orpj",
          "author": "Reivaj640",
          "text": "Thank you so much, it's super interesting to do with VS Code",
          "score": 6,
          "created_utc": "2026-02-18 16:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66pi40",
              "author": "danielhanchen",
              "text": "Yes VS Code + Colab GPUs are pretty cool! You can also launch multiple Colabs as well!",
              "score": 1,
              "created_utc": "2026-02-19 04:16:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67pgvl",
          "author": "Big-Balance-6426",
          "text": "Thank you for this guide!",
          "score": 2,
          "created_utc": "2026-02-19 09:17:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a2qyk",
          "author": "bad_gambit",
          "text": "Thanks for the guide (and the quants and your amazing finetuning library)! Is google service (BigQuery, Drive, etc) and other quirks (github lfs integration was janky, iirc) finally fixed now? Last time i used the extension it was feeling like its half-baked and quite a hassle without those features",
          "score": 1,
          "created_utc": "2026-02-19 18:00:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pxbu",
      "title": "Qwen3.5 GGUF Evaluation Results",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/0kqfgy0b1mkg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-20 08:30:37",
      "score": 121,
      "num_comments": 15,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r9pxbu/qwen35_gguf_evaluation_results/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6e47uh",
          "author": "LegacyRemaster",
          "text": "I'm using this model every day. I'm curious about Q2 quantization because I'm having fun asking sonnet 4.6 the exact same questions and comparing the answers, and I haven't seen any hallucinations yet. On the contrary. While sonnet was telling me \"the directory is wrong, here's a script to edit the Python file and adjust the paths,\" qwen was telling me \"rename the directory and the problem is solved.\" In this particular case, I smiled because I then asked sonnet: \"But isn't it better to simply rename the directory to fix the references?\" And sonnet: \"Yes, you're right!\"",
          "score": 13,
          "created_utc": "2026-02-20 08:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e23pd",
          "author": "getmevodka",
          "text": "Wait why is a normal margin of error producing better results in a q3 than in a q4?",
          "score": 9,
          "created_utc": "2026-02-20 08:36:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e2kw1",
              "author": "yoracale",
              "text": "Yes, as for statistics, there is always going to be a margin of error. To cancel it out you usually have to run the test like 10 times and get the median/average. Even then, margin of errors still can occur.\n\nOr coincidentally, it could be possible that the Q3 is in fact better than the Q4 but that is unlikely.",
              "score": 15,
              "created_utc": "2026-02-20 08:40:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ez7sf",
                  "author": "simracerman",
                  "text": "Thereâ€™s a small catch her not explaining the behavior.\n\nI believe itâ€™s the evaluation. Different prompts will reveal more cracks in the Q3 one.\n",
                  "score": 1,
                  "created_utc": "2026-02-20 13:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ezwmb",
          "author": "ciprianveg",
          "text": "can you please also test the MXFP4_MOE version?",
          "score": 4,
          "created_utc": "2026-02-20 13:11:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6izwdt",
              "author": "yoracale",
              "text": "You'll need to ask Benjamin for this: [https://x.com/bnjmn\\_marie/status/2024533203239788851](https://x.com/bnjmn_marie/status/2024533203239788851)",
              "score": 3,
              "created_utc": "2026-02-21 01:15:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6j8pci",
              "author": "Dry_Mortgage_4646",
              "text": "Please ask Benjamin. I want to know too",
              "score": 2,
              "created_utc": "2026-02-21 02:11:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k79at",
          "author": "alexp702",
          "text": "Q8 on Mac Studio subjectively feels very good - much better than iq4_NL of Qwen Coder 480.",
          "score": 2,
          "created_utc": "2026-02-21 06:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fagru",
          "author": "GCoderDCoder",
          "text": "I had an online debate with someone on this topic. I see the perplexity hits a cliff at q4 where the token accuracy compared to the original decreases dramatically BUT that doesnt mean the model is putting bad tokens, it just means the model is using a different token than the original would have. How do those affects compare to temperature changes? Xcreate shows that these models do indeed have different output but it seems to be that they lose nuance as they compress more which makes perfect sense given the way compression works. I would love to see more standardized testing around this stuff. Model providers wont even compare the same benchmarks at full size let alone quants...",
          "score": 1,
          "created_utc": "2026-02-20 14:09:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fneqj",
              "author": "yoracale",
              "text": "Mmm perplexity is generally not a good measurement for accuracy. Benjamin utilized actual prompts which are much more real world use-case based",
              "score": 2,
              "created_utc": "2026-02-20 15:15:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fq6jn",
                  "author": "GCoderDCoder",
                  "text": "Agreed! Just pointing out seeing the data about token accuracy doesn't necessarily translate 1:1 into model performance but we never get standardized data on things like quantized performance despite the fact that most of us are using quantized versions.",
                  "score": 1,
                  "created_utc": "2026-02-20 15:28:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hiotb",
          "author": "JorG941",
          "text": "Can you share a google notebook to do this?",
          "score": 1,
          "created_utc": "2026-02-20 20:28:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6izw2f",
              "author": "yoracale",
              "text": "You'll need to ask Benjamin for this: [https://x.com/bnjmn\\_marie/status/2024533203239788851](https://x.com/bnjmn_marie/status/2024533203239788851)",
              "score": 0,
              "created_utc": "2026-02-21 01:15:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6sjuha",
          "author": "Honest-Debate-6863",
          "text": "Is that good or bad? Q4 drift is fine? Just that it hallucinates more right?",
          "score": 1,
          "created_utc": "2026-02-22 16:14:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r908nn",
      "title": "New r/unsloth User Flairs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/s14byfp2lgkg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-19 14:11:58",
      "score": 30,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r908nn/new_runsloth_user_flairs/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o698enw",
          "author": "im_datta0",
          "text": "Why did you pick the sad sloth though LOL",
          "score": 2,
          "created_utc": "2026-02-19 15:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c5lng",
              "author": "yoracale",
              "text": "Because it's my favorite sloth emoji ðŸ˜­",
              "score": 1,
              "created_utc": "2026-02-20 00:20:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ae124",
          "author": "danielhanchen",
          "text": "I'm using the Heart Sloth! :)",
          "score": 2,
          "created_utc": "2026-02-19 18:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lk0c3",
          "author": "LegacyRemaster",
          "text": "all I need",
          "score": 1,
          "created_utc": "2026-02-21 13:37:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r993q3",
      "title": "Any good model that can even run on 0.5 GB of RAM (512 MB of RAM)?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r993q3/any_good_model_that_can_even_run_on_05_gb_of_ram/",
      "author": "Ok-Type-7663",
      "created_utc": "2026-02-19 19:40:20",
      "score": 15,
      "num_comments": 20,
      "upvote_ratio": 0.81,
      "text": "I'm testing local AI limits. Also recommend a OS :3 and Hugging Face repo and great quant :D",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r993q3/any_good_model_that_can_even_run_on_05_gb_of_ram/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6aqhcs",
          "author": "RudeboyRudolfo",
          "text": "functiongemma. But it's more for running structured output and tool calling. It's not really a LLM.",
          "score": 15,
          "created_utc": "2026-02-19 19:53:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b3ya2",
              "author": "Apart_Refrigerator27",
              "text": "What are the use cases for functiongemma?\nit is really worth the time?",
              "score": 4,
              "created_utc": "2026-02-19 20:59:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b7j5i",
                  "author": "RudeboyRudolfo",
                  "text": "That's your imagination. There is a lot possible with it. Pretty sure you can get it over ollama. So not much time wasted. I built a small assistant with it, which can open and edit files and search man pages.",
                  "score": 5,
                  "created_utc": "2026-02-19 21:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ave8q",
          "author": "PlayerWell",
          "text": "I think the Gemma 3 270m will work. It's not great, but it can be successful if fine-tuned",
          "score": 9,
          "created_utc": "2026-02-19 20:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bxr6f",
              "author": "--Spaci--",
              "text": "this is the only real correct option tbf qwen3 0.6b is to large with a context window",
              "score": 1,
              "created_utc": "2026-02-19 23:34:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ar209",
          "author": "Jan49_",
          "text": "No good model can run on half a GB of RAM... Lol\n\nThe smallest LLM, that I know of, that can barely form sentences is Qwen3 0.6B. The q2 quant from unsloth is sub 300mb in size. But then you would still need RAM for context and general overhead.\n\nDoes your system only have 512mb RAM? Then the OS would probably take up the whole RAM on its own. Try Linux XFCE or even better no DE at all.",
          "score": 4,
          "created_utc": "2026-02-19 19:55:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bun9h",
              "author": "logos_flux",
              "text": "It's not a system it's a ti-83",
              "score": 1,
              "created_utc": "2026-02-19 23:16:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ayjfh",
          "author": "BenniB99",
          "text": "You could try the 2bit dynamic quants of LFM2-1.2B but that is probably going to be a vegetable",
          "score": 3,
          "created_utc": "2026-02-19 20:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b7wc8",
          "author": "Significant_Fig_7581",
          "text": "Qwen 0.6B",
          "score": 3,
          "created_utc": "2026-02-19 21:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ccw9x",
          "author": "host3000",
          "text": "Qwen2.5-0.5b",
          "score": 3,
          "created_utc": "2026-02-20 01:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b3dnv",
          "author": "DuckyBlender",
          "text": "Gemma 3 270M",
          "score": 2,
          "created_utc": "2026-02-19 20:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6civqc",
          "author": "catplusplusok",
          "text": "BitNet?",
          "score": 2,
          "created_utc": "2026-02-20 01:40:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74ks8r",
              "author": "so_schmuck",
              "text": "Not BitConnect ?",
              "score": 1,
              "created_utc": "2026-02-24 12:44:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gm2b6",
          "author": "GeneralComposer5885",
          "text": "Sentence Bert",
          "score": 1,
          "created_utc": "2026-02-20 17:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p3qhq",
          "author": "Mice_With_Rice",
          "text": "There are lots of good models in that size. The real question is what sort of model are you looking for, and what is the use case.\n\nFor example, KittenTTS is <25MB. But do you want a TTS model? There are good embedding models if you want to vectorize data. If you want a custom model for a specific use you can make your own as well. Lots of options, but just asking for \"good\" is open to a lot of interpretation.",
          "score": 1,
          "created_utc": "2026-02-22 01:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rnjp2",
          "author": "mrtumederanges",
          "text": "LFM2-350MÂ from [liquid.ai](http://liquid.ai) ",
          "score": 1,
          "created_utc": "2026-02-22 13:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ylbe",
          "author": "FeiX7",
          "text": "use case?",
          "score": 1,
          "created_utc": "2026-02-23 21:42:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ar1om",
          "author": "supportend",
          "text": "Kitten TTS, 14, 40 and 80M.",
          "score": 1,
          "created_utc": "2026-02-19 19:55:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rblyus",
      "title": "How to maximize Qwen3.5 t/s?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rblyus/how_to_maximize_qwen35_ts/",
      "author": "Altruistic_Call_3023",
      "created_utc": "2026-02-22 13:33:20",
      "score": 12,
      "num_comments": 34,
      "upvote_ratio": 0.93,
      "text": "Hello all.  I am following the guide from unsloth about running qwen3.5 that says 25t/s+ with 24gb vram and 256GB RAM is possible on the 4bit Dynamic quant. Iâ€™m only seeing around 7t/s with my 3090 and 32 core Xeon on 356gb of ddr4 RAM so Iâ€™m trying to understand what I might be configuring wrong.  (Or is it just because Iâ€™m not ddr5 and more recent cpu?) I also have two 5060ti I can use - but adding those in, I donâ€™t see any real performance increase.  Iâ€™m using a current llama.cpp built just yesterday.  Thanks for any help. My settings are:\n\nctx-size = 32768\n\nbatch-size = 512\n\nubatch-size = 512\n\nthreads = 32\n\ntemp = 0.6\n\nmin-p = 0\n\ntop-p = 0.95\n\nrepeat-penalty = 1.0\n\npresence-penalty = 0.0\n\ntop-k = 20\n\nfa = on\n\ncache-type-k = q8\\_0\n\ncache-type-v = q8\\_0\n\nfit = on\n\nnp = 1",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rblyus/how_to_maximize_qwen35_ts/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6s38gc",
          "author": "suicidaleggroll",
          "text": "I think the guide is just saying you *can* hit those speeds, as in itâ€™s theoretically possible, not that you *will* hit them. Â Iâ€™m hitting 32 tok/s on my system. Â Thatâ€™s with 96 GB VRAM, but given this model size there honestly wonâ€™t be much difference in speed between 24G and 96G, pretty much everything is still running on the CPU anyway. Â I can test it again while limiting GPU usage to <24 GB to put a number on it, Iâ€™m guessing itâ€™d be close to that 25 tok/s.\n\nThatâ€™s with an EPYC 9455P with 12 channels of DDR5-6400, so around 600 GB/s memory bandwidth. Â Memory bandwidth is the big limiter on CPU inference speeds.\n\nEdit: Looks like there was a recent llama.cpp update that increased speeds since my first benchmark, re-testing it brought inference up to 39.5 as a baseline with 36 layers offloaded to the CPU and ~90 GB VRAM usage.  Increasing to 56 layers offloaded to the CPU brought VRAM usage down to <24 GB, and dropped inference speeds back down to about 33.  So 25 is definitely possible with only 24 GB of VRAM, but you need a CPU with high memory bandwidth to do it.",
          "score": 4,
          "created_utc": "2026-02-22 14:56:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6srjar",
              "author": "Altruistic_Call_3023",
              "text": "Thanks for the info.  Good to know and helpful.",
              "score": 2,
              "created_utc": "2026-02-22 16:48:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7513zn",
              "author": "Pitiful_Gene_3648",
              "text": "Can you share the command you run it? I have 96gb vram also with 256ram hitting only max 17 tps",
              "score": 1,
              "created_utc": "2026-02-24 14:17:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uij7b",
          "author": "someone383726",
          "text": "I was getting about 15 T/s with llama cpp and Q4-K-XL with 90gb on RTX6000 and the rest on my 4x64 DDR5-6000.   I thought I must be doing something wrong too.",
          "score": 2,
          "created_utc": "2026-02-22 21:52:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypft1",
              "author": "xanduonc",
              "text": "same with 8xddr4-3200",
              "score": 1,
              "created_utc": "2026-02-23 15:20:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6spd5g",
          "author": "djdeniro",
          "text": "i have only 19 t/s with full GPU loaded UD\\_Q4\\_K\\_XL model. 6xR9700 + 6x7900xtx, also super slow on PP",
          "score": 2,
          "created_utc": "2026-02-22 16:39:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6srf5g",
              "author": "Altruistic_Call_3023",
              "text": "Thanks for posting that.  I feel better that Iâ€™m not missing something obvious",
              "score": 2,
              "created_utc": "2026-02-22 16:48:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t6lsw",
                  "author": "djdeniro",
                  "text": "anyway i tested 3 times, and the maximum i got in prompt processing is 100t/s.  i also search results in OpenRouter, and we can be sure, VL models work slower than non-vision models.  But when i see here someone got 32 t/s i got questions, how RAM based launches faster than GPU..",
                  "score": 0,
                  "created_utc": "2026-02-22 17:58:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rxc2n",
          "author": "ciprianveg",
          "text": "I am getting 18.5t/s on 8 channel ddr4 3995wx cpu and 1x3090.",
          "score": 1,
          "created_utc": "2026-02-22 14:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rza8z",
              "author": "Altruistic_Call_3023",
              "text": "Maybe my dual Xeon silver is just the bottleneck.  Itâ€™s the gen before the optimizations started, and itâ€™s only actually 16 cores I see since hyperthreading.  I do have a bunch of channels of memory, but maybe thatâ€™s my breakdown point.",
              "score": 2,
              "created_utc": "2026-02-22 14:36:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rzlvd",
                  "author": "ciprianveg",
                  "text": "search for a numactl command to treat both as a unique processing unit",
                  "score": 2,
                  "created_utc": "2026-02-22 14:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sr6zl",
              "author": "EbbNorth7735",
              "text": "What's your setup and commands? I'm hitting 11tps with 5955wx. Certainly possible the reduced speed is simply from reduced NUMA or whatever the ram processor blocks are called. I think 5995 and 3995 have 4, 5955 has 1.",
              "score": 2,
              "created_utc": "2026-02-22 16:47:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6t4332",
                  "author": "ciprianveg",
                  "text": "8x64gb 2933mt/s 3995wx 1x3090 24gb\n/build/bin/llama-server  --model /home/ciprian/models/Qwen3.5-397b/Qwen3.5-397B-A17B-UD-Q4_K_XL-00001-of-00006.gguf --alias Qwen3-397B \\\n --flash-attn on   -c 128000 -ub 4096 -b 4096  -fit on  -ngl 999  -ot \"blk\\.(0|1)\\.ffn_.*_exps.*=CUDA0\"   --cpu-moe \\\n --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0   --threads 58 --host 0.0.0.0 --port 5001   --chat-template-kwargs \"{\\\"enable_thinking\\\": false}\"  \\\n   --jinja --parallel 1  --no-mmap --mmproj /home/ciprian/models/Qwen3.5-397b/mmproj-BF16.gguf",
                  "score": 3,
                  "created_utc": "2026-02-22 17:47:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6s1g3d",
              "author": "some_user_2021",
              "text": "Which quantization and how much context was being processed?",
              "score": 1,
              "created_utc": "2026-02-22 14:47:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s1us5",
                  "author": "ciprianveg",
                  "text": "UD-Q4-XL. it stays pretty constant. 18.5 4k context, 17.8 16k context",
                  "score": 2,
                  "created_utc": "2026-02-22 14:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6u8wa5",
              "author": "segmond",
              "text": "what quant?  what's speed of your ram?",
              "score": 1,
              "created_utc": "2026-02-22 21:04:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6u9s9c",
                  "author": "ciprianveg",
                  "text": "UD-Q4-XL. 2933 8 channels",
                  "score": 1,
                  "created_utc": "2026-02-22 21:08:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6t081d",
          "author": "LA_rent_Aficionado",
          "text": "I get around 40 t/s fully offloaded at Q4 with about 120-160k context, itâ€™s just not a very fast model given the multimodal piece adds complexity",
          "score": 1,
          "created_utc": "2026-02-22 17:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6u92f5",
              "author": "segmond",
              "text": "hardware specs? GPUs, cpu, memory speed?",
              "score": 1,
              "created_utc": "2026-02-22 21:04:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6uu8mv",
                  "author": "LA_rent_Aficionado",
                  "text": "Asus WRX90 SAGE, 7965wx, 384GB G.skill DDR5 @ 6000, 6000 Pro, 5090, 2x 3090ti and 4x 3090",
                  "score": 1,
                  "created_utc": "2026-02-22 22:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6vvwer",
          "author": "Fresh_Finance9065",
          "text": "mlock = on\nswa-full = on\nkvu = on\n\nmlock and kvu definitely speed things up. swa full may, but i do not understand it enough to know whether it really does or not",
          "score": 1,
          "created_utc": "2026-02-23 02:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rpb1q",
          "author": "Antique_Dot_5513",
          "text": "Pareil sur lm studio jâ€™obtiens pas du tout ces fameux 25 t/s",
          "score": 0,
          "created_utc": "2026-02-22 13:39:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rapr95",
      "title": "Subject: Seeking Validation: Strategy for Multi-LoRA Behavioral Fine-Tuning on Micro-Datasets (50-100 rows)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rapr95/subject_seeking_validation_strategy_for_multilora/",
      "author": "Scouserleemc",
      "created_utc": "2026-02-21 12:18:35",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "\n\nHi Folks,\n\nI am currently building a composite agentic system for my PhD dissertation (a Design-Based Research project). The system is a \"Purposeful Agent\" designed to act as a professional executive coach. It uses a multi-agent RAG architecture with a vLLM backend routing to multiple specialized LoRA adapters (e.g., an `adapter_empathy`, `adapter_scaffolding`, `adapter_planner`) based on the user's real-time emotional state (Valence-Arousal-Dominance).\n\nBecause my research relies on highly authentic, expert-validated facilitation transcripts, my dataset is incredibly constrained. Based on the *LIMA (Less Is More for Alignment)* hypothesis, I am attempting to do purely behavioral/stylistic fine-tuning using extremely small, highly curated datasetsâ€”specifically **only 50 to 100 rows of data per adapter**.\n\nMy goal is not to teach the model new knowledge, but to teach it a very specific facilitative stance (e.g., asking open-ended questions, mirroring, and strictly avoiding giving direct advice).\n\nGiven the high risk of catastrophic overfitting with such a small dataset, I have developed the following training strategy using Unsloth. I would love your expert feedback on whether this is viable and if there are any Unsloth-specific optimizations I should apply:\n\n**1. Data Structure: Multi-Turn ChatML Threads** Instead of single-turn Q&A pairs, I am formatting my 50-100 rows as multi-turn conversational histories (User -> Assistant -> User -> Assistant) using standard ChatML. The theory is that this will provide enough linguistic density for the attention mechanism to learn the temporal pacing of a coaching intervention (e.g., when to validate vs. when to probe) rather than just acting like a reactive search engine.\n\n**2. Data Composition: \"Hard Negatives\" to counter RLHF** Base instruction models (like Llama-3-8B-Instruct) are heavily biased toward sycophancy and immediate problem-solving due to their RLHF training. To overwrite this urge to give \"helpful advice,\" roughly 20% of my micro-dataset consists of \"hard negative\" interactions, where the user explicitly begs for advice, and the assistant actively deflects and returns agency to the user.\n\n**3. Hyperparameter Adjustments for Micro-Datasets** To prevent the loss curve from instantly crashing to zero and the model simply memorizing the 50 transcripts, I am planning the following hyperparameter constraints:\n\n* **LoRA Rank (r) & Alpha:** Very low rank (r=4 or 8) with Alpha=16 to restrict the adapter's capacity and force generalization over memorization.\n* **Dropout:** Increasing LoRA dropout to `0.05` or `0.10`.\n* **Learning Rate:** Lowering to `2e-5` for a gentler update to the stylistic weights.\n* **Epochs:** Capping at 3 to 4 epochs, utilizing a small holdout set to closely monitor Validation Loss. If validation loss spikes while training loss drops, I will trigger early stopping.\n\n**My Questions:**\n\n1. Given Unsloth's underlying optimizations, is this micro-dataset strategy (50-100 multi-turn rows) mathematically viable for behavioral cloning, or is that simply too little data for the optimizer to find a meaningful gradient?\n2. Are there any specific Unsloth arguments, parameters, or configurations (e.g., specific target modules, gradient accumulation steps, or learning rate schedulers) you would highly recommend when the dataset is this tiny?\n3. Have you seen success with multi-turn ChatML formatting in Unsloth when trying to teach conversational pacing rather than just instruction following?\n\nThank you so much for your time and for building such an incredible tool for the open-source community!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rapr95/subject_seeking_validation_strategy_for_multilora/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6magef",
          "author": "wildyam",
          "text": "I am too dumb to add value but am commenting to show support! Sounds fascinating - good luck!",
          "score": 3,
          "created_utc": "2026-02-21 16:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nappe",
          "author": "Rhinoseri0us",
          "text": "This is really exciting to me and my current focus of work/research. Would you be open to a message/connecting? My focus is partly on training small edge models and your dissertation seems extremely compelling to my line of thinking. Not trying to gas you up just saying ðŸ˜†",
          "score": 2,
          "created_utc": "2026-02-21 19:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q2ky9",
          "author": "fourwheels2512",
          "text": "Great setup â€” a few concrete answers to your three questions:\n\n\n\n\\*\\*1. Is 50-100 multi-turn rows viable?\\*\\*\n\nYes, for behavioral/stylistic cloning specifically. LIMA showed 1000 rows generalises, but you're not teaching knowledge â€” you're overwriting an attentional pattern (\"deflect advice, return agency\"). At r=4 with multi-turn ChatML you're probably updating \\~0.1% of weights. The optimizer has enough signal from 50 well-formed coaching transcripts if the examples are consistent in style. The risk isn't gradient direction, it's gradient \\*magnitude\\* â€” with tiny batches you'll see noisy norm spikes that look alarming but aren't.\n\n\n\n\\*\\*2. Unsloth-specific recommendations:\\*\\*\n\n\\- Use \\`gradient\\_accumulation\\_steps=4-8\\` to smooth out the noisy per-step gradients you'll get from batch\\_size=1-2\n\n\\- \\`warmup\\_ratio=0.1\\` (longer warmup than usual) â€” the model needs more steps before it \"commits\" to the style shift\n\n\\- \\`weight\\_decay=0.01\\` helps prevent the few-shot memorisation collapse\n\n\\- For target modules, \\`q\\_proj, v\\_proj\\` only (skip k/o/gate) â€” minimum footprint for behavioural style\n\n\n\n\\*\\*3. On your early stopping trigger:\\*\\*\n\nValidation loss \\*spikes\\* on micro-datasets are often gradient norm events rather than true divergence â€” the spike resolves within 2-3 steps. Before triggering early stopping, check if the spike recovers. A tool like ZClip (adaptive gradient clipping based on rolling norm history) handles this better than fixed \\`max\\_grad\\_norm\\` â€” it only clips when the norm is statistically anomalous vs. your run history rather than at a fixed ceiling.\n\n\n\nI ran a similar ablation on TinyLlama (200 rows, same seed) comparing plain LoRA vs LoRA + adaptive clipping â€” peak grad norm dropped 52.7% with neutral impact on final loss. For a 50-row micro-dataset the effect would likely be more pronounced. Happy to share details if useful.",
          "score": 1,
          "created_utc": "2026-02-22 05:05:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbhvrv",
      "title": "Fine-Tuning Qwen 4B: Need Tips on Configs, Overfitting & Small Datasets?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rbhvrv/finetuning_qwen_4b_need_tips_on_configs/",
      "author": "dyeusyt",
      "created_utc": "2026-02-22 09:50:35",
      "score": 9,
      "num_comments": 9,
      "upvote_ratio": 0.91,
      "text": "So am working on my thesis project which involves fine-tuning a small language model for a specific code generation task in a niche domain (Typescript)\n\nI'm leaning toward the Qwen family of models. I started by fine-tuning the 8B version, but it didn't feel like a true SLM in terms of consumer-hardware-efficiency and size, so I'm downgrading to the 4B variant for better adherence to SLM part.\n\nMy main concern is my dataset: It's high-quality but small, with only 700-800Â `{prompt,completion}`Â pairs. Some pairs are distilled from larger LLMs, while others come from real code snippets paired with synthetically generated prompts. The data is straightforward (no chain-of-thought reasoning) but it includes potential noise: like non-code elements in code files (placeholders, plain text, or image paths). I want to train the model effectively so it performs well on my use case without picking up this noise or overfitting to the limited examples\n\nFor context I'm currently training on Google Colab with an A100 GPU. Here's the configuration I'm using, based on recommendations from Reddit threads and Unsloth docs:\n\n    model = FastLanguageModel.get_peft_model(\n    Â  Â  model,\n    Â  Â  r=64,\n    Â  Â  lora_alpha=128,\n    Â  Â  lora_dropout=0.05,\n    Â  Â  target_modules=[\n    Â  Â  Â  Â  \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", Â # Self-attention\n    Â  Â  Â  Â  \"gate_proj\", Â # MLP gate for code generation patterns\n    Â  Â  ],\n    Â  Â  bias=\"none\", Â \n    Â  Â  use_gradient_checkpointing=\"unsloth\", \n    Â  Â  random_state=3407,\n    Â  Â  use_rslora=False,\n    Â  Â  loftq_config=None,\n    )\n    \n    training_args = SFTConfig(\n    Â  Â  output_dir=\"./qwen-8b-a100\",\n    Â  Â  per_device_train_batch_size=16, \n    Â  Â  gradient_accumulation_steps=2, Â \n    Â  Â  per_device_eval_batch_size=16, Â \n    \n    Â  Â  num_train_epochs=3,\n    Â  Â  max_steps=-1, Â # Use epochs (not max_steps)\n    Â  Â  learning_rate=2e-4,\n    Â  Â  lr_scheduler_type=\"cosine\",\n    Â  Â  warmup_ratio=0.05, Â # 5% warmup\n    Â  Â  optim=\"adamw_8bit\", Â # Memory efficient, works well with LoRA\n    Â  Â  weight_decay=0.01, Â  # Light regularization\n    Â  Â  fp16=False, Â # Don't use FP16 on A100\n    Â  Â  bf16=True, Â # A100 has native BF16 support - MUCH better!\n    Â  Â  tf32=True, Â # Enable TensorFloat-32 for even faster matmuls\n    Â  Â  dataloader_num_workers=4, Â # Parallel data loading\n    Â  Â  dataloader_pin_memory=True, Â # Faster GPU transfers\n    Â  Â  logging_steps=5,\n    Â  Â  eval_strategy=\"steps\",\n    Â  Â  eval_steps=10,\n    Â  Â  save_strategy=\"steps\",\n    Â  Â  save_steps=10, Â # Match eval_steps\n    Â  Â  save_total_limit=3, Â # Keep 3 best\n    Â  Â  load_best_model_at_end=True,\n    Â  Â  metric_for_best_model=\"eval_loss\",\n    Â  Â  greater_is_better=False,\n    Â  Â  packing=True,\n    Â  Â  max_seq_length=4096,\n    Â  Â  seed=3407,\n    Â  Â  report_to=\"none\",\n    Â  Â  dataset_text_field=\"text\",\n    )\n    \n    trainer = SFTTrainer(\n    Â  Â  model=model,\n    Â  Â  args=training_args,\n    Â  Â  processing_class=tokenizer,\n    Â  Â  train_dataset=train_dataset_formatted,\n    Â  Â  eval_dataset=val_dataset_formatted,\n    )\n    \n    # Using Unsloth's gradient accumulation fix\n    from unsloth import unsloth_train\n    trainer_stats = unsloth_train(trainer)\n\nI'm fairly new to fine-tuning (about 60% VibeCoding; 40% reading docs) and the results so far aren't great. The model underperforms on my tasks - The 8B one.\n\nSo I'm reaching out to folks who've worked with Qwen models: What configs have worked well for you, especially for small datasets and code generation? Any tips on preventing overfitting? Are there must-read docs or guides to get started properly?\n\nThanks in advance.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rbhvrv/finetuning_qwen_4b_need_tips_on_configs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6r7bct",
          "author": "yoracale",
          "text": "The most important read are our lora hyperparameters guide which includes overfitting/underfitting, selecting the correct hyperparamters and much much more: [https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)",
          "score": 6,
          "created_utc": "2026-02-22 11:20:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rawcl",
          "author": "NorthEastCalifornia",
          "text": "I'm working in the same direction, but in a different domain (language). I'm currently preparing a dataset, so I'm reading carefully. 8b seems like a good option, I just need to play with the parameters.",
          "score": 2,
          "created_utc": "2026-02-22 11:53:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rv0vu",
              "author": "dyeusyt",
              "text": "8B is actually usable now but its honestly too big to still call it a proper SLM. The graph difference between the smaller one and this doesnâ€™t even look that impressive anyway; Iâ€™ll probably just fine-tune both and see which one comes out on top considering resources and all.",
              "score": 1,
              "created_utc": "2026-02-22 14:12:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rsplf",
          "author": "wektor420",
          "text": "I have similiar setup after 400 steps eval loss stops improving :/",
          "score": 2,
          "created_utc": "2026-02-22 13:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rub9h",
              "author": "dyeusyt",
              "text": "I did the above because some docs on Unsloth (or related to Qwen) recommended it; But maybe 400 it is then.",
              "score": 2,
              "created_utc": "2026-02-22 14:08:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rnsla",
          "author": "Thrumpwart",
          "text": "Iâ€™m planning to do a full fine tune on Q4B with a 3 billion token dataset. Am I crazy or incredible? Please advise.",
          "score": 1,
          "created_utc": "2026-02-22 13:29:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rprta",
              "author": "dyeusyt",
              "text": "> Please advise.\n\nYou're the reason RAM costs a fortune now! /s",
              "score": 3,
              "created_utc": "2026-02-22 13:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6x2tfc",
          "author": "NoobMLDude",
          "text": "> r=64,\n> lora_alpha=128,\n\nFor a dataset with 800 samples , the Lora rank (r) seems high. A rank of 8 should be a good starting point. \n\nIf you suspect overfitting, reducing rank and lora_alpha is a good strategy. \n\nRemember Lora_rank (r) are usually multiple of 2 : 4,8,16.",
          "score": 1,
          "created_utc": "2026-02-23 08:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zcrkf",
          "author": "indicava",
          "text": "Why use PEFT? For a 4B model with 4K seq lengths, you could probably do a full parameter fine tune on that A100. \n\nYou MUST have a bigger and more diverse dataset. 600 examples is hardly enough for coding in a niche domain. \nThink 10K-30K examples, thatâ€™s the mindset you need to get into. \nAlso, critically, more is not enough. They have to be diverse across your niche domain so the model can learn to generalize around it. \n\nBuild out a good synthetic data generation pipeline, leverage SOTA modelâ€™s free tiers if you canâ€™t afford the API costs.\n\nLastly, donâ€™t set too high expectations for a 8B model, let alone a 4B one. They ainâ€™t that smart, even with proper fine tuning.",
          "score": 1,
          "created_utc": "2026-02-23 17:09:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7dcdm",
      "title": "Best Unsloth model for 12GB RAM + GTX 1050 (3GB VRAM) for inference only?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r7dcdm/best_unsloth_model_for_12gb_ram_gtx_1050_3gb_vram/",
      "author": "Ok-Type-7663",
      "created_utc": "2026-02-17 17:55:24",
      "score": 6,
      "num_comments": 16,
      "upvote_ratio": 0.75,
      "text": "Iâ€™m trying to run a local LLM using Unsloth for inference only (NOT finetuning), and I want the best model my hardware can handle smoothly.\n\n**My specs:**\n\n* RAM: 12GB\n* GPU: GTX 1050 (3GB VRAM)\n* OS: Linux\n* Goal: inference/chat, not training\n* Prefer GGUF or Unsloth-compatible models\n\n**Priorities:**\n\n* Best quality possible within my limits\n* Stable inference (no crashes / OOM)\n* Good reasoning and instruction following\n* Fast enough to be usable\n\n**Questions:**\n\n1. What is the BEST model size I can realistically run? (1B, 3B, 4B, etc)\n2. Which specific Unsloth model do you recommend?\n3. What quant should I use? (Q4\\_K\\_M, Q5\\_K\\_M, etc)\n4. Should I use GPU offloading or pure CPU with my 3GB VRAM?\n\nIf possible, please recommend exact HF model IDs.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r7dcdm/best_unsloth_model_for_12gb_ram_gtx_1050_3gb_vram/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5wktmj",
          "author": "--Spaci--",
          "text": "lfm 1.2b",
          "score": 2,
          "created_utc": "2026-02-17 18:06:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wkx4z",
              "author": "--Spaci--",
              "text": "LiquidAI/LFM2.5-1.2B-Thinking",
              "score": 2,
              "created_utc": "2026-02-17 18:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wlnzw",
                  "author": "Ok-Type-7663",
                  "text": "which quant?",
                  "score": 1,
                  "created_utc": "2026-02-17 18:10:05",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5y5bq3",
                  "author": "ctanna5",
                  "text": "This.",
                  "score": 1,
                  "created_utc": "2026-02-17 22:32:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wqigx",
          "author": "Fresh_Finance9065",
          "text": "You will want models under 1.5B on gpu, or under 3B on cpu.\n\nFor gpu, lfm2.5-1.2b is as good as it gets. You wouldn't really chat with it through, more of 1 shotting before you go oom.\n\nFor cpu, you can try granite 4 micro. It is not very smart for its size, but its also very hard to oom with it. You don't have much space to reason with. Qwen 3VL 4B and Ministral 3B are smarter for the suze but goodluck with oom.\n\nFor gpu, use Q8_K_XL from unsloth or Q8_0. For cpu, use Q6_K_XL from unsloth or any Q6_K imatrix.",
          "score": 2,
          "created_utc": "2026-02-17 18:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wu22l",
              "author": "Crafty_Ball_8285",
              "text": "I had to use iq4 instead of iq6 on a 3b model on Mac because q6 was chugging the full vram",
              "score": 1,
              "created_utc": "2026-02-17 18:48:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xncwb",
                  "author": "Fresh_Finance9065",
                  "text": "I find models this size lose too much instruction following ability and hallucinate more if they are quantized too far, thats why i prefer larger quants than other ppl",
                  "score": 1,
                  "created_utc": "2026-02-17 21:06:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o605a1i",
          "author": "Mabuse046",
          "text": "I'm not exactly following what you're asking, so just to make sure we're on the same page, Unsloth is a python library for working with / training transformers models - the primary format you find on Huggingface. It's not like an app you load the model in. You write python code and inside the code you call unsloth/transformers/trl to load the model and then you can do whatever you want to it with your code. A GGUF is a custom format used by llama.cpp - and llama.cpp comes with the python script to convert a transformers model to a GGUF and another script to Quantize it. An \"Unsloth model\" is just a normal model that the unsloth team tuned up, but it's still in the standard transformers format and then optionally converted into a handful of other proprietary formats like GGUF. So my question is, are you asking for models that you can load using the unsloth library in python, or models that have been tweaked by the unsloth team?",
          "score": 2,
          "created_utc": "2026-02-18 05:25:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ds60r",
              "author": "eagledoto",
              "text": "He's fs asking about the quantized models",
              "score": 1,
              "created_utc": "2026-02-20 07:03:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nvswp",
          "author": "souna06",
          "text": "Had similar VRAM constraints before upgrading. If you want to quickly check what fits, advisor.forwardcompute.ai shows the VRAM breakdown per model at different quantizations. With 3GB you're pretty much limited to sub-2B models on GPU, which tracks with what others said here.",
          "score": 1,
          "created_utc": "2026-02-21 20:55:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcpg7a",
      "title": "llama-server Production Ready?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rcpg7a/llamaserver_production_ready/",
      "author": "Sudden_Tennis_2067",
      "created_utc": "2026-02-23 18:26:22",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 0.8,
      "text": "Wondering if llama-server (that's part of llama.cpp) is production ready and performance is comparable to vllm?\n\nMost of the comparisons I see are between vllm and llama.cpp, and they show that vllm is significantly more performant and llama.cpp is just not production ready. But I wonder if it's a different story for llama-server?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rcpg7a/llamaserver_production_ready/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o73i5dw",
          "author": "TokenRingAI",
          "text": "It's not even remotely production ready, the regex parser triggers segfaults multiple times per day while consuming 100% cpu.",
          "score": 6,
          "created_utc": "2026-02-24 07:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73kj86",
              "author": "yoracale",
              "text": "There's a current PR which should fix the parsing issues: [https://github.com/ggml-org/llama.cpp/pull/18675](https://github.com/ggml-org/llama.cpp/pull/18675)",
              "score": 5,
              "created_utc": "2026-02-24 07:29:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73gixx",
          "author": "yoracale",
          "text": "The biggest difference I'd say if you have any CPU or RAM in your setup, llama.cpp is definitely better and faster. Llama-server is production ready and the best for single user inference (they also have multi user). There are ways to enable high throughput mode if you look through their docs.\n\nIf you mainly utilize GPUs, especially one's that are large like H100s, or if you want batched inference for multiple users, then vLLM is most likely better.",
          "score": 2,
          "created_utc": "2026-02-24 06:53:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74hlsq",
          "author": "sid_276",
          "text": "production ready you are looking at vLLM, SGLang, TensorRT-LLM",
          "score": 1,
          "created_utc": "2026-02-24 12:22:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74t2yh",
          "author": "StardockEngineer",
          "text": "No one runs llama.cpp as a production server if theyâ€™re serious.",
          "score": 1,
          "created_utc": "2026-02-24 13:34:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o750oqq",
              "author": "PaceZealousideal6091",
              "text": "I hope this changes with lcpp-hf tie up because nothing comes close to lcpp for edge devices inference.",
              "score": 1,
              "created_utc": "2026-02-24 14:15:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o752efd",
                  "author": "StardockEngineer",
                  "text": "Define what edge inference means to you? \n\nedit: https://huggingface.co/blog/ggml-joins-hf",
                  "score": 1,
                  "created_utc": "2026-02-24 14:24:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rcehjr",
      "title": "Mac Studio (M4 Max, 128GB) for FULL fine-tuning a 27B Model",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rcehjr/mac_studio_m4_max_128gb_for_full_finetuning_a_27b/",
      "author": "PlayerWell",
      "created_utc": "2026-02-23 11:00:51",
      "score": 5,
      "num_comments": 12,
      "upvote_ratio": 0.73,
      "text": "Hi,\nWe are looking to add dedicated hardware to our project pipeline specifically to fully fine-tune and run inference on a 27B parameter model (Gemma 3 27B).\n\nWe are currently considering adding a Mac Studio with the following specs:\n-M4 Max (16-Core CPU, 40-Core GPU)\n-128GB Unified Memory\n-1TB SSD\n\nFor those of you who have experience training LLMs on Apple Silicon (using MLX), I have a few specific questions:\n1. Is a single Mac Studio realistically enough for a full fine-tune of a 27B model? (not LoRA/QLoRA).\n2. If we hit a bottleneck and need more compute/memory later, is it practical to buy a second Mac Studio and cluster them together for distributed training? \n3. Would it be a much more logical scenario to skip the Mac ecosystem entirely, buy GPUs and build a standard multi-GPU workstation connected via PCIe?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rcehjr/mac_studio_m4_max_128gb_for_full_finetuning_a_27b/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6ydgvx",
          "author": "Current_Ferret_4981",
          "text": "You will always spend less money and time just renting for a given model training. Only matters if you plan to do a fine-tuning run regularly to warrant local hardware or need data privacy. \n\nThat being said, I would much rather have a GPU cluster than Mac studio for training.",
          "score": 4,
          "created_utc": "2026-02-23 14:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yup09",
              "author": "PlayerWell",
              "text": "Because of the project requirements, the data we use during training and the environment where the model will run after training must be completely local. Project budget is tight, but since we don't have a time constraint",
              "score": 2,
              "created_utc": "2026-02-23 15:45:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xqcfr",
          "author": "CKtalon",
          "text": "3. or wait for M5 (since you are going for M4 Max only, the M5 Max benchmarks will be out in about 2-3 weeks) and see if the matmul optimizations are better for training.",
          "score": 3,
          "created_utc": "2026-02-23 11:46:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o706l79",
          "author": "superSmitty9999",
          "text": "Just be aware Macs have good memory bandwidth, which is important for inference, but horrible FLOPs, which are required for training in addition to mem bandwidth.Â \n\nI personally would consider training on a Mac to be strictly speaking not worth it except for the smallest jobs.Â \n\nI bought a DGX spark and though the memory bandwidth on it sucks itâ€™s more balanced in terms of flops.Â \n\nIf youâ€™re serious about training I think 1-4 RTX Pro 6000â€™s is the minimum for decent performance. (Or the cloud, obviously)Â \n",
          "score": 3,
          "created_utc": "2026-02-23 19:26:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o740pjd",
              "author": "PlayerWell",
              "text": "After the comments here, we started looking at the RTX PRO 4000, 5000 and 6000 graphics cards.",
              "score": 1,
              "created_utc": "2026-02-24 10:02:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6yjs5e",
          "author": "LA_rent_Aficionado",
          "text": "3. Thatâ€™s certainly not enough memory for a FFT of 27B or even 14B for that matter. Perhaps 8B tops without LORA.\n\nAlso training speed on Mac will be abysmal, for any model that size you are looking at data center class GPUs if you want efficient FFT.  You could use 6000 pros but lose out in NVLink so itâ€™ll add time. \n\nEither way I would not go the route of a Mac and either rent GPUs or get ready to shell out some serious cash",
          "score": 2,
          "created_utc": "2026-02-23 14:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70yyjd",
          "author": "pl201",
          "text": "You need 512gb unified memory to do full fine tuning a 27b model on Mac Silicon. I was trying with M3/256GB for a 24b model and failed on out of memory. Ended up just do a LoTA training.",
          "score": 2,
          "created_utc": "2026-02-23 21:43:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73aczp",
          "author": "Desperate-Sir-5088",
          "text": "Please dont try that. Even 4bit QLoRA finetuning of gemma-3 is extreamly slow in M3Ultra.",
          "score": 2,
          "created_utc": "2026-02-24 06:01:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73jjso",
          "author": "SafetyGloomy2637",
          "text": "A full fine tune on a 27b model is going to need about 250-300gb of VRAM. 128gb RAM isnâ€™t enough and your bandwidth is limited too,  SFT job on a couple hundred million tokens would take 10+ hours at least unless you use adamw4bit, but I personally would stay away from anything less that Fp16",
          "score": 2,
          "created_utc": "2026-02-24 07:20:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y3wsa",
          "author": "yoracale",
          "text": "We're working on MLX support for Unsloth currently. Benchmark currently show that MLX is definitely not as optimized as it could be, I don't think it's possible FFT a 27B model on 128gb, or even 192GB tbh. I'd rather just do bf16 LoRA.",
          "score": 1,
          "created_utc": "2026-02-23 13:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yvb0h",
              "author": "PlayerWell",
              "text": "I sincerely apologize for opening this here. I know it's off-topic since you don't technically support it right now. I haven't developed on a Mac before, so I just assumed it would be supported",
              "score": 1,
              "created_utc": "2026-02-23 15:48:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xn2uc",
          "author": "No_Conversation9561",
          "text": "https://github.com/Goekdeniz-Guelmez/mlx-lm-lora\n\ncheck this out too",
          "score": 1,
          "created_utc": "2026-02-23 11:18:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra6xsg",
      "title": "What are some coding AI models I can run with my hardware?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1ra6xsg/what_are_some_coding_ai_models_i_can_run_with_my/",
      "author": "Sharp-University-555",
      "created_utc": "2026-02-20 20:44:08",
      "score": 4,
      "num_comments": 18,
      "upvote_ratio": 0.62,
      "text": "I have a PC with a 14600K, 32GB of E-die DDR4 RAM (which I could run at a stable OC upwards of 4000Mhz) and a Founders RTX 3070 8GB.\n\n\nI would appreciate any sort of feedback or direction since I have just started coding with Claude Pro Plan and want to explore moving towards a (small due to hardware constraints) local model or another alternative due to burning through tokens too fast. It would be used for HTML/CSS/JS mostly, so nothing too crazy programming wise.\n\n\nAlso, what is the best GUI I can use with local AI models? I love the Claude Desktop app interface, can I still use that or is there any other equivalent that follows the same design (showing the snippets of code it's changing in real time and having a easy to read history to go through when I want to revisit some ideas I prompted earlier.)\n\n\nThanks a lot!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1ra6xsg/what_are_some_coding_ai_models_i_can_run_with_my/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6hy5tc",
          "author": "Shoddy_Bed3240",
          "text": "The most you can run is GLM 4.7 Flash. Pick a quantization level that fits within your available VRAM and RAM. Probably you can hit 10 t/s",
          "score": 7,
          "created_utc": "2026-02-20 21:44:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ia52l",
              "author": "Sharp-University-555",
              "text": "Will give it a go tomorrow once I set up Llama.cpp, thanks Shoddy!",
              "score": 1,
              "created_utc": "2026-02-20 22:47:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k5w6w",
          "author": "el-rey-del-estiercol",
          "text": "Glm 4.7 flash",
          "score": 3,
          "created_utc": "2026-02-21 06:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kqlyt",
              "author": "Sharp-University-555",
              "text": "Will give it a go, thanks!",
              "score": 1,
              "created_utc": "2026-02-21 09:24:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6iav37",
          "author": "LegacyRemaster",
          "text": "unsloth/Qwen3-Coder-Next-GGUF best one. Consider 36 gb (real) free: try IQ3",
          "score": 2,
          "created_utc": "2026-02-20 22:50:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kqmyc",
              "author": "Sharp-University-555",
              "text": "Got it, will try it in a couple hours",
              "score": 2,
              "created_utc": "2026-02-21 09:24:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6io4vo",
          "author": "ZealousidealShoe7998",
          "text": "someone recently posted on how to run qwen coder on a similar hardware but laptop version. they got it to run very slowsly but it was running.   \nif you use a good hardness you might be able to run it and go to other things while its processing.",
          "score": 2,
          "created_utc": "2026-02-21 00:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kqq0b",
              "author": "Sharp-University-555",
              "text": "I will look for that post, I don't mind waiting but I want to see which model produces the best quality code so I'll probably try a few of them before I make my pick",
              "score": 1,
              "created_utc": "2026-02-21 09:25:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nizk0",
          "author": "kiwibonga",
          "text": "Not recommended at all, the only local models that are actually useful (approaching Sonnet 3.5 / GPT-4o quality) are at least 20-30B parameters (10-16 GB with heavy quantization, plus 10-20 GB for the context).\n\nCPU inference is miserable also. Most agents and CLI tools add workflows and embeddings to the first prompt in a session. I don't think anyone does anything serious with CPU inference for coding because just a tiny 10k context with a few code files in it will take at least 5-10 minutes to start generating any tokens.\n\nA decent 16GB consumer GPU is still $500-700, and you can stack 2 on most cheap motherboards; it'll give you an infinitely better experience than a hybrid CPU-GPU setup.",
          "score": 2,
          "created_utc": "2026-02-21 19:48:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6njtul",
              "author": "Sharp-University-555",
              "text": "Yeah, tested a few models today but I'm going back to commercial solutions. Perhaps if I had 24GB GPU like a 4090 and 4 times the run it would be worth it, but definitely not with my current setup. I had to try anyways :)",
              "score": 1,
              "created_utc": "2026-02-21 19:52:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6pdu2a",
              "author": "Conscious-Pen5811",
              "text": "This! As a subcontract programmer, I need to run local models as some of my contracts have restrictions on where I can post code.\n\n20-30B models is where the magic happens. A couple of 3060-12gb cards will get decent results. I did this for some time before upgrading to 2x 5060-ti 16gb cards.\n\nGUI. I use LM studio with MCP tools, sometimes server mode. DIY takes a bit of time getting setup managing tools, prompts and context.  I code in C++, im sure these models will do excellent with HTML/CSS/JS",
              "score": 1,
              "created_utc": "2026-02-22 02:13:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n6ye2",
          "author": "laurenthu",
          "text": "Also interested in the GUI part of the question - I feel like the models are \"easy\" to run locally now, but the Claude Desktop app is hard to beat - Open WebUI doesn't come close honestly?",
          "score": 1,
          "created_utc": "2026-02-21 18:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n8rsp",
              "author": "Sharp-University-555",
              "text": "I just ran Qwen2.5 7B through LM Studio using Open WebUI and itâ€™s decent but it doesnâ€™t come close to Anthropicâ€™s models and the Desktop app",
              "score": 1,
              "created_utc": "2026-02-21 18:56:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6odupq",
          "author": "Top_Oven8236",
          "text": "Install LMstudio and search models, it will automatically show offloading status before you download based on your hardware specs, but if you are low on specs search models finetuned by Unsloth they are great fit than primary models.",
          "score": 1,
          "created_utc": "2026-02-21 22:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qa8i2",
          "author": "johndeuff",
          "text": "I never found anything remotely useful at coding. It makes zero sense to adopt a much worse model at coding instead of just paying a sub for the best model.",
          "score": 1,
          "created_utc": "2026-02-22 06:08:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qr6ow",
              "author": "Sharp-University-555",
              "text": "Spot on, after trying out the few models I could fully run on my hardware, it's not up to par or close, even, to what Claude Code allows me to do, happy I tried it at least!",
              "score": 1,
              "created_utc": "2026-02-22 08:46:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6re5fj",
          "author": "llllJokerllll",
          "text": "If u wanna get more BEST results on speed, use better vLLM program",
          "score": 1,
          "created_utc": "2026-02-22 12:20:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6szp9r",
          "author": "Zestyclose-Shift710",
          "text": "I have the same 32gb ram and 8gb vram\n\nBasically your best bet are either models up to \\~8b fully in vram, or MoEs (Like GLM 4.7 Flash already recommended here a lot, and 30b a3b Qwens, Nemotron 3 nano, etc) with --cpu-moe and vram filled with context\n\nFor me 200k context quantized to Q8 fits in the gpu when running GLM 4.7 Flash with --cpu-moe for example\n\nAs for the gui I'd recommend llama.cpp with their new router mode and model ini file support, there's a nice webui you can view in the browser",
          "score": 1,
          "created_utc": "2026-02-22 17:26:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}