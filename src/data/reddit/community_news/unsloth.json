{
  "metadata": {
    "last_updated": "2026-02-04 02:59:57",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 10,
    "total_comments": 70,
    "file_size_bytes": 64001
  },
  "items": [
    {
      "id": "1quvrmn",
      "title": "Qwen3-Coder-Next is released! üíú",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/7kswd313pahg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-03 15:59:43",
      "score": 347,
      "num_comments": 56,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1quvrmn/qwen3codernext_is_released/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3d2cke",
          "author": "danielhanchen",
          "text": "MXFP4 MoE and FP8-Dynamic quants are still converting!",
          "score": 22,
          "created_utc": "2026-02-03 16:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dabs3",
              "author": "GlobalLadder9461",
              "text": "How do you rate MXFP4 vs UD Q4 K XL in terms of quality and speed ?\n\nAny chance of getting KL divergence graph. Between them also adding q4_1. These are new quants added.\n\nHopefully we get a reply",
              "score": 7,
              "created_utc": "2026-02-03 16:48:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dr16r",
                  "author": "sourceholder",
                  "text": "Related question: is \"UD Q4 K XL\" able to leverage fast Blackwell 4-bit registers or does it fallback to 8-bits?   The primary appeal of MXFP4¬†is 4-bit native acceleration.",
                  "score": 4,
                  "created_utc": "2026-02-03 18:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dtk12",
              "author": "yoracale",
              "text": "They're all up now!",
              "score": 3,
              "created_utc": "2026-02-03 18:15:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3de1qg",
              "author": "StardockEngineer",
              "text": "Let‚Äôs gooooooooooooo üëè üéâ",
              "score": 1,
              "created_utc": "2026-02-03 17:05:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d2100",
          "author": "qwen_next_gguf_when",
          "text": "This is perfection ü§©",
          "score": 6,
          "created_utc": "2026-02-03 16:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dekui",
          "author": "Effective_Head_5020",
          "text": "Thank you so much!\n\n\nI always wondered about the VRAM requirement. If I have 64gb of RAM only, will it work or will I have performance degradation?",
          "score": 4,
          "created_utc": "2026-02-03 17:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhj1f",
              "author": "yoracale",
              "text": "Absolutely you can. More VRAM will just make it faster.",
              "score": 2,
              "created_utc": "2026-02-03 17:21:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d2jpi",
          "author": "msrdatha",
          "text": "Thanks for the quick release...! \n\nWill there be an IQ4\\_NL Quant also?",
          "score": 5,
          "created_utc": "2026-02-03 16:11:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d5cew",
              "author": "yoracale",
              "text": "Yes it's converting right now!",
              "score": 2,
              "created_utc": "2026-02-03 16:24:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d3hyp",
          "author": "oldassveteran",
          "text": "Let‚Äôs gooooo!",
          "score": 4,
          "created_utc": "2026-02-03 16:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dthbw",
          "author": "brhkim",
          "text": "Okay I've never attempted to run an agentic coding LLM locally before -- seemed totally out of reach and not worth it v. paying for Claude. But this is WILD.\n\nHow do the hardware requirements scale when you're running subagents? If you have 3 separate subagents running with their own context (in addition to an orchestrator agent you're interacting with directly), how much more RAM/VRAM do you need to make things continue to run smoothly? Does that make sense? I assume tok/sec gen gets spread across parallel running subagents, and the added context per session means there's a lot more RAM usage just to context. But the model can be loaded \"centrally\", right? Or can it not run parallel sessions at all, they'd end up being sequential by query?",
          "score": 4,
          "created_utc": "2026-02-03 18:15:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dzqq3",
          "author": "flavio_geo",
          "text": "Great performance on single XTX 7900 + Ryzen 7 9700X CPU 2x48gb 6000 MT/s with Q4\\_K\\_XL\n\n29.5 tokens/s (21.5 GB VRAM used)\n\nllama.cpp config:\n\n\"-ot\", \"blk.(2\\[0-9\\]|\\[3-4\\]\\[0-9\\]).ffn\\_.\\*\\_exps.=CPU\",\n\nUsing K and V type q8\\_0 and 64k token context setup\n\nNow lets go test the model in the daily works",
          "score": 4,
          "created_utc": "2026-02-03 18:43:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fsezt",
              "author": "BigYoSpeck",
              "text": "You should try the MXFP4 and if you aren't already ROCm for the backend. I'm on a Ryzen 9 5900X but only use 8 threads as performance caps out, 64gb DDR4 and a 16gb RX 6800 XT\n\nhttps://preview.redd.it/wlrgal8nadhg1.png?width=1368&format=png&auto=webp&s=e16215d54da118c756b7dea63c5c038f405f3179",
              "score": 2,
              "created_utc": "2026-02-03 23:56:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d5pit",
          "author": "Zeranor",
          "text": "Nice, let's see how this does compared to GLM 4.7 flash and Devstral 2 Small. But quick question: WHERE can I find the MXFP4 quants? :D I only find the \"regular\" quants.",
          "score": 3,
          "created_utc": "2026-02-03 16:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d6edn",
              "author": "yoracale",
              "text": "Sorry they're still converting ahaha will let u know once theyre up\n\nEdit: they're out now: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF?show_file_info=Qwen3-Coder-Next-MXFP4_MOE.gguf",
              "score": 7,
              "created_utc": "2026-02-03 16:29:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3d6vem",
                  "author": "Zeranor",
                  "text": "ahh yes, nice, I see, sorry for being too excited ;)",
                  "score": 2,
                  "created_utc": "2026-02-03 16:31:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d6xxq",
          "author": "FullstackSensei",
          "text": "Guess it's still uploading? Q8 isn't there yet üòÇ",
          "score": 2,
          "created_utc": "2026-02-03 16:32:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dlpuy",
              "author": "yoracale",
              "text": "Should be all up now!",
              "score": 2,
              "created_utc": "2026-02-03 17:40:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3dnovw",
                  "author": "FullstackSensei",
                  "text": "Thanks! Already halfway through the download\n\nWas checking the page every couple of mins üòÇ",
                  "score": 2,
                  "created_utc": "2026-02-03 17:49:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d7nn2",
              "author": "yoracale",
              "text": "You're right lol, I just realised. Will need to wait a few more mins xD",
              "score": 1,
              "created_utc": "2026-02-03 16:35:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ewzku",
          "author": "Suitable-Program-181",
          "text": "Damn this is so gooooood!",
          "score": 2,
          "created_utc": "2026-02-03 21:18:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fz8z3",
              "author": "Motor_Ocelot_1547",
              "text": "more story pz",
              "score": 1,
              "created_utc": "2026-02-04 00:33:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d3eu2",
          "author": "PrefersAwkward",
          "text": "Can anyone recommend a good tool that can use a local LLM like this for development?¬†",
          "score": 2,
          "created_utc": "2026-02-03 16:15:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d4u53",
              "author": "yoracale",
              "text": "We made a guide for Codex and Claude Code which you can view here: [https://unsloth.ai/docs/models/qwen3-coder-next#improving-generation-speed](https://unsloth.ai/docs/models/qwen3-coder-next#improving-generation-speed)",
              "score": 9,
              "created_utc": "2026-02-03 16:22:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3dt2m4",
              "author": "synth_mania",
              "text": "Aider's community fork, \"cecli\" is a good bet.\n\n[https://github.com/dwash96/cecli](https://github.com/dwash96/cecli)",
              "score": 3,
              "created_utc": "2026-02-03 18:13:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d6q62",
              "author": "dsartori",
              "text": "Cline recommends qwen3-coder and they work really well together. This should be good too.",
              "score": 1,
              "created_utc": "2026-02-03 16:31:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ddua6",
          "author": "Fox-Lopsided",
          "text": "I wonder how fast it would be with 16 VRAM and 32DRAM",
          "score": 1,
          "created_utc": "2026-02-03 17:04:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhs4v",
              "author": "yoracale",
              "text": "10+ tokens/s",
              "score": 1,
              "created_utc": "2026-02-03 17:22:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3de3wz",
          "author": "KillerX629",
          "text": "Is there any chance of getting a QAD version? Very interested in looking at how that performs",
          "score": 1,
          "created_utc": "2026-02-03 17:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dtnfl",
              "author": "yoracale",
              "text": "QAD or QAT?",
              "score": 1,
              "created_utc": "2026-02-03 18:16:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3frk96",
                  "author": "KillerX629",
                  "text": "QAD, the one recently proposed by nvidia",
                  "score": 1,
                  "created_utc": "2026-02-03 23:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dfd7k",
          "author": "Proper_Taste_6778",
          "text": "Could you make your version of Step 3.5 Flash?",
          "score": 1,
          "created_utc": "2026-02-03 17:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhl5b",
              "author": "yoracale",
              "text": "I'm not sure if llama.cpp supports it tbh",
              "score": 2,
              "created_utc": "2026-02-03 17:21:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3diwpy",
                  "author": "Proper_Taste_6778",
                  "text": "They're working on it probablyüòÖ thx for fast answer \n\nhttps://github.com/stepfun-ai/Step-3.5-Flash/blob/main/llama.cpp/docs/step3.5-flash.md",
                  "score": 2,
                  "created_utc": "2026-02-03 17:27:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dil5r",
          "author": "alfpacino2020",
          "text": "funvionara  con 16vram y 48 ram?  en llm studio?",
          "score": 1,
          "created_utc": "2026-02-03 17:26:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dkolh",
              "author": "yoracale",
              "text": "Yes it works, just follow our guide: [https://unsloth.ai/docs/models/qwen3-coder-next](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 1,
              "created_utc": "2026-02-03 17:35:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dmeep",
          "author": "ChopSticksPlease",
          "text": "Any comparision to Devstral small 2, Qwen3 coder and GLM-4.7-Flash ?",
          "score": 1,
          "created_utc": "2026-02-03 17:43:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dv2tk",
          "author": "milkipedia",
          "text": "Nice that there is an MXFP4 quant in there! Going to give this a try soon",
          "score": 1,
          "created_utc": "2026-02-03 18:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dy30q",
          "author": "fernando782",
          "text": "Any benchmark comparison with Claude Sonnet 4.5, Claude Opus 4.5? those are the best coding models out there!\n\n",
          "score": 1,
          "created_utc": "2026-02-03 18:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3etnod",
          "author": "turbomedoqa",
          "text": "I tried the MXFP4 version and it flies at 50 t/s on 48GB VRAM. I am using it at Temperature 0.1. Is there any reason why would I run it at 1.0 for coding or instructions?",
          "score": 1,
          "created_utc": "2026-02-03 21:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3euuvh",
          "author": "TheSpicyBoi123",
          "text": "Do you have images of the spectrogram of the generated music? Would be very interresting what it actually makes. Additionally, on which data was it trained? Its not exactly a \\*garden variety\\* project to find \\~thousands of hours of genuine lossless music. Either way, solid job!",
          "score": 1,
          "created_utc": "2026-02-03 21:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f1kxl",
          "author": "Skt_97",
          "text": "Has anyone had a chance to compare it with a 3.5 step flash? It would be interesting to see which of the two is better.",
          "score": 1,
          "created_utc": "2026-02-03 21:39:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fa3do",
          "author": "Status_Contest39",
          "text": "The open-source model supports the first echelon of speed, and this operation is so great that it takes off directly!",
          "score": 1,
          "created_utc": "2026-02-03 22:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fxlfx",
          "author": "LittleBlueLaboratory",
          "text": "Anyone else getting this error when¬† trying to use Q6_K_XL in llama.cpp?\n\n\nLlama_model_load: error loading model: missing tensor 'blk.0.ssm_in.weight'\n\n\nI have downloaded the model twice already thinking I just got a corrupted download or something but it keeps happening.",
          "score": 1,
          "created_utc": "2026-02-04 00:24:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g4ljy",
          "author": "DaringNinja",
          "text": "I am definitely doing something wrong seeing everyone else's token numbers. Using a 3090 and 128gb RAM only seeing 7 tokens/s with MXFP4 on LM Studio.",
          "score": 1,
          "created_utc": "2026-02-04 01:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dwej6",
          "author": "No_Afternoon_4260",
          "text": "How benchmaxxed is it?",
          "score": 1,
          "created_utc": "2026-02-03 18:28:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d0tei",
          "author": "Otherwise_Wave9374",
          "text": "That 256K context + \"agentic coding\" angle is really interesting, local agent setups get way more usable once you can keep a lot of repo + docs in context without constant chunking. Have you noticed any gotchas with tool calling or long horizon tasks (like refactors) vs quick one shot codegen?\n\nIm always looking for notes on building coding agents and evaling them, a few posts Ive bookmarked are here: https://www.agentixlabs.com/blog/",
          "score": -6,
          "created_utc": "2026-02-03 16:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dd177",
              "author": "pokemonplayer2001",
              "text": "You‚Äôre such a shill, FO.",
              "score": 7,
              "created_utc": "2026-02-03 17:00:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3daw4d",
          "author": "getmevodka",
          "text": "How big is that ? I have 96gb vram available üòäüòÖüëç",
          "score": -4,
          "created_utc": "2026-02-03 16:50:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dg7gg",
              "author": "some_user_2021",
              "text": "The answer is on the post",
              "score": 2,
              "created_utc": "2026-02-03 17:15:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dgb02",
                  "author": "getmevodka",
                  "text": "Yes it is indeed. Thanks",
                  "score": 3,
                  "created_utc": "2026-02-03 17:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpbmrt",
      "title": "You can now run Kimi K2.5 locally!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/nwp8ammpf3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 13:40:33",
      "score": 250,
      "num_comments": 22,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qpbmrt/you_can_now_run_kimi_k25_locally/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o281v90",
          "author": "m98789",
          "text": "On a single H100 (80GB) VM with 256 GB of ram and 1TB of ssd and plenty of cpu cores, how fast in tokens / sec can we expect?",
          "score": 12,
          "created_utc": "2026-01-28 14:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284n8h",
              "author": "hudimudi",
              "text": "That would be very interesting to know",
              "score": 6,
              "created_utc": "2026-01-28 15:07:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o284ovk",
              "author": "yoracale",
              "text": "You can run the 2bit one possible with that. Maybe like 20 tokens?",
              "score": 7,
              "created_utc": "2026-01-28 15:07:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o28c5jm",
                  "author": "m98789",
                  "text": "So if this set up works, it‚Äôs about $3/hour on demand as an instance in a major cloud. So say we get Claude Opus level for $3/hour. And at 20 tokens a second, that‚Äôs 72,000 output tokens per hour. using the api for opus for that amount of tokens costs about $2. If using a reserved instance of the cloud vm, the cost goes to about $2/hour. So it‚Äôs a wash on cost effectiveness. But on privacy and control, it‚Äôs a win. Assuming quality is effectively the same.",
                  "score": 8,
                  "created_utc": "2026-01-28 15:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28ew1j",
          "author": "illkeepthatinmind",
          "text": "I mean.... \\*I\\* can't...",
          "score": 4,
          "created_utc": "2026-01-28 15:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2j647h",
          "author": "BeginningReveal2620",
          "text": "Cool",
          "score": 3,
          "created_utc": "2026-01-30 02:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28h6dn",
          "author": "joninco",
          "text": "Did you apply any unsloth fixes? I ran the K2.5 from moonshot's hf page and was unimpressed with the coding results for creating a single page html tetris app. I used 1.0 temp, 0.95 topp.",
          "score": 2,
          "created_utc": "2026-01-28 16:03:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d0d8h",
              "author": "yoracale",
              "text": "There's no unsloth fixes for this model :(. Have you tried their API?",
              "score": 2,
              "created_utc": "2026-01-29 05:30:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2evu8w",
                  "author": "joninco",
                  "text": "Only locally with sglang. I haven't tried their API yet, but I will give it a shot and compare with my results to see if they are consistent!\n\nFor SGLANG on RTX PRO 6000s, disabling DEEPGEMM and ensuring temp 1.0, top p 0.95 and min p 0.01 made a material difference. \n\nMy only problem now is that it's materially slowing down as context grows -- assuming triton attn isn't optimal for this model.",
                  "score": 3,
                  "created_utc": "2026-01-29 14:11:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28lhoq",
          "author": "nomorebuttsplz",
          "text": "Hi -- why is the K.2.5 Q 3 XL UD quite a bit larger than the same quant for K2 thinking?",
          "score": 2,
          "created_utc": "2026-01-28 16:22:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28q2mv",
          "author": "Nooddlleee",
          "text": "Does anyone test the code quality? I am curious how it performs on the complex tasks and big projects",
          "score": 2,
          "created_utc": "2026-01-28 16:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28sxci",
          "author": "davidl002",
          "text": "Cannot imagine running it locally anytime soon but thanks for the work!\n\nHope in 5 years hardware can be affordable enough to run this locally cheaply.",
          "score": 2,
          "created_utc": "2026-01-28 16:54:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xywgu",
              "author": "Certain_Cricket7958",
              "text": "I‚Äôm afraid in five years, it won‚Äôt be as interesting",
              "score": 1,
              "created_utc": "2026-02-01 09:17:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o29lpma",
          "author": "emperorofrome13",
          "text": "That is the flux 2 dev of coding ai. Way too big to be useful.",
          "score": 2,
          "created_utc": "2026-01-28 18:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28p0fc",
          "author": "maxtheman",
          "text": "Would be VERY interested in the vision support, but already awesome work.",
          "score": 1,
          "created_utc": "2026-01-28 16:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r6ij8",
          "author": "Significant-Taro409",
          "text": "Is there any info on fine-tuning available",
          "score": 1,
          "created_utc": "2026-01-31 07:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2skoz9",
              "author": "yoracale",
              "text": "The model is gigantic. You'll need at least 8x B200's to finetune it",
              "score": 2,
              "created_utc": "2026-01-31 14:27:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xgec7",
          "author": "No-Intention-5521",
          "text": "Huhh i only have one A100 .... seems i have to buy apissss",
          "score": 1,
          "created_utc": "2026-02-01 06:29:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xtgny",
              "author": "yoracale",
              "text": "How much RAM do you have? If it adds to 240gb then itll work you dont need api",
              "score": 1,
              "created_utc": "2026-02-01 08:26:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o32gr45",
          "author": "texasdude11",
          "text": "I could not get it to do function calling. Any sample command for that?",
          "score": 1,
          "created_utc": "2026-02-02 00:18:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqc06x",
      "title": "How to Run Local LLMs with Claude Code & OpenAI Codex!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/6mhzmpzd6bgg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-29 15:42:38",
      "score": 125,
      "num_comments": 27,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qqc06x/how_to_run_local_llms_with_claude_code_openai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2fnep4",
          "author": "__Maximum__",
          "text": "Fine-tune?",
          "score": 3,
          "created_utc": "2026-01-29 16:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2frhwe",
              "author": "yoracale",
              "text": "Yep fine-tune! We use glm flash to autonomously fine-tune an LLM with unsloth",
              "score": 3,
              "created_utc": "2026-01-29 16:38:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2fr3lz",
              "author": "moonflowerseed",
              "text": "On Mac/Apple Silicon?",
              "score": 1,
              "created_utc": "2026-01-29 16:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2fre76",
                  "author": "yoracale",
                  "text": "We're working on Mac support for real. Optimizations are done, only thing next is checking, benchmarking and Integra tion",
                  "score": 9,
                  "created_utc": "2026-01-29 16:38:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2j0zdt",
          "author": "PixelatedCaffeine",
          "text": "Is there a way to change the Claude Code limit to match the model‚Äôs limit? It always seems to default to 200k, and I would love to use the auto compact feature based on that",
          "score": 2,
          "created_utc": "2026-01-30 02:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g6qtr",
          "author": "ethereal_intellect",
          "text": "They lose web search capability when linked to local models right?",
          "score": 1,
          "created_utc": "2026-01-29 17:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kd5p2",
              "author": "admajic",
              "text": "Not is you ask it to build you a mcp search tool.",
              "score": 1,
              "created_utc": "2026-01-30 07:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gahui",
          "author": "Glittering-Call8746",
          "text": "Prompt \"You can only work in the cwd project/. Do not search for CLAUDE.md - this is it. Install Unsloth via a virtual environment via uv. See https://unsloth.ai/docs/get-started/install/pip-install on how (get it and read). Then do a simple Unsloth finetuning run described in https://github.com/unslothai/unsloth. You have access to 1 GPU.\" What's the model it's finetuning..",
          "score": 1,
          "created_utc": "2026-01-29 18:04:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrpqc",
              "author": "yoracale",
              "text": "Llama most likely",
              "score": 1,
              "created_utc": "2026-01-29 22:14:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gu6cv",
          "author": "creminology",
          "text": "Has Anthropic ever given any indication that they view this as a breach of terms of service? Asking because they have come down on hard on using Claude Code subscriptions in other environments, although this is doing the reverse.",
          "score": 1,
          "created_utc": "2026-01-29 19:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrmg0",
              "author": "yoracale",
              "text": "Oh no, they allow this because Claude Code was meant to be used locally!",
              "score": 3,
              "created_utc": "2026-01-29 22:13:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2luumi",
              "author": "Otherwise-Way1316",
              "text": "They don‚Äôt like their models being used in other platforms, like OpenCode.\n\nAll indications are that they are ok with Claude Code being used with other models.",
              "score": 2,
              "created_utc": "2026-01-30 14:20:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kpq0d",
          "author": "No-Weird-7389",
          "text": "Still waiting for nvfp4",
          "score": 1,
          "created_utc": "2026-01-30 09:34:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l4ryc",
              "author": "yoracale",
              "text": "We're working on it! :) Might not be for this model but for future ones",
              "score": 1,
              "created_utc": "2026-01-30 11:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l3ars",
          "author": "SatoshiNotMe",
          "text": "Last I checked, running glm-4.7-flash with CC on my M1 Pro Max 64GB MacBook via llama-server got me an abysmal 3 tok/s, for less than the 20 tok/s I got with Qwen3-30B-A3B. I use this setup to hook up CC with local models:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nCurious what llama-server settings you recommend to get good performance with GLM-4.7-flash",
          "score": 1,
          "created_utc": "2026-01-30 11:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2x6voc",
              "author": "yoracale",
              "text": "When was the last time you tried it? A week ago llamacpp was updated to imrpove speed a lot for it",
              "score": 1,
              "created_utc": "2026-02-01 05:15:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oegdi",
          "author": "stuckinmotion",
          "text": "Does this work for anyone? I followed the steps, set ANTHROPIC\\_BASE\\_URL to my llama-server instance, but I'm getting \"Missing API key\"\n\nedit: Ok so exporting ANTHROPIC\\_API\\_KEY=sk-1234 got it working. Maybe the guide can be updated",
          "score": 1,
          "created_utc": "2026-01-30 21:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pq0sy",
              "author": "yoracale",
              "text": "Ooo ok interesting we'll update it in our guide then thanks for the feedback",
              "score": 1,
              "created_utc": "2026-01-31 01:30:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2x752j",
              "author": "yoracale",
              "text": "We just added it to our guide: [https://unsloth.ai/docs/basics/claude-codex](https://unsloth.ai/docs/basics/claude-codex)\n\nThanks so much for your feedback!",
              "score": 1,
              "created_utc": "2026-02-01 05:17:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xb6bx",
                  "author": "stuckinmotion",
                  "text": "Hey nice! Thanks for the work. It's been interesting playing with Claude code locally though it makes it obvious how much worse it is without their models",
                  "score": 1,
                  "created_utc": "2026-02-01 05:47:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2vuwel",
          "author": "JonatasLaw",
          "text": "Can I run it in a rtx 3090 + 64gb RAM?",
          "score": 1,
          "created_utc": "2026-02-01 00:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2x6zfd",
              "author": "yoracale",
              "text": "Yes ofc, it'll be fast for you. You can even run the 8-bit one",
              "score": 1,
              "created_utc": "2026-02-01 05:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fnjti",
          "author": "toreobsidian",
          "text": "This is awesome. I'll Test this with Just a dataset I'm currently preparing that Features content of a famous german political figure. Too bad I have so little time for this nonsens Project but this should be a nice boost üòÖ",
          "score": 1,
          "created_utc": "2026-01-29 16:21:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt7q4c",
      "title": "I bullied my dual 3060s into ruinning GLM-4.7-Flash  500+ T/s @ 70k Context on a Ryzen 2500 Potato. (Two Configs: \"Daily Driver\" vs. \"The Diesel Factory\")",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/gallery/1qt7q4c",
      "author": "MohammedGomaa",
      "created_utc": "2026-02-01 19:15:29",
      "score": 58,
      "num_comments": 41,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qt7q4c/i_bullied_my_dual_3060s_into_ruinning_glm47flash/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o310uy4",
          "author": "mukz_mckz",
          "text": "Please stop writing posts with AI.",
          "score": 21,
          "created_utc": "2026-02-01 19:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o312ok4",
              "author": "MohammedGomaa",
              "text": "i am not a native english speaker , i used ai to polish it",
              "score": 4,
              "created_utc": "2026-02-01 20:03:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31gwup",
                  "author": "--Spaci--",
                  "text": "Id rather struggle to read your paragraph, than have to read an AI's attempt at conveying information",
                  "score": 15,
                  "created_utc": "2026-02-01 21:13:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o34s89y",
              "author": "marko_mavecki",
              "text": "Huh? Maybe he can't? I am here for information. Not for formatting or the beauty of words. Besides, in few months you will no longer be able to differentiate between human and machine. Will it then be acceptable for you?",
              "score": 1,
              "created_utc": "2026-02-02 10:19:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3277zm",
          "author": "i_wayyy_over_think",
          "text": "Thanks for the detailed run configs. What do you use to actually use 64 concurrent quest at once besides running a benchmark? Can moltbot use that many concurrent requests effectively to get actual useful work done? I‚Äôve not tried it yet.",
          "score": 3,
          "created_utc": "2026-02-01 23:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o329d4z",
              "author": "MohammedGomaa",
              "text": "A swarm of AI agents plus multiple instance open code and the other coding agents",
              "score": 1,
              "created_utc": "2026-02-01 23:38:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o348hst",
          "author": "Previous_Nature_5319",
          "text": "good job, I think it will be useful for many people. thank you!",
          "score": 2,
          "created_utc": "2026-02-02 07:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37j6xu",
          "author": "Shivam_R_A",
          "text": "This is just awesome man!",
          "score": 2,
          "created_utc": "2026-02-02 19:25:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o316ecp",
          "author": "heaven00",
          "text": "Interesting stuff will have to try out",
          "score": 3,
          "created_utc": "2026-02-01 20:21:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31mixg",
          "author": "AbheekG",
          "text": "How do you handle the cache going stale? Doesn‚Äôt that happen fairly frequently with a codebase?",
          "score": 1,
          "created_utc": "2026-02-01 21:40:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31nnv4",
              "author": "MohammedGomaa",
              "text": "Sglang finds previously computed tokens and reuses it , very useful for ai agents  play books , system prompts and skills, i ran scheduled script    that removes chunks not accessed in the last 7 days",
              "score": 3,
              "created_utc": "2026-02-01 21:45:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31nsth",
                  "author": "AbheekG",
                  "text": "Very cool, thanks!",
                  "score": 2,
                  "created_utc": "2026-02-01 21:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33h8sb",
          "author": "Soft_Syllabub_3772",
          "text": "Can modify to use with rtx3090 x 2 and 192gb ram and 1tb nvme?",
          "score": 1,
          "created_utc": "2026-02-02 03:47:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33hp44",
              "author": "MohammedGomaa",
              "text": "Go fo it , the sky is the limit",
              "score": 1,
              "created_utc": "2026-02-02 03:50:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34ef1f",
          "author": "Aggravating_Bee3757",
          "text": "what model did you use? can i use it in my single 5060 ti 16/16?",
          "score": 1,
          "created_utc": "2026-02-02 08:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34lc83",
              "author": "MohammedGomaa",
              "text": "**GLM-4.7-Flash**¬†(the¬†`QuantTrio-AWQ`¬†flavor) , i dont think so , not without c\\[u offloading tanking performance",
              "score": 2,
              "created_utc": "2026-02-02 09:12:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34u3r2",
          "author": "jannemansonh",
          "text": "impressive setup for agent swarms... curious though - for production agent workflows, ended up using needle app since you just describe what you want vs configuring cuda graphs and cache layers. kept the self-hosted setup for experimenting but way easier when agents need to understand code/docs",
          "score": 1,
          "created_utc": "2026-02-02 10:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o355g99",
              "author": "MohammedGomaa",
              "text": "sorry , i dont realy get what you are asking about , BTW setting \n\n    --cuda-graph-bs 4 16 32  # makes sure that single requist get 20-70 t/s , depending on context cach , ie single  agent get 20-70 t/s depending on caching context and concurancy , with 450 + t/s on max concurancy",
              "score": 1,
              "created_utc": "2026-02-02 12:12:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38kj0z",
          "author": "SectionCrazy5107",
          "text": "i have not been able to get vllm to work with glm 4.7 flash on my v100+3080 so far, does sglang work out of the box? or any specific cuda + python + .. required?",
          "score": 1,
          "created_utc": "2026-02-02 22:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b3u9r",
              "author": "MohammedGomaa",
              "text": "uv pip install sglang==0.3.2.dev9039+pr-17247.g90c446848 --extra-index-url [https://sgl-project.github.io/whl/pr/](https://sgl-project.github.io/whl/pr/)\n\nuv pip install git+https://github.com/huggingface/transformers.git@76732b4e7120808ff989edbd16401f61fa6a0afa",
              "score": 1,
              "created_utc": "2026-02-03 08:07:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31gezn",
          "author": "alhinai_03",
          "text": "no one will bother reading AI produced text.",
          "score": 2,
          "created_utc": "2026-02-01 21:10:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32d7hy",
              "author": "MohammedGomaa",
              "text": "You know you are on an AI focused subreddit almost 99% of AI mentioned here argued towards Text generation",
              "score": 7,
              "created_utc": "2026-02-01 23:59:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32gt6c",
                  "author": "alhinai_03",
                  "text": "Brother, this doesn't mean I'm okay with reading AI generated text. I get it if it's not your first language but I'd rather read human written content no matter how bad the grammar is. I'm sure you have good intentions but people are generally getting tired of it.",
                  "score": 2,
                  "created_utc": "2026-02-02 00:19:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qr4i8a",
      "title": "Experimental DeepSeek-V3.2 Dynamic GGUFs",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/",
      "author": "danielhanchen",
      "created_utc": "2026-01-30 12:46:49",
      "score": 30,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "We made some experimental DeepSeek-V3.2 GGUFs for those interested! [https://huggingface.co/unsloth/DeepSeek-V3.2-GGUF](https://huggingface.co/unsloth/DeepSeek-V3.2-GGUF)\n\nThey **don't need any llama.cpp updates or special forks** \\- these should work in llama.cpp, LM Studio, Ollama (UD-TQ1\\_0).\n\nDeepSeek Sparse Attention (DSA) is disabled for now, and this mostly acts like a normal DeepSeek V3.1 model. However, we had to cook up the chat\\_template.jinja from scratch.\n\nUse [https://unsloth.ai/docs/models/tutorials/deepseek-v3.1-how-to-run-locally](https://unsloth.ai/docs/models/tutorials/deepseek-v3.1-how-to-run-locally) and replace \"DeepSeek-V3.1\" with \"DeepSeek-V3.2\"\n\nAn example Flappy Bird game in HTML with the UD-Q2\\_K\\_XL quant:\n\nhttps://preview.redd.it/a5d7sugrfhgg1.png?width=1547&format=png&auto=webp&s=26f2c96289c84fe8cace79c30a633f7a8e3b5a62\n\nLet us know how it goes!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qruejs",
      "title": "cerebras MiniMax M2.1 REAP gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qruejs/cerebras_minimax_m21_reap_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-31 06:03:05",
      "score": 15,
      "num_comments": 6,
      "upvote_ratio": 0.94,
      "text": "[https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B](https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B)\n\nmradermacher GGUF dont work, only Unsloth has best chat template fixes ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qruejs/cerebras_minimax_m21_reap_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2qzkik",
          "author": "StardockEngineer",
          "text": "I use the mrader reap daily.  Didn‚Äôt do anything special to make it work.",
          "score": 2,
          "created_utc": "2026-01-31 06:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ric8p",
              "author": "LegacyRemaster",
              "text": "me too.",
              "score": 1,
              "created_utc": "2026-01-31 09:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2u41ym",
                  "author": "ClimateBoss",
                  "text": "M2 not M2.1 dat came out like yday bruh",
                  "score": 1,
                  "created_utc": "2026-01-31 18:58:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qs8gcp",
      "title": "Should I use UnslothTrainer or SFTTrainer for Continued Pre-training (Raw Text) to create a LoRA for later merging?",
      "subreddit": "unsloth",
      "url": "https://arxiv.org/abs/2507.18294",
      "author": "choco132134",
      "created_utc": "2026-01-31 17:28:47",
      "score": 13,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qs8gcp/should_i_use_unslothtrainer_or_sfttrainer_for/",
      "domain": "arxiv.org",
      "is_self": false,
      "comments": [
        {
          "id": "o2ujqvz",
          "author": "Educational_Rent1059",
          "text": "UnslothTrainer is the same thing with additional support for these internal params which are needed for continued pre training  args = UnslothTrainingArguments(\n        ....\n        learning_rate = 5e-5,\n        embedding_learning_rate = 5e-6, # 2-10x smaller than learning_rate",
          "score": 1,
          "created_utc": "2026-01-31 20:13:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xti8b",
          "author": "yoracale",
          "text": "Use unsloth trainer if it works as it has continued pretraining benefits. If it doesn't work then use SFTTrainer",
          "score": 1,
          "created_utc": "2026-02-01 08:27:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o323ker",
          "author": "de4dee",
          "text": "I do CPT on Instruct model, using UnslothTrainer and it is looking ok..",
          "score": 1,
          "created_utc": "2026-02-01 23:06:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq0m1x",
      "title": "Guidance Needed: GPT-OSS 20B Fine-Tuning with Unsloth ‚Üí GGUF ‚Üí Ollama ‚Üí Triton (vLLM / TensorRT-LLM)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qq0m1x/guidance_needed_gptoss_20b_finetuning_with/",
      "author": "Double_Tourist3600",
      "created_utc": "2026-01-29 06:07:11",
      "score": 12,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "I am currently fine-tuning the **GPT-OSS 20B** model using **Unsloth** with **HuggingFace TRL (SFTTrainer)**.\n\n**Long-term goal**\n\n* Serve the model in production using **Triton** with either **vLLM** or **TensorRT-LLM** as the backend\n* **Short-term / initial deployment** using **Ollama (GGUF)**\n\n**Current challenge**  \nGPT-OSS uses a **Harmony-style chat template**, which includes:\n\n* `developer` role\n* Explicit EOS handling\n* `thinking` / `analysis` channels\n* Tool / function calling structure\n\nWhen converting the fine-tuned model to **GGUF** and deploying it in **Ollama** using the **default GPT-OSS Modelfile**, I am running into ambiguity around:\n\n1. Whether the **default Jinja chat template** provided by GPT-OSS should be **modified** for Ollama compatibility\n2. How to correctly handle:\n   * EOS token behavior\n   * Internal reasoning / analysis channels\n   * Developer role alignment\n3. How to do this **without degrading the model‚Äôs default performance or alignment**\n\n**Constraints / Intent**\n\n* I already have training data prepared strictly in **system / user / assistant** format\n* I want to:\n   * Preserve GPT-OSS‚Äôs native behavior as much as possible\n   * Perform **accurate, non-destructive fine-tuning**\n   * Avoid hacks that work short-term but break compatibility with **vLLM / TensorRT-LLM** later\n\n**What I‚Äôm looking for**\n\n* Has anyone successfully:\n   * Fine-tuned GPT-OSS\n   * Converted it to GGUF\n   * Deployed it with **Ollama**\n   * While preserving the Harmony template behavior?\n* If yes:\n   * Did you modify the **chat template / Modelfile**?\n   * How did you handle EOS + reasoning channels?\n   * Any pitfalls to avoid to keep it production-ready for Triton later?\n\nAny concrete guidance, references, or proven setups would be extremely helpful.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qq0m1x/guidance_needed_gptoss_20b_finetuning_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2d9qb5",
          "author": "max6296",
          "text": "https://github.com/openai/harmony",
          "score": 1,
          "created_utc": "2026-01-29 06:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2df7hg",
              "author": "Double_Tourist3600",
              "text": "can you please elaborate, as per my understanding you are suggesting to use this library to prepare data my question is then which chat template to use?",
              "score": 1,
              "created_utc": "2026-01-29 07:30:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2lclhf",
                  "author": "danielhanchen",
                  "text": "For Ollama maybe use the modelfile here: https://ollama.com/library/gpt-oss:20b/blobs/fa6710a93d78",
                  "score": 1,
                  "created_utc": "2026-01-30 12:39:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2j85ai",
          "author": "Phaelon74",
          "text": "I would skip GGUF entirely then, and roll directly from full model, to INT4/8/FP4/8 as vllm and TensorRT do absolutely HORRIBLE at GGUF serving.  Train your base model, and skip GGUF quanting.",
          "score": 1,
          "created_utc": "2026-01-30 02:56:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o398uja",
          "author": "arman-d0e",
          "text": "Yea my solution was to ignore gguf exports and focus on just merging in 16bit. Then you can use llama.cpp (`convert_hf_to_gguf.py` and `llama-quantize`) to convert afterwards.\n\nAs for the mention of harmony I did find it more reliable to use the OpenAI harmony library to encode all my conversations to harmony perfectly and had several verification checks along the way to verifying my formatting and masking before training. Best of luck!",
          "score": 1,
          "created_utc": "2026-02-03 00:32:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs6esv",
      "title": "Are Usnloth planning to provide a notebook for the Ministral 3 text?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qs6esv/are_usnloth_planning_to_provide_a_notebook_for/",
      "author": "kompania",
      "created_utc": "2026-01-31 16:13:01",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.83,
      "text": "I tried tuning the Ministral 3 3B model by swapping the training sets provided by Unsloth notebook with my own. I tried tuning the VL and Sudoku versions using the Alpaca dataset.\n\nUnfortunately, I was unsuccessful. Both Gemini and ChatGPT claim that this is currently impossible due to the lack of MistralAI support.\n\nDoes Unsloth plan to provide notebooks for Colab for tuning Ministral 3 using text?\n\nI also want to thank the people behind this system/library. I'm 63, and thanks to their extensive guides, I've made some very satisfying tweaks for Gemma 3. Thank you, Unsloth, for your work!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qs6esv/are_usnloth_planning_to_provide_a_notebook_for/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2w5qy7",
          "author": "yoracale",
          "text": "You need to disable the vision component, it is quite simple. Use the same notebook and just follow the vision finetuning guide: https://unsloth.ai/docs/basics/vision-fine-tuning\n\nE.g. turn fine-tune vision layers = off in the guide",
          "score": 1,
          "created_utc": "2026-02-01 01:19:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y15vy",
              "author": "kompania",
              "text": "Unfortunately, this solution doesn't work.\n\nI tried it myself, but unfortunately, despite entering the correct name, the model doesn't download.\n\nI get the error:\n\nRuntimeError: Unsloth: No config file found \"unsloth/Ministral-3-3B-Instruct-2512-bnb-4bit\"\n\nAnalysis of the configuration files (config.json) reveals that the model architecture is defined as Mistral3ForConditionalGeneration. This is a significant departure from the standard MistralForCausalLM or LlamaForCausalLM architectures, which are natively and seamlessly supported by the FastLanguageModel class in the Unsloth library. The Mistral3ForConditionalGeneration class implies the presence of visual projection layers and mechanisms for combining text and visual embeddings, which complicates the process of initializing the model as a pure text generator.\n\nI tried to bypass this, but encountered another problem. The main challenge is that the FastLanguageModel expects an architecture that follows the Causal LM paradigm. Forcing this class to load Ministral-3-3B requires precise dependency management, including installing the unsloth\\_zoo module, which contains definitions for non-standard architectures. The absence of this component leads to import errors or an inability to find the configuration file, which is one of the most frequently reported issues in GitHub repositories related to this model.\n\nI have unsloth\\_zoo, but it doesn't help.\n\nI tried solving the problem with Gemini Pro and ChatGPT, but they also failed.\n\nI'm at a loss here. I'm not an engineer and I can't progress any further.",
              "score": 2,
              "created_utc": "2026-02-01 09:38:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o397yxo",
                  "author": "arman-d0e",
                  "text": "This is a transformers version issue with the dependencies. Add a line after the provided dependencies cell to install the ministral transformers fork mentioned in their model README. Had the same issue and solved it like this",
                  "score": 1,
                  "created_utc": "2026-02-03 00:27:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qurswa",
      "title": "Vllm is not supported in Asus GX10 machine",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qurswa/vllm_is_not_supported_in_asus_gx10_machine/",
      "author": "maayon",
      "created_utc": "2026-02-03 13:23:20",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "The SFT notebooks run properly in ASUS gx10 but when i try to run GRPO the vLLM installation corrupts the venv installations.\n\nIs there anyway to run GRPO notebooks without vllm ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qurswa/vllm_is_not_supported_in_asus_gx10_machine/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o3c9b5p",
          "author": "Final-Rush759",
          "text": "Pip install accelerator, I think.",
          "score": 2,
          "created_utc": "2026-02-03 13:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ccir8",
              "author": "eleqtriq",
              "text": "This sounds right",
              "score": 1,
              "created_utc": "2026-02-03 14:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3c7knx",
          "author": "eleqtriq",
          "text": "Can you be more specific?  What notebook?  What is the error?",
          "score": 1,
          "created_utc": "2026-02-03 13:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3c831y",
              "author": "maayon",
              "text": "[https://github.com/unslothai/unsloth/issues/3976](https://github.com/unslothai/unsloth/issues/3976)",
              "score": 1,
              "created_utc": "2026-02-03 13:37:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3c8c1j",
                  "author": "eleqtriq",
                  "text": "You‚Äôre in a container?  What container did you use?",
                  "score": 1,
                  "created_utc": "2026-02-03 13:38:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}