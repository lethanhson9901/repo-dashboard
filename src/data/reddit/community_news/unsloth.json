{
  "metadata": {
    "last_updated": "2026-02-20 03:06:13",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 17,
    "total_comments": 105,
    "file_size_bytes": 113750
  },
  "items": [
    {
      "id": "1r6564b",
      "title": "Qwen3.5 is out now!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/xtgnyvb2stjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-16 09:28:34",
      "score": 341,
      "num_comments": 44,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r6564b/qwen35_is_out_now/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5nqycb",
          "author": "Zestyclose-Shift710",
          "text": "Cant wait to run this on my 8gb vram 32gb ram system",
          "score": 34,
          "created_utc": "2026-02-16 10:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nrau1",
              "author": "yoracale",
              "text": "They're apparently releasing smaller models soon, so you should wait for that.",
              "score": 17,
              "created_utc": "2026-02-16 10:09:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nrvf8",
                  "author": "Zestyclose-Shift710",
                  "text": "Oh that's exciting\n\nRunning sparse midsized MoEs like 30b a3b with experts on CPU and the rest on GPU is so nice on normal hardware",
                  "score": 7,
                  "created_utc": "2026-02-16 10:14:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q5fpo",
              "author": "Neither-Phone-7264",
              "text": "i mean, with enough storage you can. won't be fun, but it's doable.",
              "score": 1,
              "created_utc": "2026-02-16 18:24:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5tmb44",
                  "author": "Zestyclose-Shift710",
                  "text": "It'll be like that computer from the hitchhikers guide to the Galaxy¬†\n\n\nGonna take a billion years to output 42",
                  "score": 2,
                  "created_utc": "2026-02-17 06:14:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nu76s",
          "author": "joninco",
          "text": "Whelp.. I was gonna sleep in, but now gotta get up.",
          "score": 7,
          "created_utc": "2026-02-16 10:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oj9sa",
          "author": "jikilan_",
          "text": "Ok I am downloading more rams now. Will report back the token/s on the software ram",
          "score": 3,
          "created_utc": "2026-02-16 13:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oav1p",
          "author": "getmevodka",
          "text": "4 bit mxpf it is then ü§©üòç",
          "score": 3,
          "created_utc": "2026-02-16 12:50:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oflri",
          "author": "EbbNorth7735",
          "text": "This is really close in size MOE to Llama 3 Mavrick if I recall correctly. Interesting how that is. META should have continued working on it.",
          "score": 3,
          "created_utc": "2026-02-16 13:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q60fk",
          "author": "RandomUserRU123",
          "text": "Hoping for Qwen3.5-VL-32B-Instruct soon",
          "score": 3,
          "created_utc": "2026-02-16 18:27:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zw8vn",
              "author": "Resident_Suit_9916",
              "text": "They will not be releasing VL models",
              "score": 1,
              "created_utc": "2026-02-18 04:22:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ouoea",
          "author": "xor_2",
          "text": "I am 70GB short to fit this model in my ram... and by this model I mean Q2 version.\n\nOtherwise Qwen3-Max was my favorite non-local model so its nice to get an upgrade.\n\nWaiting for 35B version to release. It will be slow but heck, maybe it will be smarter than GLM 4.7 Flash",
          "score": 2,
          "created_utc": "2026-02-16 14:44:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t2xj2",
              "author": "yoracale",
              "text": "You should try MiniMax-2.5 instead: [https://unsloth.ai/docs/models/minimax-2.5](https://unsloth.ai/docs/models/minimax-2.5)",
              "score": 2,
              "created_utc": "2026-02-17 03:51:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v4ws2",
                  "author": "xor_2",
                  "text": "Thanks for the suggestion but I only have only 64GB RAM + 24GB VRAM and on computer I actually use and have a lot of RAM used at all times so MinMax is too big to fit.",
                  "score": 1,
                  "created_utc": "2026-02-17 13:45:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q0acp",
          "author": "txgsync",
          "text": "YAY! Unsloth is on the MXFP4 train now! WooWoo!",
          "score": 2,
          "created_utc": "2026-02-16 18:01:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t2tl1",
              "author": "yoracale",
              "text": "Let us know if it's faster for you :)",
              "score": 1,
              "created_utc": "2026-02-17 03:51:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tu0yi",
                  "author": "txgsync",
                  "text": "Alas. I  slumming it on a M4 Max 128GB. Gotta REAP it before I can play.",
                  "score": 1,
                  "created_utc": "2026-02-17 07:22:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5xo6yi",
                  "author": "SquirrelEStuff",
                  "text": "I keep trying to install this in LM Studio on Mac Studio M3 Ultra with 256GB but keep getting loading errors. How do you recommend I install it?\n\nTIA!",
                  "score": 1,
                  "created_utc": "2026-02-17 21:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rieqs",
          "author": "ChadThunderDownUnder",
          "text": "Can‚Äôt wait to run this on my dual 5090 + 512GB DDR5 setup",
          "score": 2,
          "created_utc": "2026-02-16 22:21:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t2usj",
              "author": "yoracale",
              "text": "You can run full Q8 very well with that setup",
              "score": 1,
              "created_utc": "2026-02-17 03:51:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tcw3k",
                  "author": "ChadThunderDownUnder",
                  "text": "Yep. Looking forward to seeing what this model can do. It‚Äôs the most promising so far. GPT-OSS-120B was solid but too sanitized for my liking.",
                  "score": 2,
                  "created_utc": "2026-02-17 05:01:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60qkcy",
          "author": "pppreddit",
          "text": "If only we had prompt caching locally...",
          "score": 2,
          "created_utc": "2026-02-18 08:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sqzv5",
          "author": "ButCaptainThatsMYRum",
          "text": "Ok but will it think and talk like a moody teenager? The previous qwen were fairly smart but sometimes insufferable.",
          "score": 1,
          "created_utc": "2026-02-17 02:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5us90w",
          "author": "depressedclassical",
          "text": "How well does it perform on 16GB VRAM+256GB RAM in terms of t/s (or s/t)?",
          "score": 1,
          "created_utc": "2026-02-17 12:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yif9d",
              "author": "greatwilt",
              "text": "the guide says 25 tps with 24GB of VRAM and 256GB of RAM (It doesnt specify unified, but maybe it needs to be) but I cant seem to get more than 10 tps with 4x 48GB GPUS and 512GB of system RAM.\n\nedit: got up to 15 tps with the 4bit quant and offloading, and 60tps with the 2bit quant",
              "score": 2,
              "created_utc": "2026-02-17 23:43:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o61tj1u",
          "author": "PsychologicalDemand0",
          "text": "Will any smaller versions be also released at sometime ?",
          "score": 1,
          "created_utc": "2026-02-18 13:33:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61uyas",
              "author": "yoracale",
              "text": "Hopefully, apparently according to leaks they are going to",
              "score": 1,
              "created_utc": "2026-02-18 13:41:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o62nycz",
          "author": "gracchusjanus",
          "text": "17B means it hopefully, with thoughts and prayers, runs on my 16GB 9070xt, right?",
          "score": 1,
          "created_utc": "2026-02-18 16:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62tc3k",
              "author": "Ok_Bug1610",
              "text": "They state at the bottom that at 4-bit (average most use them with say Ollama, etc.) it uses 256GB RAM... and while you can use less, you will see a huge performance loss (offloading to system memory). So don't get your hopes up... but if you have 220GB+ space available (UD-Q4\\_K\\_XL 219 GB), give it a shot.",
              "score": 1,
              "created_utc": "2026-02-18 16:28:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62u7z5",
                  "author": "gracchusjanus",
                  "text": "I was joking because of the active parameter number ;D\n\n\nWith my setup, 9070xt + 32GB DDR4 , I only use Gemma, OSS and Mistral. GLM also works but not in my native language (which is the language my docs are in).",
                  "score": 1,
                  "created_utc": "2026-02-18 16:32:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nob3j",
          "author": "Zerokx",
          "text": "Seems very good. But won't run on my machine. Also I thought you need enough VRAM to hold the model, how would it get 20 token/s just running from System RAM?",
          "score": 1,
          "created_utc": "2026-02-16 09:41:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5npde4",
              "author": "yoracale",
              "text": "I get 20 tokens/s on my unified memory Mac laptop!  \nWe're uploading smaller ones as we speak :)",
              "score": 5,
              "created_utc": "2026-02-16 09:51:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nqh31",
                  "author": "Final-Rush759",
                  "text": "Mlx or GGUF?",
                  "score": 2,
                  "created_utc": "2026-02-16 10:01:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5r13ga",
                  "author": "captainhukk",
                  "text": "Wait you‚Äôre getting that on a 128GB MacBook?",
                  "score": 1,
                  "created_utc": "2026-02-16 20:56:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o1nq8",
          "author": "SectionCrazy5107",
          "text": "please advise best config for vibe coding with claude code: should be thinking or non-thinking and the temp etc details please. Thanks",
          "score": -2,
          "created_utc": "2026-02-16 11:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5onuir",
              "author": "yoracale",
              "text": "We have a guide for Claude Code: [https://unsloth.ai/docs/basics/claude-codex](https://unsloth.ai/docs/basics/claude-codex)\n\nJust apply our Qwen3.5 settings in there.",
              "score": 2,
              "created_utc": "2026-02-16 14:07:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5p3sqv",
                  "author": "ochonueve89",
                  "text": "Super useful, thanks!1",
                  "score": 2,
                  "created_utc": "2026-02-16 15:30:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5rv4wj",
                  "author": "SectionCrazy5107",
                  "text": "Many many thanks",
                  "score": 2,
                  "created_utc": "2026-02-16 23:29:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5oht1d",
              "author": "siggystabs",
              "text": "It just came out. You could be the first to try it.",
              "score": 1,
              "created_utc": "2026-02-16 13:34:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ollr0",
                  "author": "SectionCrazy5107",
                  "text": "understand thanks. I downloaded UD-Q3\\_K\\_XL, running on V100, 176GB DDR5, getting around 9-10 t/s. just followed the unsloth guide, responses are same as in their online chat site.",
                  "score": 3,
                  "created_utc": "2026-02-16 13:55:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r5f4v8",
      "title": "Run MiniMax-2.5 locally Guide!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/61b97oryxnjg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-15 13:51:53",
      "score": 202,
      "num_comments": 37,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r5f4v8/run_minimax25_locally_guide/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5iap76",
          "author": "Status_Contest39",
          "text": "This is super and you are always the best !",
          "score": 9,
          "created_utc": "2026-02-15 14:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5if6vn",
              "author": "yoracale",
              "text": "Thanks a lot! Let us know if you encounter any issues. üôè There were previously some identification issues with Qwen3 coder next where it was detected nor loading.",
              "score": 6,
              "created_utc": "2026-02-15 14:27:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5or9kc",
          "author": "randomh4cker",
          "text": "I set this up on a MBP M4 Max w/128GB using the 3-bit dynamic quant and it's fast and smart!  With an empty context it runs about 40t/sec.  I was able to fit about 64k (65535) tokens and keep the machine stable.  More than this number of tokens context anyway takes too long to process and my integrations timeout.\n\nSo far this works very well with OpenClaw for coding tasks and tool calling. It does get a little bogged down when using a browser with Playwright. It seems to be very good at Kubernetes as well so it could be useful as a DevOps agent.\n\nThanks Unsloth for making this possible!",
          "score": 3,
          "created_utc": "2026-02-16 14:25:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vso57",
              "author": "mr_chillout",
              "text": "Thanks for sharing the information about your observations. Would it make sense to go for 256GB RAM M3 Ultra to make use of the AI for the big iOS project (200+ swift files)? ",
              "score": 1,
              "created_utc": "2026-02-17 15:48:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iauwa",
          "author": "Sad-Bat6310",
          "text": "Many thanks folks, you are truly the best !",
          "score": 2,
          "created_utc": "2026-02-15 14:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jirfz",
          "author": "flavio_geo",
          "text": "Unsloth, first thank you very much for the work bringing the quality quants to us that fast.\n\nI run local models on a AMD XTX 7900 24GB + Ryzen 7 9700 96GB. I am using the Q3\\_K\\_XL and getting \\~12 tokens/s (separating VRAM for 48k context in KV q8). In the announcement it is stated that with 16GB VRAM + 96GB RAM one could run it 25+ tokens. Could you please share what would be the optimal offload configuration to get there?",
          "score": 2,
          "created_utc": "2026-02-15 17:45:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5majvh",
              "author": "yoracale",
              "text": "Did you use fiton? 15 tokens/s doesn't sound too bad but yes it's slightly slower than expected",
              "score": 2,
              "created_utc": "2026-02-16 02:53:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nkr0s",
                  "author": "flavio_geo",
                  "text": "Yep, using --fit -on gets me about the same results as trying regex with -ot, which is 12.2 tokens/s.\n\nUsing llama.cpp latest docker rocm img.",
                  "score": 2,
                  "created_utc": "2026-02-16 09:07:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l9fsg",
          "author": "ozzeruk82",
          "text": "Do we have a way of knowing the quality loss from say using the Q3 model suggested in the tutorial? People seem unsure as to whether we're losing 25% quality, or 5% quality, or very minimal loss. It would be great for some of the main benchmarks to test some of these quantised versions.",
          "score": 2,
          "created_utc": "2026-02-15 23:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mxdw7",
          "author": "johndeuff",
          "text": "Impressive!!!",
          "score": 2,
          "created_utc": "2026-02-16 05:36:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ibuw7",
          "author": "Local-Cartoonist3723",
          "text": "I cincerely feel as a community we‚Äôve almost been to quiet on this model, this size and that strong is ridiculous.\n\nThanks unsloth team for making this avail!",
          "score": 2,
          "created_utc": "2026-02-15 14:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iqfor",
          "author": "Lesteriax",
          "text": "Will it work on rtx pro 6000 and 96gb ram?",
          "score": 2,
          "created_utc": "2026-02-15 15:27:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j5qvn",
              "author": "yoracale",
              "text": "Yes of course! Run a bigger one and it'll be even faster",
              "score": 1,
              "created_utc": "2026-02-15 16:42:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iqpdu",
          "author": "Porespellar",
          "text": "Amazing job guys! Thanks so much for getting your quants out so quickly. Is there any chance that you guys could start including the quant perplexity information with your releases so it‚Äôs easier to compare your quants with Ubergarm‚Äôs quants and others who include this information?",
          "score": 1,
          "created_utc": "2026-02-15 15:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mabqz",
              "author": "yoracale",
              "text": "Perplexity is a poor measurement for accuracy and one of the worst. See: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs#calibration-dataset-overfitting](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs#calibration-dataset-overfitting)\n\nPerplexity is just so popular because it's the easiest to benchmark",
              "score": 2,
              "created_utc": "2026-02-16 02:51:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jo6qf",
          "author": "alex_bit_",
          "text": "4 x 3090s will do it?",
          "score": 1,
          "created_utc": "2026-02-15 18:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mafub",
              "author": "yoracale",
              "text": "No, unfortunately it doesn't fit. But you can run a slightly smaller one. How much extra RAM do you have?",
              "score": 2,
              "created_utc": "2026-02-16 02:52:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nogsc",
                  "author": "alex_bit_",
                  "text": "256GB of DDR4 in a X299 system.",
                  "score": 1,
                  "created_utc": "2026-02-16 09:42:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5mxkqg",
              "author": "johndeuff",
              "text": "Man I wish",
              "score": 1,
              "created_utc": "2026-02-16 05:37:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k9xb3",
          "author": "rytheguy88",
          "text": "What size context window and tokens per second would be possible on an M3 ultra with 128 or 256GB ram?",
          "score": 1,
          "created_utc": "2026-02-15 19:58:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mahe9",
              "author": "yoracale",
              "text": "For 128 maybe 20K context? 256 can do 100K",
              "score": 1,
              "created_utc": "2026-02-16 02:52:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tgnk4",
          "author": "WonderfulNectarine87",
          "text": "Got M3 Max with 96gb RAM and 40 GPU.. guess it can work on my device?",
          "score": 1,
          "created_utc": "2026-02-17 05:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vrc6c",
          "author": "mr_chillout",
          "text": "Amazing work guys! thanks for doing this and sharing!  \nWhat Macc should I buy (the lowest price possible ;) ) so that I can run it against big (200 files) iOS project codebase? ",
          "score": 1,
          "created_utc": "2026-02-17 15:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xof55",
              "author": "yoracale",
              "text": "128gb ram id say",
              "score": 1,
              "created_utc": "2026-02-17 21:11:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5xom6c",
                  "author": "mr_chillout",
                  "text": "256 GB would be I assume a room for future growth? ",
                  "score": 1,
                  "created_utc": "2026-02-17 21:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5imzqm",
          "author": "Mysterious_Value_219",
          "text": "Would it work with the 128GB GX10 or the ryzen ai max+ 395?",
          "score": 1,
          "created_utc": "2026-02-15 15:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j3u4p",
              "author": "etcetera0",
              "text": "I tried smaller quantization models but it doesn't load for me. I'll try again today",
              "score": 1,
              "created_utc": "2026-02-15 16:32:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5jzd8f",
              "author": "StardockEngineer",
              "text": "You're going to have to use q3 quants or wait for REAP.",
              "score": 1,
              "created_utc": "2026-02-15 19:06:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iu39g",
          "author": "jellydn",
          "text": "Do you know if that's running okay on a Mac M1/M2 laptop? Or what is the recommended spec for the Mac Mini to run it smoothly? I really appreciate any help you can provide.",
          "score": 1,
          "created_utc": "2026-02-15 15:45:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j5l99",
              "author": "yoracale",
              "text": "If it fits it should mostly be ok. I have a M3 and get 20 toks",
              "score": 1,
              "created_utc": "2026-02-15 16:41:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5j8lw6",
                  "author": "jellydn",
                  "text": "Cool. Thanks, let me try and get back if any issues.",
                  "score": 1,
                  "created_utc": "2026-02-15 16:55:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6we96",
      "title": "All Qwen3.5-397B-A17B GGUFs are up!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/8ah65mf7gzjg1.jpeg",
      "author": "yoracale",
      "created_utc": "2026-02-17 04:30:55",
      "score": 172,
      "num_comments": 37,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r6we96/all_qwen35397ba17b_ggufs_are_up/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5tbknc",
          "author": "AppleBottmBeans",
          "text": "I remember in 1999 having to beg my best friends mom to drive me to CompUSA because after 3 months, I finally saved up enough money to buy a new GPU and finally play Everquest. It required at least 4MB of VRAM and that's the card I got. \n\nI wonder if in 10 years we will look back and look at something like 462GB VRAM and laugh at how small that ends up being. ",
          "score": 25,
          "created_utc": "2026-02-17 04:51:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tx081",
              "author": "DertekAn",
              "text": "Hmm, that's already the case with servers, but for home use... forget it.....\n\nIf you look at home users and upcoming technologies like the Steam Machine, which has 16GB of RAM...\n\nMy first PC from 2015 already had 32GB of RAM. And as for VRAM, my GTX 980 had 4GB, and after that I had an RTX 3060 Ti with 8GB, and now in 2026, 8GB is still very common, and it's becoming more prevalent again due to the RAM shortage.\n\nLuckily, I upgraded to an RX 9060 XT 16GB card at the end of last year, right when they were cheapest.",
              "score": 3,
              "created_utc": "2026-02-17 07:50:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o61gc8v",
                  "author": "rorykoehler",
                  "text": "China will come in with oversupply. They have already started to plug the gap. There is no way they will let this opportunity slip. They are probably also preparing subsidies for when the bubble pops to derisk the scale up for companies. We will have TB of ram for pennies in 3 years. ",
                  "score": 2,
                  "created_utc": "2026-02-18 12:12:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5tgv16",
          "author": "BABA_yaaGa",
          "text": "1 bit still needs more than 100GB ü•≤ü•≤ü•≤",
          "score": 9,
          "created_utc": "2026-02-17 05:31:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tigf8",
              "author": "yoracale",
              "text": "Yep unfortunately, you're better off running MiniMax-2.5 until Qwen releases a smaller model: https://unsloth.ai/docs/models/minimax-2.5",
              "score": 8,
              "created_utc": "2026-02-17 05:43:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5yw801",
                  "author": "garlic-silo-fanta",
                  "text": "Can I get a 0.1-bit? Maybe that‚Äôll fit",
                  "score": 1,
                  "created_utc": "2026-02-18 00:59:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o68v9t2",
                  "author": "RedParaglider",
                  "text": "Wouldn't it be better if we have 128gb like 125gb useable vram to just run qwen3 coder next in a Q5 or Q6?  Would there be a benefit to running minimax in a Q3?  If there is I just found my new fun task for tonight.",
                  "score": 1,
                  "created_utc": "2026-02-19 14:27:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5tfw38",
          "author": "nok01101011a",
          "text": "How nice! I could theoretically run the TQ1_0 Model, lol. Is there actually any thinkable scenario where someone would/could benefit from any of those Q1/Q2 models?",
          "score": 4,
          "created_utc": "2026-02-17 05:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tidyu",
              "author": "yoracale",
              "text": "Usually Q1 or Q2 is very effective for large models. Qwen3.5 is ok ish to run since it's 400b. You can see benchmarks for dynamic 1 and 2-bit here: https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs",
              "score": 3,
              "created_utc": "2026-02-17 05:43:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tt7mh",
                  "author": "alhinai_03",
                  "text": "Is the unsloth qwen coder next Q2_K_XL worth it, or am I better off running the 60B REAM at Q4?",
                  "score": 1,
                  "created_utc": "2026-02-17 07:15:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5tdwby",
          "author": "LA_rent_Aficionado",
          "text": "Edit: user error",
          "score": 2,
          "created_utc": "2026-02-17 05:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tia4x",
              "author": "yoracale",
              "text": "Weird can you try another quant and see?",
              "score": 1,
              "created_utc": "2026-02-17 05:42:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tkoon",
                  "author": "LA_rent_Aficionado",
                  "text": "Same results with IQ4\\_NL, I even tried with the mmproj from [https://huggingface.co/DevQuasar/Qwen.Qwen3.5-397B-A17B-GGUF](https://huggingface.co/DevQuasar/Qwen.Qwen3.5-397B-A17B-GGUF) to no avail",
                  "score": 1,
                  "created_utc": "2026-02-17 06:01:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5tkx8e",
              "author": "EbbNorth7735",
              "text": "Mine nailed image analysis perfectly. Like scary perfectly. Way better than qwen3 vl which made occasional mistakes. I have the mxfp4 version I think it's called from unsloth. Using the mmproj 32. Latest llama.cpp release on cuda 13.1",
              "score": 1,
              "created_utc": "2026-02-17 06:03:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5trdl9",
                  "author": "LA_rent_Aficionado",
                  "text": "I accidently fat fingered batch size at 1028 vs 1024 and apparently that was enough to break everything... problem solved ",
                  "score": 4,
                  "created_utc": "2026-02-17 06:58:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5unlp1",
          "author": "timbo2m",
          "text": "Anyone have a single 512 Mac studio benchmark for each quant for each context size?",
          "score": 2,
          "created_utc": "2026-02-17 11:53:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tzqor",
          "author": "iTh0R-y",
          "text": "Need mlx‚Äôs",
          "score": 1,
          "created_utc": "2026-02-17 08:16:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u6lgk",
              "author": "yoracale",
              "text": "Maybe in the future",
              "score": 1,
              "created_utc": "2026-02-17 09:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vwef7",
                  "author": "ProtoSkutR",
                  "text": "it‚Äôs already out",
                  "score": 1,
                  "created_utc": "2026-02-17 16:06:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vwoui",
          "author": "ProtoSkutR",
          "text": "I tried running this with the latest VLLM nightly on macOS, that‚Äôs 14.X, I also tried it on VLLM on Cuda, again the most recent nightly wheel, 16.0rc1, still got all of these errors!!!",
          "score": 1,
          "created_utc": "2026-02-17 16:07:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63so7q",
          "author": "Artistic_Ladder9570",
          "text": "maybe i am not understanding ... who runs these models? who has a b200 gpu to run this? ",
          "score": 1,
          "created_utc": "2026-02-18 19:05:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65gk2t",
              "author": "yoracale",
              "text": "It's also for people with a lot of ram or people who have a lot of unified memory like MacS or DGX Spark etc",
              "score": 1,
              "created_utc": "2026-02-18 23:52:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o69ousn",
                  "author": "Artistic_Ladder9570",
                  "text": "yea i just learned that... (sigh's in broke)... i just learned a mac studio has 512gb ram (for like $8k)...",
                  "score": 1,
                  "created_utc": "2026-02-19 16:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5u7xv9",
          "author": "Impossible_Art9151",
          "text": "under llama.cpp I tried:  \nMXFP4\\_MOE, UD-Q4\\_K\\_XL, Q4\\_K\\_M  \n  \nRunning with   \n llama-server     --model unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf     --alias \"unsloth/Qwen3.5-397B-A17B\"     --temp 0.6     --top-p 0.95     --ctx-size 261000     --top-k 20     --min-p 0.00     --port 8090 --jinja --no-mmap -fa on\n\nRunning without SSL init: using 23 threads for HTTP server start: binding port with default address family main: loading model srv load\\_model: loading model 'unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf' common\\_init\\_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on llama\\_model\\_load: error loading model: error loading model architecture: unknown model architecture: 'qwen35moe' llama\\_model\\_load\\_from\\_file\\_impl: failed to load model llama\\_params\\_fit: encountered an error while trying to fit params to free device memory: failed to load model llama\\_params\\_fit: fitting params to free memory took 0.27 seconds llama\\_model\\_load\\_from\\_file\\_impl: using device CUDA0 (NVIDIA RTX A6000) (0000:00:10.0) - 48299 MiB free llama\\_model\\_loader: additional 5 GGUFs metadata loaded. llama\\_model\\_loader: loaded meta data with 48 key-value pairs and 1098 tensors from unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf (version GGUF V3 (latest)) llama\\_model\\_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. llama\\_model\\_loader: - kv 0: general.architecture str = qwen35moe .... \\[deleted due to size\\] llama\\_model\\_loader: - type mxfp4: 352 tensors print\\_info: file format = GGUF V3 (latest) print\\_info: file type = Q4\\_K - Medium print\\_info: file size = 199.66 GiB (4.33 BPW) llama\\_model\\_load: error loading model: error loading model architecture: unknown model architecture: 'qwen35moe' llama\\_model\\_load\\_from\\_file\\_impl: failed to load model common\\_init\\_from\\_params: failed to load model 'unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf' srv load\\_model: failed to load model, 'unsloth/Qwen3.5-397B-A17B-GGUF/UD-Q4\\_K\\_XL/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf' srv operator(): operator(): cleaning up before exit... main: exiting due to model loading error\n\nAny hints what I am doing wrong?  \n",
          "score": 0,
          "created_utc": "2026-02-17 09:35:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ugir3",
              "author": "yoracale",
              "text": "Did you use the latest version of llama.cpp?",
              "score": 1,
              "created_utc": "2026-02-17 10:53:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ugunl",
                  "author": "Impossible_Art9151",
                  "text": "Yes - I did, or at least I thought, I did. BUT DUE TO AN UNDERDOSIS OF COFFEE - I failed using the actual version. :-(",
                  "score": 1,
                  "created_utc": "2026-02-17 10:56:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5uya3e",
          "author": "tist20",
          "text": "Does it run on Strix Halo?",
          "score": -1,
          "created_utc": "2026-02-17 13:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vpg7p",
              "author": "yoracale",
              "text": "Yes if you ahve enough ram",
              "score": 1,
              "created_utc": "2026-02-17 15:32:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5w45bk",
              "author": "starkruzr",
              "text": "feel like it's hard for that math to math when the thing has almost 400 parameters and the machine has a max total RAM of 128GB unless you're quantizing the hell out of it",
              "score": 1,
              "created_utc": "2026-02-17 16:45:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5xld5t",
              "author": "doingmarvelous",
              "text": "You could run it on two strix halo machines with 128GB RAM each.  Using rdma network cards you can get good scaling with vllm.\n\nhttps://youtu.be/nnB8a3OHS2E?si=0eyLSi4JjaUv_hja\n\nStill need a quantized version though.",
              "score": 0,
              "created_utc": "2026-02-17 20:57:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3jvp8",
      "title": "Unsloth is trending on GitHub today!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/c9kgw2yv28jg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-13 08:29:44",
      "score": 150,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r3jvp8/unsloth_is_trending_on_github_today/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54s76e",
          "author": "PaceZealousideal6091",
          "text": "Keep it up guys! You have made local llm possible for everyone. Along with lcpp, you guys have been instrumental in people like me to experiment with ai. Thanks a lot. Looking forward to continue to learn and grow with you guys.",
          "score": 8,
          "created_utc": "2026-02-13 08:38:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54v22a",
              "author": "yoracale",
              "text": "Thank you so much for the support!! <3",
              "score": 4,
              "created_utc": "2026-02-13 09:05:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o55k7cv",
          "author": "m98789",
          "text": "You guys are great. But we worry that you will get bought out by an evil mega corp and all of us who build on your library will be at risk. Getting investment is one thing, but we hope you can pledge to remain open source and independent.",
          "score": 4,
          "created_utc": "2026-02-13 12:38:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56qmun",
              "author": "yoracale",
              "text": "Definitely not, the package will always have an open source license and we will always continue our open source work!!",
              "score": 3,
              "created_utc": "2026-02-13 16:21:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56hsca",
          "author": "segmond",
          "text": "unsloth so important it's pinned on my browser\n\nhttps://preview.redd.it/yn30zpdv7ajg1.png?width=928&format=png&auto=webp&s=7117d0d63336756e9ed726940298744cd2209167\n\n",
          "score": 5,
          "created_utc": "2026-02-13 15:39:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56qg45",
              "author": "yoracale",
              "text": "Oh wow! Well what can I say, you have very good taste!! üëèü¶•",
              "score": 2,
              "created_utc": "2026-02-13 16:20:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bzaf5",
          "author": "Comacdo",
          "text": "And you truly deserve it ! Congrats üëè",
          "score": 1,
          "created_utc": "2026-02-14 12:53:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r85r5x",
      "title": "You can now train LLMs in VS Code for free via Colab!",
      "subreddit": "unsloth",
      "url": "https://v.redd.it/k5cjdy4ss9kg1",
      "author": "yoracale",
      "created_utc": "2026-02-18 15:19:57",
      "score": 114,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r85r5x/you_can_now_train_llms_in_vs_code_for_free_via/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o62orpj",
          "author": "Reivaj640",
          "text": "Thank you so much, it's super interesting to do with VS Code",
          "score": 7,
          "created_utc": "2026-02-18 16:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66pi40",
              "author": "danielhanchen",
              "text": "Yes VS Code + Colab GPUs are pretty cool! You can also launch multiple Colabs as well!",
              "score": 1,
              "created_utc": "2026-02-19 04:16:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67pgvl",
          "author": "Big-Balance-6426",
          "text": "Thank you for this guide!",
          "score": 2,
          "created_utc": "2026-02-19 09:17:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a2qyk",
          "author": "bad_gambit",
          "text": "Thanks for the guide (and the quants and your amazing finetuning library)! Is google service (BigQuery, Drive, etc) and other quirks (github lfs integration was janky, iirc) finally fixed now? Last time i used the extension it was feeling like its half-baked and quite a hassle without those features",
          "score": 1,
          "created_utc": "2026-02-19 18:00:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r525jp",
      "title": "Best coding model to use with 48GB vram and 90GB ram?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r525jp/best_coding_model_to_use_with_48gb_vram_and_90gb/",
      "author": "StartupTim",
      "created_utc": "2026-02-15 01:45:32",
      "score": 43,
      "num_comments": 23,
      "upvote_ratio": 0.98,
      "text": "I have a system with a RTX 5090 32GB vram and a RTX 5070Ti with 16GB vram.\n\nWhich would be the best model to run for doing JS, html (node/react) type of development?  The goal would be as big of a context window as possible as well.\n\nAlso, would you recommend llama.cpp normal or compile with any specific flags?\n\n\nThanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r525jp/best_coding_model_to_use_with_48gb_vram_and_90gb/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5hpzrw",
          "author": "Holiday_Purpose_3166",
          "text": "Reddit seems to error to post my comment, so had to break into 4 parts and LLM condensed it:\n\n# PART 1\n\nIt depends on which pipeline you‚Äôre using (Kilocode, Opencode, Openclaw, etc.) ‚Äî the same model can behave very differently depending on chat templates, tool-call reliability, and context handling.\n\n# Models I‚Äôm actively using\n\nMy current rotation: **Devstral Small 2**, **GLM 4.7 Flash**, **GPT-OSS-20B**, **GPT-OSS-120B**.\n\n# Devstral Small 2 (main workhorse)\n\n* Most reliable for **repo work** overall.\n* Runs well in **Kilocode** and **Mistral Vibe**.\n* **Opencode + Openclaw** won‚Äôt work well out-of-the-box unless you fix the **chat template** (I have the fix on my git listed below).\n* Low latency, **no-reasoning style**, very strong for **Rust + NextJS + ML** repos.\n* Can miss some deeper architectural leaps, but consistently ships working code.\n* Don‚Äôt go below **Q8** if you want it to stay strong; lower quants tend to ‚Äúspend context‚Äù fixing itself.\n* KV cache: **Q8** is great; **Q4** only for lighter tasks.\n\n# GLM 4.7 Flash (fast JS/TS repo helper)\n\n* Great for repo work involving **JS/TS**, but not as consistently strong as Devstral for full repo correctness.\n* Works best in **Opencode**; also works in **Kilocode** but occasionally misses tool calls.\n* Also works in **Openclaw**.\n* Faster than Devstral, reasoning isn‚Äôt verbose, less brittle than GPT-OSS-20B.\n* Main drawback: inference speed drops hard above \\~**40k context** (long-horizon tasks get mildly painful, especially prompt reprocessing).\n* Tested on **Q5**.\n* KV cache: prefers **F16** for speed; Q8 is fine too. Context window processing is bearable up to \\~100k\n\n#",
          "score": 19,
          "created_utc": "2026-02-15 11:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hq33s",
              "author": "Holiday_Purpose_3166",
              "text": "# PART 2\n\n# GPT-OSS-20B (great until complexity spikes)\n\n* Strong for repo work **as long as it‚Äôs not too complex**; can hard-refuse when ingesting very large files (e.g., **900+ LoC**).\n* Works well in **Opencode** and **Openclaw** out-of-the-box.\n* Weaker in **Kilocode** (tool/API calls fail too often for me).\n* Best at **medium reasoning**. High reasoning gives a small edge, but CoT verbosity kills completion time and risks falling in repetition - sampling below helps.\n* Strong speed across full context.\n* I run **MXFP4**, **no KV cache compression**.\n\n# GPT-OSS-120B (planning/docs monster, good complement to 20B)\n\n* Great for repo work; oddly, I‚Äôve seen **20B beat it** in some coding areas.\n* Where 120B shines: **planning + documentation**, and it pairs well with 20B.\n* Works with **Opencode + Kilocode**.\n* Felt a bit ‚Äúdumb‚Äù in **Openclaw**, but I haven‚Äôt used it long enough there.\n* Best at **medium reasoning**; high reasoning helps planning/docs more than code execution.\n* Stronger CoT than 20B and more token-efficient.\n* Also **MXFP4**, **no KV cache compression**.\n* Speed stays excellent even offloaded (activated params keep it lighter than it sounds).\n\n# ",
              "score": 12,
              "created_utc": "2026-02-15 11:27:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5hq4gy",
                  "author": "Holiday_Purpose_3166",
                  "text": "# PART 3\n\n# Models I didn‚Äôt keep using (yet)\n\n**Nemotron 3 Nano**\n\n* Extremely fast, tiny reasoning traces.\n* Not stable for agentic repo work (NVIDIA team themselves noted stability issues in agentic settings).\n* Good for chat-turn tasks, but GPT-OSS-20B can be significantly more intelligent, although Nemotron dataset is more up to date.\n\n**Qwen3 Coder Next**\n\n* Not weak ‚Äî but it breaks during inference on llama.cpp for me despite staying updated.\n* When it runs, it‚Äôs efficient in Kilocode/Opencode, but I can‚Äôt replicate reliably due to inference failures.\n* If someone has a stable llama.cpp setup for it, I‚Äôd love to see it.\n\n# General llama.cpp flags (what‚Äôs worked for me)\n\n* `--numa numactl` can give a small edge on non-server CPUs.\n* llama.cpp default RAM cache \\~7.5GB; I use `--cache-ram 10000`. Bigger sometimes slows prompt processing for full GPU-offload models, but can help offloaded MoE due to shared bandwidth.\n* `--no-mmap` improves stability virtually in all cases.\n* Offload tuning (hybrid GPU/CPU): `-ot \".ffn_(up)_exps.=CPU\"` \\+ `--n-cpu-moe <N>` can help with your specs.\n* Threads: more isn‚Äôt always better as CPU bottlenecks. On **Ryzen 9 9950X**:\n   * full GPU-offload tends to like `-t 8`\n   * GPT-OSS-120B offloaded liked `-t 10` with my flags\n* KV cache: generally **Q8** is the best speed/context tradeoff (higher context return vs. speed sacrifice) ‚Äî **except GPT-OSS** models, where compression hits harder.\n* Batch sizes `-b` / `-ub` are rig/model-specific:\n   * I use `16384` for GPT-OSS-20B\n   * `2048` for GPT-OSS-120B\n   * elsewhere `512` is often a solid baseline\n\n# ",
                  "score": 14,
                  "created_utc": "2026-02-15 11:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5hkan0",
          "author": "loadsamuny",
          "text": "pick your quants wisely\n\nhttps://electricazimuth.github.io/LocalLLM_VisualCodeTest/results/2026.02.04_quant/",
          "score": 8,
          "created_utc": "2026-02-15 10:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ghhpi",
          "author": "TokenRingAI",
          "text": "GLM 4.7 Flash",
          "score": 4,
          "created_utc": "2026-02-15 04:37:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fuda0",
          "author": "ClimateBoss",
          "text": "qwen coder next MXFP4 bruh",
          "score": 9,
          "created_utc": "2026-02-15 01:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g2e2a",
              "author": "larrytheevilbunnie",
              "text": "Yeah, qwen3 coder next is best right now, he doesn't even need the 5070ti for that tbh, should reserve that one for other agents",
              "score": 3,
              "created_utc": "2026-02-15 02:47:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ha3nt",
              "author": "StartupTim",
              "text": "Hey thanks, can llama.cpp run that?",
              "score": 1,
              "created_utc": "2026-02-15 08:54:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5j453a",
              "author": "raphh",
              "text": "Is there a noticeable difference between MXFP4 and the UD-Q4\\_K\\_XL variant ?",
              "score": 1,
              "created_utc": "2026-02-15 16:34:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5kc3ak",
                  "author": "Old-Cardiologist-633",
                  "text": "MXFP4 should be faster and better :)",
                  "score": 1,
                  "created_utc": "2026-02-15 20:10:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ivx1r",
          "author": "YearnMar10",
          "text": "Minimax 2.5 probably in q4ish",
          "score": 1,
          "created_utc": "2026-02-15 15:54:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o2s6g",
          "author": "Mangostickyrice1999",
          "text": "I wonder how stable multiples GPUs work. I have a 3090 and am considering turning a second card for a heavier model",
          "score": 1,
          "created_utc": "2026-02-16 11:50:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pahpt",
          "author": "zaddyninja",
          "text": "Look at k2",
          "score": 1,
          "created_utc": "2026-02-16 16:01:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5plzng",
          "author": "Confusion_Senior",
          "text": "qwen coder next¬†unsloth dynamic q4 gguf",
          "score": 1,
          "created_utc": "2026-02-16 16:54:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zlikl",
          "author": "Mabuse046",
          "text": "Hey, quick aside - you can go to Nvidia's NIM site and test out a bunch of different models on their web app for free without having to download them all just to find out you hate them, and if you sign up for a free account with a US phone number you can also use their free API with your own front end as well.",
          "score": 1,
          "created_utc": "2026-02-18 03:14:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ilk05",
          "author": "No_Strain_2140",
          "text": "**Best coding model for your dual RTX setup (JS/Node/React, max context):**\n\n**DeepSeek-Coder-V2-Lite-Instruct (16B Q4\\_K\\_M)**\n\n* Top for JS/TS/React, native 128k context, fits one GPU, \\~25-35 tok/s multi-GPU.\n\n**Bigger alternatives**:\n\n* Qwen3-Coder-32B (256k context, Q3\\_K\\_M) ‚Äì stronger for complex Node.\n* WizardCoder-70B (up to 512k RoPE) ‚Äì for huge codebases, Q2\\_K needed (\\~15 tok/s).",
          "score": 0,
          "created_utc": "2026-02-15 15:02:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vylza",
              "author": "maxwell321",
              "text": "This comment was written by Qwen 3 8B",
              "score": 1,
              "created_utc": "2026-02-17 16:17:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3sqgt",
      "title": "qwen3-coder-next ggufs updated?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r3sqgt/qwen3codernext_ggufs_updated/",
      "author": "Clank75",
      "created_utc": "2026-02-13 15:51:05",
      "score": 32,
      "num_comments": 18,
      "upvote_ratio": 0.97,
      "text": "I just noticed (because llama decided to download the quants all over again) that Qwen3-Coder-Next GGUFs all seem to have been updated (judging by the filetimes on Huggingface, about 13 hours ago.)\n\nAny ideas what's changed?  (Hoping/praying for something that fixes let's-read-this-file-over-and-over-again toolcalling problems ;-).)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r3sqgt/qwen3codernext_ggufs_updated/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o56mahv",
          "author": "yoracale",
          "text": "Some GGUFs weren't dynamic so we've updated some of them to be dynamic.\nYou won't see much improvement unless you use lower bit quants",
          "score": 15,
          "created_utc": "2026-02-13 16:01:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56p0ps",
              "author": "ywis797",
              "text": "But please add some update notes so we won't download again. I am too eager to download newer for better performance.",
              "score": 6,
              "created_utc": "2026-02-13 16:14:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56q993",
                  "author": "yoracale",
                  "text": "You can redownload if you want and you may see some minor improvements at higher bits.",
                  "score": 1,
                  "created_utc": "2026-02-13 16:20:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56pvuu",
              "author": "Clank75",
              "text": "Ahh, OK, so unlikely to be lifechanging at Q8?  Anyway, thanks for the quick reply!",
              "score": 1,
              "created_utc": "2026-02-13 16:18:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o56rrvz",
                  "author": "yoracale",
                  "text": "You can try it and see which is better. Wont be life-changing no but you may see some differences",
                  "score": 1,
                  "created_utc": "2026-02-13 16:27:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ao6ox",
              "author": "UmpireBorn3719",
              "text": "any improvement on MXFP4?",
              "score": 1,
              "created_utc": "2026-02-14 05:40:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o570fcb",
          "author": "Phantasmagoriosa",
          "text": "Ah so its not just me with the toolcalling errors all the time then:  \n`‚Üê Write lambda/src/dispatcher-handler/agentcore.ts Error: The write tool was called with invalid arguments: [   {     \"expected\": \"string\",     \"code\": \"invalid_type\",     \"path\": [       \"content\"     ],     \"message\": \"Invalid input: expected string, received undefined\"   } ]. Please rewrite the input so it satisfies the expected schema.`",
          "score": 2,
          "created_utc": "2026-02-13 17:08:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5738yz",
              "author": "Clank75",
              "text": "Nope.  I have high hopes that they'll eventually get fixed - there are some *probably* relevant fixes happening to prompt template parsing in Llama.cpp at the moment - e.g. https://github.com/ggml-org/llama.cpp/pull/18675 - but for the timebeing it's essentially unusable.  Which is a shame, because it seems smart.\n\nOne of these days I'mma get round to getting vLLM working, but every time I've fallen down that rabbit hole it's been a world of hurt...",
              "score": 2,
              "created_utc": "2026-02-13 17:22:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o58g8ur",
              "author": "__SlimeQ__",
              "text": "i was getting that like crazy using the unofficial ollama release from frob, on the official one i am getting pretty great behavior in both qwen code and openclaw",
              "score": 1,
              "created_utc": "2026-02-13 21:22:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57aizj",
          "author": "klop2031",
          "text": "Sheesh again?",
          "score": 2,
          "created_utc": "2026-02-13 17:57:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bda1l",
          "author": "nunodonato",
          "text": "My server less container was working fine with the previous version and now fails to load üòì (Q6 btw)",
          "score": 1,
          "created_utc": "2026-02-14 09:34:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4o39t",
      "title": "Is there a Problem with Qwen3 Coder Next Q6_K_XL?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/gallery/1r4o39t",
      "author": "AIMasterChief",
      "created_utc": "2026-02-14 15:54:42",
      "score": 21,
      "num_comments": 9,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r4o39t/is_there_a_problem_with_qwen3_coder_next_q6_k_xl/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5dbch6",
          "author": "Particular-Way7271",
          "text": "I think you should just try using llama.cpp directly...",
          "score": 11,
          "created_utc": "2026-02-14 17:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dpbkt",
          "author": "yoracale",
          "text": "We saw this issue and going to investigate further. Might be a lm studio specific issue since it works fine in llama.cpp",
          "score": 5,
          "created_utc": "2026-02-14 18:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fj8f4",
          "author": "admajic",
          "text": "Cant you override lmstudio by holding down Ctrl key or shift key? When it had red boxes?",
          "score": 1,
          "created_utc": "2026-02-15 00:40:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dfvgh",
          "author": "TaroOk7112",
          "text": "Same here. I used llama.cpp, but fails with opencode with streaming. There is a proxy that converts llama.cpp into a non-streaming api. But I wanted to use LM Studio for now and also is great to test configurations quickly. I tested unsloth's Qwen3-Coder-Next in MXFP4\\_MOE, Q6\\_K and UD-Q8\\_K\\_XL. Only MXFP4 is correctly detected in LM Studio 4.2.2 X86 Linux appimage.\n\nhttps://preview.redd.it/bobnzzi7zhjg1.png?width=754&format=png&auto=webp&s=6cbb5145f831f2d620de537e786dd3b49fd84a3d\n\n",
          "score": 1,
          "created_utc": "2026-02-14 17:45:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dgvxv",
              "author": "TaroOk7112",
              "text": "Is there a TUI or any other app that helps loading models in plain llama.cpp like LM Studio does? It would be great to select parameters with helpers that warn you if the parameters doesn't make sense or even have a suggestion of the best parameters to use, and from there you tweak and test the best config.",
              "score": 2,
              "created_utc": "2026-02-14 17:50:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dg8ii",
              "author": "TaroOk7112",
              "text": "When I open the model for loading it doesn't know how long is the supported context and doesn't load the model.\n\nhttps://preview.redd.it/6lle6jzizhjg1.png?width=757&format=png&auto=webp&s=de1aba75a59e4539427e33ff16491feb6ecbc8f7\n\n",
              "score": 1,
              "created_utc": "2026-02-14 17:47:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dpz52",
              "author": "yoracale",
              "text": "Really it didn't work for you in llama.cpp? What was the error, it worked for us",
              "score": 1,
              "created_utc": "2026-02-14 18:35:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ek8dt",
                  "author": "TaroOk7112",
                  "text": "Is not a fail of the GGUF, is a problem with llama.cpp streaming. Plain llama.cpp works ok, but not with opencode.\n\nEdit: I checked the integrity of the GGUFs with sha256. There are all ok.",
                  "score": 1,
                  "created_utc": "2026-02-14 21:15:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5e1lhl",
          "author": "blackhawk00001",
          "text": "Qwen3 coder next is a great opportunity to explore running llama.cpp server directly.  Lm studio uses it but not always the most recent version.  I noticed a decent speed improvement moving to llama server and it works better with vs code Kline code extension.  Llama.cpp has been having cuda optimization issues with qwen next but a few prs were merged this morning that I can‚Äôt wait to get home and pull in to test this evening.  Finding what works best can be a day or two of trouble but the results are much better.  I‚Äôm also now starting up a temporary llamacpp server from within comfy UI nodes to enhance prompts during the workflow.",
          "score": 1,
          "created_utc": "2026-02-14 19:34:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r908nn",
      "title": "New r/unsloth User Flairs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/s14byfp2lgkg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-19 14:11:58",
      "score": 21,
      "num_comments": 3,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r908nn/new_runsloth_user_flairs/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o698enw",
          "author": "im_datta0",
          "text": "Why did you pick the sad sloth though LOL",
          "score": 2,
          "created_utc": "2026-02-19 15:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c5lng",
              "author": "yoracale",
              "text": "Because it's my favorite sloth emoji üò≠",
              "score": 1,
              "created_utc": "2026-02-20 00:20:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ae124",
          "author": "danielhanchen",
          "text": "I'm using the Heart Sloth! :)",
          "score": 2,
          "created_utc": "2026-02-19 18:53:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mgfg",
      "title": "Unsloth Model Quantization: When is the MiniMax M2.5 REAP GGUF coming?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r4mgfg/unsloth_model_quantization_when_is_the_minimax/",
      "author": "Leolin7519",
      "created_utc": "2026-02-14 14:47:56",
      "score": 17,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "I know everyone‚Äôs waiting for the GGUF of the older models, but we need to prioritize MiniMax M2.5. This 10B active parameter MoE is already so efficient that even the FP8 version runs like a dream. It‚Äôs SOTA (80.2% SWE-Bench) and acts as a Real World Coworker for $1/hour. The RL scaling they‚Äôve done is more impressive than any simple quantization. If you want a model that actually reasons through a linting error instead of just guessing, M2.5 is the only one in this size category that‚Äôs truly industry-leading.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r4mgfg/unsloth_model_quantization_when_is_the_minimax/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5cr6at",
          "author": "raysar",
          "text": "We need to wait than people working on training this reap gguf quantisation üòÑ\nwe want high quality not just fast delivery üòä",
          "score": 5,
          "created_utc": "2026-02-14 15:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eavni",
          "author": "segmond",
          "text": "IMO, from my experience trying REAP versions, tho shall not REAP, just pick a smaller quant size.",
          "score": 5,
          "created_utc": "2026-02-14 20:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ss5ab",
              "author": "RedParaglider",
              "text": "Or just be lazy like me and wait for really nice distillations.",
              "score": 1,
              "created_utc": "2026-02-17 02:43:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u0sqa",
          "author": "noctrex",
          "text": "Well we gonna have to wait until somebody does it who has the available compute.\n\nOr even better, a REAM version.",
          "score": 1,
          "created_utc": "2026-02-17 08:26:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60bw7n",
          "author": "EzraWinner",
          "text": "Wait, has anyone else noticed the REAP variants already popping up on HF? I saw a few different percentages (19%, 29%, etc.) from independent uploaders yesterday.",
          "score": 1,
          "created_utc": "2026-02-18 06:18:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60cbk1",
          "author": "WAYXL",
          "text": "FP8 is smooth, but a solid GGUF for the Mac Studio crowd would be huge. 128GB Unified Memory seems like the target spot for this.",
          "score": 1,
          "created_utc": "2026-02-18 06:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60dvhc",
          "author": "Previous-Shop6033",
          "text": "Finally, a model that doesn't just guess the fix. The way M2.5 actually \"thinks\" through the linting errors reminds me of working with a senior dev who actually reads the logs.",
          "score": 1,
          "created_utc": "2026-02-18 06:35:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ixc5",
          "author": "785496",
          "text": "Unsloth usually moves fast on these. If they apply their Dynamic Quantization v2.0 to M2.5, the perplexity loss should be negligible compared to standard GGUF.",
          "score": 1,
          "created_utc": "2026-02-18 07:19:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60jzvw",
          "author": "Beautiful-Use6759",
          "text": "Is it just me, or is the $1/hour price point basically making the \"local vs API\" debate irrelevant for everything except privacy?",
          "score": 1,
          "created_utc": "2026-02-18 07:28:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60kzrm",
          "author": "No_Imagination_2813",
          "text": "I've been testing the 10B active MoE setup - it's punchy. It handles multi-file edits much better than the older, larger models that usually just get confused by the context.",
          "score": 1,
          "created_utc": "2026-02-18 07:37:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60lsi8",
          "author": "DeanICER",
          "text": "The RL scaling they mentioned in the tech blog is the real secret sauce. Quantization is great, but the base reasoning capability is what sets M2.5 apart.",
          "score": 1,
          "created_utc": "2026-02-18 07:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60nrkh",
          "author": "No-Friendship-3645",
          "text": "Need that GGUF so I can run it in LM Studio and stop paying the \"convenience tax\" on other platforms.",
          "score": 1,
          "created_utc": "2026-02-18 08:03:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60owqf",
          "author": "Least_Interest_6726",
          "text": "80.2% on SWE-Bench is insane for this size. If the quantization holds up, this becomes the default choice for any local agentic workflow.",
          "score": 1,
          "created_utc": "2026-02-18 08:13:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60q0fd",
          "author": "Complex_Shape4188",
          "text": "Everyone is hyped for Llama 4 or whatever, but MiniMax is quietly shipping the most practical dev tool of the year.",
          "score": 1,
          "created_utc": "2026-02-18 08:24:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4jqpn",
      "title": "Updates to Qwen3-Coder-Next broke my setup! :(",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r4jqpn/updates_to_qwen3codernext_broke_my_setup/",
      "author": "nunodonato",
      "created_utc": "2026-02-14 12:44:54",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 0.82,
      "text": "Hi guys,\n\nToday my container downloaded the new GGUFs that were recently updated, and since then I haven't been able to use the model.\n\nIt loads fine, but when I try to make a request it crashes\n\n`[2026-02-14T12:33:58.483Z] [zm62x] srv params_from_: Chat format: Qwen3 Coder`  \n`[2026-02-14T12:33:58.483Z] [zm62x] slot get_availabl: id 0 | task -1 | selected slot by LRU, t_last = -1`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot launch_slot_: id 0 | task -1 | sampler chain: logits -> penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> ?temp-ext -> dist`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot launch_slot_: id 0 | task 0 | processing task, is_child = 0`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot update_slots: id 0 | task 0 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 123`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot update_slots: id 0 | task 0 | n_tokens = 0, memory_seq_rm [0, end)`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot update_slots: id 0 | task 0 | prompt processing progress, n_tokens = 123, batch.n_tokens = 123, progress = 1.000000`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot update_slots: id 0 | task 0 | prompt done, n_tokens = 123, batch.n_tokens = 123`\n\n`[2026-02-14T12:33:58.483Z] [zm62x] slot init_sampler: id 0 | task 0 | init sampler, took 0.03 ms, tokens: text = 123, total = 123`\n\n`[2026-02-14T12:33:58.697Z] [zm62x] /app/ggml/src/ggml-cuda/ggml-cuda.cu:97: CUDA error`\n\n`[2026-02-14T12:33:58.697Z] [zm62x] CUDA error: an illegal memory access was encountered`\n\n`[2026-02-14T12:33:58.697Z] [zm62x] current device: 0, in function launch_mul_mat_q at /app/ggml/src/ggml-cuda/template-instances/../mmq.cuh:3893`\n\n`[2026-02-14T12:33:58.697Z] [zm62x] cudaFuncSetAttribute((mul_mat_q<type, mmq_x, false>), cudaFuncAttributeMaxDynamicSharedMemorySize, nbytes_shared)`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libggml-base.so.0(+0x1826b)[0x7edca2b7926b]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libggml-base.so.0(ggml_print_backtrace+0x21c)[0x7edca2b796cc]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libggml-base.so.0(ggml_abort+0x15b)[0x7edca2b798ab]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(_Z15ggml_cuda_errorPKcS0_S0_iS0_+0xb7)[0x7edc9a963057]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(+0x726e8c)[0x7edc9aec4e8c]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(_Z19ggml_cuda_mul_mat_qR25ggml_backend_cuda_contextPK11ggml_tensorS3_S3_PS1_+0xb63)[0x7edc9a991ba3]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(+0x1d6af4)[0x7edc9a974af4]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(+0x1db507)[0x7edc9a979507]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/libggml-cuda.so(+0x1ddd2e)[0x7edc9a97bd2e]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libggml-base.so.0(ggml_backend_sched_graph_compute_async+0x817)[0x7edca2b95e37]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libllama.so.0(_ZN13llama_context13graph_computeEP11ggml_cgraphb+0xa1)[0x7edca2cd7dc1]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libllama.so.0(_ZN13llama_context14process_ubatchERK12llama_ubatch14llm_graph_typeP22llama_memory_context_iR11ggml_status+0x114)[0x7edca2cd9884]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libllama.so.0(_ZN13llama_context6decodeERK11llama_batch+0x386)[0x7edca2ce0d76]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] libllama.so.0(llama_decode+0xf)[0x7edca2ce280f]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/llama-server(+0x152118)[0x61809b240118]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/llama-server(+0x199b0e)[0x61809b287b0e]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/llama-server(+0xb2920)[0x61809b1a0920]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca)[0x7edca25e41ca]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b)[0x7edca25e428b]`\n\n`[2026-02-14T12:33:58.735Z] [zm62x] /app/llama-server(+0xb7b25)[0x61809b1a5b25]`\n\nAlready tried reducing context significantly, but the problem seems to be somewhere else :/\n\n**startup params**: -hf unsloth/Qwen3-Coder-Next-GGUF:Q6\\_K -c 32000 -ngl 99 -np 1 -t 16 -cb --port 8080 --host [0.0.0.0](http://0.0.0.0) \\-b 8192 -ub 4096 -fa auto --no-mmap --no-warmup --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --repeat-penalty 1.05 --jinja --seed 3407\n\n**hardware:** RTX PRO 6000\n\n**llama-server release** 8040  (latest)\n\n**base image:** [ghcr.io/ggml-org/llama.cpp:server-cuda13](http://ghcr.io/ggml-org/llama.cpp:server-cuda13)\n\nhelp?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r4jqpn/updates_to_qwen3codernext_broke_my_setup/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5c19iv",
          "author": "nunodonato",
          "text": "Tried Q8\\_0 and it worked... ",
          "score": 2,
          "created_utc": "2026-02-14 13:08:00",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5dej35",
              "author": "TokenRingAI",
              "text": "FWIW, VLLM is a better choice right now for this particular model on an RTX 6000",
              "score": 2,
              "created_utc": "2026-02-14 17:38:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dpsw2",
                  "author": "yoracale",
                  "text": "On a RTX 6000 though. Most people run models via unified memory Macs or vram + extra ram",
                  "score": 3,
                  "created_utc": "2026-02-14 18:34:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5cdhbw",
              "author": "some_user_2021",
              "text": "Probably because the fix was applicable to lower quantizations.",
              "score": 1,
              "created_utc": "2026-02-14 14:25:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cnnjl",
          "author": "Responsible-Stock462",
          "text": "Try the llama-fit-params -m model [special params that affect context and or size]\n\nIf it outputs ot use them for your llama-server",
          "score": 2,
          "created_utc": "2026-02-14 15:22:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5byih7",
          "author": "Altruistic_Call_3023",
          "text": "I know there were updates to llama recently to make this work.  Do you know when the container was last updated?",
          "score": 1,
          "created_utc": "2026-02-14 12:48:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c0m52",
              "author": "yoracale",
              "text": "Do you know which updates? Do you have a link to the PR thanks!",
              "score": 1,
              "created_utc": "2026-02-14 13:03:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5c1847",
                  "author": "Altruistic_Call_3023",
                  "text": "I should wait for coffee before posting. I mean the changes that were made for performance.  The changes to make things work were for stepfun.  Although a couple weeks ago there was a PR for coder-next that did require new GGUF iirc?  I just was wondering how old the build of container was.  If more than a week, might we be that.  Sorry for my early confusing posts.",
                  "score": 1,
                  "created_utc": "2026-02-14 13:07:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5okiqq",
          "author": "LA_rent_Aficionado",
          "text": "I‚Äôd recommend building llama-server locally from source, no idea why you‚Äôre using a docker, it‚Äôs probably misconfigured for your setup or an older version",
          "score": 1,
          "created_utc": "2026-02-16 13:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5on3p8",
              "author": "nunodonato",
              "text": "I'm using the latest version.¬†\nIt's docker because it's for a production deployment",
              "score": 1,
              "created_utc": "2026-02-16 14:03:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ptro8",
                  "author": "LA_rent_Aficionado",
                  "text": "Fair, if you can configure the llama.cpp build settings and rebuild I recons various combinations of MMQ, Cublas and Graphs on/off until it works.\n\nIt seems like a kernel level error and I saw some recent PRs for Next graph duplication - the llama.cpp team has acknowledged Next support has some gaps so I assume the default docket build is likely pipelined into some of these gaps - especially if other models work.",
                  "score": 1,
                  "created_utc": "2026-02-16 17:30:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r993q3",
      "title": "Any good model that can even run on 0.5 GB of RAM (512 MB of RAM)?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r993q3/any_good_model_that_can_even_run_on_05_gb_of_ram/",
      "author": "Ok-Type-7663",
      "created_utc": "2026-02-19 19:40:20",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 0.82,
      "text": "I'm testing local AI limits. Also recommend a OS :3 and Hugging Face repo and great quant :D",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r993q3/any_good_model_that_can_even_run_on_05_gb_of_ram/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6aqhcs",
          "author": "RudeboyRudolfo",
          "text": "functiongemma. But it's more for running structured output and tool calling. It's not really a LLM.",
          "score": 7,
          "created_utc": "2026-02-19 19:53:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b3ya2",
              "author": "Apart_Refrigerator27",
              "text": "What are the use cases for functiongemma?\nit is really worth the time?",
              "score": 2,
              "created_utc": "2026-02-19 20:59:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b7j5i",
                  "author": "RudeboyRudolfo",
                  "text": "That's your imagination. There is a lot possible with it. Pretty sure you can get it over ollama. So not much time wasted. I built a small assistant with it, which can open and edit files and search man pages.",
                  "score": 1,
                  "created_utc": "2026-02-19 21:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ave8q",
          "author": "PlayerWell",
          "text": "I think the Gemma 3 270m will work. It's not great, but it can be successful if fine-tuned",
          "score": 7,
          "created_utc": "2026-02-19 20:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bxr6f",
              "author": "--Spaci--",
              "text": "this is the only real correct option tbf qwen3 0.6b is to large with a context window",
              "score": 1,
              "created_utc": "2026-02-19 23:34:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ar209",
          "author": "Jan49_",
          "text": "No good model can run on half a GB of RAM... Lol\n\nThe smallest LLM, that I know of, that can barely form sentences is Qwen3 0.6B. The q2 quant from unsloth is sub 300mb in size. But then you would still need RAM for context and general overhead.\n\nDoes your system only have 512mb RAM? Then the OS would probably take up the whole RAM on its own. Try Linux XFCE or even better no DE at all.",
          "score": 3,
          "created_utc": "2026-02-19 19:55:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bun9h",
              "author": "logos_flux",
              "text": "It's not a system it's a ti-83",
              "score": 1,
              "created_utc": "2026-02-19 23:16:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ayjfh",
          "author": "BenniB99",
          "text": "You could try the 2bit dynamic quants of LFM2-1.2B but that is probably going to be a vegetable",
          "score": 2,
          "created_utc": "2026-02-19 20:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b3dnv",
          "author": "DuckyBlender",
          "text": "Gemma 3 270M",
          "score": 2,
          "created_utc": "2026-02-19 20:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b7wc8",
          "author": "Significant_Fig_7581",
          "text": "Qwen 0.6B",
          "score": 2,
          "created_utc": "2026-02-19 21:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ccw9x",
          "author": "host3000",
          "text": "Qwen2.5-0.5b",
          "score": 2,
          "created_utc": "2026-02-20 01:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6civqc",
          "author": "catplusplusok",
          "text": "BitNet?",
          "score": 2,
          "created_utc": "2026-02-20 01:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ar1om",
          "author": "supportend",
          "text": "Kitten TTS, 14, 40 and 80M.",
          "score": 1,
          "created_utc": "2026-02-19 19:55:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7dcdm",
      "title": "Best Unsloth model for 12GB RAM + GTX 1050 (3GB VRAM) for inference only?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r7dcdm/best_unsloth_model_for_12gb_ram_gtx_1050_3gb_vram/",
      "author": "Ok-Type-7663",
      "created_utc": "2026-02-17 17:55:24",
      "score": 6,
      "num_comments": 14,
      "upvote_ratio": 0.8,
      "text": "I‚Äôm trying to run a local LLM using Unsloth for inference only (NOT finetuning), and I want the best model my hardware can handle smoothly.\n\n**My specs:**\n\n* RAM: 12GB\n* GPU: GTX 1050 (3GB VRAM)\n* OS: Linux\n* Goal: inference/chat, not training\n* Prefer GGUF or Unsloth-compatible models\n\n**Priorities:**\n\n* Best quality possible within my limits\n* Stable inference (no crashes / OOM)\n* Good reasoning and instruction following\n* Fast enough to be usable\n\n**Questions:**\n\n1. What is the BEST model size I can realistically run? (1B, 3B, 4B, etc)\n2. Which specific Unsloth model do you recommend?\n3. What quant should I use? (Q4\\_K\\_M, Q5\\_K\\_M, etc)\n4. Should I use GPU offloading or pure CPU with my 3GB VRAM?\n\nIf possible, please recommend exact HF model IDs.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r7dcdm/best_unsloth_model_for_12gb_ram_gtx_1050_3gb_vram/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5wktmj",
          "author": "--Spaci--",
          "text": "lfm 1.2b",
          "score": 2,
          "created_utc": "2026-02-17 18:06:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wkx4z",
              "author": "--Spaci--",
              "text": "LiquidAI/LFM2.5-1.2B-Thinking",
              "score": 2,
              "created_utc": "2026-02-17 18:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wlnzw",
                  "author": "Ok-Type-7663",
                  "text": "which quant?",
                  "score": 1,
                  "created_utc": "2026-02-17 18:10:05",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5y5bq3",
                  "author": "ctanna5",
                  "text": "This.",
                  "score": 1,
                  "created_utc": "2026-02-17 22:32:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wqigx",
          "author": "Fresh_Finance9065",
          "text": "You will want models under 1.5B on gpu, or under 3B on cpu.\n\nFor gpu, lfm2.5-1.2b is as good as it gets. You wouldn't really chat with it through, more of 1 shotting before you go oom.\n\nFor cpu, you can try granite 4 micro. It is not very smart for its size, but its also very hard to oom with it. You don't have much space to reason with. Qwen 3VL 4B and Ministral 3B are smarter for the suze but goodluck with oom.\n\nFor gpu, use Q8_K_XL from unsloth or Q8_0. For cpu, use Q6_K_XL from unsloth or any Q6_K imatrix.",
          "score": 2,
          "created_utc": "2026-02-17 18:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wu22l",
              "author": "Crafty_Ball_8285",
              "text": "I had to use iq4 instead of iq6 on a 3b model on Mac because q6 was chugging the full vram",
              "score": 1,
              "created_utc": "2026-02-17 18:48:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xncwb",
                  "author": "Fresh_Finance9065",
                  "text": "I find models this size lose too much instruction following ability and hallucinate more if they are quantized too far, thats why i prefer larger quants than other ppl",
                  "score": 1,
                  "created_utc": "2026-02-17 21:06:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o605a1i",
          "author": "Mabuse046",
          "text": "I'm not exactly following what you're asking, so just to make sure we're on the same page, Unsloth is a python library for working with / training transformers models - the primary format you find on Huggingface. It's not like an app you load the model in. You write python code and inside the code you call unsloth/transformers/trl to load the model and then you can do whatever you want to it with your code. A GGUF is a custom format used by llama.cpp - and llama.cpp comes with the python script to convert a transformers model to a GGUF and another script to Quantize it. An \"Unsloth model\" is just a normal model that the unsloth team tuned up, but it's still in the standard transformers format and then optionally converted into a handful of other proprietary formats like GGUF. So my question is, are you asking for models that you can load using the unsloth library in python, or models that have been tweaked by the unsloth team?",
          "score": 2,
          "created_utc": "2026-02-18 05:25:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7c9ip",
      "title": "Creating Dynamic 2.0 quants",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r7c9ip/creating_dynamic_20_quants/",
      "author": "de4dee",
      "created_utc": "2026-02-17 17:17:46",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "How do I create Unsloth Dynamic 2.0 quants (UD-Q4\\_K\\_XL ...) ?\n\nThanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r7c9ip/creating_dynamic_20_quants/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5xh2l3",
          "author": "yoracale",
          "text": "Utilize llama.cpp, utilize the imatrix files and investigate layers which are quantized or updated to create ones for the certain model yourself.",
          "score": 2,
          "created_utc": "2026-02-17 20:36:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r46cl8",
      "title": "GLM-4.7-Flash-GGUF missing first  <think>",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r46cl8/glm47flashgguf_missing_first_think/",
      "author": "techmago",
      "created_utc": "2026-02-14 00:43:02",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.67,
      "text": "Hello.  \nI'm using:\n\n[hf.co/unsloth/GLM-4.7-Flash-GGUF:Q8\\_0](http://hf.co/unsloth/GLM-4.7-Flash-GGUF:Q8_0)\n\nwith ollama 1.16.1 + openwebui.\n\nWhen GLM does the thinking, it's not oppening the thinking block\n\nThis make a mess... a bunch o redundant text, a random </thinking> closing nothing.\n\n    \\``docker run -d --name ollama `\n    --restart=unless-stopped \\\n    --gpus=all \\\n    -v /mnt/nvme/ollama/.ollama:/root/.ollama \\\n    --network=host \\\n    -e OLLAMA_VULKAN=0 \\\n    -e OLLAMA_FLASH_ATTENTION=0 \\\n    -e OLLAMA_KV_CACHE_TYPE=q8_0 \\\n    -e OLLAMA_NEW_ENGINE=1 \\\n    -e OLLAMA_NUM_PARALLEL=1 \\\n    -e OLLAMA_DEBUG=0 \\\n    -e GIN_MODE=release \\\n    -e OLLAMA_NEW_ESTIMATES=1 \\\n    -e OLLAMA_MAX_LOADED_MODELS=2 \\\n    -e OLLAMA_KEEP_ALIVE=320 \\\n    -e OLLAMA_CONTEXT_LENGTH=48128 \\\n    -e OLLAMA_NUM_PREDICT=600 \\\n    $IMAGE:$IMAGE_TAG\n    \\```\n\nAm i doing something wrong, or is the model that is broke?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r46cl8/glm47flashgguf_missing_first_think/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o59hbki",
          "author": "yoracale",
          "text": "We wrote many times in our guide not to use Ollama with Glm-4.7-flash gguf it doesn't work: https://unsloth.ai/docs/models/glm-4.7-flash",
          "score": 15,
          "created_utc": "2026-02-14 00:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cros0",
              "author": "_twrecks_",
              "text": "You need to put glm4 7 render and parser parameters in the ollama modelfile. Look at the modelfile from a glm4.7 family model from ollama.com.",
              "score": 1,
              "created_utc": "2026-02-14 15:43:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59ynau",
              "author": "techmago",
              "text": "I thought it was already fixed on the last version. Sorry.",
              "score": -1,
              "created_utc": "2026-02-14 02:36:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5atp72",
          "author": "eleqtriq",
          "text": "Dude.  Don‚Äôt use Ollama in a container.  At least use llama.cpp.",
          "score": 7,
          "created_utc": "2026-02-14 06:28:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a7tim",
          "author": "admajic",
          "text": "Research how to use it with llama.cpp\nIt dosen't work with vllm with a gguf atm either.\n\nThen go for a walk as it does it's thing. It's very slow on my 3090",
          "score": 5,
          "created_utc": "2026-02-14 03:37:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ab20w",
              "author": "techmago",
              "text": "The reason i don't use llama is ease of use. I did pilot it with llama-swap, but it was still... a chore.  \nI have around 10\\~15 models, i swap then A LOT and i use different context sizes depending on the caller. lamma cpp is to stiff for the way i use.\n\nI know the ollama crew is not exactly... friendly. But in my defense, there is people who like apple, and that company is the most anti-consumer thing i know.",
              "score": 0,
              "created_utc": "2026-02-14 04:00:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5b0sdd",
                  "author": "Zestyclose-Shift710",
                  "text": "llama.cpp has a router mode now, and supports .ini file models configuration¬†\n\n\nThey have official docker images too¬†\n\n\nAnd a new builtin webui\n\n\nIt's gotten a lot easier to use, is what I'm saying¬†",
                  "score": 8,
                  "created_utc": "2026-02-14 07:33:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5b8wc0",
                  "author": "admajic",
                  "text": "You can define every tiny detail ctx size temperature etc using llama-swap or llama.cpp and an ini",
                  "score": 5,
                  "created_utc": "2026-02-14 08:51:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r48dzu",
      "title": "First time fine tuning and need advice for tuning unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r48dzu/first_time_fine_tuning_and_need_advice_for_tuning/",
      "author": "ClientPrize9151",
      "created_utc": "2026-02-14 02:17:40",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "Hi, guys any advice would be nice. I will provide my current settings that I will be using and would appropriate any feedback to ensure as much accuracy  from the input and output from my dataset without over fitting. Any advice on the settings and if I can improved them to get better results would be really appropriated. Thanks.\n\nfrom unsloth import FastLanguageModel\n\nimport torch\n\nmodel\\_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n\nmax\\_seq\\_length = 2048  # Choose sequence length\n\ndtype = None  # Auto detection\n\n\\# Load model and tokenizer\n\nmodel, tokenizer = FastLanguageModel.from\\_pretrained(\n\nmodel\\_name=model\\_name,\n\nmax\\_seq\\_length=max\\_seq\\_length,\n\ndtype=dtype,\n\nload\\_in\\_4bit=True,\n\n)\n\n\\# Add LoRA adapters\n\nmodel = FastLanguageModel.get\\_peft\\_model(\n\nmodel,\n\nr=64,  # LoRA rank - higher = more capacity, more memory\n\ntarget\\_modules=\\[\n\n\"q\\_proj\", \"k\\_proj\", \"v\\_proj\", \"o\\_proj\",\n\n\"gate\\_proj\", \"up\\_proj\", \"down\\_proj\",\n\n\\],\n\nlora\\_alpha=128,  # LoRA scaling factor (usually 2x rank)\n\nlora\\_dropout=0,  # Supports any, but = 0 is optimized\n\nbias=\"none\",     # Supports any, but = \"none\" is optimized\n\nuse\\_gradient\\_checkpointing=\"unsloth\",  # Unsloth's optimized version\n\nrandom\\_state=3407,\n\nuse\\_rslora=False,  # Rank stabilized LoRA\n\nloftq\\_config=None, # LoftQ\n\n)\n\nfrom trl import SFTTrainer\n\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n\nmodel=model,\n\ntokenizer=tokenizer,\n\ntrain\\_dataset=dataset,\n\ndataset\\_text\\_field=\"text\",\n\nmax\\_seq\\_length=max\\_seq\\_length,\n\ndataset\\_num\\_proc=2,\n\nargs=TrainingArguments(\n\nper\\_device\\_train\\_batch\\_size=1,\n\ngradient\\_accumulation\\_steps=8,\n\ngradient\\_checkpointing=True,\n\nwarmup\\_steps=10,\n\nnum\\_train\\_epochs=3,\n\nlearning\\_rate=2e-4,\n\nfp16=not torch.cuda.is\\_bf16\\_supported(),\n\nbf16=torch.cuda.is\\_bf16\\_supported(),\n\nlogging\\_steps=25,\n\noptim=\"adamw\\_8bit\",\n\nweight\\_decay=0.01,\n\nlr\\_scheduler\\_type=\"linear\",\n\nseed=3407,\n\noutput\\_dir=\"outputs\",\n\nsave\\_strategy=\"epoch\",\n\nsave\\_total\\_limit=2,\n\ndataloader\\_pin\\_memory=False,\n\n),\n\n)\n\nExample of my dataset shown below- input receipt data and output is insight data.\n\n    [\n      {\n        \"id\": 1,\n        \"period_days\": 3,\n        \"receipts\": [\n          {\n            \"merchant_name\": \"WH Smith\",\n            \"date\": \"Jan 29, 2026\",\n            \"currency\": \"¬£\",\n            \"total\": 5.31,\n            \"category\": \"Other\"\n          },\n          {\n            \"merchant_name\": \"WH Smith\",\n            \"date\": \"Jan 29, 2026\",\n            \"currency\": \"¬£\",\n            \"total\": 15.07,\n            \"category\": \"Other\"\n          },\n          {\n            \"merchant_name\": \"Card Factory\",\n            \"date\": \"Jan 29, 2026\",\n            \"currency\": \"¬£\",\n            \"total\": 5.82,\n            \"category\": \"Other\"\n          },\n          {\n            \"merchant_name\": \"Tesco\",\n            \"date\": \"Jan 30, 2026\",\n            \"currency\": \"¬£\",\n            \"total\": 72.92,\n            \"category\": \"Groceries\"\n          }\n        ],\n        \"insights\": [\n          {\n            \"title\": \"You spent ¬£26.\",\n            \"category_tag\": \"Spending Insight\",\n            \"last_days\": \"Last 3 Days\",\n            \"date_generated\": \"Jan 30, 2026\",\n            \"description\": \"You spent ¬£26.20 on other 3 times. Small reductions here could add up significantly.\",\n            \"tag\": \"Other\"\n          },\n          {\n            \"title\": \"Groceries totaled ¬£72.\",\n            \"category_tag\": \"Spending Insight\",\n            \"last_days\": \"Last 3 Days\",\n            \"date_generated\": \"Jan 30, 2026\",\n            \"description\": \"Groceries totaled ¬£72.92 this period. Compare prices across stores for better deals.\",\n            \"tag\": \"Groceries\"\n          }\n        ]\n\nStep | Training Loss so far\n\nhttps://preview.redd.it/oiezdk0mkdjg1.png?width=804&format=png&auto=webp&s=628a824d36f704627c79b0e90ba1b6d5ed7cceb8\n\nNote: I have an i9, 4070 8gb vram and 32gb ram- Lenovo Legion 5 Pro.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r48dzu/first_time_fine_tuning_and_need_advice_for_tuning/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5c0qdv",
          "author": "yoracale",
          "text": "Everything you need for overfitting etc is in our docs: [https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)",
          "score": 1,
          "created_utc": "2026-02-14 13:04:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c3q2a",
              "author": "ClientPrize9151",
              "text": "Thanks got what i needed.",
              "score": 1,
              "created_utc": "2026-02-14 13:24:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r5b5gu",
      "title": "How can I train a small model to self-correct without encouraging it to deliberately answer wrong at first?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r5b5gu/how_can_i_train_a_small_model_to_selfcorrect/",
      "author": "PlayerWell",
      "created_utc": "2026-02-15 10:13:52",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 0.81,
      "text": "I want to finetune a small model which is Gemma 3 1b, to do some tasks and learn how to make self correction. I'm training it using conversation-style examples in two formats:\n\n\n\n**Plain task examples:**\n\n`User: Task question`\n\n`Model: Output`\n\n\n\n**Self-correction examples:**\n\n`User: Task question`\n\n`Model: Output`\n\n`User: Please correct the output using these steps. The output is wrong.`\n\n`Model: New Output`\n\n\n\nWill training with these \"self-correction\" dialogues cause the model to intentionally produce wrong initial outputs just to trigger corrections later? If that's a possible failure, how can I avoid it while still teaching reliable self-correction?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r5b5gu/how_can_i_train_a_small_model_to_selfcorrect/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o5hk6kl",
          "author": "Responsible-Stock462",
          "text": "You can only change the model with fine tuning. If that works depends on the base knowledge of the model and the task you have.",
          "score": 3,
          "created_utc": "2026-02-15 10:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hpfa8",
          "author": "aaronr_90",
          "text": "Masking‚Ä¶it would be something similar to Unsloth‚Äôs ‚Äútrain on responses only‚Äù method but would need a custom implementation to mask everything before the New Output.",
          "score": 3,
          "created_utc": "2026-02-15 11:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5k7a63",
              "author": "PlayerWell",
              "text": "Yes. Thank you. I'll try to do some custom masking",
              "score": 1,
              "created_utc": "2026-02-15 19:45:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}