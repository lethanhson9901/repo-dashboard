{
  "metadata": {
    "last_updated": "2026-02-12 17:15:22",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 6,
    "total_comments": 21,
    "file_size_bytes": 30664
  },
  "items": [
    {
      "id": "1r10d9l",
      "title": "GLM-4.7-Flash is now the #1 most downloaded model on Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/n9wlycyu2oig1.png",
      "author": "yoracale",
      "created_utc": "2026-02-10 13:12:55",
      "score": 289,
      "num_comments": 40,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r10d9l/glm47flash_is_now_the_1_most_downloaded_model_on/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4m748w",
          "author": "RedKnightRG",
          "text": "*Raises a toast to everyone who bought 128GB of RAM and dual 3090s or similar thousands of dollars ago*",
          "score": 16,
          "created_utc": "2026-02-10 14:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7sdz",
              "author": "yoracale",
              "text": "You don't need that much to run the 30B model. 24GB is enough to get it working well with quality",
              "score": 7,
              "created_utc": "2026-02-10 14:09:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m8f6l",
                  "author": "RedKnightRG",
                  "text": "Oh my B I saw GLM but read Qwen and thought this was the 80B model.  You're right 1 3090 is fine.   So I'll raise a toast to the guys who bought their single 3090 hundreds of dollars ago!  üòÖ",
                  "score": 3,
                  "created_utc": "2026-02-10 14:13:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ohfii",
              "author": "timbo2m",
              "text": "So mad I didn't stack my machine with 256GB ram when I bought it a couple of years ago and it was like a couple of hundred bucks for 32GB. 20/20 hindsight!",
              "score": 2,
              "created_utc": "2026-02-10 20:35:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4x1hap",
                  "author": "epicskyes",
                  "text": "I‚Äôm so mad i didn‚Äôt stack up on 1tb of ram it was only a couple g‚Äôs now my 256gb costs the same amount as 1tb did fml ü§¶",
                  "score": 1,
                  "created_utc": "2026-02-12 02:57:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lz42d",
          "author": "Far-Donut-1177",
          "text": "I envy folks with the hardware that could run 100B above. The best I could do is 30 and that's stretching it.",
          "score": 12,
          "created_utc": "2026-02-10 13:21:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m0j7v",
              "author": "Significant_Fig_7581",
              "text": "Heyyy let's just be happy cause this post just proved that most of us are and these download numbers are surely going to be an encouragement for most of them to release good models at that size, Qwen is releasing a 35B MOE btw...",
              "score": 8,
              "created_utc": "2026-02-10 13:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4m3n6z",
                  "author": "ismaelgokufox",
                  "text": "Indeed. Being able to run this at good speeds in 16GB VRAM is great!",
                  "score": 3,
                  "created_utc": "2026-02-10 13:47:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mujj1",
                  "author": "ScoreUnique",
                  "text": "35B MoE sounds like a dream after seeing qwen 3 next coder.",
                  "score": 3,
                  "created_utc": "2026-02-10 16:04:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m5rwq",
          "author": "MrMrsPotts",
          "text": "I need a version I can squeeze into 12GB of VRAM.",
          "score": 8,
          "created_utc": "2026-02-10 13:58:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ming1",
              "author": "eesnimi",
              "text": "Unload the expert layers to CPU and it will be a nice squeeze :) I have squeezed a Q6\\_K\\_XL quant with 130k token window to 11GB VRAM with all the expert layers unloaded to CPU, getting around 11-13t/s that's quite usable.",
              "score": 6,
              "created_utc": "2026-02-10 15:06:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mqv58",
                  "author": "MrMrsPotts",
                  "text": "Is there something I can read about how to do that?",
                  "score": 2,
                  "created_utc": "2026-02-10 15:46:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4o7yqv",
                  "author": "ethereal_intellect",
                  "text": "Commenting so I'll remember to try this later. I'm on lm studio out of convenience, which app are you on",
                  "score": 1,
                  "created_utc": "2026-02-10 19:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4thuda",
                  "author": "Confident-Ad-2688",
                  "text": "Can you share the command with all parameters I ,also have 12gb vram but not able to run a good speed .",
                  "score": 1,
                  "created_utc": "2026-02-11 16:04:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4m8sty",
              "author": "NorthEastCalifornia",
              "text": "glm-4.7-flash REAP 50%",
              "score": 1,
              "created_utc": "2026-02-10 14:15:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ma897",
                  "author": "paq85",
                  "text": "I've tried the REAP version and couldn't make it work with open code at all... As if it end in some loop, even though I run it with same llamacpp params as the full version.",
                  "score": 3,
                  "created_utc": "2026-02-10 14:22:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m9zsu",
          "author": "paq85",
          "text": "This model is definitely the best 30b agentic coding model I've seen so far.",
          "score": 6,
          "created_utc": "2026-02-10 14:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mml3u",
          "author": "alfpacino2020",
          "text": "[https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) mucho mejor este mas lento pero   no falla como loco   no se tilda como gml 4.7",
          "score": 3,
          "created_utc": "2026-02-10 15:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mr3bb",
              "author": "MrMrsPotts",
              "text": "I would need the 2 bit quantisation though which might not be good.",
              "score": 1,
              "created_utc": "2026-02-10 15:47:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4msb5x",
                  "author": "alfpacino2020",
                  "text": "https://preview.redd.it/k4kc0it8voig1.png?width=974&format=png&auto=webp&s=5c2f0a31e47adbbb79d2bae65ac01c10de4a04be\n\nejemplo en iq4 me da casi  23 tokens x seg el qwen me da 15 x seg osea no gran diferencia usando Qwen3-Coder-Next-MXFP4\\_MOE.gguf",
                  "score": 1,
                  "created_utc": "2026-02-10 15:53:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4muk1k",
                  "author": "alfpacino2020",
                  "text": "tengo 5070 16gb vram 48ram  aca el qwen \n\nhttps://preview.redd.it/916th7phxoig1.png?width=1024&format=png&auto=webp&s=78fb7ae7e7bc1dd6ca8581429839edb61bc221e0\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 16:04:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4q1vqh",
          "author": "PassageInfamous3639",
          "text": "Everyone‚Äôs chasing 100B+ like it‚Äôs the promised land and I‚Äôm over here just wanting a 7‚Äì30B model that doesn‚Äôt hallucinate my whole repo üò≠\n\nIf GLM-4.7-Flash is actually dependable, that‚Äôs a huge W.",
          "score": 3,
          "created_utc": "2026-02-11 01:31:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qyfsm",
              "author": "yoracale",
              "text": "Very soon there will be more!",
              "score": 1,
              "created_utc": "2026-02-11 05:00:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nk2o0",
          "author": "Megalion75",
          "text": "What's the best model for local SWE?  Or is there a model that can run locally that is reliable?",
          "score": 1,
          "created_utc": "2026-02-10 18:01:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50340j",
          "author": "Lesteriax",
          "text": "What is the best available model in could run on rtx pro 6000?",
          "score": 1,
          "created_utc": "2026-02-12 16:12:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r13pk4",
      "title": "Faster MoE LLM Training now in Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/qfoq8mirjoig1.png",
      "author": "yoracale",
      "created_utc": "2026-02-10 15:25:44",
      "score": 136,
      "num_comments": 17,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r13pk4/faster_moe_llm_training_now_in_unsloth/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4mp3bh",
          "author": "joninco",
          "text": "Multi gpu?",
          "score": 5,
          "created_utc": "2026-02-10 15:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mqz3p",
              "author": "yoracale",
              "text": "Works already but still preliminary. It's out soon.\n\nGuide for now: [https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth](https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth)",
              "score": 4,
              "created_utc": "2026-02-10 15:47:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4muop1",
          "author": "arman-d0e",
          "text": "Transformers v5 fully optimized? Do the optimizations apply to Qwen3-Coder-Next?",
          "score": 4,
          "created_utc": "2026-02-10 16:04:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mvode",
              "author": "danielhanchen",
              "text": "Not yet - we plan to add support for Qwen3-Next next!",
              "score": 11,
              "created_utc": "2026-02-10 16:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mwszz",
                  "author": "arman-d0e",
                  "text": "Much appreciated, thanks for the updates.",
                  "score": 3,
                  "created_utc": "2026-02-10 16:14:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4qyfj9",
                  "author": "Desperate-Sir-5088",
                  "text": "Would you make 4bit-bnb quant for the Qwen3-Coder-Next model? I expected QLoRA finetuing of this model with only one H100 80GB or RTX PRO 6000 96GB",
                  "score": 1,
                  "created_utc": "2026-02-11 05:00:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n0rp8",
          "author": "sterby92",
          "text": "Will we also get vulkan or rocm support at some point? ",
          "score": 3,
          "created_utc": "2026-02-10 16:32:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n5cn4",
              "author": "yoracale",
              "text": "Yes, we're announcing very soon. As soon as this month. It already works: https://unsloth.ai/docs/get-started/install/amd",
              "score": 6,
              "created_utc": "2026-02-10 16:53:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nobiz",
                  "author": "sterby92",
                  "text": "Amazing, thank you üôå",
                  "score": 1,
                  "created_utc": "2026-02-10 18:21:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qmmrc",
          "author": "Old-Nobody-2010",
          "text": "What is the minimum VRAM required to fine-tune GLM-4.7-Flash with Unsloth      30b a3b model",
          "score": 2,
          "created_utc": "2026-02-11 03:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qs17q",
              "author": "yoracale",
              "text": "I think around 20GB VRAM",
              "score": 1,
              "created_utc": "2026-02-11 04:14:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sv1cm",
          "author": "Mac_NCheez_TW",
          "text": "What did we do to deserve this work üò≠. You folks are like world heros to poor ram serfs, the Robin hood to us low end peasents stealing from the Nvidia's tax demands and giving us another couple years on our garbage rigs.¬†",
          "score": 2,
          "created_utc": "2026-02-11 14:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pj0od",
          "author": "codeblockzz",
          "text": "Train or fine tune?",
          "score": 0,
          "created_utc": "2026-02-10 23:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ptl9r",
              "author": "yoracale",
              "text": "Both, works for prettaining, FFT and Lora",
              "score": 4,
              "created_utc": "2026-02-11 00:42:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4q3e6p",
          "author": "PassageInfamous3639",
          "text": "12x faster + less VRAM is actually insane. This is exactly the kind of ‚Äúmake local practical‚Äù progress I love seeing.\n\nAlso shoutout to the RTX 3090 mention ‚Äî my GPU just felt seen üòÇ",
          "score": 0,
          "created_utc": "2026-02-11 01:40:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2soh5",
      "title": "Run GLM-5 Locally Guide!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/u0l6522992jg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-12 12:55:16",
      "score": 56,
      "num_comments": 9,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r2soh5/run_glm5_locally_guide/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4z3l8b",
          "author": "arm2armreddit",
          "text": "Too large for my potato (2x48 GB VRAM). I dream of getting more money to get more VRAM... you think you are almost there, then bam! 1.5 TB VRAM required.",
          "score": 14,
          "created_utc": "2026-02-12 13:08:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4xwc",
              "author": "danielhanchen",
              "text": "RAM offloading also works via --fit on, but yes more VRAM the better :(",
              "score": 3,
              "created_utc": "2026-02-12 13:16:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zitig",
          "author": "kingabzpro",
          "text": "I am running (2-bit model) on the single H200 and getting 9 tps. ",
          "score": 3,
          "created_utc": "2026-02-12 14:33:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zwg3y",
              "author": "fragment_me",
              "text": "Wow, what's the prompt processing speed?",
              "score": 1,
              "created_utc": "2026-02-12 15:41:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zxbip",
                  "author": "kingabzpro",
                  "text": "16 tps. \n\n\\>I think. The full model need to be run on 8 H200 to get 200K context window and also best speed. ",
                  "score": 1,
                  "created_utc": "2026-02-12 15:45:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zjegx",
          "author": "false79",
          "text": "Love these guides...\n\n\nDon't love when I don't have the VRAM requirements :(",
          "score": 3,
          "created_utc": "2026-02-12 14:36:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zkadt",
          "author": "fragment_me",
          "text": "Yeah let me just fire up my 1TB VRAM machine",
          "score": 2,
          "created_utc": "2026-02-12 14:41:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o505ka0",
          "author": "No_Conversation9561",
          "text": "Does this model use less memory for KV cache because of DSA?",
          "score": 1,
          "created_utc": "2026-02-12 16:24:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx1ye4",
      "title": "koute/GLM-4.7-Flash-Derestricted",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qx1ye4/kouteglm47flashderestricted/",
      "author": "Witty_Mycologist_995",
      "created_utc": "2026-02-05 23:56:38",
      "score": 27,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "https://huggingface.co/koute/GLM-4.7-Flash-Derestricted",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qx1ye4/kouteglm47flashderestricted/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qyopks",
      "title": "When replacing embed_tokens and lm_head with those from another model, is this implementation correct?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qyopks/when_replacing_embed_tokens_and_lm_head_with/",
      "author": "choco132134",
      "created_utc": "2026-02-07 20:38:05",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "In the KnitLM paper ([https://openreview.net/forum?id=2uctT30vTS](https://openreview.net/forum?id=2uctT30vTS)), they train a LoRA adapter on a **base** model and then merge/apply that adapter onto an **instruct** model. To keep the two models consistent, they replace the base model‚Äôs token embeddings (and also the LM head if it is not tied to the embeddings) with those from the instruct model.\n\nI‚Äôm trying to implement this with **Qwen3-8B**, and I‚Äôd like to ask whether the implementation below looks correct. I ran this on **Google Colab with an A100**. When I tried the same thing on an **L4**, I ran into OOM-related issues and ended up getting **meta tensors**, so it didn‚Äôt work properly.\n\nAlso, as far as I understand, **Qwen3-8B uses** `tie_word_embeddings = False`, so the input embeddings and `lm_head` are *not* tied, which is why I‚Äôm copying both.\n\n`%%capture`\n\n`import os, re`\n\n`if \"COLAB_\" not in \"\".join(os.environ.keys()):`\n\n`!pip install unsloth`\n\n`else:`\n\n`# Do this only in Colab notebooks! Otherwise use pip install unsloth`\n\n`import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)`\n\n`xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")`\n\n`!pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo`\n\n`!pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer`\n\n`!pip install --no-deps unsloth`\n\n`!pip install transformers==4.56.2`\n\n`!pip install --no-deps trl==0.22.2`\n\n`# =============================================================================`\n\n`# Hyperparameter configuration`\n\n`# =============================================================================`\n\n`LORA_R = 16`\n\n`LORA_ALPHA = 16`\n\n`PER_DEVICE_TRAIN_BATCH_SIZE = 16`\n\n`GRADIENT_ACCUMULATION_STEPS = 1`\n\n`PACKING = True`\n\n`NUM_TRAIN_EPOCHS = 1`\n\n`LEARNING_RATE = 2e-4`\n\n`MAX_SEQ_LENGTH = 2048`\n\n`# Model configuration`\n\n`BASE_MODEL = \"unsloth/Qwen3-8B-Base\"`\n\n`INSTRUCT_MODEL = \"unsloth/Qwen3-8B\"`\n\n`USE_INSTRUCT_EMBEDDINGS = True`\n\n`from unsloth import FastLanguageModel`\n\n`import torch`\n\n`# 1. Load the Base LLM`\n\n`print(\"[1/4] Loading Base LLM (backbone)...\")`\n\n`base_model, base_tokenizer = FastLanguageModel.from_pretrained(`\n\n`model_name = BASE_MODEL,`\n\n`max_seq_length = MAX_SEQ_LENGTH,`\n\n`load_in_4bit = False,`\n\n`)`\n\n`# 2. Load the Instruct LLM`\n\n`print(\"[2/4] Loading Instruct LLM (for embeddings)...\")`\n\n`instruct_model, instruct_tokenizer = FastLanguageModel.from_pretrained(`\n\n`model_name = INSTRUCT_MODEL,`\n\n`max_seq_length = MAX_SEQ_LENGTH,`\n\n`load_in_4bit = False,`\n\n`)`\n\n`def _is_meta(t: torch.Tensor) -> bool:`\n\n`return hasattr(t, \"device\") and t.device.type == \"meta\"`\n\n`def copy_qwen_embed_and_lm_head_exact(base_model, instruct_model, *, verbose: bool = True):`\n\n`\"\"\"`\n\n`Assumptions:`\n\n`- The Base and Instruct models have identical vocab_size / hidden_size (exact match).`\n\n`- For Qwen-style models where embeddings are NOT tied, copy both \\embed_tokens\\` and \\`lm_head\\`.\\``\n\n`What it does:`\n\n`- Prints the parameter shapes.`\n\n`- Copies weights in-place under torch.no_grad() (does NOT use .data).`\n\n`\"\"\"`\n\n`base_in = base_model.get_input_embeddings() # nn.Embedding`\n\n`inst_in = instruct_model.get_input_embeddings()`\n\n`base_out = base_model.get_output_embeddings() # nn.Linear (lm_head)`\n\n`inst_out = instruct_model.get_output_embeddings()`\n\n`if base_in is None or inst_in is None:`\n\n`raise ValueError(\"get_input_embeddings() returned None. Please check the model implementation.\")`\n\n`if base_out is None or inst_out is None:`\n\n`raise ValueError(\"get_output_embeddings() returned None. Please make sure this is a CausalLM.\")`\n\n`# Meta guard (prevents copying from tensors with no real storage)`\n\n`if _is_meta(inst_in.weight) or _is_meta(inst_out.weight):`\n\n`raise RuntimeError(\"instruct_model weights are on the 'meta' device (likely not fully loaded yet).\")`\n\n`# Get shapes`\n\n`base_in_shape = tuple(base_in.weight.shape)`\n\n`inst_in_shape = tuple(inst_in.weight.shape)`\n\n`base_out_shape = tuple(base_out.weight.shape)`\n\n`inst_out_shape = tuple(inst_out.weight.shape)`\n\n`# Print shapes`\n\n`if verbose:`\n\n`print(\"[Shapes]\")`\n\n`print(f\" base input_embeddings : {base_in_shape}\")`\n\n`print(f\" inst input_embeddings : {inst_in_shape}\")`\n\n`print(f\" base lm_head : {base_out_shape}\")`\n\n`print(f\" inst lm_head : {inst_out_shape}\")`\n\n`# Enforce exact match`\n\n`if base_in_shape != inst_in_shape:`\n\n`raise ValueError(f\"Input embedding shape mismatch: base={base_in_shape}, inst={inst_in_shape}\")`\n\n`if base_out_shape != inst_out_shape:`\n\n`raise ValueError(f\"LM head shape mismatch: base={base_out_shape}, inst={inst_out_shape}\")`\n\n`# Copy weights`\n\n`with torch.no_grad():`\n\n`base_in.weight.copy_(inst_in.weight)`\n\n`base_out.weight.copy_(inst_out.weight)`\n\n`if verbose:`\n\n`print(\"‚úì Copied input_embeddings and lm_head weights (exact match).\")`\n\n`return base_model`\n\n`copy_qwen_embed_and_lm_head_exact(base_model, instruct_model, verbose=True)`\n\n`# KnitLM-style assumption: use the Instruct tokenizer`\n\n`tokenizer = instruct_tokenizer`\n\n`print(f\"[Tokenizer] using instruct tokenizer. len(tokenizer)={len(tokenizer)}, vocab_size={tokenizer.vocab_size}\")`\n\n`# Safety check: ensure tokenizer IDs fit within the embedding matrix`\n\n`print(\"max token id (instruct tokenizer):\", max(instruct_tokenizer.get_vocab().values()))`\n\n`print(\"embedding rows:\", base_model.get_input_embeddings().weight.shape[0])`\n\nOutput:  \n\\[Shapes\\]\n\nbase  input\\_embeddings : (151936, 4096)\n\ninst  input\\_embeddings : (151936, 4096)\n\nbase  lm\\_head          : (151936, 4096)\n\ninst  lm\\_head          : (151936, 4096)\n\n‚úì Copied input\\_embeddings and lm\\_head weights (exact match).\n\n\\[Tokenizer\\] using instruct tokenizer. len(tokenizer)=151669, vocab\\_size=151643\n\nmax token id (instruct tokenizer): 151668\n\nembedding rows: 151936\n\nIf you think anything is missing, please let me know.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qyopks/when_replacing_embed_tokens_and_lm_head_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r116zw",
      "title": "Finetuning query for gpt-oss 20b model",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r116zw/finetuning_query_for_gptoss_20b_model/",
      "author": "Double_Tourist3600",
      "created_utc": "2026-02-10 13:47:55",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "We are facing a **thinking-loop issue** after fine-tuning a reasoning-enabled model and would appreciate guidance.\n\n**Setup**\n\n* Created a custom medical dataset and prepared it using the OpenAI Harmony format\n* Fine-tuned using Unsloth (analysis samples included)\n* Converted to GGUF via `llama.cpp`, quantized to **Q4\\_K\\_M**, and deployed with Ollama\n* For short/simple prompts, outputs are correct; however, as conversation context grows, the model remains in continuous reasoning (‚Äúthinking‚Äù) and does not produce the final response\n\n**Questions**\n\n1. What are the common causes of this behavior (chat template mismatch, stop-token issues, reasoning token configuration, RLHF override during SFT, etc.)?\n2. What checks or precautions should be taken during fine-tuning, GGUF conversion, quantization, and Ollama model file setup to prevent reasoning loops?\n3. Are there recommended template or stop-sequence configurations specifically for reasoning-enabled models to ensure the model exits the thinking phase properly?\n\nAny debugging checklist or best practices would be very helpful.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r116zw/finetuning_query_for_gptoss_20b_model/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o4mspu4",
          "author": "arman-d0e",
          "text": "Train on assistant only loss, include stop tokens (training with harmony has been weird for me, but this helped a lot) but most of all just try lowering temperature with inference you‚Äôll see better results",
          "score": 2,
          "created_utc": "2026-02-10 15:55:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r0dr3",
              "author": "Double_Tourist3600",
              "text": "    trainer = train_on_responses_only(\n            trainer,\n            instruction_part=\"<|start|>user<|message|>\",\n            response_part=\"<|start|>assistant\",\n            force_match=True,\n        )\n    I used this to apply loss on assistant messages/channels, Included stop tokens in each examples as <|end|> for channel and EOS = <|return|>\n    Lowering the temperature works for short queries but as context grows it start repeating the thinking..\n    This are the above measures that I have taken please let me know if any inputs are there..",
              "score": 1,
              "created_utc": "2026-02-11 05:15:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}