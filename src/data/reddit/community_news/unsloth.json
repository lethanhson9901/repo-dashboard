{
  "metadata": {
    "last_updated": "2026-02-10 17:16:23",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 8,
    "total_comments": 35,
    "file_size_bytes": 50133
  },
  "items": [
    {
      "id": "1qwp508",
      "title": "We created a Tool Calling Guide for LLMs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/ttfrqhl48phg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-05 16:00:39",
      "score": 236,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qwp508/we_created_a_tool_calling_guide_for_llms/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3uat10",
          "author": "fat_fun_xox",
          "text": "Kudos to you and team",
          "score": 5,
          "created_utc": "2026-02-06 03:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uznm8",
              "author": "yoracale",
              "text": "Thank you appreciate the support :)",
              "score": 2,
              "created_utc": "2026-02-06 06:44:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3szflf",
          "author": "RMK137",
          "text": "This is awesome. I've been wanting to get into this and build my own custom functions for Nemotron nano and Devstral small 2.",
          "score": 3,
          "created_utc": "2026-02-05 23:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t83s6",
          "author": "paul_tu",
          "text": "Great",
          "score": 2,
          "created_utc": "2026-02-05 23:52:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o495pen",
          "author": "JMowery",
          "text": "Unsloth team... Any idea why GLM 4.7 Flash does not work in Pydantic AI? It is the only model out of like 15 I have tested that fails. Even models at a fraction of its size work fine.\n\nGemini (which I was using to help me diagnose the issue) has proposed that 4.7 Flash refuses to adopt Pydantic AI tool calling methods/structure. But it is just strange that it is the only model that suffers from that.",
          "score": 1,
          "created_utc": "2026-02-08 14:10:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvt6qy",
      "title": "Qwen3-Coder-Next GGUFs updated - now produces much better outputs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/yoeghkey7ihg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-04 16:28:56",
      "score": 215,
      "num_comments": 60,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qvt6qy/qwen3codernext_ggufs_updated_now_produces_much/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3kl6da",
          "author": "Nieles1337",
          "text": "Hey, just wanted to check the update and noticed the MXFP4\\_MOE is still from yesterday, is this still cooking? Or is this the \"This week we'll also release a new MoE update\" you mention? Thanks!",
          "score": 8,
          "created_utc": "2026-02-04 18:10:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ks81r",
              "author": "GlobalLadder9461",
              "text": "Also q8 are not updated",
              "score": 3,
              "created_utc": "2026-02-04 18:42:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3limxk",
                  "author": "NebulaBetter",
                  "text": "I am waiting for this one too",
                  "score": 1,
                  "created_utc": "2026-02-04 20:46:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m7tb3",
                  "author": "Zc5Gwu",
                  "text": "Welp, that was a waste of bandwidth, sorry huggingface.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:48:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3n65m3",
                  "author": "danielhanchen",
                  "text": "Q8_0 and Q8_K_XL should be fine - updating llama.cpp is all you need - sorry should have mentioned it earlier",
                  "score": 1,
                  "created_utc": "2026-02-05 02:00:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m3ic7",
              "author": "StardockEngineer",
              "text": "u/yorascale Is mxfp4 ok or?",
              "score": 2,
              "created_utc": "2026-02-04 22:26:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n69fs",
                  "author": "danielhanchen",
                  "text": "MXFP4 is fine - just a llama.cpp update!",
                  "score": 1,
                  "created_utc": "2026-02-05 02:00:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3n62px",
              "author": "danielhanchen",
              "text": "Oh hey so sorry - MXFP4-MoE is fine - just a llama.cpp update would work!",
              "score": 1,
              "created_utc": "2026-02-05 01:59:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3qs32k",
              "author": "StardockEngineer",
              "text": "Looks like it updated overnight.",
              "score": 1,
              "created_utc": "2026-02-05 16:46:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lpe5v",
          "author": "zoyer2",
          "text": "Many thanks! The IQ4\\_XS now performs really good compared to before! This is for sure a great replacement for GPT-OSS 120B for coding\n\nhttps://preview.redd.it/51hzzo02ojhg1.png?width=1578&format=png&auto=webp&s=701d6334c31ff311ca5504e4a1d3910ac9a0ed45\n\n",
          "score": 5,
          "created_utc": "2026-02-04 21:18:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6jjv",
              "author": "danielhanchen",
              "text": "Fantastic!",
              "score": 1,
              "created_utc": "2026-02-05 02:02:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kjisa",
          "author": "CogahniMarGem",
          "text": "Sorry but how about the tool call, are there anyone test it.",
          "score": 3,
          "created_utc": "2026-02-04 18:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lsg8t",
              "author": "zoyer2",
              "text": "tested IQ4\\_XS now webgl and javascript repo. No looping, loaded 100k context. Actually performs really well",
              "score": 3,
              "created_utc": "2026-02-04 21:33:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n6cg2",
                  "author": "danielhanchen",
                  "text": "Oh nice!",
                  "score": 1,
                  "created_utc": "2026-02-05 02:01:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ko1fk",
          "author": "DocWolle",
          "text": "was the update needed? For me it seems fixed after updating llama.cpp.",
          "score": 3,
          "created_utc": "2026-02-04 18:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6bnq",
              "author": "danielhanchen",
              "text": "Yes imatrix ones still need them - BF16, Q8_0, Q8_K_XL, MXFP4 are fine, but the rest need updating",
              "score": 2,
              "created_utc": "2026-02-05 02:01:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lxk05",
          "author": "Zeranor",
          "text": "Oh boy.... this is an excellent model for (minor) local coding tasks. This is working WAY better than my recent attempts with GLM 4.7 flash, Devstral 2 Small or Qwen3 Next .... nice",
          "score": 3,
          "created_utc": "2026-02-04 21:57:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6p6p",
              "author": "danielhanchen",
              "text": "Yes the model is really good! Qwen really cooked this time!",
              "score": 2,
              "created_utc": "2026-02-05 02:03:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3roa8r",
          "author": "turbomedoqa",
          "text": "2 days ago I run Qwen Coder Next MXFP4 and I was impressed (it run 50 t/s). Blackwell 5000 RTX PRO was utilized by 100%. I had LM Studio 0.4.1 and Llama 2.01 runtime.\n\nToday, I updated LM Studio to 0.4.2 and I downloaded a fixed Qwen Coder Next MXFP4 model. I also plugged in 3060tu on my PCI4 slot and moved my monitors off blackwell which now runs on 16x PCI5. So my Blackwell is totally dedicated for LLM work.\n\nNow the inference speed is 46 t/s, but utilization of Blackwell is 50-60%. How can I run it at 100%? Is this a bug or something else? \n\nI loaded Qwen 3 Coder 32b Q8 and it runs at \\~100% utlization with 150 t/s speed. I wonder why is blackwell under utilized on Blackwell and MXFP4 Qwen Coder Next model?\n\nHere are model configs: \n\nhttps://preview.redd.it/mida3ocs6qhg1.png?width=390&format=png&auto=webp&s=3c500fecd3029fb2e7a97352a4017b894554db4d\n\nAnyone experiencing something like that? It looks like something else might be the bottleneck or runtime issues? Thanks for any hints.",
          "score": 3,
          "created_utc": "2026-02-05 19:14:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k87c2",
          "author": "FinalsMVPZachZarba",
          "text": "Did the bug affect qwen3-next too?",
          "score": 2,
          "created_utc": "2026-02-04 17:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3koujm",
              "author": "DocWolle",
              "text": "not sure if new ggufs were needed.Â \nThe prompt fromÂ https://github.com/ggml-org/llama.cpp/issues/19305\n\n\nworks by just updating llama.cpp",
              "score": 3,
              "created_utc": "2026-02-04 18:27:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n6hj1",
                  "author": "danielhanchen",
                  "text": "Yes! But imatrix needs updating - I'll need to redo the other Qwen3-Next ones as well",
                  "score": 1,
                  "created_utc": "2026-02-05 02:02:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kikud",
              "author": "DocWolle",
              "text": "I think so",
              "score": 2,
              "created_utc": "2026-02-04 17:59:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kknyn",
          "author": "[deleted]",
          "text": "Is it possible to make torrents or diffs to prevent having to redownload gigs of data again? Or are the models changed that much?",
          "score": 2,
          "created_utc": "2026-02-04 18:08:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nnn49",
              "author": "illkeepthatinmind",
              "text": "Boggles my mind torrents arent a thing with models.",
              "score": 2,
              "created_utc": "2026-02-05 03:41:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3n6j2u",
              "author": "danielhanchen",
              "text": "Sadly imatrix changed a lot",
              "score": 1,
              "created_utc": "2026-02-05 02:02:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o40pswp",
          "author": "Ok-Shower7286",
          "text": "Oh. Thanks a lot. ",
          "score": 2,
          "created_utc": "2026-02-07 03:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0ck9",
          "author": "boatbomber",
          "text": "Why does Qwen refuse to contribute to prevent these problems? It makes their models look worse on release when they underperform with community implementations of their architecture",
          "score": 2,
          "created_utc": "2026-02-04 16:35:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k1yb5",
              "author": "yoracale",
              "text": "This isn't related to them not contributing or not. There was no need for custom/new implementation because it's just using Qwen3 next arch which was already supported in llama.cpp\n\nIt was a normal human accident that caused this issue and it happens and was fixed. Even if Qwen helped for this release, this would've likely happened as it was a needle in a haystack situation.\n\nQwen team also previously contributed to llama.cpp for Qwen3 release. They need to allocate their time for smaller model releases like Qwen3-next otherwise there would a million repos to implement and contribute to\n\nI'm guessing they're working with the llama.cpp team for the next important Qwen model release.",
              "score": 24,
              "created_utc": "2026-02-04 16:42:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3k4oiw",
                  "author": "fancyrocket",
                  "text": "Is it worth downloading now or will i have to re-download it again in the near future for updates? Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-04 16:55:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lrdxn",
          "author": "kryptkpr",
          "text": "In case anyone hasn't learned this lesson yet you should wait a week before downloading any new model the first implementation is broke 95% of the tome",
          "score": 1,
          "created_utc": "2026-02-04 21:27:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ly9yg",
              "author": "ServiceOver4447",
              "text": "probably too much AI to release it\n\nokay, i'll leave myself out now.",
              "score": 1,
              "created_utc": "2026-02-04 22:00:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3n6lf4",
              "author": "danielhanchen",
              "text": "Yes it's best probably to wait 3 days ish - we also do bug fixes on day 1 or 2",
              "score": 1,
              "created_utc": "2026-02-05 02:02:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3qwg30",
              "author": "TokenRingAI",
              "text": "This bug affected the original Qwen Next which was released months ago",
              "score": 1,
              "created_utc": "2026-02-05 17:06:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3rkuzy",
              "author": "TaroOk7112",
              "text": "And miss all the fun?!",
              "score": 1,
              "created_utc": "2026-02-05 18:59:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mju9f",
          "author": "lukepacman",
          "text": "the version i downloaded about 12h ago generated at speed of 18 tok/s on apple silicon m1  \n  \nim on llama.cpp 7930 and IQ3\\_XXS\n\nwould the new version improve the speed?",
          "score": 1,
          "created_utc": "2026-02-04 23:54:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6n2e",
              "author": "danielhanchen",
              "text": "Speed not so much, but definitely accuracy",
              "score": 1,
              "created_utc": "2026-02-05 02:02:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nryk2",
                  "author": "lukepacman",
                  "text": "yeah im re-downloading the model and will retry to see how it's going then.\n\njust curious, since this is also a MoE 3B active params model, do you know why it's significantly slower than the others in the same class?",
                  "score": 1,
                  "created_utc": "2026-02-05 04:09:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mqqjl",
          "author": "iadanos",
          "text": "Does this change affect previous Qwen-Next models?",
          "score": 1,
          "created_utc": "2026-02-05 00:32:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n6nll",
              "author": "danielhanchen",
              "text": "Yes sadly I have to reupload",
              "score": 3,
              "created_utc": "2026-02-05 02:03:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n99o2",
                  "author": "iadanos",
                  "text": "Would you kindly give a notice once it's done?",
                  "score": 1,
                  "created_utc": "2026-02-05 02:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pzyvs",
          "author": "wisepal_app",
          "text": "In your site says: \"If your quant fully fits on your device, expect 20+ tokens/s. If it doesn't fit, it'll still work by offloading but it will be slower.\"  \nWhat do you mean with \"fully fit\"? Fully fit to VRAM or fully fit to \"disk space + RAM + VRAM\"? And with which quant and context window 20+ tokens/s we get? (i have a 16 gb vram+96 gb ddr5 ram laptop)",
          "score": 1,
          "created_utc": "2026-02-05 14:31:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qjex7",
          "author": "Rand_o",
          "text": "This update actually broke things for me. Previous version didnt have issues, this new one has many issues with tool calls failing",
          "score": 1,
          "created_utc": "2026-02-05 16:06:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rfnyp",
          "author": "iadanos",
          "text": "Sorry for asking each time, but, did anyone check that these fixes don't improve Qwen3-30B-A3B and the respective Coder model? Llama.cpp had some important fixes during last months and it might improve old arch models as well. (or, please correct me if I'm wrong)",
          "score": 1,
          "created_utc": "2026-02-05 18:35:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zfhlz",
              "author": "yoracale",
              "text": "No these changes only affect Qwen3-Next and qwen3nextcoder",
              "score": 1,
              "created_utc": "2026-02-06 22:36:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44w9jj",
                  "author": "iadanos",
                  "text": "Good. Thanks.",
                  "score": 1,
                  "created_utc": "2026-02-07 20:04:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o46o00l",
          "author": "raiffuvar",
          "text": "46gb ram? Can it run on CPU?",
          "score": 1,
          "created_utc": "2026-02-08 02:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47w2s6",
              "author": "yoracale",
              "text": "Yes it can yes",
              "score": 1,
              "created_utc": "2026-02-08 07:48:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4bbuk5",
          "author": "shrug_hellifino",
          "text": "I am getting some funky behavior I posted a little info here: [https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o4b93l1](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o4b93l1) basically getting \\`forcing full prompt re-processing due to lack of cache dataÂ \\`",
          "score": 1,
          "created_utc": "2026-02-08 20:38:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r10d9l",
      "title": "GLM-4.7-Flash is now the #1 most downloaded model on Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/n9wlycyu2oig1.png",
      "author": "yoracale",
      "created_utc": "2026-02-10 13:12:55",
      "score": 99,
      "num_comments": 26,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r10d9l/glm47flash_is_now_the_1_most_downloaded_model_on/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4lz42d",
          "author": "Far-Donut-1177",
          "text": "I envy folks with the hardware that could run 100B above. The best I could do is 30 and that's stretching it.",
          "score": 8,
          "created_utc": "2026-02-10 13:21:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m0j7v",
              "author": "Significant_Fig_7581",
              "text": "Heyyy let's just be happy cause this post just proved that most of us are and these download numbers are surely going to be an encouragement for most of them to release good models at that size, Qwen is releasing a 35B MOE btw...",
              "score": 5,
              "created_utc": "2026-02-10 13:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4m3n6z",
                  "author": "ismaelgokufox",
                  "text": "Indeed. Being able to run this at good speeds in 16GB VRAM is great!",
                  "score": 3,
                  "created_utc": "2026-02-10 13:47:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mujj1",
                  "author": "ScoreUnique",
                  "text": "35B MoE sounds like a dream after seeing qwen 3 next coder.",
                  "score": 2,
                  "created_utc": "2026-02-10 16:04:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m5rwq",
          "author": "MrMrsPotts",
          "text": "I need a version I can squeeze into 12GB of VRAM.",
          "score": 6,
          "created_utc": "2026-02-10 13:58:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ming1",
              "author": "eesnimi",
              "text": "Unload the expert layers to CPU and it will be a nice squeeze :) I have squeezed a Q6\\_K\\_XL quant with 130k token window to 11GB VRAM with all the expert layers unloaded to CPU, getting around 11-13t/s that's quite usable.",
              "score": 2,
              "created_utc": "2026-02-10 15:06:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mqv58",
                  "author": "MrMrsPotts",
                  "text": "Is there something I can read about how to do that?",
                  "score": 2,
                  "created_utc": "2026-02-10 15:46:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4m8sty",
              "author": "NorthEastCalifornia",
              "text": "glm-4.7-flash REAP 50%",
              "score": 1,
              "created_utc": "2026-02-10 14:15:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ma897",
                  "author": "paq85",
                  "text": "I've tried the REAP version and couldn't make it work with open code at all... As if it end in some loop, even though I run it with same llamacpp params as the full version.",
                  "score": 2,
                  "created_utc": "2026-02-10 14:22:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m748w",
          "author": "RedKnightRG",
          "text": "*Raises a toast to everyone who bought 128GB of RAM and dual 3090s or similar thousands of dollars ago*",
          "score": 5,
          "created_utc": "2026-02-10 14:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7sdz",
              "author": "yoracale",
              "text": "You don't need that much to run the 30B model. 24GB is enough to get it working well with quality",
              "score": 3,
              "created_utc": "2026-02-10 14:09:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m8f6l",
                  "author": "RedKnightRG",
                  "text": "Oh my B I saw GLM but read Qwen and thought this was the 80B model.  You're right 1 3090 is fine.   So I'll raise a toast to the guys who bought their single 3090 hundreds of dollars ago!  ðŸ˜…",
                  "score": 1,
                  "created_utc": "2026-02-10 14:13:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m9zsu",
          "author": "paq85",
          "text": "This model is definitely the best 30b agentic coding model I've seen so far.",
          "score": 5,
          "created_utc": "2026-02-10 14:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mml3u",
          "author": "alfpacino2020",
          "text": "[https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) mucho mejor este mas lento pero   no falla como loco   no se tilda como gml 4.7",
          "score": 1,
          "created_utc": "2026-02-10 15:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mr3bb",
              "author": "MrMrsPotts",
              "text": "I would need the 2 bit quantisation though which might not be good.",
              "score": 1,
              "created_utc": "2026-02-10 15:47:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4msb5x",
                  "author": "alfpacino2020",
                  "text": "https://preview.redd.it/k4kc0it8voig1.png?width=974&format=png&auto=webp&s=5c2f0a31e47adbbb79d2bae65ac01c10de4a04be\n\nejemplo en iq4 me da casi  23 tokens x seg el qwen me da 15 x seg osea no gran diferencia usando Qwen3-Coder-Next-MXFP4\\_MOE.gguf",
                  "score": 1,
                  "created_utc": "2026-02-10 15:53:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4muk1k",
                  "author": "alfpacino2020",
                  "text": "tengo 5070 16gb vram 48ram  aca el qwen \n\nhttps://preview.redd.it/916th7phxoig1.png?width=1024&format=png&auto=webp&s=78fb7ae7e7bc1dd6ca8581429839edb61bc221e0\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 16:04:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r13pk4",
      "title": "Faster MoE LLM Training now in Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/qfoq8mirjoig1.png",
      "author": "yoracale",
      "created_utc": "2026-02-10 15:25:44",
      "score": 50,
      "num_comments": 7,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r13pk4/faster_moe_llm_training_now_in_unsloth/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4muop1",
          "author": "arman-d0e",
          "text": "Transformers v5 fully optimized? Do the optimizations apply to Qwen3-Coder-Next?",
          "score": 3,
          "created_utc": "2026-02-10 16:04:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mvode",
              "author": "danielhanchen",
              "text": "Not yet - we plan to add support for Qwen3-Next next!",
              "score": 5,
              "created_utc": "2026-02-10 16:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mwszz",
                  "author": "arman-d0e",
                  "text": "Much appreciated, thanks for the updates.",
                  "score": 2,
                  "created_utc": "2026-02-10 16:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mp3bh",
          "author": "joninco",
          "text": "Multi gpu?",
          "score": 2,
          "created_utc": "2026-02-10 15:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mqz3p",
              "author": "yoracale",
              "text": "Works already but still preliminary. It's out soon.\n\nGuide for now: [https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth](https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth)",
              "score": 2,
              "created_utc": "2026-02-10 15:47:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n0rp8",
          "author": "sterby92",
          "text": "Will we also get vulkan or rocm support at some point? ",
          "score": 1,
          "created_utc": "2026-02-10 16:32:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n5cn4",
              "author": "yoracale",
              "text": "Yes, we're announcing very soon. As soon as this month. It already works: https://unsloth.ai/docs/get-started/install/amd",
              "score": 1,
              "created_utc": "2026-02-10 16:53:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvvks9",
      "title": "TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF Â· Hugging Face",
      "subreddit": "unsloth",
      "url": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF",
      "author": "No-Intention-5521",
      "created_utc": "2026-02-04 17:54:22",
      "score": 47,
      "num_comments": 19,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qvvks9/teichaiglm47flashclaudeopus45highreasoningdistillg/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o3kkfeq",
          "author": "KvAk_AKPlaysYT",
          "text": "Just 250 examples enough for a valuable distillation? Where are the benches?",
          "score": 10,
          "created_utc": "2026-02-04 18:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lh2i4",
              "author": "Zyguard7777777",
              "text": "Thinking the same, would be nice to have even a small bench to compare",
              "score": 4,
              "created_utc": "2026-02-04 20:38:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mlias",
                  "author": "arman-d0e",
                  "text": "I was thinking HumanEval and maybe LiveCodeBench? Or would you suggest others",
                  "score": 1,
                  "created_utc": "2026-02-05 00:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ml4hv",
              "author": "arman-d0e",
              "text": "Our goal with this model (as with all our others) has been to distill CoT not knowledge. 250 is still small donâ€™t get me wrong, however itâ€™s surprisingly proven to still successfully teach the student model to â€œthink likeâ€ the teacher (given the diverse types of prompts). Iâ€™m currently looking at rentable gpus to do some benchmarks. Feel free to suggest what benchmarks youâ€™d like to see for this model.\n\nI wouldnâ€™t have much expectation though as the model could use some form of RL to better adapt to itâ€™s new CoT",
              "score": 3,
              "created_utc": "2026-02-05 00:01:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sivsf",
                  "author": "Frequent-Mud8705",
                  "text": "have you tried training on a bunch of claude code logs? I have all of my history with the tool saved, do you think it would make it noticeably better?",
                  "score": 1,
                  "created_utc": "2026-02-05 21:40:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3lnl5z",
              "author": "ethereal_intellect",
              "text": "Afaik the whole idea was to lessen the abysmally long thinking, opus thinks 50x less. But yeah benches would be nice to see for a lot of these :/",
              "score": 2,
              "created_utc": "2026-02-04 21:09:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3md9kz",
                  "author": "ClimateBoss",
                  "text": "or try it can it even code bruh ?",
                  "score": 1,
                  "created_utc": "2026-02-04 23:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mw0be",
              "author": "CoruNethronX",
              "text": "Check this: [Less Is More for Reasoning](https://arxiv.org/abs/2502.03387)",
              "score": 2,
              "created_utc": "2026-02-05 01:01:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o4z9d",
                  "author": "Thrumpwart",
                  "text": "Then check this: [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)",
                  "score": 3,
                  "created_utc": "2026-02-05 05:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o46e3bw",
              "author": "[deleted]",
              "text": "The guy was a rookie... it's not a real distill. Just some amature listing crap on HF. \n\n100,000 minimum for a distill. Get this clown out of there. ",
              "score": 1,
              "created_utc": "2026-02-08 01:11:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mypb8",
          "author": "CoruNethronX",
          "text": "Checked the actual dataset and looks, like it's 90% filled with the same question \"what is requerments\", rephrased in different ways. But also includes more practical questions, like \"how to subtract 47 from 89\". Not sure that it's really useful for programming and especially for agentic skills, that are strongest sides of glm 4.7 flash in comparison to other open models of the similar size.",
          "score": 8,
          "created_utc": "2026-02-05 01:17:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nsvrg",
          "author": "Successful_Bit7710",
          "text": "Anyone use and perform benchmarks?",
          "score": 1,
          "created_utc": "2026-02-05 04:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oz7pu",
              "author": "zoyer2",
              "text": "Tested it, it seems promising, does things well but always messes up something. I'm not sure how it's trained etc but this looks very similar to the first version of glm 4.7 flash from unsloth which made silly mistakes",
              "score": 3,
              "created_utc": "2026-02-05 10:17:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ofk9b",
              "author": "kironlau",
              "text": "worst than original one, tested MXFP4 quant, a few days ago.",
              "score": 2,
              "created_utc": "2026-02-05 07:09:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3oalfu",
              "author": "TokenRingAI",
              "text": "Not needed, falls apart under basic testing.",
              "score": 1,
              "created_utc": "2026-02-05 06:26:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qx1ye4",
      "title": "koute/GLM-4.7-Flash-Derestricted",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qx1ye4/kouteglm47flashderestricted/",
      "author": "Witty_Mycologist_995",
      "created_utc": "2026-02-05 23:56:38",
      "score": 27,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "https://huggingface.co/koute/GLM-4.7-Flash-Derestricted",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qx1ye4/kouteglm47flashderestricted/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qw0ch3",
      "title": "FSDP with Unsloth",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qw0ch3/fsdp_with_unsloth/",
      "author": "Potential_Nerve_4381",
      "created_utc": "2026-02-04 20:43:51",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I'm trying to load Qwen3-30B-A3B model on my g5.12xlarge GPU. I need to shard the model as it doesn't fit in one GPU. Does anyone have an example working script that runs FSDP with Unsloth and Hugging face Trainer? I can't seem to find one anywhere ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qw0ch3/fsdp_with_unsloth/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o3ot2ct",
          "author": "rjtannous",
          "text": "unsloth optimized FSDP is coming soon , however FSDP should still work if you disable torch compile by setting os.environ\\[\"TORCH\\_COMPILE\\_DISABLE\"\\] = \"1\"",
          "score": 1,
          "created_utc": "2026-02-05 09:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xl44b",
              "author": "Potential_Nerve_4381",
              "text": "I tried. I couldn't make it work. It'd be great if you share a working scriptÂ ",
              "score": 1,
              "created_utc": "2026-02-06 17:10:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45pp0f",
          "author": "Mac_NCheez_TW",
          "text": "Call me ignorant and put lipnstick on me, but what is FSDP? Nvm I'll find it and learn. I'm running multigpu setup am I missing something? I'm sure I am I never research enough. brb.Â ",
          "score": 1,
          "created_utc": "2026-02-07 22:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45q2bt",
              "author": "Mac_NCheez_TW",
              "text": "Okay it's for training, I haven't even started down that path I just built my EPYC system after struggling to afford ram.Â ",
              "score": 2,
              "created_utc": "2026-02-07 22:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyopks",
      "title": "When replacing embed_tokens and lm_head with those from another model, is this implementation correct?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qyopks/when_replacing_embed_tokens_and_lm_head_with/",
      "author": "choco132134",
      "created_utc": "2026-02-07 20:38:05",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "In the KnitLM paper ([https://openreview.net/forum?id=2uctT30vTS](https://openreview.net/forum?id=2uctT30vTS)), they train a LoRA adapter on a **base** model and then merge/apply that adapter onto an **instruct** model. To keep the two models consistent, they replace the base modelâ€™s token embeddings (and also the LM head if it is not tied to the embeddings) with those from the instruct model.\n\nIâ€™m trying to implement this with **Qwen3-8B**, and Iâ€™d like to ask whether the implementation below looks correct. I ran this on **Google Colab with an A100**. When I tried the same thing on an **L4**, I ran into OOM-related issues and ended up getting **meta tensors**, so it didnâ€™t work properly.\n\nAlso, as far as I understand, **Qwen3-8B uses** `tie_word_embeddings = False`, so the input embeddings and `lm_head` are *not* tied, which is why Iâ€™m copying both.\n\n`%%capture`\n\n`import os, re`\n\n`if \"COLAB_\" not in \"\".join(os.environ.keys()):`\n\n`!pip install unsloth`\n\n`else:`\n\n`# Do this only in Colab notebooks! Otherwise use pip install unsloth`\n\n`import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)`\n\n`xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")`\n\n`!pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo`\n\n`!pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer`\n\n`!pip install --no-deps unsloth`\n\n`!pip install transformers==4.56.2`\n\n`!pip install --no-deps trl==0.22.2`\n\n`# =============================================================================`\n\n`# Hyperparameter configuration`\n\n`# =============================================================================`\n\n`LORA_R = 16`\n\n`LORA_ALPHA = 16`\n\n`PER_DEVICE_TRAIN_BATCH_SIZE = 16`\n\n`GRADIENT_ACCUMULATION_STEPS = 1`\n\n`PACKING = True`\n\n`NUM_TRAIN_EPOCHS = 1`\n\n`LEARNING_RATE = 2e-4`\n\n`MAX_SEQ_LENGTH = 2048`\n\n`# Model configuration`\n\n`BASE_MODEL = \"unsloth/Qwen3-8B-Base\"`\n\n`INSTRUCT_MODEL = \"unsloth/Qwen3-8B\"`\n\n`USE_INSTRUCT_EMBEDDINGS = True`\n\n`from unsloth import FastLanguageModel`\n\n`import torch`\n\n`# 1. Load the Base LLM`\n\n`print(\"[1/4] Loading Base LLM (backbone)...\")`\n\n`base_model, base_tokenizer = FastLanguageModel.from_pretrained(`\n\n`model_name = BASE_MODEL,`\n\n`max_seq_length = MAX_SEQ_LENGTH,`\n\n`load_in_4bit = False,`\n\n`)`\n\n`# 2. Load the Instruct LLM`\n\n`print(\"[2/4] Loading Instruct LLM (for embeddings)...\")`\n\n`instruct_model, instruct_tokenizer = FastLanguageModel.from_pretrained(`\n\n`model_name = INSTRUCT_MODEL,`\n\n`max_seq_length = MAX_SEQ_LENGTH,`\n\n`load_in_4bit = False,`\n\n`)`\n\n`def _is_meta(t: torch.Tensor) -> bool:`\n\n`return hasattr(t, \"device\") and t.device.type == \"meta\"`\n\n`def copy_qwen_embed_and_lm_head_exact(base_model, instruct_model, *, verbose: bool = True):`\n\n`\"\"\"`\n\n`Assumptions:`\n\n`- The Base and Instruct models have identical vocab_size / hidden_size (exact match).`\n\n`- For Qwen-style models where embeddings are NOT tied, copy both \\embed_tokens\\` and \\`lm_head\\`.\\``\n\n`What it does:`\n\n`- Prints the parameter shapes.`\n\n`- Copies weights in-place under torch.no_grad() (does NOT use .data).`\n\n`\"\"\"`\n\n`base_in = base_model.get_input_embeddings() # nn.Embedding`\n\n`inst_in = instruct_model.get_input_embeddings()`\n\n`base_out = base_model.get_output_embeddings() # nn.Linear (lm_head)`\n\n`inst_out = instruct_model.get_output_embeddings()`\n\n`if base_in is None or inst_in is None:`\n\n`raise ValueError(\"get_input_embeddings() returned None. Please check the model implementation.\")`\n\n`if base_out is None or inst_out is None:`\n\n`raise ValueError(\"get_output_embeddings() returned None. Please make sure this is a CausalLM.\")`\n\n`# Meta guard (prevents copying from tensors with no real storage)`\n\n`if _is_meta(inst_in.weight) or _is_meta(inst_out.weight):`\n\n`raise RuntimeError(\"instruct_model weights are on the 'meta' device (likely not fully loaded yet).\")`\n\n`# Get shapes`\n\n`base_in_shape = tuple(base_in.weight.shape)`\n\n`inst_in_shape = tuple(inst_in.weight.shape)`\n\n`base_out_shape = tuple(base_out.weight.shape)`\n\n`inst_out_shape = tuple(inst_out.weight.shape)`\n\n`# Print shapes`\n\n`if verbose:`\n\n`print(\"[Shapes]\")`\n\n`print(f\" base input_embeddings : {base_in_shape}\")`\n\n`print(f\" inst input_embeddings : {inst_in_shape}\")`\n\n`print(f\" base lm_head : {base_out_shape}\")`\n\n`print(f\" inst lm_head : {inst_out_shape}\")`\n\n`# Enforce exact match`\n\n`if base_in_shape != inst_in_shape:`\n\n`raise ValueError(f\"Input embedding shape mismatch: base={base_in_shape}, inst={inst_in_shape}\")`\n\n`if base_out_shape != inst_out_shape:`\n\n`raise ValueError(f\"LM head shape mismatch: base={base_out_shape}, inst={inst_out_shape}\")`\n\n`# Copy weights`\n\n`with torch.no_grad():`\n\n`base_in.weight.copy_(inst_in.weight)`\n\n`base_out.weight.copy_(inst_out.weight)`\n\n`if verbose:`\n\n`print(\"âœ“ Copied input_embeddings and lm_head weights (exact match).\")`\n\n`return base_model`\n\n`copy_qwen_embed_and_lm_head_exact(base_model, instruct_model, verbose=True)`\n\n`# KnitLM-style assumption: use the Instruct tokenizer`\n\n`tokenizer = instruct_tokenizer`\n\n`print(f\"[Tokenizer] using instruct tokenizer. len(tokenizer)={len(tokenizer)}, vocab_size={tokenizer.vocab_size}\")`\n\n`# Safety check: ensure tokenizer IDs fit within the embedding matrix`\n\n`print(\"max token id (instruct tokenizer):\", max(instruct_tokenizer.get_vocab().values()))`\n\n`print(\"embedding rows:\", base_model.get_input_embeddings().weight.shape[0])`\n\nOutput:  \n\\[Shapes\\]\n\nbase  input\\_embeddings : (151936, 4096)\n\ninst  input\\_embeddings : (151936, 4096)\n\nbase  lm\\_head          : (151936, 4096)\n\ninst  lm\\_head          : (151936, 4096)\n\nâœ“ Copied input\\_embeddings and lm\\_head weights (exact match).\n\n\\[Tokenizer\\] using instruct tokenizer. len(tokenizer)=151669, vocab\\_size=151643\n\nmax token id (instruct tokenizer): 151668\n\nembedding rows: 151936\n\nIf you think anything is missing, please let me know.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qyopks/when_replacing_embed_tokens_and_lm_head_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    }
  ]
}