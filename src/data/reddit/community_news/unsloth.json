{
  "metadata": {
    "last_updated": "2026-01-24 16:49:57",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 11,
    "total_comments": 62,
    "file_size_bytes": 86063
  },
  "items": [
    {
      "id": "1qhscts",
      "title": "Run GLM-4.7-Flash locally Guide! (24GB RAM)",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/hgng1lc5ufeg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-20 05:22:23",
      "score": 208,
      "num_comments": 46,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qhscts/run_glm47flash_locally_guide_24gb_ram/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mccxs",
          "author": "TaroOk7112",
          "text": "Thanks!!!  \nDoes llama.cpp need to add some code to improve support or with this PR it's all supported?  \n[https://github.com/ggml-org/llama.cpp/pull/18936](https://github.com/ggml-org/llama.cpp/pull/18936)",
          "score": 7,
          "created_utc": "2026-01-20 05:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mdiub",
              "author": "danielhanchen",
              "text": "It should work in llama.cpp main now!",
              "score": 5,
              "created_utc": "2026-01-20 06:00:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0n9xr7",
                  "author": "goniz",
                  "text": "Does the repetitions happen on Q8 and BF16 GGUFs as well? Or just lower quants?",
                  "score": 3,
                  "created_utc": "2026-01-20 10:51:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0q5ocg",
                  "author": "TaroOk7112",
                  "text": "Be aware that flash attention degrades speed a lot, there are several PR in the works to improve support for  GLM-4.7-Flash in llama.cpp. It works, but is slower than even gpt-oss 120, so for now not too interesting.   \n[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/5#696f93fcbc8028d97e6fdf2b](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/5#696f93fcbc8028d97e6fdf2b)",
                  "score": 1,
                  "created_utc": "2026-01-20 19:59:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzq84",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:35:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0o83m1",
          "author": "maxpayne07",
          "text": "So far, yes",
          "score": 2,
          "created_utc": "2026-01-20 14:35:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzs2f",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qiwhk",
          "author": "Dramatic-Rub-7654",
          "text": "Is this model actually dumber than Qwen 3 Coder Flash, or is it just overly sensitive? To the point that with the --n-cpu-moe flag it gets stuck in an infinite loop repeating a single word, and without that flag it keeps creating endless files, all with errors, until the window runs out?",
          "score": 2,
          "created_utc": "2026-01-20 21:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rc1ba",
              "author": "yoracale",
              "text": "It's very sensitive. Did you try using our recommended parameters? It seems to be a must for the this model",
              "score": 2,
              "created_utc": "2026-01-20 23:23:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rm20t",
                  "author": "Dramatic-Rub-7654",
                  "text": "I tried using the parameters recommended on the Hugging Face page for llama-b7782/llama-server:\n\n-m GLM-4.7-Flash-Q4_K_M.gguf --host 0.0.0.0 --n-gpu-layers 999 -fa on -t 14 -n -1 -c 16384 --jinja --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1 \n\nThe only changes I experimented with were adding the --n-cpu-moe flag, which caused the model to bug out with severe repetition issues, and increasing the temperature to 1.0.\n\nAt temperature 1.0, the model‚Äôs reasoning and responses appear coherent, but when I try to use it with tools like Cline, it clearly doesn‚Äôt know what it‚Äôs doing. It can create and edit files and interact with the terminal, but it consistently outputs broken code and introduces errors when editing files.\n\nIn contrast, Qwen, even in version Q4, is capable of providing a fully functional implementation of Flappy Bird from start to finish. based on the tests I ran, the GGUF versions still need further refinement. I tested the model using the version available on OpenRouter, where it performs significantly better than in my GGUF-based tests. However, Coder Flash still demonstrates superior intelligence compared to this model.",
                  "score": 1,
                  "created_utc": "2026-01-21 00:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzv12",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 0,
              "created_utc": "2026-01-21 10:37:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mldpf",
          "author": "RMK137",
          "text": "Great turnaround! I just got my 5090 so this is perfect timing.",
          "score": 2,
          "created_utc": "2026-01-20 07:05:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzs82",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:36:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mjz8z",
          "author": "Psyko38",
          "text": "On a GPU with 16GB of VRAM, we're good at Q3?",
          "score": 1,
          "created_utc": "2026-01-20 06:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mma9p",
              "author": "Conscious_Chef_3233",
              "text": "you should go higher, i run qwen3 30b q4 on my 4070 12g",
              "score": 1,
              "created_utc": "2026-01-20 07:13:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mmg8z",
                  "author": "Psyko38",
                  "text": "But the weights, where do you put them in the RAM and you put the MoE in the VRAM?",
                  "score": 1,
                  "created_utc": "2026-01-20 07:14:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0tzsmj",
                  "author": "yoracale",
                  "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
                  "score": 1,
                  "created_utc": "2026-01-21 10:36:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzsdw",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u46lj",
                  "author": "Psyko38",
                  "text": "I already downloaded the model yesterday with your recommended settings and the result was not too bad. I'll try the new ones again, but in Q4 with the MoE weights on the GPU and the unused ones on the CPU.",
                  "score": 1,
                  "created_utc": "2026-01-21 11:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mmhfv",
          "author": "Unlikely_Database_87",
          "text": "In lm studio the model with those parameters you provided, cannot even give any clear response and it infinitely generates nonsense. Simple prompt I use - write java method  to merge two sorted arrays, no tests no explanation just code. My config 5080 and 64GB RAM",
          "score": 1,
          "created_utc": "2026-01-20 07:14:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mohp7",
              "author": "yoracale",
              "text": "You need to use dry multiplier which has the biggest impact, because LM Studio does not have it, you need to disable repeat penalty entirely.",
              "score": 4,
              "created_utc": "2026-01-20 07:32:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0mq2e5",
                  "author": "Unlikely_Database_87",
                  "text": "Thanks that helps",
                  "score": 1,
                  "created_utc": "2026-01-20 07:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tztmu",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 2,
              "created_utc": "2026-01-21 10:36:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mnfbw",
          "author": "Prudent-Ad4509",
          "text": "I'm not sure what they mean by full precision. The original seems to require 64Gb even with near zero context.",
          "score": 1,
          "created_utc": "2026-01-20 07:23:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n1ywr",
          "author": "Kirito_5",
          "text": "Thank you as always! \nWill try to see how well it compares to others on my 3090.",
          "score": 1,
          "created_utc": "2026-01-20 09:38:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nfyub",
              "author": "siggystabs",
              "text": "I really like GLM 4.7 Flash, but i can only fit a small context (like 5000 chars) on a single 3090 ‚Äî Not using System RAM. I end up having to split it across two 3090s to use it with decent context, and then it isn‚Äôt as fast.\n\nBut GLM 4.7 Flash is really exciting. The quality is excellent. If I didn‚Äôt have specific needs for my self-hosted apps, I‚Äôd probably use that exclusively for chatting. It‚Äôs definitely smarter than Qwen3 30B A3B in my usage.\n\nFor my workhorse applications I went back to Qwen3 30B A3B. Reluctantly. It‚Äôs just faster and better on VRAM, and ‚Äúgood enough‚Äù.",
              "score": 2,
              "created_utc": "2026-01-20 11:42:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tzu6e",
                  "author": "yoracale",
                  "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
                  "score": 1,
                  "created_utc": "2026-01-21 10:36:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzu0f",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nh57o",
          "author": "xanduonc",
          "text": "FP8 is half precision, original weights from glm are BF16",
          "score": 1,
          "created_utc": "2026-01-20 11:51:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nvq0g",
              "author": "yoracale",
              "text": "Yes thanks I edited my post",
              "score": 1,
              "created_utc": "2026-01-20 13:28:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nl0u3",
          "author": "maxpayne07",
          "text": "On LM studio, on the last step of reasoning, its starts a loop of repetition.  \n\n||\n|:-|",
          "score": 1,
          "created_utc": "2026-01-20 12:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0no5zm",
              "author": "yoracale",
              "text": "Did you disable repeat penalty?",
              "score": 1,
              "created_utc": "2026-01-20 12:41:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0np93r",
                  "author": "maxpayne07",
                  "text": "yes, just put it to value 1, solved!!! Thanks",
                  "score": 1,
                  "created_utc": "2026-01-20 12:49:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tzuge",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": 1,
              "created_utc": "2026-01-21 10:36:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pkbck",
          "author": "DuckyBlender",
          "text": "Will dynamic nvfp4 quants come out?",
          "score": 1,
          "created_utc": "2026-01-20 18:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0q1qym",
              "author": "Opposite-Station-337",
              "text": "https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4/tree/main\n\nAlready are. \n\n:edit: sorry, missed dynamic.",
              "score": 1,
              "created_utc": "2026-01-20 19:41:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tzur2",
              "author": "yoracale",
              "text": "After llamacpp fixed some bugs, we've updated and reuploaded GLM-47-Flash.\n\nYou should now see much much better outputs!! üôè\n\nYou will need to redownload and follow Zai's parameters which is on the model card!",
              "score": -1,
              "created_utc": "2026-01-21 10:37:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qiu5w8",
      "title": "GLM-4.7-Flash GGUFs updated - now produces much better outputs!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qiu5w8/glm47flash_ggufs_updated_now_produces_much_better/",
      "author": "yoracale",
      "created_utc": "2026-01-21 10:13:23",
      "score": 149,
      "num_comments": 30,
      "upvote_ratio": 0.99,
      "text": "Hey guys after the issues in the past day or so, llama.cpp has fixed a bug which caused the model to loop and produce poor outputs. Huge thanks to the `llama.cpp` team and all contributors for the fix: [https://github.com/ggml-org/llama.cpp/pull/18980](https://github.com/ggml-org/llama.cpp/pull/18980)\n\nWe‚Äôve reconverted and reuploaded the model, so **you‚Äôll need to re-download it** for the fix to take effect:  \n[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nThe issue was GLM 4.7 Flash did not set `\"scoring_func\": \"sigmoid\"`in the config.json file. We added the metadata in, so no need to reinstall llama.cpp, just re-download the quants.\n\nAfter our testing, outputs are **significantly improved**, and you should be able to use Z.ai‚Äôs recommended sampling settings with great results:\n\n* **General use:** `--temp 1.0 --top-p 0.95`\n* **Tool-calling:** `--temp 0.7 --top-p 1.0`\n* If using llama.cpp, don't forget to set `--min-p 0.01` as the default is 0.1\n\nNo need to update llama.cpp, just redownload the quants.\n\nGuide: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)\n\nLet us know if you notice the improvement!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qiu5w8/glm47flash_ggufs_updated_now_produces_much_better/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0tzhea",
          "author": "PixelatedCaffeine",
          "text": "Thank you!",
          "score": 6,
          "created_utc": "2026-01-21 10:33:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0u095h",
              "author": "danielhanchen",
              "text": ":) Let me know how it goes!",
              "score": 2,
              "created_utc": "2026-01-21 10:40:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0wpnwy",
                  "author": "PixelatedCaffeine",
                  "text": "It's working without any loops now, thanks! I couldn't compare the generation speed, but looking good so far!",
                  "score": 1,
                  "created_utc": "2026-01-21 19:14:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uiuwb",
          "author": "[deleted]",
          "text": "Confirmed fixed in llama.cpp and the new GLM-4.7-Flash-Q4\\_K\\_M.gguf is sweet. \n\nThe generation speed slowed a lot I don't know which part is responsible.",
          "score": 4,
          "created_utc": "2026-01-21 13:00:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ulii6",
              "author": "yoracale",
              "text": "Awesome, mmm might be a FA issue, i remember theyre still trying to optimize it.",
              "score": 2,
              "created_utc": "2026-01-21 13:16:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0u6kgj",
          "author": "Appropriate_Car_5599",
          "text": "is GLM good model for general reasoning not just for coding tasks? or will qwen3 be better for that case?",
          "score": 2,
          "created_utc": "2026-01-21 11:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ubmfc",
              "author": "danielhanchen",
              "text": "Yes GLM is good for all! I would actually try using it as a speculator for the larger ones as well maybe!",
              "score": 3,
              "created_utc": "2026-01-21 12:12:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0wbhyp",
              "author": "zoyer2",
              "text": "so far code-wise imo its better than **Qwen3 30B A3B instruct**. I would say it might be a bit more stable than **Qwen3-Next-80B-A3B** but **Next** seems to be able to complete more complex tasks but fails a bit more often.",
              "score": 1,
              "created_utc": "2026-01-21 18:12:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0uax2s",
          "author": "alhinai_03",
          "text": "Has the inference speed issue been fixed as well? And can we use ```-fa on``` now?",
          "score": 2,
          "created_utc": "2026-01-21 12:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ubkes",
              "author": "danielhanchen",
              "text": "I think you're referring to https://github.com/ggml-org/llama.cpp/pull/18953 right - there seems to be some issues on the PR :(",
              "score": 1,
              "created_utc": "2026-01-21 12:11:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ud1lc",
                  "author": "alhinai_03",
                  "text": "Yes, I will keep waiting until the PR gets merged. My system always offloads things to system ram for some reason with -fa off which massively degrades performance for me. I really can't wait to try this model, seeing how well it performs while people are running it broken is crazy.",
                  "score": 3,
                  "created_utc": "2026-01-21 12:22:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0x185t",
                  "author": "TokenRingAI",
                  "text": "The PR is working fine for me",
                  "score": 1,
                  "created_utc": "2026-01-21 20:06:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uepwf",
          "author": "zoyer2",
          "text": "Thanks! Works as intended now I believe (not sure if the notable speed drop after some context load is fixable), managed to pass my coding tests! Looks like a solid model",
          "score": 2,
          "created_utc": "2026-01-21 12:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11p9eo",
              "author": "yoracale",
              "text": "Awesome, thanks for trying. have you tried disabling flashattention? might help speed",
              "score": 1,
              "created_utc": "2026-01-22 13:57:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14zk11",
                  "author": "zoyer2",
                  "text": "yep sure helps! but only for a short while :,D waiting for llama.cpp fix i guess",
                  "score": 1,
                  "created_utc": "2026-01-22 23:15:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uzby4",
          "author": "Sad-Masterpiece-4730",
          "text": "Can you explain please what is the right way to understand how much q8_k_xl is better than q4_k_xl? Are there any benchmarks or is there a scientific way to see this info? Regarding glm4.7 flash or in general usecase. Thanks.",
          "score": 2,
          "created_utc": "2026-01-21 14:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v29at",
              "author": "yoracale",
              "text": "Different models will always have difference quantization sensitivity but for now, you can view some analysis and graphs here: [https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)",
              "score": 2,
              "created_utc": "2026-01-21 14:46:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0x45xp",
                  "author": "NoahFect",
                  "text": "Is there *any* document that explains what the different parts of the filenames mean?  q4/q8 are obvious enough, but what are _k_ and _xl_ and _iq_ and the rest?",
                  "score": 1,
                  "created_utc": "2026-01-21 20:20:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11egz4",
          "author": "marko_mavecki",
          "text": "Works way better now. I am getting 45 t/s on my dual RTX3060 with the following complete command line. Remember that this is for CUDA only and you have to modify \"\\~/models/\" path - this is the place where you have to have the model downloaded to.\n\ndocker run --gpus all -p 11434:11434 -v \\~/models/:/models ghcr.io/ggml-org/llama.cpp:full-cuda --server -m /models/GLM-4.7-Flash-UD-Q4\\_K\\_XL.gguf --jinja --threads -8 --ctx-size 20000 --temp 0.7 --top-p 0.95  --port 11434 --host 0.0.0.0\n\nThreads param is only needed in case CPU needs to take over some calculations. But if the whole thing fits your GPU then it is almost useless.  \nThe model has been downloaded from [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/blob/main/GLM-4.7-Flash-UD-Q4\\_K\\_XL.gguf](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/blob/main/GLM-4.7-Flash-UD-Q4_K_XL.gguf)\n\nKudos to u/danielhanchen for quick reaction to feedback!",
          "score": 2,
          "created_utc": "2026-01-22 12:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11p1xw",
              "author": "yoracale",
              "text": "Amazing thanks so much for trying again! And awesome to hear it works for you",
              "score": 1,
              "created_utc": "2026-01-22 13:56:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0u06sk",
          "author": "danielhanchen",
          "text": "As an example after fixing the `\"scoring_func\": \"sigmoid\"` issue, we tried a long convo:\n\n    Hi\n    What is 2+2\n    Create a Python Flappy Bird game\n    Create a totally different game in Rust\n    Find bugs in both\n    Make the 1st game I mentioned but in a standalone HTML file\n    Find bugs and show the fixed game\n\nAnd we get:\n\nhttps://preview.redd.it/h8ptha1floeg1.png?width=1422&format=png&auto=webp&s=4539e6bc931b10d31df9de9336f2416a244cd2ff\n\n**For LM Studio, disable** `repeat_penalty` (this causes issues rather) or set it to 1.0! And use `--temp 1.0 --min-p 0.01 --top-p 0.95`",
          "score": 2,
          "created_utc": "2026-01-21 10:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0u1xb4",
              "author": "Medium_Chemist_4032",
              "text": "Which quants?",
              "score": 2,
              "created_utc": "2026-01-21 10:55:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0u1yn2",
                  "author": "yoracale",
                  "text": "Q4\\_K\\_XL and Q2\\_K\\_XL",
                  "score": 3,
                  "created_utc": "2026-01-21 10:55:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0ujmwa",
              "author": "__Maximum__",
              "text": "What about other game in Rust? And 2+2",
              "score": 1,
              "created_utc": "2026-01-21 13:05:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0wc5yq",
              "author": "zoyer2",
              "text": "https://preview.redd.it/4ar6zdf8uqeg1.png?width=1503&format=png&auto=webp&s=fa87585b583bf8112337d2d6b9d19f8fd8bac10a\n\ncreate in one html file using canvas a 2d platformer game, features: camera following the player. Procedural generated world with trees, rocks. Collision system, weather system. Make it complete and complex, fully experience.\n\n\\- worked great, better output than Qwen Next 80B and other 30B models.",
              "score": 1,
              "created_utc": "2026-01-21 18:15:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjy258",
      "title": "Fine-tuning Embedding models in Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/i0ojvxvg9xeg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-22 15:49:36",
      "score": 107,
      "num_comments": 5,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qjy258/finetuning_embedding_models_in_unsloth/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o12jo43",
          "author": "danielhanchen",
          "text": "Some benchmarks for 4bit QLoRA - more in our docs [https://unsloth.ai/docs/new/embedding-finetuning](https://unsloth.ai/docs/new/embedding-finetuning)\n\nhttps://preview.redd.it/jhnixy1pfxeg1.png?width=1109&format=png&auto=webp&s=142af4191cfc72fd509c349b719958fef63c31e4",
          "score": 3,
          "created_utc": "2026-01-22 16:23:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12ovqu",
          "author": "larrytheevilbunnie",
          "text": "Does this work for clip/siglip?",
          "score": 2,
          "created_utc": "2026-01-22 16:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12srjc",
              "author": "danielhanchen",
              "text": "Oh we do support VLMs like Gemma which has a siglip part - so I guess yes? Maybe try loading it and see if it works (any model name works)",
              "score": 4,
              "created_utc": "2026-01-22 17:04:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o18dn02",
          "author": "AgileEfficiency2775",
          "text": "Awesome. Are full finetuning support for embeding model?",
          "score": 2,
          "created_utc": "2026-01-23 13:13:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18n4wb",
              "author": "yoracale",
              "text": "Yes definitely!",
              "score": 1,
              "created_utc": "2026-01-23 14:05:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ql8nnq",
      "title": "For GLM-4.7-Flash TURN OFF REPEAT PENALTY!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1ql8nnq/for_glm47flash_turn_off_repeat_penalty/",
      "author": "yoracale",
      "created_utc": "2026-01-24 00:56:19",
      "score": 82,
      "num_comments": 21,
      "upvote_ratio": 0.99,
      "text": "I've seen and spoken to over 40 people and it seems a lot of people are still experiencing issues with GLM-4.7-Flash but after they disable repeat penalty or set it to 1.0, it all got solved.\n\nSo please, turn it off as it screws up the model badly and maybe set by default for you! [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nRemember\n\n* For general use-case:  `--temp 1.0 --top-p 0.95`\n* For tool-calling:  `--temp 0.7 --top-p 1.0`\n* If using llama.cpp, set `--min-p 0.01` as llama.cpp's default is 0.1\n* Repeat penalty: Disable it, or set `--repeat-penalty 1.0`\n\nLet us know if you're still receiving bad outputs after this (keep in mind sometimes you may get bad outputs or looping - as such with any other model like GPT5 or Gemini, this is normal, but if it happens a lot this isn't normal).\n\nHave a good friday and weekend!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1ql8nnq/for_glm47flash_turn_off_repeat_penalty/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o1codiv",
          "author": "ScoreUnique",
          "text": "Seems like that's what I needed to get my open coder going, thanks a lot. I'm on Unsloth Q5, yesterday I found them radically bad bur today they've made up for it after I use your recommended settings. Finally something that is very independent and agentic that runs locally.",
          "score": 7,
          "created_utc": "2026-01-24 01:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cqg7z",
              "author": "yoracale",
              "text": "Amazing to hear and glad it worked for you :)",
              "score": 1,
              "created_utc": "2026-01-24 02:04:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1d4pm3",
                  "author": "ClimateBoss",
                  "text": "how do you \"disable\" it ? 1.0 = disable ?",
                  "score": 1,
                  "created_utc": "2026-01-24 03:28:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dfw0l",
          "author": "Thrumpwart",
          "text": "Thank you. I set the temp, top-p, and min-p yesterday on the unsloth UD model. I ran into errors. Sat down just now, loaded up reddit, saw this post, and the model is humming nicely now in LM Studio.\n\nEdit: For anyone curious, I'm running the Unsloth Q6_K_XL UD quant in the latest LLM. I run ROCM 7.2 on an AMD W7900 on Ubuntu 24.04.3. I gave the model a 6200 token document, added a 20 word prompt, and I got 4.13s tts and 25.16 tok/sec. The quality is very, very good for it's size - the output is clear, concise, and very relevant to my word prompt.",
          "score": 5,
          "created_utc": "2026-01-24 04:41:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e6nbf",
              "author": "danielhanchen",
              "text": "Nice to hear that!",
              "score": 1,
              "created_utc": "2026-01-24 08:17:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eg5qr",
          "author": "gigascake",
          "text": "I hope this model where Blackwell pro 6000 based vllm.\nBut very slower then glm-4.5-air-fp8.\nWhat problem this model architecture? 80 token/s vs 10 token/s\nüò≠",
          "score": 3,
          "created_utc": "2026-01-24 09:44:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ejhhe",
              "author": "yoracale",
              "text": "This was before llama.cpp introduced FA optimizations, it should be faster now\n\nCould you test again and see?",
              "score": 1,
              "created_utc": "2026-01-24 10:15:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1en8ia",
              "author": "TaroOk7112",
              "text": "With llama.cpp we needed several days of improvements until GLM-4.7-Flash worked more or less fast and reliably.  \nI'm amazed at how fast and well still runs gpt-oss 120B. But this one is getting closer at half the size, very nice!",
              "score": 1,
              "created_utc": "2026-01-24 10:49:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1epay6",
                  "author": "Mr_Back",
                  "text": "GPT OSS 120b works perfectly for me, even with a large context. I'm getting around 10 tokens per second. GLM 4.7 air is performing terribly. The speed is awful. The most reasonable speed is with q4, followed by f16, and surprisingly, the slowest speed is with q2. I set the context to 128k, and the requests were between 33k and 40k tokens, depending on the model. The most interesting thing is that q4 had a good processing speed, better than nemotron 3 nano, but a terrible output speed. With smaller requests, q4 glm produced acceptable results.",
                  "score": 1,
                  "created_utc": "2026-01-24 11:08:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d1x6s",
          "author": "some_user_2021",
          "text": "Newbie here. Can't these default settings be part of the model?",
          "score": 2,
          "created_utc": "2026-01-24 03:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d6uhs",
              "author": "yoracale",
              "text": "You can only set it in some palces like Ollama, you cant set it auto for GGUFs in LM Studio.\n\nBut the issue is GLM Flash GGUFs don't work in Ollama, so unfortunately you have to manually set it.",
              "score": 3,
              "created_utc": "2026-01-24 03:41:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fjea7",
          "author": "LightBrightLeftRight",
          "text": "I was having all kinds of problems with loops on LM Studio, but it turns out it their built-in tools (wikipedia) just werent returning data to GLM properly. As soon as I turned their tools off with your general use settings it worked brilliantly.\n\nAlso, thank you unsloth, we love you",
          "score": 2,
          "created_utc": "2026-01-24 14:37:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1edt4e",
          "author": "cangaroo_hamam",
          "text": "(newbie) Is this something that can be set in LM Studio? I checked 'Edit model default parameters' but I don't see it.",
          "score": 1,
          "created_utc": "2026-01-24 09:23:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f2l4i",
              "author": "yoracale",
              "text": "repeat penalty should be here \n\nbelow are the default setttings. you have to turn it off or set it = 1.0\n\nhttps://preview.redd.it/s37xzradoafg1.png?width=644&format=png&auto=webp&s=9bfd8b62d742329478780d523957702f4dc89809",
              "score": 3,
              "created_utc": "2026-01-24 12:55:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ei1gb",
          "author": "Bluethefurry",
          "text": "still have issues with it where it will just repeat a token ad infinitum, even with repeat penalty off and the tool calling settings set, seems to be worse with thinking enabled but still sometimes happens with thinking off as well.",
          "score": 1,
          "created_utc": "2026-01-24 10:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1f2n55",
              "author": "yoracale",
              "text": "mmm interesting, what platform are you currently using? llama.cpp?",
              "score": 1,
              "created_utc": "2026-01-24 12:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1g6ama",
                  "author": "Bluethefurry",
                  "text": "Yes.\n\nCUDA\\_VISIBLE\\_DEVICES=0,1 LLAMA\\_CACHE=/models/cache GGML\\_CUDA\\_GRAPH\\_OPT=1 /app/llama-server --port ${PORT} -hf unsloth/GLM-4.7-Flash-GGUF:Q4\\_K\\_XL --ctx-size 131072 --temp 1.0 --top-p 0.95 --min-p 0.01 --no-webui --no-warmup --jinja --main-gpu 0 --flash-attn auto --cache-reuse 128 --no-mmap --slot-save-path /models/cache/slots/ --parallel 1 --threads 1 --batch-size 4096\n\nRTX 3090 + RTX 3060 running b7819 (557515be1)\n\ni noticed i used the default temp/top-p here so i will try switching to the tool calling recommendation to see if i still get looping or not.",
                  "score": 1,
                  "created_utc": "2026-01-24 16:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g3jq7",
          "author": "NoahFect",
          "text": "Something I've been seeing with llama-server that I haven't noticed before is that the tokens/sec output rate steadily decreases from one prompt to the next.  Typically I'll hit 'New chat' each time I prompt it, and it will start out near 100 tokens/sec and eventually end up near 10 after a few chat sessions have been run.\n\nAnyone else seeing this behavior?  Hardware is 96GB Blackwell, command line is:\n\n    llama-server ^\n     --model GLM-4.7-Flash-BF16-00001-of-00002.gguf ^\n     --jinja              ^\n     --threads -1         ^\n     --ctx-size 131072    ^\n     --repeat-penalty 1.0 ^\n     --temp 1.0           ^\n     --top-p 0.95         ^\n     --min-p 0.01         ^\n     --port 2080          ^\n     --log-file glm47g.log\n\nAm I missing any useful/necessary parameters?\n\nHave to say, so far at least, GLM 4.7 Flash is not holding up very well next to Qwen (specifically Qwen30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf.)  That one has really been punching above its weight class.",
          "score": 1,
          "created_utc": "2026-01-24 16:16:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1g5onw",
          "author": "JMowery",
          "text": "Have these settings set and I get infinite loops all the time with Pydantic AI with GLM 4.7 Flash via llama.cpp. Can't even do the simplest test response. No other model I have installed (around 2 dozen) has any issue.",
          "score": 1,
          "created_utc": "2026-01-24 16:26:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhbr0p",
      "title": "glm 4.7 flash is out gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-19 18:08:55",
      "score": 18,
      "num_comments": 7,
      "upvote_ratio": 0.79,
      "text": "Guys do you plan to release quantisation variants of GLM-4.7 flash ? Its 30b a3b, unsloth chat template fixes are da best.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qhbr0p/glm_47_flash_is_out_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0mfece",
          "author": "yoracale",
          "text": "It's out now! GGUF: [https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n\nTweet: [https://x.com/UnslothAI/status/2013482180564132092](https://x.com/UnslothAI/status/2013482180564132092)\n\nGuide: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)",
          "score": 1,
          "created_utc": "2026-01-20 06:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iq727",
          "author": "loadsamuny",
          "text": "architecture looks like a renamed deepseekv3, a pull request is in for it in llama.cpp so maybe tomorrow‚Ä¶.",
          "score": 5,
          "created_utc": "2026-01-19 18:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jnj3f",
          "author": "bobeeeeeeeee8964",
          "text": "i test it by the f16 gguf, and it seems the runing in a good speed, BUT it output garbage instead of proper text. We need waiting and see what going in this PR https://github.com/ggml-org/llama.cpp/pull/18936",
          "score": 3,
          "created_utc": "2026-01-19 20:55:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0k8is8",
              "author": "remghoost7",
              "text": "Pull request was merged and closed about an hour ago.  \nIt seems like they figured it out.",
              "score": 2,
              "created_utc": "2026-01-19 22:38:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0j9v1l",
          "author": "neph1010",
          "text": "[https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF](https://huggingface.co/ngxson/GLM-4.7-Flash-GGUF)",
          "score": 1,
          "created_utc": "2026-01-19 19:51:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jk10j",
              "author": "noctrex",
              "text": "Still needs more work, its looping indefinitely",
              "score": 3,
              "created_utc": "2026-01-19 20:39:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0jzshk",
              "author": "Clqgg",
              "text": "this one is omega broken",
              "score": 2,
              "created_utc": "2026-01-19 21:55:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qi3m95",
      "title": "Is GLM-4.7-Flash still looping / repeating for you?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qi3m95/is_glm47flash_still_looping_repeating_for_you/",
      "author": "yoracale",
      "created_utc": "2026-01-20 15:11:00",
      "score": 17,
      "num_comments": 37,
      "upvote_ratio": 0.96,
      "text": "Hey guys many of you are still experiencing looping/repetition issues.\n\n**Jan 21 UPDATE: llama.cpp has fixed a bug which caused the model to loop and produce poor outputs. We have reconverted and reuploaded the model so outputs should be much much better now.**\n\nYou can now use Z.ai's recommended parameters and get great results:  \n\\- For general use-case:  `--temp 1.0 --top-p 0.95`  \n\\- For tool-calling:  `--temp 0.7 --top-p 1.0`\n\nIf you still experience looping issues even after following all these steps, please let us know!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qi3m95/is_glm47flash_still_looping_repeating_for_you/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0ppvlj",
          "author": "Scared_Mycologist_92",
          "text": "i use lm-studio and try repeat penality 1,2 ..this fixed it for me and i think temp 0,6 or 0,9 or something. anything else beside repeat penality couldnt fix overthinking and those psychotic loops at any answer i tried. the model itself is pretty amazing",
          "score": 1,
          "created_utc": "2026-01-20 18:47:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0raa4l",
              "author": "yoracale",
              "text": "Interesting, for many people, adding repeat penalty didn't do anything for them unfortunately.",
              "score": 1,
              "created_utc": "2026-01-20 23:14:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0tvekv",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0q2llt",
          "author": "zoyer2",
          "text": "No loop or repeating but just in general suck at coding, making small mistakes here and there (Have only tested the quant versions). Something needs to be fixed",
          "score": 1,
          "created_utc": "2026-01-20 19:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tveuj",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0u3wbc",
                  "author": "zoyer2",
                  "text": "damn, much better, great work! :)",
                  "score": 2,
                  "created_utc": "2026-01-21 11:12:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0q4czx",
          "author": "epigen01",
          "text": "Yup unsloths are not usable for me gonna wait it out",
          "score": 1,
          "created_utc": "2026-01-20 19:53:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvf1q",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qepz7",
          "author": "Final-Rush759",
          "text": "No problem,  I use mlx version.",
          "score": 1,
          "created_utc": "2026-01-20 20:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvf6d",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 2,
              "created_utc": "2026-01-21 09:56:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s6fur",
          "author": "Calm_Management_5090",
          "text": "Hi, I am using Q5\\_K\\_M on lm studio and get frequent looping with both unsloth and [z.ai](http://z.ai) suggested options above once the context gets a few thousand tokens long. Thank you for all the unsloth work on this and previous models, I will keep my eyes open for updates. Best regards.",
          "score": 1,
          "created_utc": "2026-01-21 02:13:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tba8e",
              "author": "kripper-de",
              "text": "Did you also try Q4?",
              "score": 1,
              "created_utc": "2026-01-21 06:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tvfka",
                  "author": "yoracale",
                  "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
                  "score": 1,
                  "created_utc": "2026-01-21 09:56:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0tvff3",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:56:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11khx9",
          "author": "imqqmi",
          "text": "I've downloaded the updated version of the model after you've fixed it: GLM-4.7-Flash-UD-Q6\\_K\\_XL.gguf in LM-Studio and tried with a lot of combinations of settings ie temp=0 0.5 0.8 1.0, top\\_k = 0, 20, 40, top\\_p = 0.5 0.8 0.95 1, min\\_p 0 0.01 0.05, repeat penalty 1.1 1.2 1.8. Results remain the same. Also tried turning off flash attention. I even tried to compile the latest llama.cpp release in wsl ubuntu. I got similar speeds (90-100t/s) on my 5090 32GB and 4070ti 12GB combo, same results. The model doesn't appear stable to me. I also downloaded the LM-Studio version Q4, same issue. I've also increased the number of experts (ie 8 16 or 20) used to hopefully get a better response but alas.\n\nHere's the llama-server command:\n\nllama-server -m /mnt/e/LMStudioModels/unsloth/GLM-4.7-Flash-GGUF/GLM-4.7-Flash-UD-Q6\\_K\\_XL.gguf --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --n-gpu-layers 99 --flash-attn on --metrics --ubatch-size 512 --batch-size 512 --presence-penalty 1.5 --ctx-size 16000 --temp 1.0 --top-k 20 --top-p 0.95 --min-p 0.01 --repeat-penalty 1.1 --threads 12 --threads-http 5 --no-mma\n\np -kvo -dev CUDA0,CUDA1 --override-kv llama.expert\\_used\\_count=int:8 --batch-size 512 --jinja\n\nBut the output remains the same: garbage and repetition while still thinking, no final output. It also often lists numbers with one number on a line as if to count the characters but no characters appear. And it counts to 16 then starts repeating the pattern.",
          "score": 1,
          "created_utc": "2026-01-22 13:31:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o176rry",
              "author": "yoracale",
              "text": "I think the biggest issue is you turned on repeat penalty. You must turn it off! Over 10 people said after turning repeat penalty off, there's no more bad outputs",
              "score": 1,
              "created_utc": "2026-01-23 07:21:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17jbo1",
                  "author": "imqqmi",
                  "text": "Ah I must have missed that comment. It works, thanks!",
                  "score": 2,
                  "created_utc": "2026-01-23 09:15:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0p3js8",
          "author": "Sensitive_Song4219",
          "text": "Happy to test, have just grabbed *unsloth/GLM-4.7-Flash-GGUF* in LM Studio: what are the recommended settings  for it? (We don't seem to have a dry-multiplier?)",
          "score": 1,
          "created_utc": "2026-01-20 17:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0r9z4b",
              "author": "yoracale",
              "text": "Edit: llamacpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)\nI wrote it in my post. Use the parameters above and disable repeat penalty.",
              "score": 1,
              "created_utc": "2026-01-20 23:12:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rd2hu",
                  "author": "Sensitive_Song4219",
                  "text": "I see that now - thank you! I also see there's *zai-org/glm-4.7-flash* available now in LM Studio as-of an hour or two ago (I assume that's z-ai's release).\n\nWill test them both properly! In my early testing thinking seems to run fine (no loops); performance at small contexts (<4k) on my machine seems similar to Qwen3 30B A3B 2507. Looking forward to seeing how the output quality is (and how performance is at larger contexts!)",
                  "score": 2,
                  "created_utc": "2026-01-20 23:29:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0pvp4m",
          "author": "ZeWishmastR",
          "text": "Same issue here",
          "score": 1,
          "created_utc": "2026-01-20 19:13:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tvdn5",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0q6gfx",
          "author": "Rektile142",
          "text": "The model is responding well after using the recommended parameters, but throughput is utterly scuffed as context grows.\n\nMaybe the llama.cpp team still needs time to cook.",
          "score": 0,
          "created_utc": "2026-01-20 20:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qbd9j",
              "author": "Mr_Back",
              "text": "I really hope so. Nemotron 3 nano and GPT OSS 120b run ten times faster on my hardware when the context is increased.",
              "score": 1,
              "created_utc": "2026-01-20 20:26:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tvdw1",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pz5ow",
          "author": "Mr_Back",
          "text": "I'm more concerned about performance. I have a weak system, but even with it, similarly sized models perform much better (GPT OSS 120b, Qwen Next q6, Qwen3 Coder 30B, Nemotron 3 Nano). It's not about your specific quantization, but about the model as a whole.  \nI'm using a relatively large context (128k). The speed is still acceptable for small requests, but when I try a request with 35-40k tokens, I don't even want to wait for a response, it's so slow.\n\nMy PC: i5 12400, 96gb ram. 4070 - 12gb vram.",
          "score": -1,
          "created_utc": "2026-01-20 19:29:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tve65",
              "author": "yoracale",
              "text": "llama.cpp fixed a bug in the implementation. We have updated all the quants and results should now be much better! Give it a test and let us know :)",
              "score": 1,
              "created_utc": "2026-01-21 09:55:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0xl09d",
                  "author": "Mr_Back",
                  "text": "https://preview.redd.it/x3mu0jnvtreg1.png?width=1280&format=png&auto=webp&s=df7e4507e88931948f20addf4ab66090e654c41c\n\nNemotron 3 nano F16 vs GLM 4.7 air Q4 UD. And yet GLM's response is shit(  \nI will wait for further improvements üí™.",
                  "score": 1,
                  "created_utc": "2026-01-21 21:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qgvg1u",
      "title": "Fine tuning Gpt oss on thinking dataset , which tokens to mask ?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qgvg1u/fine_tuning_gpt_oss_on_thinking_dataset_which/",
      "author": "Hulksulk666",
      "created_utc": "2026-01-19 05:23:52",
      "score": 16,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "from the official unsloth [notebook ](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb#) for fine tuning Gpt oss 20b ,they used\n\n    from unsloth.chat_templates import train_on_responses_only\n    gpt_oss_kwargs = dict( instruction_part = \"<|start|>user<|message|>\", response_part = \"<|start|>assistant<|channel|>final<|message|>\" )\n     trainer = train_on_responses_only( trainer, **gpt_oss_kwargs, )\n\nBut doesn't this effectively mean the thinking tokens are also being masked ? if so , how is the model actually learning from the thinking tokens of the dataset ? or am i missing something .",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qgvg1u/fine_tuning_gpt_oss_on_thinking_dataset_which/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o0g9qyp",
          "author": "im_datta0",
          "text": "Hey u/Hulksulk666  \nThanks for noticing this. Yeah if I read it right, this might be masking \"thinking/analysis\" tokens from the training loss.  \nEdit: Talked to my friends and he mentioned that masking the analysis/thinking seems to produce better down stream results and hence the decision",
          "score": 2,
          "created_utc": "2026-01-19 09:54:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hbime",
              "author": "Hulksulk666",
              "text": "Thanks , i was confused whether or not this masking meant the thinking tokens would not be part of training loss and how not being part of the training loss makes the model learn from the data . From my intuitive understanding it seemed the model should take loss if the data has some ground truth outcome like math if the solution step by step is passed onto thinking , ig i lack some deeper understanding and should resort to some papers",
              "score": 1,
              "created_utc": "2026-01-19 14:29:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0on6kq",
          "author": "WolfeheartGames",
          "text": "The reason thinking works is because it causes a dependency. What comes after thinking depends on the thinking.\n\nIf you train a model's thinking to be too far out of its own distribution, it will degrade.",
          "score": 1,
          "created_utc": "2026-01-20 15:50:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tojyj",
              "author": "Hulksulk666",
              "text": "That makes sense",
              "score": 2,
              "created_utc": "2026-01-21 08:49:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ucp5p",
          "author": "burntoutdev8291",
          "text": "Hmmm doesn't the think tokens come after that response_part? I could be wrong, never fine tuned gpt oss.",
          "score": 0,
          "created_utc": "2026-01-21 12:19:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk109a",
      "title": "The best (tiny) model I can run on my phone",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qk109a/the_best_tiny_model_i_can_run_on_my_phone/",
      "author": "gized00",
      "created_utc": "2026-01-22 17:36:05",
      "score": 11,
      "num_comments": 10,
      "upvote_ratio": 0.93,
      "text": "I work in ML and I am quite familiar with Llama, fine tuning, etc. but I always work on 10s of billions parameters. \n\nI would like to train a tiny model that I can run on my phone (Pixel 8) and unsloth seems the right place to start with this (but feel free to suggest other solutions). I have some difficulties to identify what can realistically run (with a decent num tokens/s). Is a 1B model a reasonable choice if I am quantizing it?\n\nAny other suggestions?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qk109a/the_best_tiny_model_i_can_run_on_my_phone/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o134ezq",
          "author": "BenniB99",
          "text": "I believe Gemma3n was made specifically with phone and edge usage in mind.  \nIt is also a pretty decent model for its size imo (and multimodal).  \nAfaik it is also possible to switch between 2B and 4B effective parameter on the fly with it.",
          "score": 10,
          "created_utc": "2026-01-22 17:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13ow28",
              "author": "gized00",
              "text": "Nice! I will take a look.\nThank you",
              "score": 2,
              "created_utc": "2026-01-22 19:27:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17icir",
          "author": "schlammsuhler",
          "text": "I like trinity nano a 6b moe with 1b active. Nice persona\n\nhttps://huggingface.co/arcee-ai/Trinity-Nano-Preview-GGUF",
          "score": 2,
          "created_utc": "2026-01-23 09:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13flhy",
          "author": "Late_Huckleberry850",
          "text": "If you get the Apollo app that has some. Idk if they have an android version or not though",
          "score": 1,
          "created_utc": "2026-01-22 18:46:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13ub5t",
          "author": "[deleted]",
          "text": "Function Gemma or maybe liquid foundation models will be best fit for it",
          "score": 1,
          "created_utc": "2026-01-22 19:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13woxx",
          "author": "maxtheman",
          "text": "I have a pixel 9 and am working on fine-tuning functionalgemma, which is working great, but it really depends on your task. 1B or less can work great on a distilled task, but don't expect 90%+ perf unless you overfit the shit out of it and consider doing multiple types of fine-tuning.\n\n  \nOn pixel the hardest part, for me at least, will be getting it on an api that can actually access your gpu. I am targeting huggingfacejs for now due to the ease of use, but I don't know a better way to deploy than that or get on the google npu.",
          "score": 1,
          "created_utc": "2026-01-22 20:03:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o141j0z",
              "author": "gized00",
              "text": "It's a fairly simple task but it will require a bit of RL",
              "score": 1,
              "created_utc": "2026-01-22 20:26:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14w8qp",
          "author": "Azuriteh",
          "text": "The best model for on-device usage right now is probably [https://huggingface.co/LiquidAI/LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B), with 4 bit quantization it'd probably be alright!",
          "score": 1,
          "created_utc": "2026-01-22 22:58:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1684aw",
          "author": "Sure_Explorer_6698",
          "text": "Ive used Llama-3 1-3B, and Qwen-2&3 1-3B with llama.cpp on:\n\nSamusng A16 4Gb, and Samsung S20FE 6Gb. Both work great, but Llama seems more conversational.",
          "score": 1,
          "created_utc": "2026-01-23 03:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o138iyq",
          "author": "Large-Example-1275",
          "text": "You can use the ¬´¬†Locally¬†¬ª app to check supported models on your device, but I don't know if it's available on Android.",
          "score": 0,
          "created_utc": "2026-01-22 18:15:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3ti9",
      "title": "Train Llama-3.2-11b-Vision-Instruct with GRPO",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qj3ti9/train_llama3211bvisioninstruct_with_grpo/",
      "author": "darkwigga",
      "created_utc": "2026-01-21 17:13:39",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.91,
      "text": "Hi,\n\nI was working on training Llama-3.2-11b-Vision-Instruct with GRPO using unsloth and trl grpotrainer.\n\nAfter starting the training, I am getting the following error\n\n`raise ValueError(\"\\`aspect\\_ratio\\_ids\\` must be provided if \\`pixel\\_values\\` is provided\")\\`\n\nMy trainer code worked for Qwen and Gemma.\n\n  \nCode for trainer and config\n\n    from\n     trl \n    import\n     GRPOConfig, GRPOTrainer\n    \n    \n    training_args = GRPOConfig(\n    ¬† ¬† learning_rate=learning_rate,\n    ¬† ¬† adam_beta1=adam_beta1,\n    ¬† ¬† adam_beta2=adam_beta2,\n    ¬† ¬† weight_decay=weight_decay,\n    ¬† ¬† warmup_ratio=warmup_ratio,\n    ¬† ¬† lr_scheduler_type=lr_scheduler_type,\n    ¬† ¬† optim=optim,\n    ¬† ¬† logging_steps=logging_steps,\n    ¬† ¬† log_completions=log_completions,\n    ¬† ¬† per_device_train_batch_size=per_device_train_batch_size,\n    ¬† ¬† gradient_accumulation_steps=gradient_accumulation_steps, ¬†\n    # Increase to 4 for smoother training\n    ¬† ¬† num_generations=num_generations, ¬†\n    # Decrease if out of memory\n    ¬† ¬† max_prompt_length=max_prompt_length,\n    ¬† ¬† max_completion_length=max_completion_length,\n    ¬† ¬† num_train_epochs=num_train_epochs, ¬†\n    # Set to 1 for a full training run\n    ¬† ¬† \n    # max_steps = 60,\n    ¬† ¬† save_steps=save_steps,\n    ¬† ¬† max_grad_norm=max_grad_norm,\n    ¬† ¬† report_to=report_to, ¬†\n    # Can use Weights & Biases\n    ¬† ¬† output_dir=output_dir,\n    ¬† ¬† \n    # # Below enables GSPO:\n    ¬† ¬† importance_sampling_level=importance_sampling_level,\n    ¬† ¬† mask_truncated_completions=mask_truncated_completions,\n    ¬† ¬† loss_type=loss_type,\n    )\n    \n    \n    from\n     unsloth.trainer \n    import\n     UnslothVisionDataCollator\n    \n    \n    trainer = GRPOTrainer(\n    ¬† ¬† model=model,\n    ¬† ¬† args=training_args,\n    ¬† ¬† \n    # Pass the processor to handle multimodal inputs\n    ¬† ¬† data_collator=UnslothVisionDataCollator(model, processor),\n    ¬† ¬† processing_class=processor,\n    ¬† ¬† reward_funcs=[\n    ¬† ¬† ¬† ¬† get_reward,\n    ¬† ¬† ],\n    ¬† ¬† train_dataset=train_ds,\n    ¬† ¬† eval_dataset=test_ds,\n    )\n    \n    \n    trainer.train()\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qj3ti9/train_llama3211bvisioninstruct_with_grpo/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o10dbtw",
          "author": "im_datta0",
          "text": "Hey u/darkwigga it would be helpful if you can tell us what dataset are you using and if you're doing any pre proessing...\n\nI think you might want to look at this notebook [https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2\\_5\\_7B\\_VL\\_GRPO.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb) for reference.\n\nThis is very possibly some issue with preprocessing the dataset. Would be happy to help if you can provide further information\n\nEdit: If you need any further assistance, feel free to open a github issue or hop on to our discord server",
          "score": 1,
          "created_utc": "2026-01-22 07:44:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10g9ll",
              "author": "im_datta0",
              "text": "For example, if you do something like\n\n\n\n    instruction = \"You are an expert radiographer. Describe accurately what you see in this image.\"\n    # image is a PIL object\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction}\n        ]}\n    ]\n    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n    inputs = tokenizer(\n        image,\n        input_text,\n        add_special_tokens = False,\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n\n\n\nThe inputs would contain input\\_ids, attention\\_mask, pixel\\_values, aspect\\_ratio\\_ids, aspect\\_ratio\\_mask....",
              "score": 1,
              "created_utc": "2026-01-22 08:10:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o18w7ug",
                  "author": "darkwigga",
                  "text": "Hi, my dataset is a custom one formatted like similar to the notebook you shared. My training works perfectly with Qwen, but I was just facing the issue with Llama",
                  "score": 1,
                  "created_utc": "2026-01-23 14:51:21",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkntk1",
      "title": "RL for learning math",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qkntk1/rl_for_learning_math/",
      "author": "goldlord44",
      "created_utc": "2026-01-23 10:57:30",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.81,
      "text": "Hi there,\n\nI was wondering if anyone here has some advice for using unsloth to train models to be better at math?\n\nI am looking at using math text books and research papers to be able to post-train my models, specifically maths, physics and statistics. (And maybe some HF datasets).\n\nI am not sure which is the ideal post training technique for this and am looking for some direction advice before I dive head first into this.\n\nI am happy both with training on the raw text, but also understand that some post-processing is always required.\n\nI have a single Rtx Pro 6000 96GB so was hoping to train something like OSS-120B or some of the mid sized models like qwen3 30B.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qkntk1/rl_for_learning_math/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o17xu2v",
          "author": "yoracale",
          "text": "We have many RL notebooks for math, that might be a good starting point: [https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl](https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl)\n\nE.g. our Qwen3-Advanced GRPO notebook has a concrete example for math: [https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3\\_(4B)-GRPO.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)\n\nhttps://preview.redd.it/bh5w1be733fg1.png?width=2590&format=png&auto=webp&s=5f4cd2800213de88afd18c2b5d8d7dfec5959a1a",
          "score": 4,
          "created_utc": "2026-01-23 11:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a989h",
              "author": "samplebitch",
              "text": "FYI I think reddit messed up your link - here's the working URL for anyone else who might want to follow it:\n\nhttps://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb",
              "score": 2,
              "created_utc": "2026-01-23 18:35:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1b5kdr",
                  "author": "yoracale",
                  "text": "Oh thank you you're right, idk why reddit always does that üòÖ",
                  "score": 1,
                  "created_utc": "2026-01-23 21:06:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qk1qy4",
      "title": "Guide to use unsloth on windows",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qk1qy4/guide_to_use_unsloth_on_windows/",
      "author": "LahmeriMohamed",
      "created_utc": "2026-01-22 18:02:54",
      "score": 6,
      "num_comments": 15,
      "upvote_ratio": 0.88,
      "text": "hello guys hope i recieve help , i have recently installed unsloth to try it fine-tuning process , but due to dependencies  conflicts i had to remove it , if anyone can help me to fix this issue , my current env \npython 3.11.2\ntorch 2.5.1+cu121 \nif i ran install unsloth , it remove the cuda installation , so i used --no-deps instruction , but when running it , it require vllm , accelerate error .. .\ncan  you provide me with better/compatible versions ? \nthank you",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qk1qy4/guide_to_use_unsloth_on_windows/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o149lq3",
          "author": "Educational_Rent1059",
          "text": "Install WSL2 I highly recommend that. And hook up VSCode and you are good to go with Linux within windows without any overhead. The benchmarks in training vs native Ubuntu is between 1-4% diffs (native ahead) , if you do pure GPU work and ML, it's no diff at all.",
          "score": 3,
          "created_utc": "2026-01-22 21:04:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o149z5e",
              "author": "LahmeriMohamed",
              "text": "since i have 1ssd main system and hdd for env saving , can i save the wls env in the hdd and use it in env-variable to be accessible globaly ?",
              "score": 1,
              "created_utc": "2026-01-22 21:05:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14ag0l",
                  "author": "Educational_Rent1059",
                  "text": "Your WSL ext4 drive will be a single file that you can move anywhere into any HDD you like in windows. If you prefer to have it on a different disk etc that works as well. Everything you do will be stored within that single file (it's basically your entire OS and all files within it in one file on windows)",
                  "score": 2,
                  "created_utc": "2026-01-22 21:08:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o14aosb",
                  "author": "Educational_Rent1059",
                  "text": "You can also create another \"drive\" (single file) and attach it into linux, it's a bit messy as you need to attach it in windows command shell first, and then go into the linux WsL2 cli and attach it there too, but use GPT for guidance and do that. Be careful as so you don't delete any files by wrong instructions from GPT as they hallucinate alot",
                  "score": 1,
                  "created_utc": "2026-01-22 21:09:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15bmc0",
              "author": "fiery_prometheus",
              "text": "Except that wsl likes to crash under heavy workloads, I've had so many crashes when fine-tuning¬†",
              "score": 1,
              "created_utc": "2026-01-23 00:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15vh4u",
                  "author": "Educational_Rent1059",
                  "text": "I'm running WSL on 2  machines  with 4 gpus never had a single issue in years. I think the issue is either your bad/faulty hardware (probably memory or something else) or your environment/drivers. Yeah, just blaming WSL randomly for a crash without any input into what CAUSED the crash is not helpful at all - rather misleading.",
                  "score": 1,
                  "created_utc": "2026-01-23 02:08:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14t3zp",
          "author": "yoracale",
          "text": "We have this Windows guide which we revamped around a month ago, especially for WSL: [https://unsloth.ai/docs/get-started/install/windows-installation](https://unsloth.ai/docs/get-started/install/windows-installation)",
          "score": 2,
          "created_utc": "2026-01-22 22:41:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13lzmu",
          "author": "immediate_a982",
          "text": "Use virtual environments",
          "score": 1,
          "created_utc": "2026-01-22 19:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o145msj",
              "author": "LahmeriMohamed",
              "text": "did not work",
              "score": 0,
              "created_utc": "2026-01-22 20:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13mp3c",
          "author": "CMPUTX486",
          "text": "Use docker.. I think it save more time",
          "score": 1,
          "created_utc": "2026-01-22 19:18:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o145kpy",
              "author": "LahmeriMohamed",
              "text": "i am low on ram 1 * 8gb , so avoid it",
              "score": 0,
              "created_utc": "2026-01-22 20:45:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o18rmc7",
                  "author": "StardockEngineer",
                  "text": "You don‚Äôt have enough system in any case.  Use colabs",
                  "score": 2,
                  "created_utc": "2026-01-23 14:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}