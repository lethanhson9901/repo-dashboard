{
  "metadata": {
    "last_updated": "2026-02-25 03:09:26",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 17,
    "total_comments": 153,
    "file_size_bytes": 135631
  },
  "items": [
    {
      "id": "1rdmqp8",
      "title": "Qwen3.5 Medium models out now!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/vztwlpot9hlg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-24 17:33:48",
      "score": 297,
      "num_comments": 82,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rdmqp8/qwen35_medium_models_out_now/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o76v3p1",
          "author": "Look_0ver_There",
          "text": "Wow, just wow, but especially this ->  Qwen3.5 122B-A10B (MoE ‚Ä¢ 70GB)\n\nThank you so much Qwen team for looking after everyone at all deployment levels.  You guys are the best!",
          "score": 22,
          "created_utc": "2026-02-24 19:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76cktf",
          "author": "Significant_Fig_7581",
          "text": "Wow you guys are great! Thank you all",
          "score": 17,
          "created_utc": "2026-02-24 17:58:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76rcob",
          "author": "GCoderDCoder",
          "text": "I thought glm4.7flash was cool but Qwen never disappoints.... having vision in everything! I run everything locally and I'm about to subscribe to qwen somehow just on principle!!!",
          "score": 5,
          "created_utc": "2026-02-24 19:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76brsm",
          "author": "planetearth80",
          "text": "I have a Mac Studio Ultra (192 GB unified memory). What‚Äôs the latest model you would recommend?",
          "score": 7,
          "created_utc": "2026-02-24 17:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76cf67",
              "author": "yoracale",
              "text": "This one once it finishes uploading: [https://huggingface.co/unsloth/Qwen3.5-122B-A10B-GGUF](https://huggingface.co/unsloth/Qwen3.5-122B-A10B-GGUF)\n\nOtherwise MiniMax 2.5: [https://unsloth.ai/docs/models/minimax-m25](https://unsloth.ai/docs/models/minimax-m25)",
              "score": 11,
              "created_utc": "2026-02-24 17:57:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77mgij",
                  "author": "_VirtualCosmos_",
                  "text": "MiniMax 2.5 is great, from its quantized Q3 version, which one would you say is the best? (I have 128 GB unified memory)",
                  "score": 1,
                  "created_utc": "2026-02-24 21:27:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76c5uz",
          "author": "PixelatedCaffeine",
          "text": "I already downloaded the UD-Q2\\_K\\_XL used it in OpenCode to commit something in my project, you guys are awesome!",
          "score": 7,
          "created_utc": "2026-02-24 17:56:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77e76v",
              "author": "Turbulent_Dot3764",
              "text": "Hey, what model and context size?",
              "score": 2,
              "created_utc": "2026-02-24 20:49:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77mhvq",
                  "author": "PixelatedCaffeine",
                  "text": "It was the 35B-A3B with 131k of context size",
                  "score": 5,
                  "created_utc": "2026-02-24 21:27:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76d54r",
          "author": "Zestyclose839",
          "text": "So fast! You all have this down to a science. That was like sub-1hr post-release",
          "score": 5,
          "created_utc": "2026-02-24 18:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76ivm7",
          "author": "getpodapp",
          "text": "Can‚Äôt wait for coder variants",
          "score": 5,
          "created_utc": "2026-02-24 18:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76kqe7",
          "author": "Remarkable_Tea8039",
          "text": "Excited to give these a try! Qwen3-Coder-Next has been great already",
          "score": 6,
          "created_utc": "2026-02-24 18:34:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76wonn",
          "author": "vesuraychev",
          "text": "What speeds do you get? I get 137t/s with rtx 4090 and the mxfp4 quant of 30b 3a to fully fit in the video ram, but I think it is bottlenecked on my CPU actually.",
          "score": 3,
          "created_utc": "2026-02-24 19:27:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7845rp",
              "author": "Holiday_Purpose_3166",
              "text": "You probably meant the 35B variant.\n\nIf it fully fits in GPU then CPU is usually not a bottleneck unless you have huge prompt caching, but you can always tweak your -b and -ub flags and whatever you are using.",
              "score": 1,
              "created_utc": "2026-02-24 22:52:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o769cik",
          "author": "PaceZealousideal6091",
          "text": "Thanks a lot guys! Quick questions- \n1.no need for mmproj files for this?\n2. And any bugs inherited from Qwen next implementation for lcpp we need to keep in mind? \n3. Are you working on liquid lfm 2.5 24B a2B UD quants?",
          "score": 3,
          "created_utc": "2026-02-24 17:43:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76af9j",
              "author": "yoracale",
              "text": "You do can have mmproj files yes but it's not compulsory. 2. no. 3. we will work on them",
              "score": 2,
              "created_utc": "2026-02-24 17:48:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76d0gb",
                  "author": "PaceZealousideal6091",
                  "text": "Thanks. I was asking about mmproj files coz, they seem to be too small in size. 1.47 KB looks way too small. Am I missing something?\n\nhttps://preview.redd.it/334piexyehlg1.jpeg?width=1065&format=pjpg&auto=webp&s=f018c1f398d60ac362e2d1254f541002f414c6d5",
                  "score": 2,
                  "created_utc": "2026-02-24 18:00:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76lvhg",
          "author": "Soft-Barracuda8655",
          "text": "ohhh baby, the 35b and 27b looking real nice for my 3090",
          "score": 3,
          "created_utc": "2026-02-24 18:39:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76x97g",
          "author": "liviuberechet",
          "text": "Really excited for the 122B model",
          "score": 3,
          "created_utc": "2026-02-24 19:30:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78218a",
              "author": "yoracale",
              "text": "They all should be up now!",
              "score": 1,
              "created_utc": "2026-02-24 22:41:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76zsl4",
          "author": "xmikjee",
          "text": "GGUF WHEN..... oh wait!\n\nhehehe, thanks :)",
          "score": 3,
          "created_utc": "2026-02-24 19:42:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o782jpi",
              "author": "yoracale",
              "text": "All should now be uploaded üôè",
              "score": 2,
              "created_utc": "2026-02-24 22:44:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77r8u5",
          "author": "grassmunkie",
          "text": "Nice job. Using the UD Q4 on my gaming rig (5090) and getting 56t/s consistently.\n\nThe quality and style of the responses so far is impressive.",
          "score": 3,
          "created_utc": "2026-02-24 21:49:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781yyq",
              "author": "yoracale",
              "text": "Oh wow that's very quick and glad to hear that!",
              "score": 2,
              "created_utc": "2026-02-24 22:41:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o787wpa",
              "author": "boyobob55",
              "text": "Which variation? 35B or the 27B? Interested because i have a 5090 as well lol",
              "score": 1,
              "created_utc": "2026-02-24 23:11:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78z2gf",
                  "author": "grassmunkie",
                  "text": "27b",
                  "score": 1,
                  "created_utc": "2026-02-25 01:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76gn4m",
          "author": "zp-87",
          "text": "Thank you! You guys are awesome",
          "score": 2,
          "created_utc": "2026-02-24 18:16:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76oqrm",
          "author": "CATLLM",
          "text": "Amazing! Can‚Äôt wait for the Dynamic quants!",
          "score": 2,
          "created_utc": "2026-02-24 18:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o782lf2",
              "author": "yoracale",
              "text": "All should now be uploaded! üôè",
              "score": 2,
              "created_utc": "2026-02-24 22:44:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76y3ce",
          "author": "boyobob55",
          "text": "What would perform bettter: the 122B variant heavily quantized, or the 35B variant with a less aggressive quant? üßê",
          "score": 2,
          "created_utc": "2026-02-24 19:34:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77sggb",
              "author": "SpicyWangz",
              "text": "Depends how quantized you're talking. IQ1 on the 122B might be braindead enough to be outperformed by the 35B model. But if you're running Q4 on the 122b, there's no way Q8 or even full size is gonna outperform it",
              "score": 2,
              "created_utc": "2026-02-24 21:54:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o782bcl",
                  "author": "yoracale",
                  "text": "Actually if you check out these benchmarks, the 1-bit surprisingly does insanely well: [https://x.com/i/status/2025951400119751040](https://x.com/i/status/2025951400119751040)\n\nhttps://preview.redd.it/g41sc6fetilg1.png?width=813&format=png&auto=webp&s=f7344153636a40270723890bb8997543ca9a48f8",
                  "score": 2,
                  "created_utc": "2026-02-24 22:42:51",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o78770q",
                  "author": "boyobob55",
                  "text": "I'm thinking the same thing, it'd be a fun benchmark to test out though and compare!",
                  "score": 1,
                  "created_utc": "2026-02-24 23:08:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o782fko",
              "author": "yoracale",
              "text": "Check out qwen3.5 400b 1-bit benchmarks: [https://x.com/i/status/2025951400119751040](https://x.com/i/status/2025951400119751040)\n\nhttps://preview.redd.it/gotxoe5itilg1.png?width=813&format=png&auto=webp&s=acc07fe69e9aa265649de5f1e4fabb46b4412e34",
              "score": 2,
              "created_utc": "2026-02-24 22:43:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76yvy5",
          "author": "DavidNorena",
          "text": "I have 8vram on a AMD GPU, and 96 of RAM which one do you guys recommend ? \n\nAMAZING",
          "score": 2,
          "created_utc": "2026-02-24 19:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o782i96",
              "author": "yoracale",
              "text": "Probably the big 112 one in 4 or 5bit",
              "score": 1,
              "created_utc": "2026-02-24 22:43:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o773rkp",
          "author": "someone383726",
          "text": "Woohoo!  This is exciting.",
          "score": 2,
          "created_utc": "2026-02-24 20:00:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o774dai",
          "author": "Daniel_H212",
          "text": "Which mmproj file should we be using? Does it depend on the quant?",
          "score": 2,
          "created_utc": "2026-02-24 20:03:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o782u0r",
              "author": "yoracale",
              "text": "You can use either shouldn't matter",
              "score": 1,
              "created_utc": "2026-02-24 22:45:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77pb50",
          "author": "JustSayin_thatuknow",
          "text": "Is it working with lcpp right out of the box? Will try it as soon as I get home!!",
          "score": 2,
          "created_utc": "2026-02-24 21:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o781xdf",
              "author": "yoracale",
              "text": "Yes it should be as long as u update to the latest",
              "score": 1,
              "created_utc": "2026-02-24 22:40:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77w02m",
          "author": "Eastern-Group-1993",
          "text": "35B? I can fit UD-IQ3_XSS on 16GB vram with 1.3GiG context(16384/8192).",
          "score": 2,
          "created_utc": "2026-02-24 22:11:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o783361",
              "author": "yoracale",
              "text": "How fast is it? You're lucky!",
              "score": 1,
              "created_utc": "2026-02-24 22:46:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o790gs9",
                  "author": "Eastern-Group-1993",
                  "text": "I haven't tested it out yet, would have to update ollama.  \nI'm on v0.16.X still updated couple days ago.  \nI get 20T/s on gpt-oss:20B and glm-4.7-Flash.\n\nBut I sometimes get crashes using glm-4.7-Flash(Q3\\_K\\_XL unsloth) for more than 1-2 responses.           \n\nThe context VRAM size is an estimate from what gpt-oss:20b and glm-4.7-Flash context VRAM cost is.",
                  "score": 1,
                  "created_utc": "2026-02-25 01:49:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7810nt",
          "author": "el-rey-del-estiercol",
          "text": "Gracias QWEN!!!!",
          "score": 2,
          "created_utc": "2026-02-24 22:36:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78167b",
          "author": "el-rey-del-estiercol",
          "text": "Qwen la mejir empresa del mundo!!! Tenemos que apoyarlos y ayudarles!!!",
          "score": 2,
          "created_utc": "2026-02-24 22:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78amui",
          "author": "romayojr",
          "text": "amazing! i have a 5090 32gb vram and 64gb ram, which one do you recommend?",
          "score": 2,
          "created_utc": "2026-02-24 23:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78g9nk",
              "author": "yoracale",
              "text": "122b 6bit, you can see here: https://unsloth.ai/docs/models/qwen3.5#usage-guide",
              "score": 1,
              "created_utc": "2026-02-24 23:57:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o78dabu",
          "author": "Thrumpwart",
          "text": "Are we expecting smaller models too? Would love a 7-9B, maybe a 15B, and a 4B.",
          "score": 2,
          "created_utc": "2026-02-24 23:40:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78fhbs",
              "author": "yoracale",
              "text": "Yes according to Qwen Jungyang",
              "score": 2,
              "created_utc": "2026-02-24 23:53:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76g9mo",
          "author": "KittyPigeon",
          "text": "So if one wants performance go for the 27b dense over the 35b MoE right?",
          "score": 1,
          "created_utc": "2026-02-24 18:14:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76i02u",
              "author": "yoracale",
              "text": "Nop, for speed 35B is faster. For accuracy then 27B in most cases yes",
              "score": 5,
              "created_utc": "2026-02-24 18:22:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o76n4oc",
              "author": "wesmo1",
              "text": "From the unsloth guide \"Between 27B and 35B-A3B, use 27B if you want slightly more accurate results and can't fit in your device. Go for 35B-A3B if you want much faster inference.\"",
              "score": 1,
              "created_utc": "2026-02-24 18:44:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76mtxw",
          "author": "Crinkez",
          "text": "122B-A10B beating Sonnet 4.5 across the board? I call BS on those benchmarks.",
          "score": 1,
          "created_utc": "2026-02-24 18:43:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76teb3",
          "author": "charmander_cha",
          "text": "Quais beanchs me ajudariam a saber o t√£o quanto bom um modelo seria √∫til se usado no opencode ou Claude  code?",
          "score": 1,
          "created_utc": "2026-02-24 19:12:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76yf1h",
          "author": "waltpinkman",
          "text": "We really need real vllm support now with all these gguf models popping up",
          "score": 1,
          "created_utc": "2026-02-24 19:35:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76zy5u",
          "author": "tinkerman46",
          "text": "With a mac studio 36GM RAM, which is better; 27B 8bit or 35-A3B 4bit?",
          "score": 1,
          "created_utc": "2026-02-24 19:42:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o782rv0",
              "author": "yoracale",
              "text": "For speed, 35b, for accuracy 27b. We wrote here: https://unsloth.ai/docs/models/qwen3.5#usage-guide",
              "score": 1,
              "created_utc": "2026-02-24 22:45:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o773o13",
          "author": "Correct-Wing-6884",
          "text": "https://preview.redd.it/2ihde0l8zhlg1.png?width=1326&format=png&auto=webp&s=1ecb2b831afe0141157923e966342073dac4e1c9\n\nDownloading Qwen3.5 27B UDQ5KXL right now‚Äîfront row seat for this release! üöÄ",
          "score": 1,
          "created_utc": "2026-02-24 20:00:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o776ebe",
          "author": "DanielWe",
          "text": "Thanks. Plans for dynamic quants for vllm (at least q8)?",
          "score": 1,
          "created_utc": "2026-02-24 20:12:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o782v9j",
              "author": "yoracale",
              "text": "Yes probably in the works",
              "score": 1,
              "created_utc": "2026-02-24 22:45:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o778iyr",
          "author": "MaCl0wSt",
          "text": "I'll see what I can do with 12VRAM and 32RAM",
          "score": 1,
          "created_utc": "2026-02-24 20:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o782wpa",
              "author": "yoracale",
              "text": "35b or 27b should run very nicely",
              "score": 1,
              "created_utc": "2026-02-24 22:45:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77bstt",
          "author": "Myonneutrino",
          "text": "Will these be a good match for speculative decoding with the large Qwen3.5 on m3ultra 512gb? I hope caching for those will come to llamacpp soon",
          "score": 1,
          "created_utc": "2026-02-24 20:38:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77cvcr",
          "author": "Mr_Back",
          "text": "First of all, a huge thank you to unsloth!  \nI managed to download Qwen3.5-35B-A3B-MXFP4\\_MOE and Qwen3.5-122B-A10B-UD-Q4\\_K\\_XL.  \nI've only managed to run a couple of queries so far, and I'm still looking into it, but here's my initial impression:  \nThe 35B model is incredibly fast, generating almost 30 tokens per second on my relatively weak configuration. I fed it some .NET code for review, and it highlighted all the problem areas.  \nThe 122B model, on the same queries, takes a long time to process, is slower than a similar-sized gpt oss120b model, and while its responses seem intelligent and profound, they are actually bordering on nonsense. I hope I just got unlucky with the initial seed.",
          "score": 1,
          "created_utc": "2026-02-24 20:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77q77z",
          "author": "sieskei",
          "text": "27B is better than 35B-A3B?",
          "score": 1,
          "created_utc": "2026-02-24 21:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o782y3e",
              "author": "yoracale",
              "text": "In accuracy yes. In speed, definitely not. You can read more here: https://unsloth.ai/docs/models/qwen3.5#usage-guide",
              "score": 1,
              "created_utc": "2026-02-24 22:46:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o784fa5",
          "author": "el-rey-del-estiercol",
          "text": "Hay que suscribirse a qwen para apoyarlos y agradecerselo aunque solo sea para eso!!! Pido a todos vuestra ayuda para qwen!!",
          "score": 1,
          "created_utc": "2026-02-24 22:53:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o784s5o",
          "author": "el-rey-del-estiercol",
          "text": "VIVA CHINA!!! QWEN üòáüòáüòéüòéüëçüôÄ ARRIBA OPENAI üëéüòøABAJO!!!!",
          "score": 1,
          "created_utc": "2026-02-24 22:55:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78b0z7",
          "author": "MiguelAraCo",
          "text": "What would have better performance overall? 122B-A10B in 16-bit (244gb), or 397B-A17B in UD-Q4\\_K\\_XL (219gb). I haven't found a comparison chart that includes these options.",
          "score": 1,
          "created_utc": "2026-02-24 23:28:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78dr6h",
          "author": "untitleXYZ",
          "text": "what speeds are people getting on MacBooks? I'm getting 5 TPS on my M1 Max (32gb) for the 27b model. I feel like it should be higher... ?",
          "score": 1,
          "created_utc": "2026-02-24 23:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78la7v",
          "author": "g4vg4i",
          "text": "In those benchmarks: the 27B is pretty close to the 122B: impressive!",
          "score": 1,
          "created_utc": "2026-02-25 00:25:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78srns",
          "author": "himefei",
          "text": "So excited to try out!!",
          "score": 1,
          "created_utc": "2026-02-25 01:05:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7929fq",
          "author": "Reddit_User_Original",
          "text": "Can we use a better fucking color palette? I literally cannot read this graph",
          "score": 1,
          "created_utc": "2026-02-25 01:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78o6z6",
          "author": "russmur",
          "text": "How great it is compared to Qwen-Image-Edit-2511 for image generations and image edits? I could not find the comparisons against its older models",
          "score": 0,
          "created_utc": "2026-02-25 00:40:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78siuu",
              "author": "muskillo",
              "text": "It's an LLM, it has nothing to do with Qwen-Image-Edit-2511",
              "score": 2,
              "created_utc": "2026-02-25 01:03:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9wkxd",
      "title": "100,000+ models trained with Unsloth have been open-sourced",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/6f7zem4yrnkg1.png",
      "author": "danielhanchen",
      "created_utc": "2026-02-20 14:19:29",
      "score": 249,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r9wkxd/100000_models_trained_with_unsloth_have_been/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6fiw48",
          "author": "SnooPuppers4132",
          "text": "wow",
          "score": 7,
          "created_utc": "2026-02-20 14:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fnlqw",
          "author": "immediate_a982",
          "text": "With open weights you get the model‚Äôs parameters ie the actual numbers so you can run and fine-tune the model, you don‚Äôt necessarily get the **training data, training code**, needed to reproduce the gguf from scratch.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
          "score": 9,
          "created_utc": "2026-02-20 15:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6isms7",
              "author": "zp-87",
              "text": "So basically in terms of software, you get compiled .exe file but not the source code. So it is not open source, it is free to use",
              "score": 5,
              "created_utc": "2026-02-21 00:32:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qw6t1",
                  "author": "cibernox",
                  "text": "Not really because you can modify an exe to make to work differently. I think that metaphor isn‚Äôt right.",
                  "score": 2,
                  "created_utc": "2026-02-22 09:34:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fkc51",
          "author": "Comrade-Porcupine",
          "text": "Basically none of this is \"open source\".  It's open weight. There is an important difference -- apart from AllenAI almost none of the openweight models on huggingface have their \"sources\" open.\n\nUsing the term \"open source\" for these models is an abuse of the term.",
          "score": 6,
          "created_utc": "2026-02-20 15:00:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fpk0j",
              "author": "Rhinoseri0us",
              "text": "I believe you are correct. What‚Äôs a good AllenAI model to start with?",
              "score": 1,
              "created_utc": "2026-02-20 15:25:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6fogx8",
              "author": "yoracale",
              "text": "I think we're getting stressed over semantics for no reason. The models are released under an opensource license aka APACHE 2.0 and this qualify for it to be open source.",
              "score": 0,
              "created_utc": "2026-02-20 15:20:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6grcsc",
                  "author": "Tema_Art_7777",
                  "text": "Does ‚Äòsource‚Äô definition include training data?",
                  "score": 1,
                  "created_utc": "2026-02-20 18:19:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6k23z7",
          "author": "No_Conversation9561",
          "text": "So let me get this straight.\n\n- These guys took weights of an existing model (for free)\n- They used Unsloth‚Äôs toolkit (for free)\n- They finetuned the existing model using Unsloth‚Äôs toolkit using their own dataset. (compute not free)\n- They released only the finetuned weights on huggingface but not the dataset.\n\nIf you don‚Äôt release the dataset then there‚Äôs no distinction between open source community and big corporations.",
          "score": 2,
          "created_utc": "2026-02-21 05:35:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6prppo",
              "author": "NorthEastCalifornia",
              "text": "Datasets are there. You can check the models description.",
              "score": 2,
              "created_utc": "2026-02-22 03:47:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vkugv",
          "author": "cashtec",
          "text": "How and where is this fine tuned? Who is funding the GPUs to do all this? I am just curious",
          "score": 1,
          "created_utc": "2026-02-23 01:27:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcjrux",
      "title": "Qwen3-Coder-Next is now the #1 most downloaded model on Unsloth!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/o3qtavcee9lg1.jpeg",
      "author": "yoracale",
      "created_utc": "2026-02-23 15:02:44",
      "score": 186,
      "num_comments": 31,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rcjrux/qwen3codernext_is_now_the_1_most_downloaded_model/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ypidx",
          "author": "mukz_mckz",
          "text": "I wish more companies also released a Lite/Smaller versions of their models so that GPU poor plebs like us can actually use them for agentic workflows. I hope they see this response to GLM Flash and Qwen Next Coder and reconsider dropping a few blazing fast models in the future. I'm hopeful.",
          "score": 13,
          "created_utc": "2026-02-23 15:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrgp2",
              "author": "yoracale",
              "text": "According to Qwen team e.g. Jungyang they are in fact coming very soon",
              "score": 9,
              "created_utc": "2026-02-23 15:30:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77fviy",
                  "author": "mzinz",
                  "text": "Did they say how many parameters to expect in future releases?",
                  "score": 1,
                  "created_utc": "2026-02-24 20:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71d547",
              "author": "starkruzr",
              "text": "I wonder what hardware people are running this on. only 3B active parameters means you *can* run it well with a variety of GPUs, *provided* you have a lot of fast system RAM for moving experts in and out of VRAM as needed. I guess it would run very well on STXH.",
              "score": 2,
              "created_utc": "2026-02-23 22:55:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zd87p",
          "author": "flavio_geo",
          "text": "Qwen3-Coder-Next is the main man in my local agentic workflow, very consistent, running on consumer grade accessible hardware, in Q6\\_K\\_XL.\n\n  \nGreat model. Great quant.\n\n  \nThank you for the great work Unsloth, you are making the difference for us.",
          "score": 7,
          "created_utc": "2026-02-23 17:11:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72x3ct",
              "author": "yoracale",
              "text": "Thanks for using our quants we appreciate it! \\^\\^",
              "score": 3,
              "created_utc": "2026-02-24 04:21:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ze6wq",
          "author": "HlddenDreck",
          "text": "This small model performs exceptional!\nI'm using it since unsloth released their quants, it's amazing. Hope the performance with llama.cpp will improve. I get 25t/s tg, this very poor for such a small model.\nRunning GPT-OSS-120B I get 80t/s tg, however GPT is really bad at coding.",
          "score": 3,
          "created_utc": "2026-02-23 17:16:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72x6v1",
              "author": "yoracale",
              "text": "Thank you! Which specific quant are you using btw?",
              "score": 1,
              "created_utc": "2026-02-24 04:22:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o73hvp1",
                  "author": "HlddenDreck",
                  "text": "I always aim for the highest I can run completely from VRAM.\nIn case of this model I'm running Q4_K_XL.\nWell, I could run Q6_K_XL, however benchmarks indicate there's almost no accuracy difference between them, so I can run additionally a smaller model for other tasks.",
                  "score": 1,
                  "created_utc": "2026-02-24 07:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zkdrg",
          "author": "TaroOk7112",
          "text": "With this model, UD_Q8_K_XL, I finally have useful local inference solving real problems. Changing Linux configuration, writing scripts, explaining code, ... Very solid compared with previous <235B models.",
          "score": 3,
          "created_utc": "2026-02-23 17:45:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703wzr",
          "author": "omercelebi00",
          "text": "i get 15tok/s with 6600xt and the model is very good at chat. Not able to use with RooCode yet. Q4\\_K\\_XL",
          "score": 3,
          "created_utc": "2026-02-23 19:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72xbf5",
              "author": "yoracale",
              "text": "Llama.cpp fixed some parsing issues a few days ago, try it now and see if it fixed",
              "score": 1,
              "created_utc": "2026-02-24 04:23:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71ufj5",
          "author": "simracerman",
          "text": "And it‚Äôs not even for the coding part (which is awesome too). The model is just amazing at everything I‚Äôve thrown at it so far.",
          "score": 3,
          "created_utc": "2026-02-24 00:31:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o700ox7",
          "author": "dmter",
          "text": "can confirm it's the best atm - using q8 quant from developers, it just consumes 12-16gb or vram if you fully unload moe so you can use other gpu intensive apps and that's at 15t/s.\n\nalso I tried mm2.5 q4 and was extremely disappointed. the math errors it makes are unbelievable. q3cn looks like a genius in comparison.",
          "score": 2,
          "created_utc": "2026-02-23 18:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708kfn",
          "author": "LegacyRemaster",
          "text": "Question: I'm using Step Fun 3.5 a lot, but I haven't seen Unsloth's gguf. Is there a specific reason?",
          "score": 2,
          "created_utc": "2026-02-23 19:36:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72hoey",
              "author": "yoracale",
              "text": "There were some issues with conversion unfortunately",
              "score": 2,
              "created_utc": "2026-02-24 02:45:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71aw8o",
          "author": "Prudent-Ad4509",
          "text": "Still falling into infinite loops in opencode with UD Q4 quant though.",
          "score": 2,
          "created_utc": "2026-02-23 22:43:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72hk53",
              "author": "yoracale",
              "text": "Have you tried Q6 and see any improvements? It was only recently llama.cpp fixed parsing. Did you update llama.cpp?",
              "score": 1,
              "created_utc": "2026-02-24 02:44:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o73i8w0",
                  "author": "Prudent-Ad4509",
                  "text": "I did various suggested things except moving to higher quant but it just moved the step where the looping starts. So, even if looping stops in one scenario, this does not mean much. I‚Äôll try mxfp4 quant next, it showed abnormally high results on certain benches and this looping problem just might be the reason. Grasping at straws and all that.",
                  "score": 1,
                  "created_utc": "2026-02-24 07:08:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72981l",
          "author": "OmarBessa",
          "text": "it's an amazing model",
          "score": 2,
          "created_utc": "2026-02-24 01:56:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72bdds",
          "author": "SouvikMandal",
          "text": "Really hope they release a VLM with 80b active 3B params",
          "score": 2,
          "created_utc": "2026-02-24 02:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o738g58",
          "author": "DarkZ3r0o",
          "text": "Because its really amazing ! I stayed up all night testing the model and its on fire üî• it can easily work with claude code and provide great results",
          "score": 2,
          "created_utc": "2026-02-24 05:45:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ytflm",
          "author": "joninco",
          "text": "Can we use unsloth to make an Eagle3 draft model? Asking for a friend.",
          "score": 1,
          "created_utc": "2026-02-23 15:39:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72xeiv",
              "author": "yoracale",
              "text": "Eagle3 draft? We use llama.cpp for all our quants, not unsloth. Unsloth is mainly for training and inference",
              "score": 1,
              "created_utc": "2026-02-24 04:24:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o71i0ha",
          "author": "gtrak",
          "text": "This is the best one in my informal testing for 24gb vram and 64gb dram https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF/discussions/2 .  It uses bf16 for the non moe weights and 4 bit for everything else. I think it beats q6 at a smaller size.",
          "score": 1,
          "created_utc": "2026-02-23 23:21:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73hpey",
              "author": "Slow-Ability6984",
              "text": "Can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-24 07:03:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74x2h9",
                  "author": "gtrak",
                  "text": "Read the link",
                  "score": 1,
                  "created_utc": "2026-02-24 13:56:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73wsfe",
          "author": "ZealousidealShoe7998",
          "text": "for anyone who wants to try a code harness, instead of using opencode or claude code try qwen code first. it should work out of the box. ",
          "score": 1,
          "created_utc": "2026-02-24 09:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74c5wa",
          "author": "Legitimate-Track-829",
          "text": "Does the 3-bit work on Apple Silicon 48GB unified RAM? ",
          "score": 1,
          "created_utc": "2026-02-24 11:42:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74hbku",
              "author": "yoracale",
              "text": "You'd rather use the 4-bit which works yes. Our guide uses 4-bit: [https://unsloth.ai/docs/models/qwen3-coder-next](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 2,
              "created_utc": "2026-02-24 12:20:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rbkbse",
      "title": "Qwen3-Coder-Next GGUF Aider Coding Benchmarks",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/2u2qeza4f1lg1.png",
      "author": "Etherll",
      "created_utc": "2026-02-22 12:12:31",
      "score": 180,
      "num_comments": 33,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rbkbse/qwen3codernext_gguf_aider_coding_benchmarks/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6rdogv",
          "author": "Significant_Fig_7581",
          "text": "I said the IQ3 XXS was great and people still don't believe me",
          "score": 24,
          "created_utc": "2026-02-22 12:16:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6riq5y",
              "author": "Look_0ver_There",
              "text": "I found this to also be true of Unsloth's IQ3_XXS quant of MiniMax M2.5 which allows it to fix nicely within the various MiniPC's and Mac's with 128GB of memory.  Personally I've found MiniMax to be a little more reliable than Qwen3-Coder-Next, but I think that just depends on the tasks at hand.  The main take away though is that for larger models(>50B say?) that IQ3_XXS doesn't seem to hurt as much as it does for smaller models.",
              "score": 11,
              "created_utc": "2026-02-22 12:55:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rktb0",
          "author": "define_undefine",
          "text": "Does anyone know why FP8 has a drop in performance compared to Q6 or NVFP4?",
          "score": 10,
          "created_utc": "2026-02-22 13:10:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rm7jy",
              "author": "FullstackSensei",
              "text": "Because it's a limited benchmark. Dig enough and you'll find other benchmarks where the picture is flipped.\n\nIn any case, the only thing that actually matters is whether a quant works for your uses or not.",
              "score": 9,
              "created_utc": "2026-02-22 13:19:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6sfpnx",
                  "author": "steezy13312",
                  "text": ">¬†In any case, the only thing that actually matters is whether a quant works for your uses or not.\n\nI understand what this sentence is communicating, but it‚Äôs kind of missing the point here. Many of us don‚Äôt have the time to determine if a quant or model works for every one of our use cases or not.\n\nImagined you have a friend who‚Äôs interested in buying a car, and you tell them the only way to find out what works best for them is to go test drive every variation of trim and engine package instead of at first looking at car reviews to refine their options.¬†",
                  "score": 5,
                  "created_utc": "2026-02-22 15:56:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rul62",
                  "author": "some_user_2021",
                  "text": "And because inference is a statistical process.",
                  "score": 3,
                  "created_utc": "2026-02-22 14:09:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rngdl",
          "author": "Thrumpwart",
          "text": "How does NVFP4 perform in terms of speed on AMD GPUs? Is Blackwell necessary to have. A good experience?",
          "score": 5,
          "created_utc": "2026-02-22 13:27:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rr6x3",
              "author": "blazze",
              "text": "AMD **RDNA 4**: Supports FP8/BF8 with hardware acceleration and includes improvements in AI compute performance, but¬†does not implement NVFP4¬†or similar micro-floating-point formats.",
              "score": 5,
              "created_utc": "2026-02-22 13:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s0yil",
                  "author": "Thrumpwart",
                  "text": "Yeah that‚Äôs too bad. Seems like a solid quant option.",
                  "score": 2,
                  "created_utc": "2026-02-22 14:45:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sazam",
          "author": "siegevjorn",
          "text": "NVPF4 better than BF16? How is it posssible",
          "score": 8,
          "created_utc": "2026-02-22 15:35:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rjciq",
          "author": "LegacyRemaster",
          "text": "wow",
          "score": 3,
          "created_utc": "2026-02-22 13:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6toudd",
          "author": "MaxKruse96",
          "text": "No +- error margins, no simpler quants for q2 q4 q5 q6. kinda whack",
          "score": 3,
          "created_utc": "2026-02-22 19:22:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ro12r",
          "author": "Glittering-Call8746",
          "text": "What card u using 5070ti 16gb ?",
          "score": 2,
          "created_utc": "2026-02-22 13:31:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tddtr",
          "author": "StartupTim",
          "text": "How would this sort on a system with 2x GPUs for 48GB vram and 96GB system ram?\n\nWhich model would you choose, especially when going for long context windows such as 256k or 512k?",
          "score": 2,
          "created_utc": "2026-02-22 18:29:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wa9ka",
          "author": "AdventurousGold672",
          "text": "what frameworks support running nvfp4?",
          "score": 2,
          "created_utc": "2026-02-23 04:06:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rlven",
          "author": "Prudent-Ad4509",
          "text": "I wonder why there is NVFP4 quant but no UD-Q4-K-XL quant. Is it \\*that\\* bad ?",
          "score": 4,
          "created_utc": "2026-02-22 13:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rrzfo",
              "author": "yoracale",
              "text": "The contributors who benchmarkered this on Discord did not test the Q4 quants as the gap between Q3 and full BF16 precision is already so close",
              "score": 6,
              "created_utc": "2026-02-22 13:55:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rswjm",
                  "author": "Prudent-Ad4509",
                  "text": "The problem is in the position of the UD-Q6\\_K\\_XL quant relative to the NVFP4 quant. Also, I see two different NVFP4 quants on huggingface, one made with **nvidia-modelopt** and another with **llmcompressor**. It feels like they've missed the elephant in the room. All 3 quants should have been tested I think.\n\nAlso, there is at least one more thread on reddit with this picture where people are reporting issues with Q3.",
                  "score": 4,
                  "created_utc": "2026-02-22 14:00:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rt9x9",
          "author": "Mr_Back",
          "text": "This table is confusing. I don't understand where UD-Q3\\_K\\_XL, UD-Q4\\_K\\_XL, and MXFP4\\_MOE fit in.  \nI always thought that the \"K\\_XL\" configuration offered the best balance of speed and quality ‚Äì is that not the case?  \nI just tried running UD-IQ3\\_XXS, and it's running a quarter slower than MXFP4, and its speed is comparable to UD-Q8\\_K\\_XL on my machine.",
          "score": 2,
          "created_utc": "2026-02-22 14:02:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rywod",
              "author": "yoracale",
              "text": "This benchmark is comparing REAP non Unsloth GGUFs, vs Unsloth GGUFs vs NVFP4 vs FP8. It is quite confusing. K\\_XL isn't always the best balance of speed, but it is usually quality yes.\n\nThe Q3\\_K\\_XL displayed in this graph is not of Unsloth's but rather the REAP version of the model",
              "score": 3,
              "created_utc": "2026-02-22 14:34:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s4hqw",
                  "author": "Mr_Back",
                  "text": "Regarding the Q3\\_K\\_XL REAP model on the graph ‚Äì I understand. My question is more about where the Unsloth models UD-Q3\\_K\\_XL, UD-Q4\\_K\\_XL, and MXFP4\\_MOE would be located on this graph.  \nWould they be positioned on the line between UD-IQ3\\_XXS and UD-Q6\\_K\\_XL?  \nI'm currently using MXFP4, which gives me 20 tokens per second (for video transcription and small code edits), and UD-Q8\\_K\\_XL (for agent-based encoding), which gives me 15 tokens per second.  \nLooking at this graph, I thought that UD-IQ3\\_XXS would be very good and faster than MXFP4, while also being almost as accurate as UD-Q6\\_K\\_XL, but its speed is similar to UD-Q8\\_K\\_XL.  \nIs UD-IQ3\\_XXS more accurate than MXFP4?  \nIs MXFP4 particularly fast compared to other quantization methods?  \nIs UD-IQ3\\_XXS quantization slower?  \nWhich quantization method would be best for me, offering a good balance between speed and accuracy for both casual use and more demanding tasks?",
                  "score": 0,
                  "created_utc": "2026-02-22 15:03:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rg2t6",
          "author": "Glittering-Call8746",
          "text": "Can fit consumer gaming cards ?",
          "score": 2,
          "created_utc": "2026-02-22 12:36:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rib3w",
              "author": "yoracale",
              "text": "Only if you have enough RAM. Becuase IQ3XXS is only 32.7GB youll need about 35GB total VRAM + RAM combined.\n\nSo e.g. a 16GB VRAM + 20GB RAM will work quite nicely\n\nMore deets in our guide: [https://unsloth.ai/docs/models/qwen3-coder-next](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 10,
              "created_utc": "2026-02-22 12:52:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rprj8",
                  "author": "1_7xr",
                  "text": "I have a laptop with 8GB VRAM + 24GB RAM. Would I get decent performance if I upgraded the ram to 32GB?",
                  "score": 2,
                  "created_utc": "2026-02-22 13:41:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x8evm",
          "author": "FeiX7",
          "text": "link to benchmark? and does they have similar benchmark or other models as well?\n\n",
          "score": 1,
          "created_utc": "2026-02-23 08:57:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ytfb6",
          "author": "AntuaW",
          "text": "So lame we don't get those by default and people have to do that individually. It is such a waste of time because of this info lacking on each quant :(",
          "score": 1,
          "created_utc": "2026-02-23 15:39:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t2ap3",
          "author": "alfons_fhl",
          "text": "NVFP4 is better than bf16? Does I understand it right that the Quantization has better performance as the default bf16? (bf16 is the default LLM of Qwen3-Coder-Next right?)",
          "score": 0,
          "created_utc": "2026-02-22 17:38:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72txjg",
              "author": "eXl5eQ",
              "text": "Happens to be better in this benchmark with the particular hardware and seed OP used.",
              "score": 1,
              "created_utc": "2026-02-24 04:00:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r85r5x",
      "title": "You can now train LLMs in VS Code for free via Colab!",
      "subreddit": "unsloth",
      "url": "https://v.redd.it/k5cjdy4ss9kg1",
      "author": "yoracale",
      "created_utc": "2026-02-18 15:19:57",
      "score": 135,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r85r5x/you_can_now_train_llms_in_vs_code_for_free_via/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o62orpj",
          "author": "Reivaj640",
          "text": "Thank you so much, it's super interesting to do with VS Code",
          "score": 8,
          "created_utc": "2026-02-18 16:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66pi40",
              "author": "danielhanchen",
              "text": "Yes VS Code + Colab GPUs are pretty cool! You can also launch multiple Colabs as well!",
              "score": 1,
              "created_utc": "2026-02-19 04:16:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67pgvl",
          "author": "Big-Balance-6426",
          "text": "Thank you for this guide!",
          "score": 2,
          "created_utc": "2026-02-19 09:17:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a2qyk",
          "author": "bad_gambit",
          "text": "Thanks for the guide (and the quants and your amazing finetuning library)! Is google service (BigQuery, Drive, etc) and other quirks (github lfs integration was janky, iirc) finally fixed now? Last time i used the extension it was feeling like its half-baked and quite a hassle without those features",
          "score": 1,
          "created_utc": "2026-02-19 18:00:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pxbu",
      "title": "Qwen3.5 GGUF Evaluation Results",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/0kqfgy0b1mkg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-20 08:30:37",
      "score": 121,
      "num_comments": 15,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r9pxbu/qwen35_gguf_evaluation_results/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6e47uh",
          "author": "LegacyRemaster",
          "text": "I'm using this model every day. I'm curious about Q2 quantization because I'm having fun asking sonnet 4.6 the exact same questions and comparing the answers, and I haven't seen any hallucinations yet. On the contrary. While sonnet was telling me \"the directory is wrong, here's a script to edit the Python file and adjust the paths,\" qwen was telling me \"rename the directory and the problem is solved.\" In this particular case, I smiled because I then asked sonnet: \"But isn't it better to simply rename the directory to fix the references?\" And sonnet: \"Yes, you're right!\"",
          "score": 13,
          "created_utc": "2026-02-20 08:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e23pd",
          "author": "getmevodka",
          "text": "Wait why is a normal margin of error producing better results in a q3 than in a q4?",
          "score": 10,
          "created_utc": "2026-02-20 08:36:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e2kw1",
              "author": "yoracale",
              "text": "Yes, as for statistics, there is always going to be a margin of error. To cancel it out you usually have to run the test like 10 times and get the median/average. Even then, margin of errors still can occur.\n\nOr coincidentally, it could be possible that the Q3 is in fact better than the Q4 but that is unlikely.",
              "score": 13,
              "created_utc": "2026-02-20 08:40:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ez7sf",
                  "author": "simracerman",
                  "text": "There‚Äôs a small catch her not explaining the behavior.\n\nI believe it‚Äôs the evaluation. Different prompts will reveal more cracks in the Q3 one.\n",
                  "score": 1,
                  "created_utc": "2026-02-20 13:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ezwmb",
          "author": "ciprianveg",
          "text": "can you please also test the MXFP4_MOE version?",
          "score": 5,
          "created_utc": "2026-02-20 13:11:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6izwdt",
              "author": "yoracale",
              "text": "You'll need to ask Benjamin for this: [https://x.com/bnjmn\\_marie/status/2024533203239788851](https://x.com/bnjmn_marie/status/2024533203239788851)",
              "score": 3,
              "created_utc": "2026-02-21 01:15:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6j8pci",
              "author": "Dry_Mortgage_4646",
              "text": "Please ask Benjamin. I want to know too",
              "score": 2,
              "created_utc": "2026-02-21 02:11:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k79at",
          "author": "alexp702",
          "text": "Q8 on Mac Studio subjectively feels very good - much better than iq4_NL of Qwen Coder 480.",
          "score": 2,
          "created_utc": "2026-02-21 06:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fagru",
          "author": "GCoderDCoder",
          "text": "I had an online debate with someone on this topic. I see the perplexity hits a cliff at q4 where the token accuracy compared to the original decreases dramatically BUT that doesnt mean the model is putting bad tokens, it just means the model is using a different token than the original would have. How do those affects compare to temperature changes? Xcreate shows that these models do indeed have different output but it seems to be that they lose nuance as they compress more which makes perfect sense given the way compression works. I would love to see more standardized testing around this stuff. Model providers wont even compare the same benchmarks at full size let alone quants...",
          "score": 1,
          "created_utc": "2026-02-20 14:09:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fneqj",
              "author": "yoracale",
              "text": "Mmm perplexity is generally not a good measurement for accuracy. Benjamin utilized actual prompts which are much more real world use-case based",
              "score": 2,
              "created_utc": "2026-02-20 15:15:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fq6jn",
                  "author": "GCoderDCoder",
                  "text": "Agreed! Just pointing out seeing the data about token accuracy doesn't necessarily translate 1:1 into model performance but we never get standardized data on things like quantized performance despite the fact that most of us are using quantized versions.",
                  "score": 1,
                  "created_utc": "2026-02-20 15:28:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hiotb",
          "author": "JorG941",
          "text": "Can you share a google notebook to do this?",
          "score": 1,
          "created_utc": "2026-02-20 20:28:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6izw2f",
              "author": "yoracale",
              "text": "You'll need to ask Benjamin for this: [https://x.com/bnjmn\\_marie/status/2024533203239788851](https://x.com/bnjmn_marie/status/2024533203239788851)",
              "score": 0,
              "created_utc": "2026-02-21 01:15:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6sjuha",
          "author": "Honest-Debate-6863",
          "text": "Is that good or bad? Q4 drift is fine? Just that it hallucinates more right?",
          "score": 1,
          "created_utc": "2026-02-22 16:14:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r908nn",
      "title": "New r/unsloth User Flairs!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/s14byfp2lgkg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-19 14:11:58",
      "score": 31,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Feature:FO2C6766BA42_Sloth_Salut:",
      "permalink": "https://reddit.com/r/unsloth/comments/1r908nn/new_runsloth_user_flairs/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o698enw",
          "author": "im_datta0",
          "text": "Why did you pick the sad sloth though LOL",
          "score": 2,
          "created_utc": "2026-02-19 15:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c5lng",
              "author": "yoracale",
              "text": "Because it's my favorite sloth emoji üò≠",
              "score": 1,
              "created_utc": "2026-02-20 00:20:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ae124",
          "author": "danielhanchen",
          "text": "I'm using the Heart Sloth! :)",
          "score": 2,
          "created_utc": "2026-02-19 18:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lk0c3",
          "author": "LegacyRemaster",
          "text": "all I need",
          "score": 1,
          "created_utc": "2026-02-21 13:37:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r993q3",
      "title": "Any good model that can even run on 0.5 GB of RAM (512 MB of RAM)?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1r993q3/any_good_model_that_can_even_run_on_05_gb_of_ram/",
      "author": "Ok-Type-7663",
      "created_utc": "2026-02-19 19:40:20",
      "score": 16,
      "num_comments": 20,
      "upvote_ratio": 0.83,
      "text": "I'm testing local AI limits. Also recommend a OS :3 and Hugging Face repo and great quant :D",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1r993q3/any_good_model_that_can_even_run_on_05_gb_of_ram/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6aqhcs",
          "author": "RudeboyRudolfo",
          "text": "functiongemma. But it's more for running structured output and tool calling. It's not really a LLM.",
          "score": 16,
          "created_utc": "2026-02-19 19:53:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b3ya2",
              "author": "Apart_Refrigerator27",
              "text": "What are the use cases for functiongemma?\nit is really worth the time?",
              "score": 4,
              "created_utc": "2026-02-19 20:59:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b7j5i",
                  "author": "RudeboyRudolfo",
                  "text": "That's your imagination. There is a lot possible with it. Pretty sure you can get it over ollama. So not much time wasted. I built a small assistant with it, which can open and edit files and search man pages.",
                  "score": 6,
                  "created_utc": "2026-02-19 21:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ave8q",
          "author": "PlayerWell",
          "text": "I think the Gemma 3 270m will work. It's not great, but it can be successful if fine-tuned",
          "score": 9,
          "created_utc": "2026-02-19 20:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bxr6f",
              "author": "--Spaci--",
              "text": "this is the only real correct option tbf qwen3 0.6b is to large with a context window",
              "score": 1,
              "created_utc": "2026-02-19 23:34:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ar209",
          "author": "Jan49_",
          "text": "No good model can run on half a GB of RAM... Lol\n\nThe smallest LLM, that I know of, that can barely form sentences is Qwen3 0.6B. The q2 quant from unsloth is sub 300mb in size. But then you would still need RAM for context and general overhead.\n\nDoes your system only have 512mb RAM? Then the OS would probably take up the whole RAM on its own. Try Linux XFCE or even better no DE at all.",
          "score": 6,
          "created_utc": "2026-02-19 19:55:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bun9h",
              "author": "logos_flux",
              "text": "It's not a system it's a ti-83",
              "score": 1,
              "created_utc": "2026-02-19 23:16:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ayjfh",
          "author": "BenniB99",
          "text": "You could try the 2bit dynamic quants of LFM2-1.2B but that is probably going to be a vegetable",
          "score": 3,
          "created_utc": "2026-02-19 20:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b7wc8",
          "author": "Significant_Fig_7581",
          "text": "Qwen 0.6B",
          "score": 3,
          "created_utc": "2026-02-19 21:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ccw9x",
          "author": "host3000",
          "text": "Qwen2.5-0.5b",
          "score": 3,
          "created_utc": "2026-02-20 01:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b3dnv",
          "author": "DuckyBlender",
          "text": "Gemma 3 270M",
          "score": 2,
          "created_utc": "2026-02-19 20:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6civqc",
          "author": "catplusplusok",
          "text": "BitNet?",
          "score": 2,
          "created_utc": "2026-02-20 01:40:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74ks8r",
              "author": "so_schmuck",
              "text": "Not BitConnect ?",
              "score": 1,
              "created_utc": "2026-02-24 12:44:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gm2b6",
          "author": "GeneralComposer5885",
          "text": "Sentence Bert",
          "score": 1,
          "created_utc": "2026-02-20 17:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p3qhq",
          "author": "Mice_With_Rice",
          "text": "There are lots of good models in that size. The real question is what sort of model are you looking for, and what is the use case.\n\nFor example, KittenTTS is <25MB. But do you want a TTS model? There are good embedding models if you want to vectorize data. If you want a custom model for a specific use you can make your own as well. Lots of options, but just asking for \"good\" is open to a lot of interpretation.",
          "score": 1,
          "created_utc": "2026-02-22 01:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rnjp2",
          "author": "mrtumederanges",
          "text": "LFM2-350M¬†from [liquid.ai](http://liquid.ai) ",
          "score": 1,
          "created_utc": "2026-02-22 13:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ylbe",
          "author": "FeiX7",
          "text": "use case?",
          "score": 1,
          "created_utc": "2026-02-23 21:42:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ar1om",
          "author": "supportend",
          "text": "Kitten TTS, 14, 40 and 80M.",
          "score": 1,
          "created_utc": "2026-02-19 19:55:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rblyus",
      "title": "How to maximize Qwen3.5 t/s?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rblyus/how_to_maximize_qwen35_ts/",
      "author": "Altruistic_Call_3023",
      "created_utc": "2026-02-22 13:33:20",
      "score": 12,
      "num_comments": 34,
      "upvote_ratio": 0.88,
      "text": "Hello all.  I am following the guide from unsloth about running qwen3.5 that says 25t/s+ with 24gb vram and 256GB RAM is possible on the 4bit Dynamic quant. I‚Äôm only seeing around 7t/s with my 3090 and 32 core Xeon on 356gb of ddr4 RAM so I‚Äôm trying to understand what I might be configuring wrong.  (Or is it just because I‚Äôm not ddr5 and more recent cpu?) I also have two 5060ti I can use - but adding those in, I don‚Äôt see any real performance increase.  I‚Äôm using a current llama.cpp built just yesterday.  Thanks for any help. My settings are:\n\nctx-size = 32768\n\nbatch-size = 512\n\nubatch-size = 512\n\nthreads = 32\n\ntemp = 0.6\n\nmin-p = 0\n\ntop-p = 0.95\n\nrepeat-penalty = 1.0\n\npresence-penalty = 0.0\n\ntop-k = 20\n\nfa = on\n\ncache-type-k = q8\\_0\n\ncache-type-v = q8\\_0\n\nfit = on\n\nnp = 1",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rblyus/how_to_maximize_qwen35_ts/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6s38gc",
          "author": "suicidaleggroll",
          "text": "I think the guide is just saying you *can* hit those speeds, as in it‚Äôs theoretically possible, not that you *will* hit them. ¬†I‚Äôm hitting 32 tok/s on my system. ¬†That‚Äôs with 96 GB VRAM, but given this model size there honestly won‚Äôt be much difference in speed between 24G and 96G, pretty much everything is still running on the CPU anyway. ¬†I can test it again while limiting GPU usage to <24 GB to put a number on it, I‚Äôm guessing it‚Äôd be close to that 25 tok/s.\n\nThat‚Äôs with an EPYC 9455P with 12 channels of DDR5-6400, so around 600 GB/s memory bandwidth. ¬†Memory bandwidth is the big limiter on CPU inference speeds.\n\nEdit: Looks like there was a recent llama.cpp update that increased speeds since my first benchmark, re-testing it brought inference up to 39.5 as a baseline with 36 layers offloaded to the CPU and ~90 GB VRAM usage.  Increasing to 56 layers offloaded to the CPU brought VRAM usage down to <24 GB, and dropped inference speeds back down to about 33.  So 25 is definitely possible with only 24 GB of VRAM, but you need a CPU with high memory bandwidth to do it.",
          "score": 5,
          "created_utc": "2026-02-22 14:56:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6srjar",
              "author": "Altruistic_Call_3023",
              "text": "Thanks for the info.  Good to know and helpful.",
              "score": 2,
              "created_utc": "2026-02-22 16:48:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7513zn",
              "author": "Pitiful_Gene_3648",
              "text": "Can you share the command you run it? I have 96gb vram also with 256ram hitting only max 17 tps",
              "score": 1,
              "created_utc": "2026-02-24 14:17:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uij7b",
          "author": "someone383726",
          "text": "I was getting about 15 T/s with llama cpp and Q4-K-XL with 90gb on RTX6000 and the rest on my 4x64 DDR5-6000.   I thought I must be doing something wrong too.",
          "score": 2,
          "created_utc": "2026-02-22 21:52:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypft1",
              "author": "xanduonc",
              "text": "same with 8xddr4-3200",
              "score": 1,
              "created_utc": "2026-02-23 15:20:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6spd5g",
          "author": "djdeniro",
          "text": "i have only 19 t/s with full GPU loaded UD\\_Q4\\_K\\_XL model. 6xR9700 + 6x7900xtx, also super slow on PP",
          "score": 2,
          "created_utc": "2026-02-22 16:39:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6srf5g",
              "author": "Altruistic_Call_3023",
              "text": "Thanks for posting that.  I feel better that I‚Äôm not missing something obvious",
              "score": 2,
              "created_utc": "2026-02-22 16:48:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t6lsw",
                  "author": "djdeniro",
                  "text": "anyway i tested 3 times, and the maximum i got in prompt processing is 100t/s.  i also search results in OpenRouter, and we can be sure, VL models work slower than non-vision models.  But when i see here someone got 32 t/s i got questions, how RAM based launches faster than GPU..",
                  "score": 0,
                  "created_utc": "2026-02-22 17:58:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rxc2n",
          "author": "ciprianveg",
          "text": "I am getting 18.5t/s on 8 channel ddr4 3995wx cpu and 1x3090.",
          "score": 1,
          "created_utc": "2026-02-22 14:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rza8z",
              "author": "Altruistic_Call_3023",
              "text": "Maybe my dual Xeon silver is just the bottleneck.  It‚Äôs the gen before the optimizations started, and it‚Äôs only actually 16 cores I see since hyperthreading.  I do have a bunch of channels of memory, but maybe that‚Äôs my breakdown point.",
              "score": 2,
              "created_utc": "2026-02-22 14:36:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rzlvd",
                  "author": "ciprianveg",
                  "text": "search for a numactl command to treat both as a unique processing unit",
                  "score": 2,
                  "created_utc": "2026-02-22 14:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sr6zl",
              "author": "EbbNorth7735",
              "text": "What's your setup and commands? I'm hitting 11tps with 5955wx. Certainly possible the reduced speed is simply from reduced NUMA or whatever the ram processor blocks are called. I think 5995 and 3995 have 4, 5955 has 1.",
              "score": 2,
              "created_utc": "2026-02-22 16:47:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6t4332",
                  "author": "ciprianveg",
                  "text": "8x64gb 2933mt/s 3995wx 1x3090 24gb\n/build/bin/llama-server  --model /home/ciprian/models/Qwen3.5-397b/Qwen3.5-397B-A17B-UD-Q4_K_XL-00001-of-00006.gguf --alias Qwen3-397B \\\n --flash-attn on   -c 128000 -ub 4096 -b 4096  -fit on  -ngl 999  -ot \"blk\\.(0|1)\\.ffn_.*_exps.*=CUDA0\"   --cpu-moe \\\n --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0   --threads 58 --host 0.0.0.0 --port 5001   --chat-template-kwargs \"{\\\"enable_thinking\\\": false}\"  \\\n   --jinja --parallel 1  --no-mmap --mmproj /home/ciprian/models/Qwen3.5-397b/mmproj-BF16.gguf",
                  "score": 3,
                  "created_utc": "2026-02-22 17:47:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6s1g3d",
              "author": "some_user_2021",
              "text": "Which quantization and how much context was being processed?",
              "score": 1,
              "created_utc": "2026-02-22 14:47:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s1us5",
                  "author": "ciprianveg",
                  "text": "UD-Q4-XL. it stays pretty constant. 18.5 4k context, 17.8 16k context",
                  "score": 2,
                  "created_utc": "2026-02-22 14:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6u8wa5",
              "author": "segmond",
              "text": "what quant?  what's speed of your ram?",
              "score": 1,
              "created_utc": "2026-02-22 21:04:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6u9s9c",
                  "author": "ciprianveg",
                  "text": "UD-Q4-XL. 2933 8 channels",
                  "score": 1,
                  "created_utc": "2026-02-22 21:08:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6t081d",
          "author": "LA_rent_Aficionado",
          "text": "I get around 40 t/s fully offloaded at Q4 with about 120-160k context, it‚Äôs just not a very fast model given the multimodal piece adds complexity",
          "score": 1,
          "created_utc": "2026-02-22 17:28:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6u92f5",
              "author": "segmond",
              "text": "hardware specs? GPUs, cpu, memory speed?",
              "score": 1,
              "created_utc": "2026-02-22 21:04:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6uu8mv",
                  "author": "LA_rent_Aficionado",
                  "text": "Asus WRX90 SAGE, 7965wx, 384GB G.skill DDR5 @ 6000, 6000 Pro, 5090, 2x 3090ti and 4x 3090",
                  "score": 1,
                  "created_utc": "2026-02-22 22:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6vvwer",
          "author": "Fresh_Finance9065",
          "text": "mlock = on\nswa-full = on\nkvu = on\n\nmlock and kvu definitely speed things up. swa full may, but i do not understand it enough to know whether it really does or not",
          "score": 1,
          "created_utc": "2026-02-23 02:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rpb1q",
          "author": "Antique_Dot_5513",
          "text": "Pareil sur lm studio j‚Äôobtiens pas du tout ces fameux 25 t/s",
          "score": 0,
          "created_utc": "2026-02-22 13:39:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rapr95",
      "title": "Subject: Seeking Validation: Strategy for Multi-LoRA Behavioral Fine-Tuning on Micro-Datasets (50-100 rows)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rapr95/subject_seeking_validation_strategy_for_multilora/",
      "author": "Scouserleemc",
      "created_utc": "2026-02-21 12:18:35",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "\n\nHi Folks,\n\nI am currently building a composite agentic system for my PhD dissertation (a Design-Based Research project). The system is a \"Purposeful Agent\" designed to act as a professional executive coach. It uses a multi-agent RAG architecture with a vLLM backend routing to multiple specialized LoRA adapters (e.g., an `adapter_empathy`, `adapter_scaffolding`, `adapter_planner`) based on the user's real-time emotional state (Valence-Arousal-Dominance).\n\nBecause my research relies on highly authentic, expert-validated facilitation transcripts, my dataset is incredibly constrained. Based on the *LIMA (Less Is More for Alignment)* hypothesis, I am attempting to do purely behavioral/stylistic fine-tuning using extremely small, highly curated datasets‚Äîspecifically **only 50 to 100 rows of data per adapter**.\n\nMy goal is not to teach the model new knowledge, but to teach it a very specific facilitative stance (e.g., asking open-ended questions, mirroring, and strictly avoiding giving direct advice).\n\nGiven the high risk of catastrophic overfitting with such a small dataset, I have developed the following training strategy using Unsloth. I would love your expert feedback on whether this is viable and if there are any Unsloth-specific optimizations I should apply:\n\n**1. Data Structure: Multi-Turn ChatML Threads** Instead of single-turn Q&A pairs, I am formatting my 50-100 rows as multi-turn conversational histories (User -> Assistant -> User -> Assistant) using standard ChatML. The theory is that this will provide enough linguistic density for the attention mechanism to learn the temporal pacing of a coaching intervention (e.g., when to validate vs. when to probe) rather than just acting like a reactive search engine.\n\n**2. Data Composition: \"Hard Negatives\" to counter RLHF** Base instruction models (like Llama-3-8B-Instruct) are heavily biased toward sycophancy and immediate problem-solving due to their RLHF training. To overwrite this urge to give \"helpful advice,\" roughly 20% of my micro-dataset consists of \"hard negative\" interactions, where the user explicitly begs for advice, and the assistant actively deflects and returns agency to the user.\n\n**3. Hyperparameter Adjustments for Micro-Datasets** To prevent the loss curve from instantly crashing to zero and the model simply memorizing the 50 transcripts, I am planning the following hyperparameter constraints:\n\n* **LoRA Rank (r) & Alpha:** Very low rank (r=4 or 8) with Alpha=16 to restrict the adapter's capacity and force generalization over memorization.\n* **Dropout:** Increasing LoRA dropout to `0.05` or `0.10`.\n* **Learning Rate:** Lowering to `2e-5` for a gentler update to the stylistic weights.\n* **Epochs:** Capping at 3 to 4 epochs, utilizing a small holdout set to closely monitor Validation Loss. If validation loss spikes while training loss drops, I will trigger early stopping.\n\n**My Questions:**\n\n1. Given Unsloth's underlying optimizations, is this micro-dataset strategy (50-100 multi-turn rows) mathematically viable for behavioral cloning, or is that simply too little data for the optimizer to find a meaningful gradient?\n2. Are there any specific Unsloth arguments, parameters, or configurations (e.g., specific target modules, gradient accumulation steps, or learning rate schedulers) you would highly recommend when the dataset is this tiny?\n3. Have you seen success with multi-turn ChatML formatting in Unsloth when trying to teach conversational pacing rather than just instruction following?\n\nThank you so much for your time and for building such an incredible tool for the open-source community!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rapr95/subject_seeking_validation_strategy_for_multilora/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6magef",
          "author": "wildyam",
          "text": "I am too dumb to add value but am commenting to show support! Sounds fascinating - good luck!",
          "score": 5,
          "created_utc": "2026-02-21 16:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nappe",
          "author": "Rhinoseri0us",
          "text": "This is really exciting to me and my current focus of work/research. Would you be open to a message/connecting? My focus is partly on training small edge models and your dissertation seems extremely compelling to my line of thinking. Not trying to gas you up just saying üòÜ",
          "score": 2,
          "created_utc": "2026-02-21 19:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q2ky9",
          "author": "fourwheels2512",
          "text": "Great setup ‚Äî a few concrete answers to your three questions:\n\n\n\n\\*\\*1. Is 50-100 multi-turn rows viable?\\*\\*\n\nYes, for behavioral/stylistic cloning specifically. LIMA showed 1000 rows generalises, but you're not teaching knowledge ‚Äî you're overwriting an attentional pattern (\"deflect advice, return agency\"). At r=4 with multi-turn ChatML you're probably updating \\~0.1% of weights. The optimizer has enough signal from 50 well-formed coaching transcripts if the examples are consistent in style. The risk isn't gradient direction, it's gradient \\*magnitude\\* ‚Äî with tiny batches you'll see noisy norm spikes that look alarming but aren't.\n\n\n\n\\*\\*2. Unsloth-specific recommendations:\\*\\*\n\n\\- Use \\`gradient\\_accumulation\\_steps=4-8\\` to smooth out the noisy per-step gradients you'll get from batch\\_size=1-2\n\n\\- \\`warmup\\_ratio=0.1\\` (longer warmup than usual) ‚Äî the model needs more steps before it \"commits\" to the style shift\n\n\\- \\`weight\\_decay=0.01\\` helps prevent the few-shot memorisation collapse\n\n\\- For target modules, \\`q\\_proj, v\\_proj\\` only (skip k/o/gate) ‚Äî minimum footprint for behavioural style\n\n\n\n\\*\\*3. On your early stopping trigger:\\*\\*\n\nValidation loss \\*spikes\\* on micro-datasets are often gradient norm events rather than true divergence ‚Äî the spike resolves within 2-3 steps. Before triggering early stopping, check if the spike recovers. A tool like ZClip (adaptive gradient clipping based on rolling norm history) handles this better than fixed \\`max\\_grad\\_norm\\` ‚Äî it only clips when the norm is statistically anomalous vs. your run history rather than at a fixed ceiling.\n\n\n\nI ran a similar ablation on TinyLlama (200 rows, same seed) comparing plain LoRA vs LoRA + adaptive clipping ‚Äî peak grad norm dropped 52.7% with neutral impact on final loss. For a 50-row micro-dataset the effect would likely be more pronounced. Happy to share details if useful.",
          "score": 1,
          "created_utc": "2026-02-22 05:05:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbhvrv",
      "title": "Fine-Tuning Qwen 4B: Need Tips on Configs, Overfitting & Small Datasets?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rbhvrv/finetuning_qwen_4b_need_tips_on_configs/",
      "author": "dyeusyt",
      "created_utc": "2026-02-22 09:50:35",
      "score": 8,
      "num_comments": 9,
      "upvote_ratio": 0.9,
      "text": "So am working on my thesis project which involves fine-tuning a small language model for a specific code generation task in a niche domain (Typescript)\n\nI'm leaning toward the Qwen family of models. I started by fine-tuning the 8B version, but it didn't feel like a true SLM in terms of consumer-hardware-efficiency and size, so I'm downgrading to the 4B variant for better adherence to SLM part.\n\nMy main concern is my dataset: It's high-quality but small, with only 700-800¬†`{prompt,completion}`¬†pairs. Some pairs are distilled from larger LLMs, while others come from real code snippets paired with synthetically generated prompts. The data is straightforward (no chain-of-thought reasoning) but it includes potential noise: like non-code elements in code files (placeholders, plain text, or image paths). I want to train the model effectively so it performs well on my use case without picking up this noise or overfitting to the limited examples\n\nFor context I'm currently training on Google Colab with an A100 GPU. Here's the configuration I'm using, based on recommendations from Reddit threads and Unsloth docs:\n\n    model = FastLanguageModel.get_peft_model(\n    ¬† ¬† model,\n    ¬† ¬† r=64,\n    ¬† ¬† lora_alpha=128,\n    ¬† ¬† lora_dropout=0.05,\n    ¬† ¬† target_modules=[\n    ¬† ¬† ¬† ¬† \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", ¬†# Self-attention\n    ¬† ¬† ¬† ¬† \"gate_proj\", ¬†# MLP gate for code generation patterns\n    ¬† ¬† ],\n    ¬† ¬† bias=\"none\", ¬†\n    ¬† ¬† use_gradient_checkpointing=\"unsloth\", \n    ¬† ¬† random_state=3407,\n    ¬† ¬† use_rslora=False,\n    ¬† ¬† loftq_config=None,\n    )\n    \n    training_args = SFTConfig(\n    ¬† ¬† output_dir=\"./qwen-8b-a100\",\n    ¬† ¬† per_device_train_batch_size=16, \n    ¬† ¬† gradient_accumulation_steps=2, ¬†\n    ¬† ¬† per_device_eval_batch_size=16, ¬†\n    \n    ¬† ¬† num_train_epochs=3,\n    ¬† ¬† max_steps=-1, ¬†# Use epochs (not max_steps)\n    ¬† ¬† learning_rate=2e-4,\n    ¬† ¬† lr_scheduler_type=\"cosine\",\n    ¬† ¬† warmup_ratio=0.05, ¬†# 5% warmup\n    ¬† ¬† optim=\"adamw_8bit\", ¬†# Memory efficient, works well with LoRA\n    ¬† ¬† weight_decay=0.01, ¬† # Light regularization\n    ¬† ¬† fp16=False, ¬†# Don't use FP16 on A100\n    ¬† ¬† bf16=True, ¬†# A100 has native BF16 support - MUCH better!\n    ¬† ¬† tf32=True, ¬†# Enable TensorFloat-32 for even faster matmuls\n    ¬† ¬† dataloader_num_workers=4, ¬†# Parallel data loading\n    ¬† ¬† dataloader_pin_memory=True, ¬†# Faster GPU transfers\n    ¬† ¬† logging_steps=5,\n    ¬† ¬† eval_strategy=\"steps\",\n    ¬† ¬† eval_steps=10,\n    ¬† ¬† save_strategy=\"steps\",\n    ¬† ¬† save_steps=10, ¬†# Match eval_steps\n    ¬† ¬† save_total_limit=3, ¬†# Keep 3 best\n    ¬† ¬† load_best_model_at_end=True,\n    ¬† ¬† metric_for_best_model=\"eval_loss\",\n    ¬† ¬† greater_is_better=False,\n    ¬† ¬† packing=True,\n    ¬† ¬† max_seq_length=4096,\n    ¬† ¬† seed=3407,\n    ¬† ¬† report_to=\"none\",\n    ¬† ¬† dataset_text_field=\"text\",\n    )\n    \n    trainer = SFTTrainer(\n    ¬† ¬† model=model,\n    ¬† ¬† args=training_args,\n    ¬† ¬† processing_class=tokenizer,\n    ¬† ¬† train_dataset=train_dataset_formatted,\n    ¬† ¬† eval_dataset=val_dataset_formatted,\n    )\n    \n    # Using Unsloth's gradient accumulation fix\n    from unsloth import unsloth_train\n    trainer_stats = unsloth_train(trainer)\n\nI'm fairly new to fine-tuning (about 60% VibeCoding; 40% reading docs) and the results so far aren't great. The model underperforms on my tasks - The 8B one.\n\nSo I'm reaching out to folks who've worked with Qwen models: What configs have worked well for you, especially for small datasets and code generation? Any tips on preventing overfitting? Are there must-read docs or guides to get started properly?\n\nThanks in advance.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rbhvrv/finetuning_qwen_4b_need_tips_on_configs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6r7bct",
          "author": "yoracale",
          "text": "The most important read are our lora hyperparameters guide which includes overfitting/underfitting, selecting the correct hyperparamters and much much more: [https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)",
          "score": 5,
          "created_utc": "2026-02-22 11:20:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rawcl",
          "author": "NorthEastCalifornia",
          "text": "I'm working in the same direction, but in a different domain (language). I'm currently preparing a dataset, so I'm reading carefully. 8b seems like a good option, I just need to play with the parameters.",
          "score": 2,
          "created_utc": "2026-02-22 11:53:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rv0vu",
              "author": "dyeusyt",
              "text": "8B is actually usable now but its honestly too big to still call it a proper SLM. The graph difference between the smaller one and this doesn‚Äôt even look that impressive anyway; I‚Äôll probably just fine-tune both and see which one comes out on top considering resources and all.",
              "score": 1,
              "created_utc": "2026-02-22 14:12:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rsplf",
          "author": "wektor420",
          "text": "I have similiar setup after 400 steps eval loss stops improving :/",
          "score": 2,
          "created_utc": "2026-02-22 13:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rub9h",
              "author": "dyeusyt",
              "text": "I did the above because some docs on Unsloth (or related to Qwen) recommended it; But maybe 400 it is then.",
              "score": 2,
              "created_utc": "2026-02-22 14:08:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rnsla",
          "author": "Thrumpwart",
          "text": "I‚Äôm planning to do a full fine tune on Q4B with a 3 billion token dataset. Am I crazy or incredible? Please advise.",
          "score": 1,
          "created_utc": "2026-02-22 13:29:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rprta",
              "author": "dyeusyt",
              "text": "> Please advise.\n\nYou're the reason RAM costs a fortune now! /s",
              "score": 3,
              "created_utc": "2026-02-22 13:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6x2tfc",
          "author": "NoobMLDude",
          "text": "> r=64,\n> lora_alpha=128,\n\nFor a dataset with 800 samples , the Lora rank (r) seems high. A rank of 8 should be a good starting point. \n\nIf you suspect overfitting, reducing rank and lora_alpha is a good strategy. \n\nRemember Lora_rank (r) are usually multiple of 2 : 4,8,16.",
          "score": 1,
          "created_utc": "2026-02-23 08:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zcrkf",
          "author": "indicava",
          "text": "Why use PEFT? For a 4B model with 4K seq lengths, you could probably do a full parameter fine tune on that A100. \n\nYou MUST have a bigger and more diverse dataset. 600 examples is hardly enough for coding in a niche domain. \nThink 10K-30K examples, that‚Äôs the mindset you need to get into. \nAlso, critically, more is not enough. They have to be diverse across your niche domain so the model can learn to generalize around it. \n\nBuild out a good synthetic data generation pipeline, leverage SOTA model‚Äôs free tiers if you can‚Äôt afford the API costs.\n\nLastly, don‚Äôt set too high expectations for a 8B model, let alone a 4B one. They ain‚Äôt that smart, even with proper fine tuning.",
          "score": 1,
          "created_utc": "2026-02-23 17:09:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcpg7a",
      "title": "llama-server Production Ready?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rcpg7a/llamaserver_production_ready/",
      "author": "Sudden_Tennis_2067",
      "created_utc": "2026-02-23 18:26:22",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 0.83,
      "text": "Wondering if llama-server (that's part of llama.cpp) is production ready and performance is comparable to vllm?\n\nMost of the comparisons I see are between vllm and llama.cpp, and they show that vllm is significantly more performant and llama.cpp is just not production ready. But I wonder if it's a different story for llama-server?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rcpg7a/llamaserver_production_ready/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o73i5dw",
          "author": "TokenRingAI",
          "text": "It's not even remotely production ready, the regex parser triggers segfaults multiple times per day while consuming 100% cpu.",
          "score": 11,
          "created_utc": "2026-02-24 07:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73kj86",
              "author": "yoracale",
              "text": "There's a current PR which should fix the parsing issues: [https://github.com/ggml-org/llama.cpp/pull/18675](https://github.com/ggml-org/llama.cpp/pull/18675)",
              "score": 7,
              "created_utc": "2026-02-24 07:29:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o767903",
                  "author": "TokenRingAI",
                  "text": "And while that is great, it still uses std::regex, so will still blow up and consume massive CPU and stack, but you all haven't come to terms with that yet.\n\nI suggested switching to boost::regex for a very good reason, it is distributed as a header only package with no dependencies on the rest of the boost ecosystem, does not use recursion for every character, and has consistent behavior and implementation across architectures.\n\nThe performance profile of std::regex is implementation defined and there is no configurable safeguard against recursion depth blowing up the stack.",
                  "score": 1,
                  "created_utc": "2026-02-24 17:34:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74t2yh",
          "author": "StardockEngineer",
          "text": "No one runs llama.cpp as a production server if they‚Äôre serious.",
          "score": 6,
          "created_utc": "2026-02-24 13:34:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o750oqq",
              "author": "PaceZealousideal6091",
              "text": "I hope this changes with lcpp-hf tie up because nothing comes close to lcpp for edge devices inference.",
              "score": 1,
              "created_utc": "2026-02-24 14:15:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o752efd",
                  "author": "StardockEngineer",
                  "text": "Define what edge inference means to you? \n\nedit: https://huggingface.co/blog/ggml-joins-hf",
                  "score": 1,
                  "created_utc": "2026-02-24 14:24:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73gixx",
          "author": "yoracale",
          "text": "The biggest difference I'd say if you have any CPU or RAM in your setup, llama.cpp is definitely better and faster. Llama-server is production ready and the best for single user inference (they also have multi user). There are ways to enable high throughput mode if you look through their docs.\n\nIf you mainly utilize GPUs, especially one's that are large like H100s, or if you want batched inference for multiple users, then vLLM is most likely better.",
          "score": 4,
          "created_utc": "2026-02-24 06:53:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74hlsq",
          "author": "sid_276",
          "text": "production ready you are looking at vLLM, SGLang, TensorRT-LLM",
          "score": 2,
          "created_utc": "2026-02-24 12:22:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77g3bf",
          "author": "LA_rent_Aficionado",
          "text": "If your definition of production is multi-user inference then no, if you have a production workflow with single stream inference or need support on memory constrained devices - yes.",
          "score": 2,
          "created_utc": "2026-02-24 20:58:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79d02c",
          "author": "Most_Drawing5020",
          "text": "I'm a user who use both mlx and llama.cpp. IMO llama-server is way more production ready than mlx-lm.server.\n\n",
          "score": 1,
          "created_utc": "2026-02-25 02:59:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdtqdm",
      "title": "Qwen 3.5 35B A3B verbosity issue",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rdtqdm/qwen_35_35b_a3b_verbosity_issue/",
      "author": "PaceZealousideal6091",
      "created_utc": "2026-02-24 21:42:41",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,  \n   I have been trying to test the new Qwen 3.5 35B A3B q4 k\\_l ud quant using latest lcpp build (b8145). I ran the basic test using llama cli with the following parameters:  \n\\--ctx-size 8192 \\`  --flash-attn on \\`  -ngl 99 \\`  --n-cpu-moe 40 \\`  --cache-type-k q4\\_0 \\`  --cache-type-v q4\\_0 \\`  --temp 0.6 \\`  --top-p 0.95 \\`  --min-p 0.0 \\`  --top-k 20 \\`  --repeat\\_penalty 1.0 \\`  --presence\\_penalty 0.0 \\`  --seed 3407   \nit keeps going into an infinite verbose answer. Is anybody else facing the same issue. I tried to set '--jinja' but that didnt help. when i tried to set '--chat-template-kwargs \"{\\\\\"enable\\_thinking\\\\\": false}\"' argument as described in the unsloth documentation, i am getting error:  \n\"error while handling argument \"--chat-template-kwargs\": \\[json.exception.parse\\_error.101\\] parse error at line 1, column 2: syntax error while parsing object key - invalid literal; last read: '{\\\\'; expected string literal\"",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rdtqdm/qwen_35_35b_a3b_verbosity_issue/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o77rinv",
          "author": "kironlau",
          "text": "set -ctk q8\\_0 -ctv q8\\_0Ôºårepeat\\_penalty 1.05 (keep increasing by 0.05, if problem existsÔºåbut less than 1.2)",
          "score": 5,
          "created_utc": "2026-02-24 21:50:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o789krw",
              "author": "PaceZealousideal6091",
              "text": "Increasing the context to q8 does help a bit but the thinking is way too verbose. I am just not able to turn off the thinking/reasoning mode.",
              "score": 1,
              "created_utc": "2026-02-24 23:20:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o790wk5",
                  "author": "Look_0ver_There",
                  "text": "Did you try setting the reasoning budget to zero? It doesn't work for all models, but I think Qwen supports it from memory.",
                  "score": 1,
                  "created_utc": "2026-02-25 01:51:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o783fx7",
          "author": "yoracale",
          "text": "The models are very sensitive to correct settings, did you try turning on presence penalty? Parameters: https://unsloth.ai/docs/models/qwen3.5#recommended-settings",
          "score": 2,
          "created_utc": "2026-02-24 22:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78a367",
              "author": "PaceZealousideal6091",
              "text": "Yes. I played around with it. The thinking is way too verbose. I am not able to shut it off. \"--chat-template-kwargs\" argument doesn't work. My initial issue was using the back slash in the windows. I fixed that.  Now I don't get any error. But its being ignored. Thinking just doesn't get disabled.",
              "score": 2,
              "created_utc": "2026-02-24 23:23:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o78ulvn",
          "author": "el-rey-del-estiercol",
          "text": "Teneis que usar la version de autoparse la rama commit 16875 , para poder usar las herramientas",
          "score": 2,
          "created_utc": "2026-02-25 01:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78gvh9",
          "author": "NoahFect",
          "text": "I'm seeing the same thing with 122B with the current llama-server build.  It made a very strong first impression on a couple of test prompts, but almost everything I've tried since is looping, with or without presence_penalty 1.5.\n\nWhat's the difference between repeat_penalty and presence_penalty?  Should we be using repeat_penalty as well/instead?",
          "score": 1,
          "created_utc": "2026-02-25 00:00:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcehjr",
      "title": "Mac Studio (M4 Max, 128GB) for FULL fine-tuning a 27B Model",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rcehjr/mac_studio_m4_max_128gb_for_full_finetuning_a_27b/",
      "author": "PlayerWell",
      "created_utc": "2026-02-23 11:00:51",
      "score": 6,
      "num_comments": 17,
      "upvote_ratio": 0.75,
      "text": "Hi,\nWe are looking to add dedicated hardware to our project pipeline specifically to fully fine-tune and run inference on a 27B parameter model (Gemma 3 27B).\n\nWe are currently considering adding a Mac Studio with the following specs:\n-M4 Max (16-Core CPU, 40-Core GPU)\n-128GB Unified Memory\n-1TB SSD\n\nFor those of you who have experience training LLMs on Apple Silicon (using MLX), I have a few specific questions:\n1. Is a single Mac Studio realistically enough for a full fine-tune of a 27B model? (not LoRA/QLoRA).\n2. If we hit a bottleneck and need more compute/memory later, is it practical to buy a second Mac Studio and cluster them together for distributed training? \n3. Would it be a much more logical scenario to skip the Mac ecosystem entirely, buy GPUs and build a standard multi-GPU workstation connected via PCIe?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rcehjr/mac_studio_m4_max_128gb_for_full_finetuning_a_27b/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6ydgvx",
          "author": "Current_Ferret_4981",
          "text": "You will always spend less money and time just renting for a given model training. Only matters if you plan to do a fine-tuning run regularly to warrant local hardware or need data privacy. \n\nThat being said, I would much rather have a GPU cluster than Mac studio for training.",
          "score": 5,
          "created_utc": "2026-02-23 14:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yup09",
              "author": "PlayerWell",
              "text": "Because of the project requirements, the data we use during training and the environment where the model will run after training must be completely local. Project budget is tight, but since we don't have a time constraint",
              "score": 2,
              "created_utc": "2026-02-23 15:45:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o768k58",
                  "author": "Refefer",
                  "text": "I would really consider an rtx6000 build for models of that size - we use it ourselves for fine-tuning and it's been great.  You will save huge amounts of time on train and inference.  You will also need significantly more disk space for model check pointing, training data, etc.",
                  "score": 1,
                  "created_utc": "2026-02-24 17:40:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xqcfr",
          "author": "CKtalon",
          "text": "3. or wait for M5 (since you are going for M4 Max only, the M5 Max benchmarks will be out in about 2-3 weeks) and see if the matmul optimizations are better for training.",
          "score": 3,
          "created_utc": "2026-02-23 11:46:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o706l79",
          "author": "superSmitty9999",
          "text": "Just be aware Macs have good memory bandwidth, which is important for inference, but horrible FLOPs, which are required for training in addition to mem bandwidth.¬†\n\nI personally would consider training on a Mac to be strictly speaking not worth it except for the smallest jobs.¬†\n\nI bought a DGX spark and though the memory bandwidth on it sucks it‚Äôs more balanced in terms of flops.¬†\n\nIf you‚Äôre serious about training I think 1-4 RTX Pro 6000‚Äôs is the minimum for decent performance. (Or the cloud, obviously)¬†\n",
          "score": 3,
          "created_utc": "2026-02-23 19:26:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o740pjd",
              "author": "PlayerWell",
              "text": "After the comments here, we started looking at the RTX PRO 4000, 5000 and 6000 graphics cards.",
              "score": 1,
              "created_utc": "2026-02-24 10:02:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76gl0r",
                  "author": "superSmitty9999",
                  "text": "What are you looking to train? Just curious. It‚Äôs possible you don‚Äôt need to train a model at all.¬†",
                  "score": 1,
                  "created_utc": "2026-02-24 18:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yjs5e",
          "author": "LA_rent_Aficionado",
          "text": "3. That‚Äôs certainly not enough memory for a FFT of 27B or even 14B for that matter. Perhaps 8B tops without LORA.\n\nAlso training speed on Mac will be abysmal, for any model that size you are looking at data center class GPUs if you want efficient FFT.  You could use 6000 pros but lose out in NVLink so it‚Äôll add time. \n\nEither way I would not go the route of a Mac and either rent GPUs or get ready to shell out some serious cash",
          "score": 2,
          "created_utc": "2026-02-23 14:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70yyjd",
          "author": "pl201",
          "text": "You need 512gb unified memory to do full fine tuning a 27b model on Mac Silicon. I was trying with M3/256GB for a 24b model and failed on out of memory. Ended up just do a LoTA training.",
          "score": 2,
          "created_utc": "2026-02-23 21:43:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73aczp",
          "author": "Desperate-Sir-5088",
          "text": "Please dont try that. Even 4bit QLoRA finetuning of gemma-3 is extreamly slow in M3Ultra.",
          "score": 2,
          "created_utc": "2026-02-24 06:01:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73jjso",
          "author": "SafetyGloomy2637",
          "text": "A full fine tune on a 27b model is going to need about 250-300gb of VRAM. 128gb RAM isn‚Äôt enough and your bandwidth is limited too,  SFT job on a couple hundred million tokens would take 10+ hours at least unless you use adamw4bit, but I personally would stay away from anything less that Fp16",
          "score": 2,
          "created_utc": "2026-02-24 07:20:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y3wsa",
          "author": "yoracale",
          "text": "We're working on MLX support for Unsloth currently. Benchmark currently show that MLX is definitely not as optimized as it could be, I don't think it's possible FFT a 27B model on 128gb, or even 192GB tbh. I'd rather just do bf16 LoRA.",
          "score": 1,
          "created_utc": "2026-02-23 13:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yvb0h",
              "author": "PlayerWell",
              "text": "I sincerely apologize for opening this here. I know it's off-topic since you don't technically support it right now. I haven't developed on a Mac before, so I just assumed it would be supported",
              "score": 1,
              "created_utc": "2026-02-23 15:48:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xn2uc",
          "author": "No_Conversation9561",
          "text": "https://github.com/Goekdeniz-Guelmez/mlx-lm-lora\n\ncheck this out too",
          "score": 1,
          "created_utc": "2026-02-23 11:18:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra6xsg",
      "title": "What are some coding AI models I can run with my hardware?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1ra6xsg/what_are_some_coding_ai_models_i_can_run_with_my/",
      "author": "Sharp-University-555",
      "created_utc": "2026-02-20 20:44:08",
      "score": 4,
      "num_comments": 18,
      "upvote_ratio": 0.62,
      "text": "I have a PC with a 14600K, 32GB of E-die DDR4 RAM (which I could run at a stable OC upwards of 4000Mhz) and a Founders RTX 3070 8GB.\n\n\nI would appreciate any sort of feedback or direction since I have just started coding with Claude Pro Plan and want to explore moving towards a (small due to hardware constraints) local model or another alternative due to burning through tokens too fast. It would be used for HTML/CSS/JS mostly, so nothing too crazy programming wise.\n\n\nAlso, what is the best GUI I can use with local AI models? I love the Claude Desktop app interface, can I still use that or is there any other equivalent that follows the same design (showing the snippets of code it's changing in real time and having a easy to read history to go through when I want to revisit some ideas I prompted earlier.)\n\n\nThanks a lot!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1ra6xsg/what_are_some_coding_ai_models_i_can_run_with_my/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o6hy5tc",
          "author": "Shoddy_Bed3240",
          "text": "The most you can run is GLM 4.7 Flash. Pick a quantization level that fits within your available VRAM and RAM. Probably you can hit 10 t/s",
          "score": 7,
          "created_utc": "2026-02-20 21:44:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ia52l",
              "author": "Sharp-University-555",
              "text": "Will give it a go tomorrow once I set up Llama.cpp, thanks Shoddy!",
              "score": 1,
              "created_utc": "2026-02-20 22:47:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k5w6w",
          "author": "el-rey-del-estiercol",
          "text": "Glm 4.7 flash",
          "score": 3,
          "created_utc": "2026-02-21 06:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kqlyt",
              "author": "Sharp-University-555",
              "text": "Will give it a go, thanks!",
              "score": 1,
              "created_utc": "2026-02-21 09:24:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6iav37",
          "author": "LegacyRemaster",
          "text": "unsloth/Qwen3-Coder-Next-GGUF best one. Consider 36 gb (real) free: try IQ3",
          "score": 2,
          "created_utc": "2026-02-20 22:50:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kqmyc",
              "author": "Sharp-University-555",
              "text": "Got it, will try it in a couple hours",
              "score": 2,
              "created_utc": "2026-02-21 09:24:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6io4vo",
          "author": "ZealousidealShoe7998",
          "text": "someone recently posted on how to run qwen coder on a similar hardware but laptop version. they got it to run very slowsly but it was running.   \nif you use a good hardness you might be able to run it and go to other things while its processing.",
          "score": 2,
          "created_utc": "2026-02-21 00:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kqq0b",
              "author": "Sharp-University-555",
              "text": "I will look for that post, I don't mind waiting but I want to see which model produces the best quality code so I'll probably try a few of them before I make my pick",
              "score": 1,
              "created_utc": "2026-02-21 09:25:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nizk0",
          "author": "kiwibonga",
          "text": "Not recommended at all, the only local models that are actually useful (approaching Sonnet 3.5 / GPT-4o quality) are at least 20-30B parameters (10-16 GB with heavy quantization, plus 10-20 GB for the context).\n\nCPU inference is miserable also. Most agents and CLI tools add workflows and embeddings to the first prompt in a session. I don't think anyone does anything serious with CPU inference for coding because just a tiny 10k context with a few code files in it will take at least 5-10 minutes to start generating any tokens.\n\nA decent 16GB consumer GPU is still $500-700, and you can stack 2 on most cheap motherboards; it'll give you an infinitely better experience than a hybrid CPU-GPU setup.",
          "score": 2,
          "created_utc": "2026-02-21 19:48:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6njtul",
              "author": "Sharp-University-555",
              "text": "Yeah, tested a few models today but I'm going back to commercial solutions. Perhaps if I had 24GB GPU like a 4090 and 4 times the run it would be worth it, but definitely not with my current setup. I had to try anyways :)",
              "score": 1,
              "created_utc": "2026-02-21 19:52:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6pdu2a",
              "author": "Conscious-Pen5811",
              "text": "This! As a subcontract programmer, I need to run local models as some of my contracts have restrictions on where I can post code.\n\n20-30B models is where the magic happens. A couple of 3060-12gb cards will get decent results. I did this for some time before upgrading to 2x 5060-ti 16gb cards.\n\nGUI. I use LM studio with MCP tools, sometimes server mode. DIY takes a bit of time getting setup managing tools, prompts and context.  I code in C++, im sure these models will do excellent with HTML/CSS/JS",
              "score": 1,
              "created_utc": "2026-02-22 02:13:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n6ye2",
          "author": "laurenthu",
          "text": "Also interested in the GUI part of the question - I feel like the models are \"easy\" to run locally now, but the Claude Desktop app is hard to beat - Open WebUI doesn't come close honestly?",
          "score": 1,
          "created_utc": "2026-02-21 18:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n8rsp",
              "author": "Sharp-University-555",
              "text": "I just ran Qwen2.5 7B through LM Studio using Open WebUI and it‚Äôs decent but it doesn‚Äôt come close to Anthropic‚Äôs models and the Desktop app",
              "score": 1,
              "created_utc": "2026-02-21 18:56:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6odupq",
          "author": "Top_Oven8236",
          "text": "Install LMstudio and search models, it will automatically show offloading status before you download based on your hardware specs, but if you are low on specs search models finetuned by Unsloth they are great fit than primary models.",
          "score": 1,
          "created_utc": "2026-02-21 22:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qa8i2",
          "author": "johndeuff",
          "text": "I never found anything remotely useful at coding. It makes zero sense to adopt a much worse model at coding instead of just paying a sub for the best model.",
          "score": 1,
          "created_utc": "2026-02-22 06:08:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qr6ow",
              "author": "Sharp-University-555",
              "text": "Spot on, after trying out the few models I could fully run on my hardware, it's not up to par or close, even, to what Claude Code allows me to do, happy I tried it at least!",
              "score": 1,
              "created_utc": "2026-02-22 08:46:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6re5fj",
          "author": "llllJokerllll",
          "text": "If u wanna get more BEST results on speed, use better vLLM program",
          "score": 1,
          "created_utc": "2026-02-22 12:20:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6szp9r",
          "author": "Zestyclose-Shift710",
          "text": "I have the same 32gb ram and 8gb vram\n\nBasically your best bet are either models up to \\~8b fully in vram, or MoEs (Like GLM 4.7 Flash already recommended here a lot, and 30b a3b Qwens, Nemotron 3 nano, etc) with --cpu-moe and vram filled with context\n\nFor me 200k context quantized to Q8 fits in the gpu when running GLM 4.7 Flash with --cpu-moe for example\n\nAs for the gui I'd recommend llama.cpp with their new router mode and model ini file support, there's a nice webui you can view in the browser",
          "score": 1,
          "created_utc": "2026-02-22 17:26:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rddd7q",
      "title": "Which recent model have you found most steerable for repo-specific fine-tuning (agentic use case)?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rddd7q/which_recent_model_have_you_found_most_steerable/",
      "author": "podolskyd",
      "created_utc": "2026-02-24 11:09:55",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.83,
      "text": "I‚Äôm working on an agentic setup where the model has access to tools and the end goal is solving future PRs on a specific repository. I‚Äôm fine-tuning on the repo‚Äôs codebase, past PRs, and related context so the model actually understands how this project works, its conventions, architecture, patterns, etc.\n\nThe key thing I‚Äôm optimizing for is steerability: which base model, in your experience, picks up repo-specific patterns best from fine-tuning while still retaining strong tool use and instruction following?\n\nAlso, any recommendations for the fine-tuning and training data setup?\n\nCurious what people have tried here!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rddd7q/which_recent_model_have_you_found_most_steerable/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rdbu56",
      "title": "Trained Unsloth Mistral-7B with 1024 max_seq_length ‚Äî need longer context window inference",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1rdbu56/trained_unsloth_mistral7b_with_1024_max_seq/",
      "author": "Character-Metal-9315",
      "created_utc": "2026-02-24 09:37:38",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI fine-tuned **unsloth/mistral-7b-instruct-v0.2-bnb-4bit** using Unsloth with:\n\n    max_seq_length = 1024\n\nTraining completed successfully.\n\nHowever, during inference, when I pass a longer context, I get:\n\n    Unsloth: Input IDs of shape torch.Size([1, 3013]) with length 3013 > \n    the model's max sequence length of 1024.\n    We shall truncate it ourselves. It's imperative if you correct this issue first.\n\nFor my task, I **need a longer context window during inference**, since my inputs can easily exceed 3k tokens. I am using Kaggle's T4 GPU. So resource is limited.  \nThanks In Advance",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1rdbu56/trained_unsloth_mistral7b_with_1024_max_seq/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o743awp",
          "author": "wektor420",
          "text": "set it manually in inference code",
          "score": 3,
          "created_utc": "2026-02-24 10:26:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}