{
  "metadata": {
    "last_updated": "2026-02-01 03:27:24",
    "time_filter": "week",
    "subreddit": "unsloth",
    "total_items": 12,
    "total_comments": 59,
    "file_size_bytes": 76100
  },
  "items": [
    {
      "id": "1qo5y54",
      "title": "DeepSeek releases DeepSeek-OCR 2. üêã",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/rjkucnsh3ufg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-27 06:15:19",
      "score": 344,
      "num_comments": 31,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qo5y54/deepseek_releases_deepseekocr_2/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1zg0su",
          "author": "GosuGian",
          "text": "Downloading now..",
          "score": 13,
          "created_utc": "2026-01-27 08:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20copo",
              "author": "danielhanchen",
              "text": "Let us know how it does!",
              "score": 2,
              "created_utc": "2026-01-27 13:10:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z7ocu",
          "author": "larrytheevilbunnie",
          "text": "Outperforming Gemma 3 pro is crazy",
          "score": 6,
          "created_utc": "2026-01-27 07:31:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z8wv5",
              "author": "Inflation_Artistic",
              "text": "Gemma 3 pro?",
              "score": 5,
              "created_utc": "2026-01-27 07:42:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z8ywk",
                  "author": "larrytheevilbunnie",
                  "text": "Gemini* oops\n\nI want Gemma 4 tho",
                  "score": 9,
                  "created_utc": "2026-01-27 07:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20cs28",
              "author": "danielhanchen",
              "text": "Ye it's pretty good for the reduction in edit distance!\n\nI'm sure Gemini 3 Pro does better on other image benchmarks though!",
              "score": 1,
              "created_utc": "2026-01-27 13:10:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zixpy",
          "author": "PaceZealousideal6091",
          "text": "Lcpp support?",
          "score": 3,
          "created_utc": "2026-01-27 09:15:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zjrin",
              "author": "yoracale",
              "text": "Not yet unfortunately. there was a PR back in nov 2025 but it never got merged",
              "score": 5,
              "created_utc": "2026-01-27 09:22:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o20f6ku",
          "author": "Cautious-Raccoon-364",
          "text": "This is completely insane, wow. Gonna try it now.",
          "score": 3,
          "created_utc": "2026-01-27 13:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22m80d",
              "author": "Cautious-Raccoon-364",
              "text": "Ok, tried it, worked well. I just hate the pdf to image conversion needed? Why not natively support pdf? I‚Äôm sure u could fine tune it further for your datasets, but still finding Claude a little better for bank document due diligence.",
              "score": 1,
              "created_utc": "2026-01-27 19:22:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zqgsg",
          "author": "Intelligent-Form6624",
          "text": "ROCm or vulkan?",
          "score": 2,
          "created_utc": "2026-01-27 10:24:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o207bbc",
              "author": "yoracale",
              "text": "Should work",
              "score": 2,
              "created_utc": "2026-01-27 12:36:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21lc80",
          "author": "arman-d0e",
          "text": "This feels like fate. I just started using deepseek ocr yesterday üò≠",
          "score": 2,
          "created_utc": "2026-01-27 16:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eop15",
          "author": "Late_Special_6705",
          "text": "How to launch this model? I don't want to write a python code. Maybe he works in ollama or vlm?",
          "score": 2,
          "created_utc": "2026-01-29 13:33:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21si3w",
          "author": "vertigo235",
          "text": "Does it handle checkboxes on forms?",
          "score": 1,
          "created_utc": "2026-01-27 17:15:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27zxqm",
              "author": "yoracale",
              "text": "Yes it should be able to, due to its new reading ability",
              "score": 1,
              "created_utc": "2026-01-28 14:44:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2aeyaa",
                  "author": "vertigo235",
                  "text": "Yeah seems promising",
                  "score": 1,
                  "created_utc": "2026-01-28 21:07:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21z6dl",
          "author": "AntiqueAndroid0",
          "text": "How is this compared to mistral ocr3?",
          "score": 1,
          "created_utc": "2026-01-27 17:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2800gi",
              "author": "yoracale",
              "text": "Unsure but it 'should' be better.",
              "score": 1,
              "created_utc": "2026-01-28 14:45:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o221xb6",
          "author": "samplebitch",
          "text": "Anyone know if this model knows how to generate bounding boxes?",
          "score": 1,
          "created_utc": "2026-01-27 17:55:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28586w",
              "author": "yoracale",
              "text": "I'm not sure if it'll generate bounding boxes, but it will detect them yes",
              "score": 1,
              "created_utc": "2026-01-28 15:10:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o224v6m",
          "author": "eagledoto",
          "text": "Is it a vlm? Can we use it instead of qwen 3 vl in comfy to generate prompts from images?",
          "score": 1,
          "created_utc": "2026-01-27 18:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2851bx",
              "author": "yoracale",
              "text": "Yes it is a vision model but cannot replace Qwen 3 vl",
              "score": 1,
              "created_utc": "2026-01-28 15:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o285cgk",
                  "author": "eagledoto",
                  "text": "But can we use it in ComfyUI?",
                  "score": 1,
                  "created_utc": "2026-01-28 15:10:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2a7tx8",
          "author": "Outrageous-Phase-786",
          "text": "I wonder if it is also well performing with handwriting... any result on that?",
          "score": 1,
          "created_utc": "2026-01-28 20:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c2ypz",
          "author": "6969its_a_great_time",
          "text": "Vllm example doesn‚Äôt work",
          "score": 1,
          "created_utc": "2026-01-29 02:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ccvan",
              "author": "yoracale",
              "text": "Oh whoops it doesn't work because the PR is still open. You'll need to use transformers v5 or the Unsloth guide",
              "score": 1,
              "created_utc": "2026-01-29 03:00:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dui2c",
          "author": "someone383726",
          "text": "Anyone get this working in vLLM?  I tried nightly and a PR version that had a fix and all I got were garbage outputs.",
          "score": 1,
          "created_utc": "2026-01-29 09:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zo5rt",
          "author": "loutishgamer",
          "text": "Does it have new coding and new texting generation coding and knowledge? Like when you're giving it a prompt does it give more new knowledge or coding or truth?",
          "score": 0,
          "created_utc": "2026-01-27 10:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o205cb0",
              "author": "Smilysis",
              "text": "That's a ocr model, not llm",
              "score": 2,
              "created_utc": "2026-01-27 12:23:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpbmrt",
      "title": "You can now run Kimi K2.5 locally!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/nwp8ammpf3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 13:40:33",
      "score": 212,
      "num_comments": 18,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qpbmrt/you_can_now_run_kimi_k25_locally/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o281v90",
          "author": "m98789",
          "text": "On a single H100 (80GB) VM with 256 GB of ram and 1TB of ssd and plenty of cpu cores, how fast in tokens / sec can we expect?",
          "score": 11,
          "created_utc": "2026-01-28 14:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284n8h",
              "author": "hudimudi",
              "text": "That would be very interesting to know",
              "score": 7,
              "created_utc": "2026-01-28 15:07:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o284ovk",
              "author": "yoracale",
              "text": "You can run the 2bit one possible with that. Maybe like 20 tokens?",
              "score": 5,
              "created_utc": "2026-01-28 15:07:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o28c5jm",
                  "author": "m98789",
                  "text": "So if this set up works, it‚Äôs about $3/hour on demand as an instance in a major cloud. So say we get Claude Opus level for $3/hour. And at 20 tokens a second, that‚Äôs 72,000 output tokens per hour. using the api for opus for that amount of tokens costs about $2. If using a reserved instance of the cloud vm, the cost goes to about $2/hour. So it‚Äôs a wash on cost effectiveness. But on privacy and control, it‚Äôs a win. Assuming quality is effectively the same.",
                  "score": 7,
                  "created_utc": "2026-01-28 15:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28ew1j",
          "author": "illkeepthatinmind",
          "text": "I mean.... \\*I\\* can't...",
          "score": 3,
          "created_utc": "2026-01-28 15:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2j647h",
          "author": "BeginningReveal2620",
          "text": "Cool",
          "score": 3,
          "created_utc": "2026-01-30 02:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28h6dn",
          "author": "joninco",
          "text": "Did you apply any unsloth fixes? I ran the K2.5 from moonshot's hf page and was unimpressed with the coding results for creating a single page html tetris app. I used 1.0 temp, 0.95 topp.",
          "score": 2,
          "created_utc": "2026-01-28 16:03:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d0d8h",
              "author": "yoracale",
              "text": "There's no unsloth fixes for this model :(. Have you tried their API?",
              "score": 2,
              "created_utc": "2026-01-29 05:30:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2evu8w",
                  "author": "joninco",
                  "text": "Only locally with sglang. I haven't tried their API yet, but I will give it a shot and compare with my results to see if they are consistent!\n\nFor SGLANG on RTX PRO 6000s, disabling DEEPGEMM and ensuring temp 1.0, top p 0.95 and min p 0.01 made a material difference. \n\nMy only problem now is that it's materially slowing down as context grows -- assuming triton attn isn't optimal for this model.",
                  "score": 3,
                  "created_utc": "2026-01-29 14:11:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28lhoq",
          "author": "nomorebuttsplz",
          "text": "Hi -- why is the K.2.5 Q 3 XL UD quite a bit larger than the same quant for K2 thinking?",
          "score": 2,
          "created_utc": "2026-01-28 16:22:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28q2mv",
          "author": "Nooddlleee",
          "text": "Does anyone test the code quality? I am curious how it performs on the complex tasks and big projects",
          "score": 2,
          "created_utc": "2026-01-28 16:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28sxci",
          "author": "davidl002",
          "text": "Cannot imagine running it locally anytime soon but thanks for the work!\n\nHope in 5 years hardware can be affordable enough to run this locally cheaply.",
          "score": 2,
          "created_utc": "2026-01-28 16:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29lpma",
          "author": "emperorofrome13",
          "text": "That is the flux 2 dev of coding ai. Way too big to be useful.",
          "score": 2,
          "created_utc": "2026-01-28 18:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28p0fc",
          "author": "maxtheman",
          "text": "Would be VERY interested in the vision support, but already awesome work.",
          "score": 1,
          "created_utc": "2026-01-28 16:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r6ij8",
          "author": "Significant-Taro409",
          "text": "Is there any info on fine-tuning available",
          "score": 1,
          "created_utc": "2026-01-31 07:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2skoz9",
              "author": "yoracale",
              "text": "The model is gigantic. You'll need at least 8x B200's to finetune it",
              "score": 1,
              "created_utc": "2026-01-31 14:27:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqc06x",
      "title": "How to Run Local LLMs with Claude Code & OpenAI Codex!",
      "subreddit": "unsloth",
      "url": "https://i.redd.it/6mhzmpzd6bgg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-29 15:42:38",
      "score": 102,
      "num_comments": 23,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Guide:FO2C6766BA42_Sloth_Quest:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qqc06x/how_to_run_local_llms_with_claude_code_openai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2fnep4",
          "author": "__Maximum__",
          "text": "Fine-tune?",
          "score": 5,
          "created_utc": "2026-01-29 16:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2frhwe",
              "author": "yoracale",
              "text": "Yep fine-tune! We use glm flash to autonomously fine-tune an LLM with unsloth",
              "score": 3,
              "created_utc": "2026-01-29 16:38:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2fr3lz",
              "author": "moonflowerseed",
              "text": "On Mac/Apple Silicon?",
              "score": 1,
              "created_utc": "2026-01-29 16:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2fre76",
                  "author": "yoracale",
                  "text": "We're working on Mac support for real. Optimizations are done, only thing next is checking, benchmarking and Integra tion",
                  "score": 10,
                  "created_utc": "2026-01-29 16:38:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2j0zdt",
          "author": "PixelatedCaffeine",
          "text": "Is there a way to change the Claude Code limit to match the model‚Äôs limit? It always seems to default to 200k, and I would love to use the auto compact feature based on that",
          "score": 2,
          "created_utc": "2026-01-30 02:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g6qtr",
          "author": "ethereal_intellect",
          "text": "They lose web search capability when linked to local models right?",
          "score": 1,
          "created_utc": "2026-01-29 17:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kd5p2",
              "author": "admajic",
              "text": "Not is you ask it to build you a mcp search tool.",
              "score": 1,
              "created_utc": "2026-01-30 07:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gahui",
          "author": "Glittering-Call8746",
          "text": "Prompt \"You can only work in the cwd project/. Do not search for CLAUDE.md - this is it. Install Unsloth via a virtual environment via uv. See https://unsloth.ai/docs/get-started/install/pip-install on how (get it and read). Then do a simple Unsloth finetuning run described in https://github.com/unslothai/unsloth. You have access to 1 GPU.\" What's the model it's finetuning..",
          "score": 1,
          "created_utc": "2026-01-29 18:04:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrpqc",
              "author": "yoracale",
              "text": "Llama most likely",
              "score": 1,
              "created_utc": "2026-01-29 22:14:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gu6cv",
          "author": "creminology",
          "text": "Has Anthropic ever given any indication that they view this as a breach of terms of service? Asking because they have come down on hard on using Claude Code subscriptions in other environments, although this is doing the reverse.",
          "score": 1,
          "created_utc": "2026-01-29 19:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrmg0",
              "author": "yoracale",
              "text": "Oh no, they allow this because Claude Code was meant to be used locally!",
              "score": 3,
              "created_utc": "2026-01-29 22:13:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2luumi",
              "author": "Otherwise-Way1316",
              "text": "They don‚Äôt like their models being used in other platforms, like OpenCode.\n\nAll indications are that they are ok with Claude Code being used with other models.",
              "score": 2,
              "created_utc": "2026-01-30 14:20:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kpq0d",
          "author": "No-Weird-7389",
          "text": "Still waiting for nvfp4",
          "score": 1,
          "created_utc": "2026-01-30 09:34:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l4ryc",
              "author": "yoracale",
              "text": "We're working on it! :) Might not be for this model but for future ones",
              "score": 1,
              "created_utc": "2026-01-30 11:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l3ars",
          "author": "SatoshiNotMe",
          "text": "Last I checked, running glm-4.7-flash with CC on my M1 Pro Max 64GB MacBook via llama-server got me an abysmal 3 tok/s, for less than the 20 tok/s I got with Qwen3-30B-A3B. I use this setup to hook up CC with local models:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nCurious what llama-server settings you recommend to get good performance with GLM-4.7-flash",
          "score": 1,
          "created_utc": "2026-01-30 11:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oegdi",
          "author": "stuckinmotion",
          "text": "Does this work for anyone? I followed the steps, set ANTHROPIC\\_BASE\\_URL to my llama-server instance, but I'm getting \"Missing API key\"\n\nedit: Ok so exporting ANTHROPIC\\_API\\_KEY=sk-1234 got it working. Maybe the guide can be updated",
          "score": 1,
          "created_utc": "2026-01-30 21:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pq0sy",
              "author": "yoracale",
              "text": "Ooo ok interesting we'll update it in our guide then thanks for the feedback",
              "score": 1,
              "created_utc": "2026-01-31 01:30:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2vuwel",
          "author": "JonatasLaw",
          "text": "Can I run it in a rtx 3090 + 64gb RAM?",
          "score": 1,
          "created_utc": "2026-02-01 00:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fnjti",
          "author": "toreobsidian",
          "text": "This is awesome. I'll Test this with Just a dataset I'm currently preparing that Features content of a famous german political figure. Too bad I have so little time for this nonsens Project but this should be a nice boost üòÖ",
          "score": 1,
          "created_utc": "2026-01-29 16:21:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qndg98",
      "title": "New FP8 GLM-4.7-Flash Unsloth Dynamic Quants for vLLM, SGLang",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qndg98/new_fp8_glm47flash_unsloth_dynamic_quants_for/",
      "author": "danielhanchen",
      "created_utc": "2026-01-26 11:23:41",
      "score": 69,
      "num_comments": 58,
      "upvote_ratio": 1.0,
      "text": "Hey guys as we're already in the new year, throughout the rest of the year we hope to release FP4 and FP8 quants specifically designed for fast inference and deployment via vLLM! For performance, expect **13,000 tokens / s throughput on 1xB200 (130 token / s decoding per user)!** [FP8 quants here.](https://huggingface.co/unsloth/GLM-4.7-Flash-FP8-Dynamic)\n\nThese are **dynamically quantized** using our Dynamic methodology and in the near future, we aim to calibrate them using our dataset which is specifically designed for production, chat, coding, tool-calling and long context use-cases which shall improve the quality even further. And yes, we do intend to compare the difference via harder benchmarks like our previous Aider Polyglot benchmarks.\n\nWe also have a mini guide on how to exactly use the FP8 quants via vLLM here: [https://unsloth.ai/docs/models/glm-4.7-flash#glm-4.7-flash-in-vllm](https://unsloth.ai/docs/models/glm-4.7-flash#glm-4.7-flash-in-vllm) \\- vLLM is what we tested, and works well!\n\nThe code to serve for 4 GPUs is:\n\n    export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False\n    CUDA_VISIBLE_DEVICES='0,1,2,3' vllm serve unsloth/GLM-4.7-Flash-FP8-Dynamic \\\n        --served-model-name unsloth/GLM-4.7-Flash \\\n        --tensor-parallel-size 4 \\\n        --tool-call-parser glm47 \\\n        --reasoning-parser glm45 \\\n        --enable-auto-tool-choice \\\n        --dtype bfloat16 \\\n        --seed 3407 \\\n        --max-model-len 200000 \\\n        --gpu-memory-utilization 0.95 \\\n        --max_num_batched_tokens 16384 \\\n        --port 8000 \\\n        --kv-cache-dtype fp8\n\nNow my question to you guys is what would you like to see us do next for quantization? AWQ, INT4, or what? Thanks guys!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qndg98/new_fp8_glm47flash_unsloth_dynamic_quants_for/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o1szd0p",
          "author": "debackerl",
          "text": "Awesome! I'm using FP8 a lot. Maybe mxfp4?",
          "score": 5,
          "created_utc": "2026-01-26 12:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1szv21",
              "author": "danielhanchen",
              "text": "Yes NXFP4 is next!! Or would MXFP4 be better?",
              "score": 6,
              "created_utc": "2026-01-26 12:10:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tf6pr",
                  "author": "Acceptable-State-271",
                  "text": "mxfp4 is better, and you are best",
                  "score": 4,
                  "created_utc": "2026-01-26 13:46:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1u9wir",
                  "author": "debackerl",
                  "text": "MXFP4 please. As far as I know, NVFP4 is for RTX 50xx only. At least, MXFP4 still runs on 40xx.",
                  "score": 3,
                  "created_utc": "2026-01-26 16:12:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1wexvt",
                  "author": "eleqtriq",
                  "text": "No!  NVFP4!",
                  "score": 2,
                  "created_utc": "2026-01-26 21:44:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1t6w8t",
          "author": "thepetek",
          "text": "AWQ with an example guide of how you did it would be awesome!",
          "score": 5,
          "created_utc": "2026-01-26 12:58:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tdmyg",
              "author": "danielhanchen",
              "text": "Oh ok! Thanks for the suggestion!",
              "score": 3,
              "created_utc": "2026-01-26 13:38:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1y5q49",
              "author": "Phaelon74",
              "text": "There's a boat load of documentation in llm_compressor for how to do this and I just pr'd glm4.7 modeling, so it's easy for you to do now.",
              "score": 1,
              "created_utc": "2026-01-27 03:06:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vagus",
          "author": "mister2d",
          "text": "Here's a grenade: why do vLLM AND SGLang exist? They appear very similar when you use them.",
          "score": 3,
          "created_utc": "2026-01-26 18:48:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xlr2s",
              "author": "danielhanchen",
              "text": "They're both great! :) I think both can co-exist - SGLang is probably slightly faster in some cases, whilst vLLM has more support for archs",
              "score": 4,
              "created_utc": "2026-01-27 01:17:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xpweo",
                  "author": "mister2d",
                  "text": "I wish I were smart enough to create a meaningful PR to merge the codebases. üòÑ",
                  "score": 2,
                  "created_utc": "2026-01-27 01:40:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1y4y9a",
              "author": "Phaelon74",
              "text": "Ummm. Because they are for different user bases.  Vllm is for batching and sglang is the need for speed.  Different use cases and user bases.",
              "score": 2,
              "created_utc": "2026-01-27 03:02:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24my9c",
              "author": "kripper-de",
              "text": "vLLM is designed for massive user chat and SGLang is designed for agentic multi-turn with big context use cases. But they are converging in the same direction, adopting each other's advantages.",
              "score": 2,
              "created_utc": "2026-01-28 01:06:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24ueen",
                  "author": "mister2d",
                  "text": "Thanks! I do see the overlap. Perhaps one day the two shall become one for the sake of simplicity.",
                  "score": 1,
                  "created_utc": "2026-01-28 01:46:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o20vtvy",
          "author": "Phaelon74",
          "text": "AWQ is very different to GGUF, as the later is more mathmatical, and the former has important aspects as it relates to Datasets, etc.\n\nIf you do decide to make AWQs, there are many differences that you need to be super plugged into versus GGUFs, in order of importance:  \n1). Your Calibration Dataset is the MOST important thing when it comes to AWQs.  There's several papers out now talking through this.  Make sure you choose 10-20 datasets, melded based on what you want to capture.  You want to be really good at Coding?  Build a software engineering Dataset.  Creative Writing?  Build a dataset with lots of Creative writing examples.  \n2). If doing an MOE, make sure ALL experts are activated on every calibration sample.  Generally, without a model file, only the parts of experts that relate to a sample, will be activated.  You need to force every layer to be activated.  \n3). Everything you read about Calibration Samples and Sequence length are wrong.  Your starting should be 512 Calib Samples, and 2048 Seq length.  Anything shorter on either of these WILL see accuracy degrade  \n4). Make sure to understand Group Size and how it affects Accuracy  \n5). IGNORE LM\\_HEAD.  In GGUF land ya'll quant it at 6/Q6, etc.  Quanting LM\\_HEAD will reduce/degrae accuracy.  \n6). GLM4.7\\_FLASH does not natively work with AWQ due to layer mappings.  You'll have a lot of coding to do, to make it work.\n\nSeveral of us are already working through this with LLM\\_Compressor, so if you lock in, rock sauce.  Welcome to the bizarre land of AWQ.",
          "score": 3,
          "created_utc": "2026-01-27 14:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pbes",
              "author": "danielhanchen",
              "text": "Oh thank you so much for these pointers! This is extremely helpful!\n1. Agreed calib data is extremely important!\n2. Yes actually llama.cpp also has this issue - if too few experts are activated, it's always best to get more calib data\n3. Agreed! Interestingly our calib data we use for imatrix is in the order of 18K to 30K context length!\n4. Oh interesting will check\n5. Oh ok thanks!\n6. Ooo hmm I'll check that out!",
              "score": 1,
              "created_utc": "2026-01-28 09:28:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wma5i",
          "author": "meganoob1337",
          "text": "Would love awq  int4  ,\nThank you for your work!\n\nDid the performance issues in vllm get fixed yet btw? Last time I tried the model it was super slow üò≠ the outputs were good though",
          "score": 2,
          "created_utc": "2026-01-26 22:18:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1x1v7f",
              "author": "meganoob1337",
              "text": "Slow proportionate to models of the same size esp with higher context",
              "score": 2,
              "created_utc": "2026-01-26 23:34:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xlwzo",
                  "author": "danielhanchen",
                  "text": "Oh see https://unsloth.ai/docs/models/glm-4.7-flash#vllm-glm-4.7-flash-speculative-decoding which might be relative - we found spec decoding via the MTP module to slow things by 10x - disabling it on Blackwell can get 130 tokens / s per user decoding speed now",
                  "score": 1,
                  "created_utc": "2026-01-27 01:18:27",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1y5jvn",
              "author": "Phaelon74",
              "text": "We've got them for llm_compressor out there already.",
              "score": 1,
              "created_utc": "2026-01-27 03:05:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sveb2",
          "author": "Pentium95",
          "text": "Is '--dtype bfloat16' to be used with fp8 / fp4? \n\nAre there any PPL bench with those quants?",
          "score": 4,
          "created_utc": "2026-01-26 11:36:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sztxr",
              "author": "danielhanchen",
              "text": "Yes `--dtype bfloat16` is correct! This specifies the intermediate data-types.\n\nAnd yes - we're working on benchmarks! It'll hopefully come with our NVFP4 one - it should be similar to https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs",
              "score": 3,
              "created_utc": "2026-01-26 12:10:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tqkoa",
          "author": "Large-Example-1275",
          "text": "I'm looking to run the new **Unsloth FP8 GLM-4.7-Flash Dynamic Quants** on my **DGX Spark** (GB10). Given the strict requirements for FP8 support on Blackwell, I want to make sure I pick the right Docker approach to avoid compatibility headaches with vLLM versions and CUDA kernels.\n\nWhich route would you recommend?\n\n1. **Use the official NVIDIA vLLM container?** Is the `v25.12.post1-py3` image recent enough to handle this model/architecture combo out of the box? *Ref:*[*nvcr.io/nvidia/vllm:v25.12.post1-py3*](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/vllm?version=25.12.post1-py3)\n2. **Or build a custom container from the PyTorch base?** Is it safer to start with the PyTorch container and manually install `vllm-nightly` (CUDA 13) to ensure I have the latest kernels? *Ref:*[*nvcr.io/nvidia/pytorch:25.12-py3*](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch?version=25.12-py3)\n\nAny insights on the most stable workflow for this hardware would be appreciated",
          "score": 1,
          "created_utc": "2026-01-26 14:44:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxu9e",
              "author": "danielhanchen",
              "text": "I don't think stable vLLM works - you need the nightly one I think",
              "score": 1,
              "created_utc": "2026-01-26 17:55:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27ppsl",
                  "author": "1-a-n",
                  "text": "I hope they fix the nightly soon for sm-120, looks like a regression. In the mean time, is this only supported in vllm or might sglang also work perhaps?",
                  "score": 1,
                  "created_utc": "2026-01-28 13:52:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wmekr",
              "author": "Lyuseefur",
              "text": "vllm-studio helps",
              "score": 1,
              "created_utc": "2026-01-26 22:18:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xm37n",
                  "author": "danielhanchen",
                  "text": "vllm-studio??!! This is the first time I'm hearing about this haha",
                  "score": 1,
                  "created_utc": "2026-01-27 01:19:23",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vil8g",
          "author": "flobernd",
          "text": "Perfect timing! I‚Äôm just about to play with vLLM. So far I‚Äôve only used llama.cpp and a few forks. Hope GGUF support in vLLM will also improve since a lot of good quants (mostly unsloth) are GGUF. Especially Q3_K_XL is super cool for large models like DeepSeek. Don‚Äòt think I can fit the 4-bit vLLM compatible variants in VRAM.",
          "score": 1,
          "created_utc": "2026-01-26 19:22:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xmgg1",
              "author": "danielhanchen",
              "text": "Oh GGUF support might be complex in vLLM :( SGLang was actually working with us on it, but it'll be hard - we're planning to do dynamic 4bit quants so hopefully those will work!",
              "score": 1,
              "created_utc": "2026-01-27 01:21:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1y576z",
              "author": "Phaelon74",
              "text": "GGuf is unoptimized.  Do not use them with vllm.",
              "score": 1,
              "created_utc": "2026-01-27 03:03:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wmx86",
          "author": "Lyuseefur",
          "text": "I‚Äôm so confused. Is it 1,300 or 13,000 tokens a second - I see both numbers.\n\nIs it 1xB200 or 4xB200 required to run this ?",
          "score": 1,
          "created_utc": "2026-01-26 22:21:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xoa4w",
              "author": "danielhanchen",
              "text": "Oh I rechecked just to be 100% sure:\n\n1 x B200:\n\n1. Decoding speed per user is 130 tokens / s\n2. Total throughput on 1000 requests is in fact **13,000 tokens / s!!!**",
              "score": 2,
              "created_utc": "2026-01-27 01:31:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1y0db4",
                  "author": "Lyuseefur",
                  "text": "BRB, buying a B200 fleet of 100. 1,300,000 tokens a second on 10,000 Clawdbots here I come.",
                  "score": 1,
                  "created_utc": "2026-01-27 02:37:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o216zb4",
          "author": "Phaelon74",
          "text": "I'm confused, as your \"Dynamic\" quant approach, really is not relevant to FP8.  What specifically did you do, in FP8 quantization, where it mirrors your dynamic approach to GGUF?  Your FP8 dynamic would be identical to any other FP8, when approaching a normalized path, no?",
          "score": 1,
          "created_utc": "2026-01-27 15:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pdwk",
              "author": "danielhanchen",
              "text": "Oh some layers are actually upcasted to BF16 (the important ones :)\n\nI'm also thinking of calibrating the FP8 KV cache as well",
              "score": 1,
              "created_utc": "2026-01-28 09:29:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o224w46",
          "author": "quantier",
          "text": "If this is KV Cache fixed I would love to see AWQ quants! Also the REAP edition at 23B would be great to have functional solutions for",
          "score": 1,
          "created_utc": "2026-01-27 18:08:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pf7f",
              "author": "danielhanchen",
              "text": "Yes! I'll see what we can do for AWQ quants!",
              "score": 1,
              "created_utc": "2026-01-28 09:29:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o228qkx",
          "author": "StardockEngineer",
          "text": "I get errors that basically none of the backends suppport `--kv-cache-dtype fp8`.\n\n```\nValueError: No valid attention backend found for cuda with AttentionSelectorConfig(head_size=576, dtype=torch.bfloat16, kv_cache_dtype=fp8, block_size=None, use_mla=True, has_sink=False, use_sparse=False, use_mm_prefix=False, attn_type=AttentionType.DECODER). Reasons: {FLASH_ATTN_MLA: [kv_cache_dtype not supported, compute capability not supported, FlashAttention MLA not supported on this device], FLASHMLA: [compute capability not supported, FlashMLA Dense is only supported on Hopper devices.], FLASHINFER_MLA: [compute capability not supported, FlashInfer MLA kernel requires qk_nope_head_dim == 128, but got 192], TRITON_MLA: [kv_cache_dtype not supported], FLASHMLA_SPARSE: [kv_cache_dtype not supported, non-sparse not supported, compute capability not supported]}.\n[rank0]:[W127 18:15:45.494603369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n```\n\nAny ideas?  I've turned it off for now.  GPU is an RTX Pro 6000.",
          "score": 1,
          "created_utc": "2026-01-27 18:24:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22ahmy",
              "author": "StardockEngineer",
              "text": "lol container failed to start.  Too much compute capability I guess  \n\\`\\`\\`  \n(EngineCore\\_DP0 pid=153) NotImplementedError: No compiled cutlass\\_scaled\\_mm for CUDA device capability: 120. Required capability: 90 or 100  \n\\`\\`\\`",
              "score": 1,
              "created_utc": "2026-01-27 18:32:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26pnlj",
                  "author": "danielhanchen",
                  "text": "Wait that's weird hmmm so 10.X is B200 and 9.X is H100, H200. I actually have not tested on 12.X hmm maybe that's the issue? (RTX 50X, NVIDIA RTX PRO 6000, DGX Spark)",
                  "score": 1,
                  "created_utc": "2026-01-28 09:31:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24pqfg",
          "author": "kripper-de",
          "text": "Hi Daniel, have you tried doing task-aware pruning?\nI would love to see big models like Kiki K2.5 running on a Strix Halo 128GB by pruning all neurons that are not related to coding and agentic stuff.\n\nIt could even improve the quality.",
          "score": 1,
          "created_utc": "2026-01-28 01:21:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pokp",
              "author": "danielhanchen",
              "text": "Oh maybe REAP is maybe similar to this?",
              "score": 1,
              "created_utc": "2026-01-28 09:31:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26qf7s",
                  "author": "kripper-de",
                  "text": "But it prunes whole experts, while you are quantizing specific parameters.\nCan you also quantize to 0-bits (prune)? :-)",
                  "score": 1,
                  "created_utc": "2026-01-28 09:38:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v9hqg",
          "author": "TokenRingAI",
          "text": "MXFP8 or NVFP8 for the Blackwell crowd\n\nAlso, why in the world is this model so slow? 130 tokens/second decode is not that impressive for a model this size",
          "score": 0,
          "created_utc": "2026-01-26 18:44:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xm0td",
              "author": "danielhanchen",
              "text": "It's not fully opimized at this time, hence 130 tokens / s doesn't sound that crazy - I'm sure it'll be better though over time!",
              "score": 1,
              "created_utc": "2026-01-27 01:19:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qohf9o",
      "title": "Kimi-K2.5 Prelim Dynamic 2bit 4bit GGUFs out!",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qohf9o/kimik25_prelim_dynamic_2bit_4bit_ggufs_out/",
      "author": "danielhanchen",
      "created_utc": "2026-01-27 15:46:41",
      "score": 43,
      "num_comments": 5,
      "upvote_ratio": 0.99,
      "text": "Hey everyone! we made some dynamic imatrix 1bit to 4bit ~~preliminary GGUFs~~ (now final release) for Kimi-K2.5! Currently they're text only (no vision yet) and the Dynamic 2bit, 4bit and normal 8bit quants are out at [https://huggingface.co/unsloth/Kimi-K2.5-GGUF](https://huggingface.co/unsloth/Kimi-K2.5-GGUF)\n\nHow to run dynamic 1bit:\n\n    LLAMA_SET_ROWS=1 ./llama.cpp/llama-cli \\\n        --model unsloth/Kimi-K2.5-GGUF/UD-TQ1_0/Kimi-K2.5-UD-TQ1_0-00001-of-00005.gguf \\\n        --temp 1.0 \\\n        --min_p 0.01 \\\n        --top-p 0.95 \\\n        --ctx-size 16384 \\\n        --seed 3407 \\\n        --fit on \\\n        --jinja\n\nGuide to run quants at [https://unsloth.ai/docs/models/kimi-k2.5](https://unsloth.ai/docs/models/kimi-k2.5)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qohf9o/kimik25_prelim_dynamic_2bit_4bit_ggufs_out/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o21lnfq",
          "author": "m98789",
          "text": "Is this possible to run on a single H100 server with plenty of RAM, CPU and disk?",
          "score": 2,
          "created_utc": "2026-01-27 16:45:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22dfeg",
              "author": "sjoerdmaessen",
              "text": "Yes, its literally in the link ‚ÄúYou need 247GB of disk space to run the 1bit quant!\n\nThe only requirement is disk space + RAM + VRAM ‚â• 247GB. That means you do not need to have that much RAM or VRAM (GPU) to run the model, but it will be much slower.‚Äù",
              "score": 5,
              "created_utc": "2026-01-27 18:44:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24gd29",
                  "author": "danielhanchen",
                  "text": "Yes the only requirement is (RAM + VRAM >= disk_space(GGUF)) then using `--fit on` in llama.cpp will optimally allocate space",
                  "score": 2,
                  "created_utc": "2026-01-28 00:33:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24ugxz",
          "author": "danielhanchen",
          "text": "Update: The imatrix dynamic quants are out! 1bit, 2bit are out",
          "score": 2,
          "created_utc": "2026-01-28 01:46:57",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2g1xp3",
          "author": "TheRiddler79",
          "text": "https://preview.redd.it/rp5ihtc4pbgg1.jpeg?width=1440&format=pjpg&auto=webp&s=06ce423d636bac406e0400bbc05bf843f3e8b750\n\nI'm not breaking records in speed, but it's clear that the intelligence is not degraded by compression",
          "score": 1,
          "created_utc": "2026-01-29 17:25:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr4i8a",
      "title": "Experimental DeepSeek-V3.2 Dynamic GGUFs",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/",
      "author": "danielhanchen",
      "created_utc": "2026-01-30 12:46:49",
      "score": 27,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "We made some experimental DeepSeek-V3.2 GGUFs for those interested! [https://huggingface.co/unsloth/DeepSeek-V3.2-GGUF](https://huggingface.co/unsloth/DeepSeek-V3.2-GGUF)\n\nThey **don't need any llama.cpp updates or special forks** \\- these should work in llama.cpp, LM Studio, Ollama (UD-TQ1\\_0).\n\nDeepSeek Sparse Attention (DSA) is disabled for now, and this mostly acts like a normal DeepSeek V3.1 model. However, we had to cook up the chat\\_template.jinja from scratch.\n\nUse [https://unsloth.ai/docs/models/tutorials/deepseek-v3.1-how-to-run-locally](https://unsloth.ai/docs/models/tutorials/deepseek-v3.1-how-to-run-locally) and replace \"DeepSeek-V3.1\" with \"DeepSeek-V3.2\"\n\nAn example Flappy Bird game in HTML with the UD-Q2\\_K\\_XL quant:\n\nhttps://preview.redd.it/a5d7sugrfhgg1.png?width=1547&format=png&auto=webp&s=26f2c96289c84fe8cace79c30a633f7a8e3b5a62\n\nLet us know how it goes!",
      "is_original_content": false,
      "link_flair_text": "Model Update:FO62525035BC6_Sloth_Spar:",
      "permalink": "https://reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qocvl6",
      "title": "How to develop using Apple Sillicon?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qocvl6/how_to_develop_using_apple_sillicon/",
      "author": "growndemon",
      "created_utc": "2026-01-27 12:46:14",
      "score": 14,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hi,  \nI'm developing my codebase on my macbook and afterwards submit trainingjobs to a gpu cluster. However I can't create a virtual env with unsloth and thus don't have any ide support and also can't have a dry run with a small model to test my code.\n\nIs there any workflow / workaround that is recommended or widely used by apple users working with unsloth?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qocvl6/how_to_develop_using_apple_sillicon/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o20cj6g",
          "author": "danielhanchen",
          "text": "We're working on it as we speak! :) Unsloth on Mac will come!",
          "score": 9,
          "created_utc": "2026-01-27 13:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20f4pn",
              "author": "pokemonplayer2001",
              "text": "‚ù§Ô∏è",
              "score": 2,
              "created_utc": "2026-01-27 13:23:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20qde1",
                  "author": "A-Rahim",
                  "text": "u/pokemonplayer2001   \nMeanwhile, you may try this: [https://github.com/ARahim3/unsloth-mlx](https://github.com/ARahim3/unsloth-mlx)",
                  "score": 0,
                  "created_utc": "2026-01-27 14:22:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq0m1x",
      "title": "Guidance Needed: GPT-OSS 20B Fine-Tuning with Unsloth ‚Üí GGUF ‚Üí Ollama ‚Üí Triton (vLLM / TensorRT-LLM)",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qq0m1x/guidance_needed_gptoss_20b_finetuning_with/",
      "author": "Double_Tourist3600",
      "created_utc": "2026-01-29 06:07:11",
      "score": 13,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I am currently fine-tuning the **GPT-OSS 20B** model using **Unsloth** with **HuggingFace TRL (SFTTrainer)**.\n\n**Long-term goal**\n\n* Serve the model in production using **Triton** with either **vLLM** or **TensorRT-LLM** as the backend\n* **Short-term / initial deployment** using **Ollama (GGUF)**\n\n**Current challenge**  \nGPT-OSS uses a **Harmony-style chat template**, which includes:\n\n* `developer` role\n* Explicit EOS handling\n* `thinking` / `analysis` channels\n* Tool / function calling structure\n\nWhen converting the fine-tuned model to **GGUF** and deploying it in **Ollama** using the **default GPT-OSS Modelfile**, I am running into ambiguity around:\n\n1. Whether the **default Jinja chat template** provided by GPT-OSS should be **modified** for Ollama compatibility\n2. How to correctly handle:\n   * EOS token behavior\n   * Internal reasoning / analysis channels\n   * Developer role alignment\n3. How to do this **without degrading the model‚Äôs default performance or alignment**\n\n**Constraints / Intent**\n\n* I already have training data prepared strictly in **system / user / assistant** format\n* I want to:\n   * Preserve GPT-OSS‚Äôs native behavior as much as possible\n   * Perform **accurate, non-destructive fine-tuning**\n   * Avoid hacks that work short-term but break compatibility with **vLLM / TensorRT-LLM** later\n\n**What I‚Äôm looking for**\n\n* Has anyone successfully:\n   * Fine-tuned GPT-OSS\n   * Converted it to GGUF\n   * Deployed it with **Ollama**\n   * While preserving the Harmony template behavior?\n* If yes:\n   * Did you modify the **chat template / Modelfile**?\n   * How did you handle EOS + reasoning channels?\n   * Any pitfalls to avoid to keep it production-ready for Triton later?\n\nAny concrete guidance, references, or proven setups would be extremely helpful.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qq0m1x/guidance_needed_gptoss_20b_finetuning_with/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2d9qb5",
          "author": "max6296",
          "text": "https://github.com/openai/harmony",
          "score": 1,
          "created_utc": "2026-01-29 06:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2df7hg",
              "author": "Double_Tourist3600",
              "text": "can you please elaborate, as per my understanding you are suggesting to use this library to prepare data my question is then which chat template to use?",
              "score": 1,
              "created_utc": "2026-01-29 07:30:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2lclhf",
                  "author": "danielhanchen",
                  "text": "For Ollama maybe use the modelfile here: https://ollama.com/library/gpt-oss:20b/blobs/fa6710a93d78",
                  "score": 1,
                  "created_utc": "2026-01-30 12:39:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2j85ai",
          "author": "Phaelon74",
          "text": "I would skip GGUF entirely then, and roll directly from full model, to INT4/8/FP4/8 as vllm and TensorRT do absolutely HORRIBLE at GGUF serving.  Train your base model, and skip GGUF quanting.",
          "score": 1,
          "created_utc": "2026-01-30 02:56:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qruejs",
      "title": "cerebras MiniMax M2.1 REAP gguf when?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qruejs/cerebras_minimax_m21_reap_gguf_when/",
      "author": "ClimateBoss",
      "created_utc": "2026-01-31 06:03:05",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.92,
      "text": "[https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B](https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B)\n\nmradermacher GGUF dont work, only Unsloth has best chat template fixes ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qruejs/cerebras_minimax_m21_reap_gguf_when/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2qzkik",
          "author": "StardockEngineer",
          "text": "I use the mrader reap daily.  Didn‚Äôt do anything special to make it work.",
          "score": 2,
          "created_utc": "2026-01-31 06:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ric8p",
              "author": "LegacyRemaster",
              "text": "me too.",
              "score": 1,
              "created_utc": "2026-01-31 09:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2u41ym",
                  "author": "ClimateBoss",
                  "text": "M2 not M2.1 dat came out like yday bruh",
                  "score": 1,
                  "created_utc": "2026-01-31 18:58:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qs8gcp",
      "title": "Should I use UnslothTrainer or SFTTrainer for Continued Pre-training (Raw Text) to create a LoRA for later merging?",
      "subreddit": "unsloth",
      "url": "https://arxiv.org/abs/2507.18294",
      "author": "choco132134",
      "created_utc": "2026-01-31 17:28:47",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qs8gcp/should_i_use_unslothtrainer_or_sfttrainer_for/",
      "domain": "arxiv.org",
      "is_self": false,
      "comments": [
        {
          "id": "o2ujqvz",
          "author": "Educational_Rent1059",
          "text": "UnslothTrainer is the same thing with additional support for these internal params which are needed for continued pre training  args = UnslothTrainingArguments(\n        ....\n        learning_rate = 5e-5,\n        embedding_learning_rate = 5e-6, # 2-10x smaller than learning_rate",
          "score": 1,
          "created_utc": "2026-01-31 20:13:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs6esv",
      "title": "Are Usnloth planning to provide a notebook for the Ministral 3 text?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qs6esv/are_usnloth_planning_to_provide_a_notebook_for/",
      "author": "kompania",
      "created_utc": "2026-01-31 16:13:01",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.8,
      "text": "I tried tuning the Ministral 3 3B model by swapping the training sets provided by Unsloth notebook with my own. I tried tuning the VL and Sudoku versions using the Alpaca dataset.\n\nUnfortunately, I was unsuccessful. Both Gemini and ChatGPT claim that this is currently impossible due to the lack of MistralAI support.\n\nDoes Unsloth plan to provide notebooks for Colab for tuning Ministral 3 using text?\n\nI also want to thank the people behind this system/library. I'm 63, and thanks to their extensive guides, I've made some very satisfying tweaks for Gemma 3. Thank you, Unsloth, for your work!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qs6esv/are_usnloth_planning_to_provide_a_notebook_for/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o2w5qy7",
          "author": "yoracale",
          "text": "You need to disable the vision component, it is quite simple. Use the same notebook and just follow the vision finetuning guide: https://unsloth.ai/docs/basics/vision-fine-tuning\n\nE.g. turn fine-tune vision layers = off in the guide",
          "score": 1,
          "created_utc": "2026-02-01 01:19:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmhzg4",
      "title": "How to train vision model with IterableDataset?",
      "subreddit": "unsloth",
      "url": "https://www.reddit.com/r/unsloth/comments/1qmhzg4/how_to_train_vision_model_with_iterabledataset/",
      "author": "willzocken",
      "created_utc": "2026-01-25 12:35:28",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hello I‚Äôm trying to create a IterableDataset with images to train a vision model (currently \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\").\n\nIf I use \\`Dataset.from\\_generator\\` it works, but it also loads all the training data into RAM before continuing, but my training data exceeds my 64 GB RAM I have on my disposal at the moment.\n\n    # dataset = Dataset.from_generator(Template.single_dataset)\n    dataset = IterableDataset.from_generator(Template.single_dataset)\n\nThis is my generator function:\n\n        u/staticmethod\n        def single_dataset() -> Iterator[ConversationDict]:\n            \"\"\"\n            Create template used to train 'kuzushiji-single' model\n            \"\"\"\n            conn = sql.connect(Path(\"output\") / \"single.db\")\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT * FROM prompts LIMIT 100\")\n            batch_size = 100\n    \n            while True:\n                rows: list[sql.Row] = cursor.fetchmany(batch_size)\n                if not rows:\n                    break\n                for row in rows:\n                    image = Image.open(io.BytesIO(row[1])).convert(\"RGB\")\n                    image_buffer = io.BytesIO()\n                    image.save(image_buffer, format=\"PNG\")\n                    image_bytes = image_buffer.getvalue()\n                    yield {\n                        \"messages\": [\n                            {\n                                \"role\": \"user\",\n                                \"content\": [\n                                    {\n                                        \"type\": \"text\",\n                                        \"text\": Template.single_instruction(),\n                                    },\n                                    {\n                                        \"type\": \"image\",\n                                        \"image\": image_bytes,\n                                    },\n                                ],\n                            },\n                            {\n                                \"role\": \"assistant\",\n                                \"content\": [\n                                    {\n                                        \"type\": \"text\",\n                                        \"text\": f\"{row[2]}\",\n                                    },\n                                ],\n                            },\n                        ],\n                    }\n    \n            conn.close()\n\nIf I use the value of the variable \\`image\\`, in other words just the PIL.Image or the \\`image\\_bytes\\` it works with \\`Dataset\\` but fails with \\`IterableDataset\\` even though they both create the same shape of data. For example here the first item of the dataset:\n\n    {'messages': [{'content': [{'image': None, 'text': \"You are an expert in reading old japanese handwritten kuzushiji characters. You will get an image of a kuzushiji character and you will give me only the correct modern japanese character. Nothing more. You'll always answer with just one single japanese character. May it be kanji or kana.\", 'type': 'text'}, {'image': b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x02\\x00\\x00\\x00\\xfdoH\\xc3\\x00\\x00\\x02VIDATx\\x9c\\xad\\x951h\\xf2@\\x18\\x86\\xef4\\xd8d\\xb2:\\x15\\x1b\\x82-\\xe2\\x81\\x83H[\\xd0*ZC\\x11\\x1c\\x1c\\x1c\\x1d\\xec\\xee\\xe2(\\x0eB'W\\x1d\\xdd:v)\\x142t\\xe9\\xd6\\x82\\x82\\x1ah\\x1d\\x12(RJT\\xec\\xe2\\xa6\\x92\\xf6\\x84\\xe4:\\xc8\\x1f~(\\x9a\\x1a}\\xb6\\xbb|\\xf7\\xe4\\xcd]\\xf2\\x05\\x80\\xb5\\xa4R)Y\\x96\\x11B^\\xaf\\xf7\\xf9\\xf9\\xf9\\xec\\xecl}\\xbd9\\xd1ht2\\x99\\xf0<\\xbf\\x1c&\\x93\\xc9\\x8f\\x8f\\x0f\\xa7\\xd3i\\xddH\\xd3\\xb4,\\xcb\\xb1X\\xcc\\x98a\\x18f<\\x1e_]]\\x99\\xae\\xa5V]@\\x08\\xbd\\xbe\\xbe\\xb6Z-\\x00\\x00\\x84\\xd0\\xe7\\xf3a\\x8c;\\x9d\\x8e\\xa6i\\xd6\\xa5\\x1c\\xc7)\\x8a\\xc2\\xb2l$\\x12\\xc9\\xe5r\\xd9lv6\\x9b\\xbd\\xbd\\xbd\\t\\x82`*]\\t\\xcf\\xf3\\x9a\\xa6}\\x7f\\x7f\\x13BDQ\\xacV\\xab\\x1e\\x8f\\xa7\\xddnW*\\x95H$\\x92H$\\xfc~\\xff\\xc6R\\x08a\\xa9Tj4\\x1a\\xe9t\\xda\\xe1p,'\\x05A \\xffh\\xb7\\xdb\\xd6#\\xffO\\xaf\\xd7#\\x84,\\x16\\x8b\\xfb\\xfb{\\x84\\xd0\\xb6:\\xa7\\xd3\\xd9h4TU\\xadV\\xab\\xa1P\\x08Bh\\xc5\\x02!dY\\x16\\x00@\\xd3t>\\x9f\\xff\\xfc\\xfc\\xd4u\\xfd\\xeb\\xebk\\xab\\x80\\xc1`P\\x96\\xe5z\\xbd>\\x1a\\x8dF\\xa3Q\\xb9\\\\\\xbe\\xbc\\xbc\\xd4u\\xfd\\xe4\\xe4\\xc4\\xba4\\x1e\\x8fK\\x924\\x9f\\xcf\\x05A8::\\x02\\x000\\x0c3\\x9f\\xcf/..\\xacK\\x01\\x00{{{.\\x97\\xcb\\xd8>\\x08\\xe1\\xfb\\xfb{\\xb1X4]\\xb8\\xf2\\xe5\\x07\\x00`\\x8c1\\xc6\\xc6\\x90\\x10\\xa2(\\x8a\\xdb\\xed6\\x95\\xdaL+\\x0cX\\x96\\xb5\\xd9l\\x7f9\\xf7?I\\xedv\\xfb\\xf5\\xf5\\xf5`0H&\\x93\\xaa\\xaa\\xfe=\\xc7J(\\x8az||4>$Q\\x14\\xf7\\xf7\\xf7\\xb7\\x95\\x06\\x02\\x81\\xe9t\\x8a16\\xbc\\xb7\\xb7\\xb76\\xdb\\xbaG\\\\wPK\\xce\\xcf\\xcf1\\xc6\\x14E\\x01\\x00\\x1e\\x1e\\x1e\\x08!\\xb9\\\\\\xee\\xe5\\xe5\\xa5V\\xabYOzzz:\\x1c\\x0e\\t!\\x92$\\x1d\\x1c\\x1cp\\x1c\\x871~zz\\xb2n\\\\\\xe2\\xf5zonn\\x8c^\\xd7j\\xb5\\xc6\\xe3\\xf1\\xfa\\x1d\\xd8\\x98\\xbb\\xbb\\xbb\\xe9tj\\xf4\\xc3\\xdfX\\xb9\\xdb\\xe1\\xe1a\\xb7\\xdb],\\x16;\\x93:\\x1c\\x8e\\xe3\\xe3\\xe3\\xe5\\xbfkg \\x84L{\\xd5\\xc6I3\\x99L\\xbf\\xdf\\x97$i\\x8b`\\xbfh6\\x9b\\x85Ba\\x97\\xc6p8\\xac\\xaa\\xaa\\xcf\\xe7[_\\xf6\\x03\\xd5W\\x08\\x12\\xaa'\\x16T\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\", 'text': None, 'type': 'image'}], 'role': 'user'}, {'content': [{'image': None, 'text': '„Åæ', 'type': 'text'}], 'role': 'assistant'}]}\n\nIf throughly checked it the is literally no difference between Dataset and IterableDataset when it comes to the shape of the data, but if I remove the image field then I can train with an IterableDataset!\n\nBut the moment I start training with an IterableDataset with an image field I get this cryptic error message:\n\n    ‚îÇ /home/kinski/Projects/kuzushiji/.venv/lib/python3.12/site-packages/torch/_tensor.py:1030 in split ‚îÇ\n    ‚îÇ ‚îÇ\n    ‚îÇ 1027 ‚îÇ ‚îÇ if isinstance(split_size, (int, torch.SymInt)): ‚îÇ\n    ‚îÇ 1028 ‚îÇ ‚îÇ ‚îÇ return torch._VF.split(self, split_size, dim) # type: ignore[attr-defined] ‚îÇ\n    ‚îÇ 1029 ‚îÇ ‚îÇ else: ‚îÇ\n    ‚îÇ ‚ù± 1030 ‚îÇ ‚îÇ ‚îÇ return torch._VF.split_with_sizes(self, split_size, dim) ‚îÇ\n    ‚îÇ 1031 ‚îÇ ‚îÇ\n    ‚îÇ 1032 ‚îÇ def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None): ‚îÇ\n    ‚îÇ 1033 ‚îÇ ‚îÇ r\"\"\"Returns the unique elements of the input tensor. ‚îÇ\n    ‚îÇ ‚îÇ\n    ‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ locals ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ\n    ‚îÇ ‚îÇ dim = 2 ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ self = tensor([[[[-4.7302e-03, -1.0620e-02, 5.5176e-02, ..., -1.6113e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -3.7994e-03, -4.0527e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 3.3936e-02, -9.5215e-03, -2.7466e-04, ..., -4.1260e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -2.6611e-02, -4.4434e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 1.6937e-03, 2.5513e-02, 2.7588e-02, ..., -1.2109e-01, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ -7.6294e-03, -2.2583e-02]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ..., ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[-1.6846e-02, -1.7212e-02, -1.0620e-02, ..., 8.4229e-03, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 5.0049e-02, -2.3828e-01]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[ 1.0559e-02, 9.8267e-03, 9.1553e-03, ..., -3.0884e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 3.9795e-02, -6.4697e-03]], ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ [[-2.5879e-02, 2.8442e-02, -8.4961e-02, ..., 3.3203e-02, ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 4.9072e-02, -2.8711e-01]]]], device='cuda:0', dtype=torch.bfloat16) ‚îÇ ‚îÇ\n    ‚îÇ ‚îÇ split_size = [16] ‚îÇ ‚îÇ\n    ‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ\n    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n    RuntimeError: split_with_sizes expects split_sizes to sum exactly to 1 (input tensor's size at dimension 2), but got\n    split_sizes=[16]\n\nDoes someone maybe know what I‚Äôm missing or what I‚Äôm doing wrong? Thanks in advance for your help!!!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/unsloth/comments/1qmhzg4/how_to_train_vision_model_with_iterabledataset/",
      "domain": "self.unsloth",
      "is_self": true,
      "comments": [
        {
          "id": "o1ndo6w",
          "author": "mmathew23",
          "text": "Are you using UnslothVisionDataCollator? Trl handles iterabledatasets differently than regular Datasets. When it sees iterabledatasets it switches to a shared data loader that assumes the first index is the batch index. This assumption breaks for the qwen series vl models. You have to disable dispatch_batches and split_batches in the accelerator_config.",
          "score": 1,
          "created_utc": "2026-01-25 17:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1oavuc",
              "author": "willzocken",
              "text": "Thats it!!!! Thank you!  \nYes I was using UnslothVisionDataCollator.\n\n  \nAnother question how do you know this? :D\n\nI guess its somewhere written inside the docs of trl? Because I couldn't find it inside huggingface's datasets as well as in the unsloth docs",
              "score": 1,
              "created_utc": "2026-01-25 19:27:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1qkalo",
                  "author": "mmathew23",
                  "text": "accelerator_config is in the transformers documentation under TrainingArguments and the actual keywords should be listed in the documentation for accelerate.",
                  "score": 1,
                  "created_utc": "2026-01-26 01:41:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}