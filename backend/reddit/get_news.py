import os
import logging
import time
from typing import Dict
from pathlib import Path
from dotenv import load_dotenv
from collector import RedditContentCollector
from models import TimeFilter

# Get the script's directory
SCRIPT_DIR = Path(__file__).resolve().parent

# Set up logging
def setup_logging():
    """Set up logging configuration with proper path handling."""
    # Create logs directory relative to script location
    log_dir = SCRIPT_DIR / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    log_file = log_dir / "reddit_news.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(log_file, mode='a')
        ]
    )

logger = logging.getLogger(__name__)

def get_env_variables() -> Dict[str, str]:
    """
    Get environment variables from either GitHub Actions secrets or .env file.
    Returns a dictionary of required environment variables.
    """
    # Try loading .env file if environment variables are not set
    if not os.getenv("REDDIT_CLIENT_ID"):
        env_path = SCRIPT_DIR / ".env"
        load_dotenv(env_path)
    
    # Define required variables with their environment variable names
    required_env_vars = {
        "REDDIT_CLIENT_ID": os.getenv("REDDIT_CLIENT_ID"),
        "REDDIT_CLIENT_SECRET": os.getenv("REDDIT_CLIENT_SECRET"),
        "REDDIT_USER_AGENT": os.getenv("REDDIT_USER_AGENT", "ContentCollector/1.0"),
        "REDDIT_USERNAME": os.getenv("REDDIT_USERNAME"),
        "REDDIT_PASSWORD": os.getenv("REDDIT_PASSWORD")
    }
    
    # Validate environment variables
    missing_vars = [k for k, v in required_env_vars.items() if not v]
    if missing_vars:
        raise ValueError(
            f"Missing required environment variables: {', '.join(missing_vars)}\n"
            "Please set them in GitHub Actions secrets or in a .env file"
        )
    
    return required_env_vars

def main() -> None:
    """Main entry point for fetching Reddit community news."""
    start_time = time.time()
    
    try:
        # Set up logging first
        setup_logging()
        logger.info("=" * 80)
        logger.info("ğŸš€ Báº®T Äáº¦U THU THáº¬P TIN Tá»¨C REDDIT - 24 GIá»œ QUA")
        logger.info("=" * 80)
        
        # Get environment variables
        logger.info("ğŸ“‹ Äang kiá»ƒm tra biáº¿n mÃ´i trÆ°á»ng...")
        env_vars = get_env_variables()
        logger.info("âœ… Biáº¿n mÃ´i trÆ°á»ng Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh thÃ nh cÃ´ng")
        
        # Initialize collector with proper path handling
        output_dir = Path('src/data/reddit')
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ ThÆ° má»¥c output: {output_dir.absolute()}")
        
        # Initialize collector
        logger.info("ğŸ”§ Äang khá»Ÿi táº¡o RedditContentCollector...")
        collector = RedditContentCollector(
            client_id=env_vars["REDDIT_CLIENT_ID"],
            client_secret=env_vars["REDDIT_CLIENT_SECRET"],
            user_agent=env_vars["REDDIT_USER_AGENT"],
            username=env_vars["REDDIT_USERNAME"],
            password=env_vars["REDDIT_PASSWORD"],
            output_dir=output_dir,
            comment_depth=3
        )
        logger.info("âœ… RedditContentCollector Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh cÃ´ng")

        # Fetch only daily news
        logger.info("ğŸ” Báº®T Äáº¦U THU THáº¬P TIN Tá»¨C Cá»˜NG Äá»’NG - 24 GIá»œ QUA")
        logger.info("ğŸ“Š ThÃ´ng sá»‘ thu tháº­p:")
        logger.info(f"   - Time filter: {TimeFilter.DAY}")
        logger.info(f"   - Min score: 3")
        logger.info(f"   - Limit: 1000 posts")
        logger.info(f"   - Comment depth: 3")
        
        fetch_start_time = time.time()
        state_path = output_dir / "collector_state.json"
        news_items = collector.get_community_news(
            time_filter=TimeFilter.DAY,
            min_score=3,     # Minimum score threshold
            limit=1000,      # Maximum number of posts to fetch
            comment_depth=3,
            state_path=state_path,
            rate_limit_threshold=5
        )
        fetch_end_time = time.time()
        
        fetch_duration = fetch_end_time - fetch_start_time
        logger.info(f"â±ï¸  Thá»i gian thu tháº­p: {fetch_duration:.2f} giÃ¢y")
        logger.info(f"ğŸ“ˆ Sá»‘ lÆ°á»£ng tin tá»©c thu tháº­p Ä‘Æ°á»£c: {len(news_items)} items")
        
        # Save the data
        logger.info("ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u...")
        save_start_time = time.time()
        collector.save_community_news(news_items, TimeFilter.DAY)
        save_end_time = time.time()
        save_duration = save_end_time - save_start_time
        logger.info(f"â±ï¸  Thá»i gian lÆ°u: {save_duration:.2f} giÃ¢y")
        
        # Summary
        total_duration = time.time() - start_time
        logger.info("=" * 80)
        logger.info("âœ… HOÃ€N THÃ€NH THU THáº¬P TIN Tá»¨C REDDIT")
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š Tá»”NG Káº¾T:")
        logger.info(f"   - Tá»•ng thá»i gian: {total_duration:.2f} giÃ¢y")
        logger.info(f"   - Thá»i gian thu tháº­p: {fetch_duration:.2f} giÃ¢y")
        logger.info(f"   - Thá»i gian lÆ°u: {save_duration:.2f} giÃ¢y")
        logger.info(f"   - Sá»‘ tin tá»©c thu tháº­p: {len(news_items)} items")
        logger.info(f"   - Tá»‘c Ä‘á»™ thu tháº­p: {len(news_items)/fetch_duration:.2f} items/giÃ¢y")
        logger.info("=" * 80)

    except Exception as e:
        total_duration = time.time() - start_time
        logger.error("=" * 80)
        logger.error("âŒ Lá»–I TRONG QUÃ TRÃŒNH THU THáº¬P TIN Tá»¨C")
        logger.error("=" * 80)
        logger.error(f"â±ï¸  Thá»i gian cháº¡y trÆ°á»›c khi lá»—i: {total_duration:.2f} giÃ¢y")
        logger.error(f"ğŸ” Chi tiáº¿t lá»—i: {e}")
        logger.error(f"ğŸ“‹ Loáº¡i lá»—i: {type(e).__name__}")
        logger.error("=" * 80)
        raise

if __name__ == "__main__":
    main()
